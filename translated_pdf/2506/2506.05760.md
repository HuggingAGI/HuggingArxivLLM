写作强化学习：自适应课程强化学习赋能长篇写作  

作者：雷徐安宇、李陈亮³、吴宇宁³、刘凯明²、沈伟洲³、李鹏¹,†、严明³,†、张骥³、黄飞³、刘洋¹,²  

¹ 清华大学人工智能产业研究院（AIR）  
² 清华大学计算机系人工智能研究院  
³ 阿里巴巴通义实验室  
联系邮箱：leixy24@mails.tsinghua.edu.cn；lipeng@air.tsinghua.edu.cn；ym119608@alibaba-inc.com  

（注：†符号通常表示通讯作者，中文语境下可保留或译为"通讯作者"；作者姓名按中文习惯调整为"姓+名"顺序；机构名称采用国内通用译法；技术术语"adaptive curriculum reinforcement learning"译为"自适应课程强化学习"，兼顾专业性与可读性。）

摘要

尽管大型语言模型（LLM）在长文本创作领域已取得显著突破，但传统监督微调（SFT）方法仍受制于数据瓶颈与教师信号的固有局限。为此，我们创新性地推出Writing-RL框架——这套自适应课程强化学习系统包含三大核心模块：能精准捕捉高价值样本的边际感知数据选择策略、在缺乏明确奖励时仍能提供判别信号的对比奖励机制，以及根据模型表现智能调节难度的动态参考调度方案（该方案尤为关键）。基于70亿参数写作模型的实验证明，本方案不仅显著超越SFT基准线，更展现出令人惊喜的跨任务泛化能力：经长文本输出训练的模型，竟在长上下文推理任务中表现卓越，这为重构长文本训练范式提供了全新思路。

1 引言

近年来，大型语言模型（LLMs）在指令跟随与文本生成领域突飞猛进（OpenAI，2023；DeepSeek-AI等，2025）。其中，能够创作高质量长文的生成能力，因其广泛的应用前景备受学界关注（Wu等，2025a；Bai等，2024b）。

然而现有LLMs在同时满足"长篇"与"优质"这两个维度时仍显乏力。研究表明（Bai等，2024b；Tu等，2025），模型存在固有输出长度瓶颈，且随着文本延长会出现质量滑坡（Wu等，2025b）。当前主流解决方案是通过监督微调（SFT）配合智能体流水线（Bai等，2024b）或指令回译（Pham等，2024）构建训练数据。但这类方法不仅数据制备成本高昂，还存在版权风险（Maini等，2024），更关键的是其性能天花板受限于人类专家水平，容易陷入数据饱和困境。

与此同时，基于可验证奖励的强化学习（DeepSeek-AI等，2025）在复杂推理任务中展现出突破SFT极限的潜力。但长文写作缺乏明确评判标准，使得这一技术路径面临三重挑战：
1）数据选择：如何筛选能激发模型潜力的优质训练样本？
2）奖励设计：在缺乏参考答案时，如何构建有效的质量评估机制？
3）课程调度：静态训练课程难以匹配模型动态演进的能力曲线。

为此，我们提出Writing-RL框架（图1），其创新点在于：
• 边际感知数据选择：通过模型输出与最优参考的质量差距评估学习潜力
• 成对比较奖励：要求模型必须超越给定参考才能获得奖励
• 动态参考调度：根据实时表现动态调整参考文本难度

实验表明，经我们框架训练的7B模型不仅刷新了长文生成SOTA，更展现出惊人泛化能力——在8k-2M☉长度的推理任务中，其表现竟优于专注该任务的SFT模型。这暗示长文生成训练可能意外增强了模型的上下文理解能力，为长文本处理研究开辟了新视角。

本文核心贡献：
1）首个面向长文写作的自适应课程强化学习框架
2）动态参考调度机制实现难度自适应的渐进式训练
3）首次发现长文生成与长文本推理能力的正向迁移现象

2 相关研究

长文本生成训练方法。最新研究（Bai等，2024b；Wu等，2025b）主要通过构建长文本微调数据集来提升生成能力，主要采用教师模型蒸馏（Wu等，2025b）、多智能体迭代生成（Bai等，2024b；Tu等，2025；Quan等，2024）和指令回译（Pham等，2024；Wang等，2024）等方法。但在线强化学习技术（Schulman等，2017；Shao等，2024）的应用仍显不足，制约了性能突破。

长文本质量评估。大模型长文本生成（Wu等，2025a）面临评估难题，研究者通过构建基准数据集（Wu等，2025b；Que等，2024），采用商业模型（Bai等，2024b；Paech，2023；Liu等，2024）或微调模型（Wu等，2025b；Ke等，2024）作为评判者。但存在位置偏差和自我强化偏差（Zheng等，2023）等问题，影响了大模型评估的可靠性。

课程学习优化。强化学习（Schulman等，2017；Shao等，2024；DeepSeek-AI等，2025）已成为释放大模型潜力的重要手段。为提升效率，研究者广泛采用课程学习（Bengio等，2009），包括静态难度规划（Luo等，2025；Song等，2025）和动态数据筛选（Bae等，2025；Shi等，2025）。但现有方法依赖规则化难度评估，可能导致样本选择失衡和计算开销增加。

本研究提出Writing-RL框架——一套基于自适应课程强化学习的解决方案，专门用于在指令微调基础上增强长文本生成能力。该框架包含三大核心模块：边界敏感数据筛选、对比式奖励机制及动态参考调度策略。通过将结果导向型强化学习融入长文本创作任务，我们创新性地从样本优选、激励设计和学习规划三个维度提升模型写作水平。下文将逐一详解各模块设计。

3.1 边界感知数据选择

传统数据筛选方法常以问题难度为核心指标，通过策略模型准确率（Shi等，2025；Bae等，2025）、解题步骤数等简易指标（Cheng等，2021；Yang等，2025）或人类直觉启发式规则（Hendrycks等，2021）进行评估。虽然这种难度优先策略在数学、编程等可验证奖励的强化学习任务中成效显著，但其依赖明确的标准答案来界定难度。然而在开放式写作任务中，标准答案的缺失使难度指标失去参考价值，最难题型未必是最佳学习素材。

为此，我们创新性提出边界感知数据选择法，通过策略输出与最优参考方案间的表现差距来量化学习潜力。核心思想简明扼要：具有学习价值的问题，必然存在显著改进空间。具体实现分为三大步骤：

多模型协同生成：摒弃传统单一难度评估模型（Shi等，2025；Bae等，2025），我们组建LLM精英团队${\mathcal{C}}=\{\pi,M_{1},M_{2},.\,.\,.\,\}$（含策略模型），为每道写作题生成多元解答。

多维智能评分：采用最新LLM-as-a-Judge评估体系（Liu等，2024；Wu等，2025b），对每个模型$M_{j}\in{\mathcal{C}}$生成的答案$r_{j}$进行多维度打分，记$s_{j}$为答案平均质量分。

潜力优先筛选：定义模型基准学习潜力$p$为最优竞争者与策略模型的质量分差：
$$
p=\operatorname*{max}_{j\in\mathcal{C},\;j\neq\pi}(s_{j}-s_{\pi})
$$
其中$s_{\pi}$代表策略模型得分。$p$值越大，提升空间越可观。为排除噪声干扰，首先剔除所有竞品模型均表现欠佳的样本（这类题目往往过难或本身存在缺陷），随后按潜力值$p$降序排列，精选前$\cdot k$名构建训练集。

3.2 双样本对比奖励机制

奖励函数是强化学习策略优化的核心引擎。尽管规则化结果奖励（DeepSeek-AI等，2025；Team等，2025）在需要长链推理（Wei等，2022）的任务中效果显著，但其难以直接适用于长文本创作——既缺乏标准答案，又极具主观性，这为奖励设计带来了独特挑战。

最新研究采用"LLM即裁判"（Zheng等，2023；Wu等，2025b）来评估模型输出质量，其判断与人类高度吻合。现有两种评估范式：单样本评分和双样本对比。前者虽因操作简便被广泛采用，但区分度有限且波动较大；后者通过与优质参考样本直接对比，能精准捕捉细微差距和改进空间。这种对比机制产生的差异化奖励信号，能有效驱动策略模型产出更优结果，甚至超越参考样本以获得更高奖励。具体设计如下：

$$
r_{\mathrm{equality}}(\mathbf{x})=\left\{\begin{array}{l l}{1}&{\mathrm{若评判结果}\mathbf{x}\succ\mathbf{参考}}\\ {0.5}&{\mathrm{若评判结果}\mathbf{x}\equiv\mathbf{参考}}\\ {0}&{\mathrm{若评判结果}\mathbf{x}\prec\mathbf{参考}}\end{array}\right.
$$

其中$r_{\mathrm{equity}}(\mathbf{x})$为生成文本$\mathbf{x}$的奖励值，ref代表优质参考文本，$\operatorname{Jac}$函数则由LLM裁判执行对比评估。

针对LLM裁判存在的"位置偏好"现象（Zheng等，2023）——总是倾向首位的回答，我们刻意将模型输出置于次位，人为制造"位置劣势"。这一巧思既避免了传统的位置轮换对比（评估成本直接减半），又能迫使模型在不利条件下突破极限，生成更优质内容。

3.3 动态参考调度

课程学习（Bengio等，2009）通过渐进式任务难度规划提升学习效率。现有方法采用离线计算难度（Shi等，2025；Song等，2025）或训练中引入额外rollout进行自适应采样（Bae等，2025；Yu等，2025），虽在推理型强化学习中有效，但存在难度评估不够灵活或计算开销大的局限。

针对现有课程调度适应性不足的问题，我们提出动态参考调度法：当策略模型超越当前参考时，自动引入更强对手（算法1详述），实现样本级难度动态调整与模型能力同步进化。

【数据准备】对写作指令集$W$，先采用3.1节的边界感知策略获取多组参考${\mathcal R}=\{r_{\pi},r_{1},...\}$及其LLM评分$s=\{s_{\pi},s_{1},...\}$，按质量升序排列为$\mathcal{R}_{s}=\{r_{q1},r_{q2},...\}$。特别保留初始策略模型$\pi$的响应作为基础参考，确保训练初期获得足够正向反馈。

【动态调度】训练启动时，所有指令绑定最低质量参考$r_{q1}$（与初始模型水平相当）。随着模型进化，其在生成响应时逐步超越当前参考并获得LLM对比奖励，此时自动升级被超越的参考$r_t$为$r_{t+1}$，未超越者维持原状。这种渐进式挑战机制与模型能力成长曲线完美契合，形成异步学习轨迹。如图2所示，该方法实现样本级异步调度，让任务难度与模型能力动态适配。

我们通过实验验证Writing-RL的效果：在专为写作优化的LLM模型上测试，探究该方法能否突破监督式微调的限制，实现更出色的长文本创作能力。

4.1 数据集




我们采用了两套精心设计的生成式写作数据集——LongWriter训练集（Bai等，2024b）和Writing Bench训练集（Nu等，2025b），主要用于监督微调。如3.1节所述，我们分别对这两个数据集实施了边际感知数据筛选流程：首先通过初始策略模型和四个高性能大模型（Qwen-Plus、GPT-4o、Claude-3.7和Deepseek R1）为每项写作指令生成优质参考；随后采用经过专项优化的评判模型（该模型在长文本评估方面与人类专家保持高度一致）进行多维度评分；最终从每个数据集中精选出1.5k个样本用于强化学习，每个样本包含写作指令及按质量升序排列的参考答案。

4.2 训练方案

为充分发挥强化学习优势，我们选用两个写作专用大语言模型作为基础模型进行微调

![](images/18ad64553986bcf13b5311e0855fcc597fc814ea2a5f8748b5b62baa27b90235.jpg)  

表1显示，经Writing-RL训练的模型在其所属模型家族中均取得最佳表现（粗体标注），性能媲美商用专有模型。

基础模型经Writing Bench全量训练集微调后，分别命名为Qwen2.5-7B-Writing Bench-SFT和Llama3.1-8B-Writing Bench-SFT。

我们运用提出的Writing-RL框架，采用PPO算法（Schulman等，2017）对基础模型进行长文本写作优化。训练过程中，由Qwen-Plus担任对比评判官，为策略优化提供奖励信号。最终产出的强化学习模型分别命名为Qwen2.5-7B-Writing-RL和Llama3.1-8B-Writing-RL，具体实现细节参见附录A。

4.3 基准测试与基线方案

为系统评估大语言模型的长文创作能力，我们精选三大标杆测试集：Writing Bench（Wu等，2025b）、LongBench-Write（Bai等，2024b）和EQ-Bench创意写作单元（Paech，2023）。这些测试集涵盖面广，并配备高性能评审模型进行生成内容质量检测。特别设计的是，评估采用的评审模型不仅多元化，且与训练阶段的奖励模型相互独立，从而避免陷入特定评审偏好陷阱，保障评估客观性。

基线方案包含三大类：顶尖商业模型（Yang等，2024；Hurst等，2024）、指令微调模型（Yang等，2024；Dubey等，2024）及写作专项优化模型（Wu等，2025b；Bai等，2024b；Pham等，2024）。完整评估方案详见附录B。

4.4 实验结果

表1数据显示，采用WritingRL训练的模型在三大基准测试中全面领先。其中Llama3.1-8B-WritingRL以87.14的平均分拔得头筹，Qwen2.5-7B-Writing-RL以84.49分紧随其后，二者在7B级别均展现出卓越性能。特别值得注意的是，我们的模型具备媲美甚至超越商业闭源模型的长文本生成能力，堪称开源领域的长文本生成利器。

研究还揭示了RL与SFT在强基模型上的差异化表现：尽管使用相同专家模型构建的24k训练样本，其微调效果反而不及12k样本版本，这印证了数据饱和现象——当模型能力达到一定阈值后，单纯堆砌数据已无法提升性能。而强化学习则展现出持续优化潜力：同系列的Llama3.1-8B-Writing-RL相较基准SFT版本保持稳定进步，表明在SFT遭遇瓶颈时，RL仍能推动模型能力突破。

5 输出能力的输入泛化

为探究长文本输出强化学习对模型长文本理解能力的影响，我们选用长文本推理基准LongBench v2（Bai等，2024a）进行测试。如图3所示，该基准的输入长度远超训练集数据，不仅突破输入长度上限，甚至超过了训练时的总输入输出长度。

表2结果显示，经强化学习微调的writer模型不仅提升了长文本生成能力，更展现出对超长输入推理任务的出色泛化性能，而监督微调模型则表现下滑。针对这一现象，我们给出以下洞见：

长输出为何能泛化至长输入？优质长文本生成必然建立在对上下文的深度理解之上，因此输出训练自然强化了模型的输入理解能力。

强化学习为何优于监督微调？监督微调局限在样本模仿，而强化学习通过奖励机制培养模型的核心能力，这种能力建设带来更优的泛化表现，这与近期其他领域研究（Chu等，2025；Shen等，2025）结论一致。

这对长文本训练的启示：输出与输入训练可能存在协同效应，二者结合或能催生更高效的训练方案，这一方向的系统探索将是未来研究重点。

六、讨论环节

6.1 数据优选策略解析  

我们提出的"边界感知优选法"专注于挖掘具有成长空间的训练样本。不同于传统单模型难度评估方案（Shi等，2025；Bae等，2025），该方法通过对比策略模型与顶尖大模型的表现差距，精准量化每个样本的"学习潜能"，实现样本价值的最大化释放。  

验证阶段，我们基于Writing Bench（Wu等，2025b）Hard训练集展开实验，采用Qwen-plus（Yang等，2024）生成的高质量参考答案训练Qwen2.5-7B微调模型。选择该测试平台源于其全面的文本覆盖和高效的评估体系。表3数据显示，通过优选高潜力样本，策略能显著提升学习效能。值得注意的是，相比传统难度优先方案，优选样本的初始得分更高（难度相对较低），有力印证了"学习潜能"比"绝对难度"更具数据优选价值。

6.2 奖励机制设计解析

我们创新性地采用高质量参考样本的成对对比法构建奖励机制。为验证其有效性，将其与主流逐点评分法（Zheng等，2023；Liu等，2025）进行对比——后者依赖Judge LLM给出的标量评分来评估响应质量。实验保持6.1节设定，表4数据表明：我们的方案能产生更精准的差异化奖励，显著促进模型写作能力的迭代提升。

6.3 参考质量分析

在成对比较奖励机制中，参考质量直接决定了策略模型获取正向奖励的难易程度，进而影响训练效果。为探究这一影响，我们采用多组静态参考集进行实验：每组参考由不同LLM生成，另设一组精选最优参考的组合集。特别地，我们还加入了初始策略模型自产的参考集作为基线（标记为"自生成"）。

表5结果显示，参考质量对训练效果至关重要。使用较低质量参考（如"自生成"）时，模型初期虽能快速获得足够奖励并提升表现，但很快会陷入瓶颈，胜率接近完美却难以突破。而使用过高标准的参考（如"最佳参考"），则因初期奖励信号过于稀疏，反而会拖慢学习进度、破坏训练稳定性。这些发现揭示了静态参考调度的核心缺陷：既需要人工精挑细选，又无法动态适配模型能力的成长曲线。

6.4 课程规划方案对比实验

基于第6.3节关于参考质量重要性和固定参考局限性的讨论，我们创新性地提出动态参考规划策略，使模型能够随着训练进程不断超越更优质的参考标准。为验证该策略的优越性，我们设计了包含三种训练模式的对比实验：无规划混合训练、静态参考划分训练以及动态规划训练。表3.3的实验数据表明，动态规划策略显著优于其他方案。值得注意的是，采用课程学习的静态和动态方案均超越了传统无课程基线，这充分证明了将渐进式学习策略引入强化训练流程的价值。

7 总结

本文提出Writing-RL框架——一套包含边缘感知数据筛选、成对比较奖励机制和动态参考调度的自适应课程强化学习方案。实验表明，该框架不仅能显著提升长文本写作能力，其性能增益还可从长输出生成延伸至长输入推理任务，为长上下文训练开辟了新思路。

研究局限




本文重点探讨三项核心局限。模型规模方面，虽然WritingRL在7B级写作模型上成效显著，但更大体量模型的潜力仍有待挖掘。现有研究表明（Gandhi等人，2025），基础模型的底层能力是RL效果的决定性因素。这意味着将WritingRL应用于更强模型时，不仅可能获得更显著的性能突破，还能展现从长文本生成到长输入推理的更优泛化能力。




关于RL的零样本特性，本研究揭示了强化学习在长文本生成中的双重价值：既带来显著性能增益，又能自然迁移至长输入推理任务。未来值得探索的是：直接在未调优的基础模型上应用RL，这将更纯粹地验证RL本身是否具备激发长文本生成能力的潜力。