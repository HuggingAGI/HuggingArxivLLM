如何打造优质自然语言提示？

作者：Do Xuan Long 1 , Duy $\mathbf{Dinh^{1*}}$ , Ngoc-Hai Nguyen 1 \* , Kenji Kawaguchi 1 , Nancy F. Chen 3 , Shafiq Joty 2 , Min-Yen Kan 1  
机构：1 新加坡国立大学, Salesforce AI Research  
3 新加坡科技研究局信息通信研究院  

联系方式：xuanlong.do@u.nus.edu, {dinhcongduy131200, haibeo2552001}@gmail.com, {kenji,knmnyn}@nus.edu.sg, sjoty@salesforce.com, nfychen@i2r.a-star.edu.sg

摘要

随着大型语言模型（LLMs）日益逼近人类智能并广泛应用于人机交互，提示词设计已成为关键要素。然而学界对自然语言提示词的量化标准尚未达成共识。本研究通过系统分析150余篇来自顶尖NLP/AI会议（2022-2025）和行业博客的文献，创新性地提出了包含6大维度、21项属性的提示词质量评估框架。研究发现：现有评估方法存在明显的模型/任务偏差，且多项属性研究尚属空白；通过解析优质提示词的属性关联，我们提炼出优化建议；在推理任务中的实验表明，单一属性强化往往效果最佳；更有趣的是，基于属性增强提示的指令调优能显著提升模型推理能力。这些发现不仅为提示词优化提供了科学框架，更架起了人机沟通的桥梁，为后续研究指明了新方向1。

1 引言

预训练大语言模型（Brown等，2020；Chowdhery等，2022；OpenAI，2022；Touvron等，2023a；Team等，2023；Guo等，2025）凭借出色的文本生成能力，在各类自然语言处理任务中表现卓越。虽然其效果高度依赖自然语言提示的质量（Sahoo等，2024），但优质提示的设计艺术与科学仍待深入探索。随着人机交互日益普及，深入理解这些作为人机沟通桥梁的自然语言提示变得至关重要。

尽管自然语言提示的重要性不言而喻，但其量化标准仍缺乏共识。当前方法主要采用结果导向的评估指标（Deng等，2022；Lin等，2024；Shi等，2024）和反复试错（Pryzant等，2023；Long等，2024a），可能导致提示更适配机器而非人类理解，进而引发解释验证困难，甚至诱发大语言模型的对抗行为（Zou等，2023；Zhu等，2023），影响模型的对齐性、透明度、可靠性及人机交互。

最新研究（Bsharat等，2023；Lin，2024）和指南（OpenAI，2024b；Anthropic，2024）提出了诸如"明确输出长度"等提升提示属性的建议。这些属性导向的建议聚焦提示质量而非模型表现，提供了可解释的优化策略。但其存在三大局限：缺乏统一的理论框架、通用性存疑、属性间相互作用机制不明。

为此，我们通过元分析系统研究了自然语言提示。通过梳理2022-2025年顶级会议论文和科技公司博客（详见§B），我们在六个维度上提炼出21个关键属性，建立了首个以属性和人为中心的评估框架（§3）。研究发现不同属性对模型效果的影响存在显著不平衡（§4）。基于高质量提示集，我们分析了属性关联性并给出实用设计建议（§5）。推理任务的案例研究表明：优化单一属性往往优于组合多个属性，且微调能进一步放大其效果（§6）。主要贡献包括：

1. 首创属性导向的提示评估框架，包含6大维度21项属性
2. 揭示现有研究在模型/任务间的严重不平衡
3. 提出基于属性关联的提示设计指南
4. 证实单一属性优化在推理任务中的突出效果

2 相关研究

提示分析。提示设计是释放大语言模型潜力的关键（Liu等，2023a；Sahoo等，2024），相关研究方兴未艾。现有工作主要沿两个方向展开：一是解析提示的结构要素，探究格式变化（Long等，2025a）、措辞调整（Yin等，2023）对性能的显著影响及其出现规律（Ma等，2024）；二是通过实证研究提出设计建议，如思维链提示（Wei等，2022；Kojima等，2022）、礼貌用语（Bsharat等，2023）和通用准则（Anthropic，2024；OpenAI，2024b）。但这些研究多局限于特定任务或局部特性。本研究首创性地提出统一属性框架，系统整合实践经验，为提示策略的解析与优化提供全新视角。

提示工程与优化。提示工程（Wei等，2022；Zhang等，2023；Zhou等，2023c）与优化技术（Deng等，2022；Pryzant等，2023；Long等，2024a）致力于提升模型任务表现。当前研究虽以基准提升为主，但新兴趋势开始关注提示的多元属性：清晰性（Lin，2024；Anthropic，2024）、礼貌度（Bsharat等，2023；Yin等，2024）、结构化（OpenAI，2024b）乃至输出公平性（Ji等，2023；Yuan等，2023）。这些属性的普适价值尚不明确，其协同效应更是亟待探索。我们将在第4-6章深入探讨这些关键问题。

3 提示质量评估体系

本研究系统梳理了150余篇文献，构建出多维度评估框架：

I. 沟通维度
• 精炼度：用最少token传递核心信息（如Shi等2023所示）
• 清晰度：杜绝模棱两可的表达（Anthropic 2024验证）
• 互动性："请确认是否需要更多细节"式引导（Deng等2023方案）
• 礼仪值："烦请""感谢"等敬语运用（Yin等2024研究）

II. 认知维度
• 任务拆解：将"写论文"分解为"列提纲-找论据-润色"（Zhou等2023a方案）
• 去冗余化：删除无关背景说明（OpenAI 2024b建议）
• 知识迁移："回忆相关知识点"式引导（Sun等2022策略）

III. 教学维度
• 目标可视化：明确输出格式/字数限制（Chang 2023方案）
• 工具调用："需查询最新数据"的自主判断（Yao等2023设计）
• 自我纠错："请检查结论是否矛盾"（Wang等2024方案）

IV. 逻辑维度
• 结构流：引言-论点-结论的递进关系（Wang等2024a验证）
• 语境融：跨对话轮次的概念统一（Pham等2024标准）

V. 幻觉管控
• 事实锚定："请引用可靠来源"（Gao等2023方案）
• 创意调控：广告文案vs学术论文的差异要求（Sinha等2023探索）

VI. 责任维度
• 去偏见："避免性别预设"的明确要求（Si等2023b方案）
• 安全阀：自动过滤危险操作指南（Zou等2023机制）
• 隐私盾：脱敏处理个人信息（Edemacu等2024方案）

4 特性如何塑造模型表现？

为探究第3节特性对模型性能的影响，我们系统分析了现有文献。研究任务分为六大类：(1) 真实对话：含AlpacaEval、ShareGPT等真实用户对话基准；(2) 评估套件：如MMLU、CEval等多任务评测集；(3) 推理问答：涵盖GSM8K、HotpotQA等需要逻辑推理的任务；(4) 文本生成：包括摘要、翻译等创作型任务；(5) 语言理解：如GLUE等语义理解任务；(6) 其他：涉及安全、个性化等专项任务。针对每个特性，我们从支持文献数量、适用任务和模型三个维度进行梳理，最终在表1中形成可落地的提示优化建议。

任务特性关联分析显示，不同任务对特性的需求存在显著差异。真实对话场景最重视沟通特性（支持文献最多），其次是指导和认知特性——这源于用户常需设计复杂提示来处理多轮对话，但现有提示往往存在信息冗余或焦点分散问题。评估套件则更关注认知、指导和逻辑特性，这与增强模型推理能力的需求高度契合。文本生成任务中，沟通特性（如token高效管理）最为关键，有趣的是，研究还发现礼貌用语能显著提升效果，暗示LLM存在"以礼相待"的偏好。NLU任务的研究相对匮乏，现有成果主要集中在指导和认知特性，因其需要模型深入理解语义而非表面信息。此外，精简提示可提升安全性，个性化需要增强内在特性，而减少偏差则能优化判断任务表现。

模型特性分布呈现明显失衡：OpenAI系列模型（如GPT-4）研究最充分，其次是LLaMa等开源模型和谷歌系模型。这种不均衡引发了对特性跨模型迁移效果的质疑——我们推测特性效果可能因模型架构和任务类型存在差异，该假设将在第6节验证。

研究发现，优化内在负载管理、提供示例和使用外部工具是三大通用特性，而幻觉意识和责任特性则更具任务特异性。前者暴露了LLM在任务分解能力上的不足，后者则凸显了示例教学和工具辅助的普适价值。

五大开放问题亟待探索：(Oq1) 特性效果是否具有模型依赖性？(Oq2) 增强推理深度和反思能力能否提升对话等任务表现？(Oq3) 创造力对生成任务的实际影响几何？(Oq4) 无关特性何时会意外生效？(Oq5) 特定特性与通用特性孰重孰轻？这些问题的突破将极大推动LLM的实用化进程，未来可通过跨模型对比实验、多维度评估指标构建以及混合策略设计等路径深入探索。

5 优质提示中的特性关联规律

为探究提示特性间的关联性并提炼优化建议，我们系统分析了高质量自然语言提示。测试集包含969条精选提示：765条单轮提示来自学术论文和主流提示库（如Alpaca、Natural Instructions），204条多轮对话选自真实聊天数据（LMSYSChat-1M）。采用GPT-4o作为主评估模型，配合自洽性校验（Selfconsistency），在21个维度进行评分。开源模型（DeepSeek、Mistral）因格式适配问题（准确率仅65%-71%）未纳入最终评估，相关结果由Gemini-2.0补充（见附录D）。

方法创新
针对大语言模型评估不稳定的问题（Do ost mohammadi等，2024），我们建立三重保障机制：1）人工标注50条随机提示作为基准；2）设计贴合人类判断的评估模板；3）由三位资深研究人员（本科+半年经验）交叉验证。初始采用10分制无参考评估（Zheng等，2023）时，发现与人工评分的Cohen’s Kappa一致性过低（15/21维度<0.15）。通过引入增量评分系统（类似Yuan等，2024a），显著提升评估一致性。对于仍存偏差的维度（如目标、责任等），通过明确定义评判标准使最终一致性提升至可用水平（"Ours"方案）。

核心发现
图1展示了特性相关性热力图（排除双低分组合）。关键发现包括：
1. 自然关联组：令牌效率↔清晰度/逻辑性（r≥0.7）
2. 内在关联组：结构逻辑⇄上下文逻辑；安全性⇄社会规范
3. 意外强相关：目标明确性⇄认知负载；幻觉意识⇄可靠性
这表明：优化提示时，提升目标表述会自然降低认知负荷；增强事实核查意识能同步提高输出可靠性。

优化建议
1. 简明性优化：提升直接/清晰的表达可同时改善令牌效率和逻辑连贯性
2. 结构设计：逻辑严密的提示能引导模型自主监控生成过程
3. 防幻觉机制：显式要求事实核查可增强输出可靠性
4. 协同优化：结构逻辑/上下文逻辑/表达方式等存在隐性协同效应

待解问题
1. (Oq6) 特性相关性是否因任务类型而异？
2. (Oq7) 强相关特性间是否存在因果关系？
3. (Oq8) 这些关联如何实际影响模型表现？
未来可通过结构方程建模和任务特异性实验深入探索，为提示工程提供更精确的优化路径。

图1：GPT-4o评估的特性相关性（双低分区域以斜线标注）

6 实验过程中需要优化提示属性吗？

我们初步探索了提示属性组合对模型推理效果的影响。实验采用双轨设计：(1) 提示优化（§6.1）与 (2) 微调训练（§6.2），测试数据集涵盖MMLU（Hendrycks等，2021）、CommonsenseQA（Talmor等，2019）、ARC-Challenge（Clark等，2018）及GSM8K。

6.1 属性增强提示法

我们采用Llama-3.1-8B-it（Dubey等，2024）、Qwen2.5-7B-it（Qwen团队，2024）和OpenAI o3-mini（OpenAI，2025）进行提示实验，重点考察沟通、认知负荷和指令三个维度。由于已有研究对这些属性进行了充分探讨，我们排除了示例演示、任务目标和外部工具。实验以零样本思维链（CoT）提示"请逐步回答以下问题"（Kojima等，2022）为基础，并进行了以下优化：（1）添加"请"字体现礼貌；（2）要求"先反思既有知识再深入分析问题"以激活相关认知；（3）强调"仔细验证每个推理步骤"培养元认知能力；（4）设置"每个正确步骤奖励100美元"的激励机制。

核心发现如表2所示：不同提示属性对模型的影响存在显著差异，且效果因任务而异。总体而言，多数属性组合仅对Llama-3.1模型有效，对其他模型反而产生负面影响。值得注意的是，叠加多个正向属性未必能获得累积效应，单一属性往往效果最佳。具体来看，礼貌属性使Llama在Comm.QA和ARC-C数据集表现最优，而元认知则让Qwen在所有任务中独占鳌头。属性组合实验显示：虽然礼貌和相关认知各自都能提升Llama在MMLU和ARC-C上的表现，但二者结合后效果反而不及单独使用礼貌属性。类似地，在CommQA数据集上为Llama同时启用元认知和奖励机制时也出现性能下降。出人意料的是，o3-mini模型对大多数属性提示都呈现负反馈，我们推测其可能因过度训练导致提示分布偏移。需要强调的是，未观测到改进并不意味着属性无效，更精妙的提示方法或许能激发其潜力——这将是未来研究的重要方向。

6.2 属性增强微调实验

为探究指令微调等模型特性对提示属性的影响，我们针对Qwen-2.5-7BIt模型展开专项实验。该模型在礼貌提示下表现平平，因此我们将其作为研究对象。我们采用两种数据集进行对比微调：礼貌增强版（每个指令添加"请"字）和原始版，数据均来自Alpaca-GPT-4o数据集的2500个样本。

实验发现：1）经礼貌提示微调的模型，在输入中添加"请"字后性能显著提升（表3），说明显式礼貌标记能增强模型对礼貌风格的敏感性，其效果远超单纯修改提示语（§6.1）；2）出乎意料的是，在绝大多数实验中，礼貌增强数据的微调效果均优于原始数据，这表明在指令微调中融入礼貌等特定属性，能培育出更强大、更稳健的推理模型。

7 总结  

本文创新性地从属性维度切入，系统研究了自然语言提示对模型性能的作用机制。通过对150余项研究的梳理，我们构建了涵盖21个核心属性的评估体系，揭示了当前研究对属性关注失衡的现状，并指出优化盲区。基于优质提示库的关联分析，我们提炼出具有实践指导价值的提示策略。在推理任务实验中，单一属性强化常胜过多属性堆砌，针对性微调更能提升效果——这一发现打破了"属性叠加必然增效"的固有认知。期待本研究能推动学界深入探究提示属性与模型行为的关联，完善评估体系，拓展应用边界。

研究局限

尽管我们力求研究严谨周全，但方法学上仍存在固有不足：

首先，文献覆盖存在缺口。受限于人力，我们虽尽力囊括多领域会议论文，仍难免遗漏部分研究，这可能影响结论的全面性。

其次，属性分析维度有限。精选的属性组合虽具代表性，但不同选择可能得出相异结论。我们通过人工校验确保提示词多样性，但属性选择本身的变数仍制约着结论的普适性，应用时需审慎。

再者，"责任"等复合维度（含偏见/安全/隐私等）划分较粗。因相关前期研究匮乏，我们暂采用现有框架。如表1所示，该维度尚存大量研究空白，未来将随新成果细化分类体系。

最后，提示优化实验采用基础模板，未针对模型特性调优。这虽构建了基础分析框架，但未能发挥定制化提示的优势，提示优化技术有待深入探索。

这些局限折射出宏观研究的固有挑战。本研究愿作引玉之砖，供后续研究参考完善。

伦理思考




我们的研究可能被恶意利用，例如优化生成虚假信息、仇恨言论或侵犯隐私的提示。虽然本研究初衷并非如此，但完全杜绝滥用行为存在固有难度。尽管该研究可能被用于对抗性场景，但我们认为其正面应用价值远大于潜在风险。此外，我们给予标注员每小时20美元的报酬，高于当地最低工资标准。

致谢




本研究获新加坡国家研究基金会AI新加坡计划（项目编号：AISG2-GC-2022-005、AISG2-TC2023-010-SGIL）及新加坡教育部学术研究基金Tier 1（项目编号：T1 251RES2207）资助。DXL获新加坡科技研究局计算与信息科学（ACIS）奖学金支持。衷心感谢新加坡国立大学WING团队、深度学习实验室同仁以及ACL RR匿名评审专家提出的宝贵意见。