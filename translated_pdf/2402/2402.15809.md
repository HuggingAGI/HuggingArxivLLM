行动学习：解锁大型语言模型智能体的潜能




赵海腾 1  马昶 3  王过寅 2  苏静 2  孔令鹏 3  徐晶晶 2  邓志鸿 1  杨红霞 2  
1 北京大学  2 字节跳动  3 香港大学

摘要

大型语言模型（LLM）智能体虽备受瞩目，却难以实现智能行为的关键特征——试错学习。本研究提出，从经验中学习新动作的能力是推动LLM智能体进化的核心。与人类通过实践不断拓展技能边界不同，现有LLM智能体受限于固定动作空间，发展潜力受阻。为此，我们开创性地探索了语言智能体的开放动作学习，提出LearnAct框架：通过迭代式学习策略，以Python函数形式持续优化动作库。该框架能在每次迭代中，根据失败任务反馈动态更新动作方案，实现效能跃升。在机器人规划和Alfworld场景的测试中，仅需少量训练样本，开放动作学习方法就使智能体性能显著提升（如在AlfWorld中较ReAct+Reflexion提高32%），有力验证了经验动作学习对培育更强大LLM智能体的关键价值。

1 引言

语言智能体正掀起研究热潮——它们以大型语言模型（LLM）为决策核心，通过迭代执行动作与环境交互（Yao等，2023a；Brohan等，2023）。这种范式之所以备受瞩目，在于它巧妙规避了传统强化学习"盲目试错"的缺陷，转而利用LLM强大的常识推理能力来指导智能体行为。

但研究者们也清醒意识到，LLM智能体存在"经验学习天花板"（Yao等，2023b）。由于模型参数规模庞大，直接微调策略并不可行。现有方案多将历史交互数据植入提示词（Yao等，2023a），但这种"记忆外挂"存在明显局限：既难以积累长期经验，又容易陷入单一样本的片面学习（Wang等，2023c）。

与人类通过实践不断拓展技能边界不同，当前LLM智能体被困在预设动作空间的"牢笼"中（Qin等，2023）。为此，我们开创性地提出"动作空间学习"范式，通过动态扩展和优化动作库，让任务需求与智能体的规划能力精准匹配。这种方法一举攻克了固定动作空间带来的两大顽疾：常识推理与具体动作的脱节，以及因条件缺失导致的策略失效（Gu等，2022）。

以调酒任务为例：开放动作空间下，LLM能智能规划"取龙舌兰→加冰块→搅拌"的完整流程；而封闭空间可能只允许"移动→抓取"这类机械操作，使简单任务变得异常艰难。更关键的是，当环境变化时（如糖块替换为糖浆），智能体需要实时更新动作库以避免无效操作。

基于这些洞察，我们推出LearnAct框架。其创新性体现在三方面：
1）用Python函数动态生成API化新动作，充分发挥LLM的代码生成优势
2）独创迭代优化机制：通过执行反馈不断打磨动作库，像老匠人雕琢工具般持续精进
3）构建可迁移的动作知识库，实现"举一反三"的泛化能力

如图1所示，训练阶段（左）通过"生成→反馈→优化"的闭环持续扩充动作空间；测试阶段（右）则能流畅调用学习成果进行序列决策。实验数据显示，LearnAct仅需少量样本就能掌握通用技能，在跨任务场景中表现卓越，大幅超越Reflexion等现有方案（Shinn等，2023）。这项突破为LLM智能体的进化开辟了新航道。

2 研究背景

LLM智能体演进 最新研究突破性地将大型语言模型(LLM)应用于具身智能体(Duan等, 2022; Huang等, 2022a等)。与早期思维链等纯推理方法不同，这些创新方案通过实时环境反馈不断优化决策，形成了以"学习-适应"为核心的智能闭环系统。

现有反思机制主要分为两类：ReAct框架支持单次任务内的行为反思(Yao等, 2023a)，而Reflexion体系则通过多轮试验积累经验(Shinn等, 2023)。ExpeL系统(Zhao等, 2023)进一步构建了经验检索库，Retroformer模型(Yao等, 2023b)创新性地采用提示训练来强化反思能力。

本研究的独特价值在于：(1) 直接在动作空间进行学习，大幅提升动作生成的可靠性；(2) 突破单一任务限制，实现跨场景的通用能力迁移。详见表1对比分析。

技术传承与创新 受分层强化学习启发，我们通过原子动作组合构建高阶动作体系，但突破了传统方法的局限：一方面利用编程语句实现更强大的逻辑表达(Cai等, 2023)，另一方面采用弹性层级结构替代僵化的双层框架。

（注：配图说明文字保持原样）

3 问题定义

语言智能体的任务可建模为部分可观测马尔可夫决策过程（POMDP），由五元组$\langle\mathcal{S},\mathcal{O},\mathcal{A},\mathcal{T},\mathcal{R}\rangle$定义：$s$为状态集合，$\mathcal{O}$为观测空间，$\mathcal{A}$为动作空间，$\mathcal{T}$是状态转移函数，$\mathcal{R}$是奖励函数。

在时刻$t$，智能体$G$根据策略$\pi$、任务指令$\mathcal{E}$和历史轨迹$h_{t}=\left[o_{1},a_{1},\ldots,o_{t}\right]$生成动作：

$$
a_{t}=G\big(\pi,\mathcal{E},h_{t}\big)
$$

只有当$a_{t}\in\mathcal{A}$时动作才可执行。我们采用二元奖励机制，根据任务完成情况给予奖励。

核心研究问题：如何让智能体通过经验学习提升决策能力？我们在训练-测试框架下进行研究：智能体先在训练集$\mathbb{D}_{\mathrm{train}}$上优化动作空间和策略，再处理测试集$\mathbb{D}_{\mathrm{test}}$中的任务。这些任务共享基础规则但场景各异，要求智能体掌握通用解决能力。

为此，我们提出动作学习框架（图1），将动作空间$\mathcal{A}$设计为可扩展的开放集。基础动作空间$\mathcal{A}_{0}$可通过学习新增动作类型$\mathcal{A}^{\prime}$进行扩展，对应策略指令更新为$\pi_{0}+\pi_{\mathcal{A}^{\prime}}$。扩展后，智能体可同时生成基础动作和习得动作$a_{t}\in\mathcal{A}_{0}\cup\mathcal{A}^{\prime}$，实现自主技能进化。

4 方法框架

LearnAct的工作流程如图1和算法1所示。其训练阶段分为两个步骤：首先生成新动作，再根据训练样本的误差反馈进行优化。当智能体在训练中掌握动作空间和策略指令后，便能从扩展的动作库中选取操作，逐步完成任务。具体训练流程详见$\S4.1$节，测试阶段说明见$\S4.2$节。

4.1 训练阶段

1. 动作空间拓展
我们开发了类API的增强型动作集合，让LLM能更流畅地与环境交互。这些创新动作以代码函数形式实现，通过条件判断、循环等编程结构，解锁了基于基础API构建复杂逻辑的潜能。动作创造分两步走：

(1) 函数生成：LLM根据任务说明，将高级操作封装成Python函数。这些函数可组合调用基础动作，生成后即纳入动作库。

(2) 策略升级：同步更新动作使用指南，包含函数说明和示范案例。LLM需详细描述每个新函数的预期效果、使用条件及输入输出规范，并生成典型使用场景示例。

2. 试错学习机制
初始动作可能存在缺陷，我们设计了迭代优化流程：智能体在训练实例中试运行→捕捉动作失败案例→更新动作库，直至完美通关或达到最大迭代次数。具体包含：

(1) 问题求解：在训练集上测试当前动作策略的有效性
(2) 错误定位：根据环境反馈识别无效动作、参数错误等故障点
(3) 智能修复：提供两种优化方式——修改函数代码修正逻辑缺陷，或增补使用说明引导正确操作

3. 择优采样优化
为避免低效优化，我们采用K次采样机制：每次迭代生成K个改进方案，通过双重指标筛选最优解：
- 任务成功率（pNCC）
- 动作执行效率（pspace）
最终选取综合得分μ（=psucc+psteady）最高的方案，确保动作库质量稳步提升。

4.2 测试阶段




学习阶段结束后，智能体已掌握升级的动作空间和优化后的策略指令。测试阶段中，它将沿用训练时的解题流程来应对问题。特别值得一提的是，尽管先前多数研究（Sun等人，2023；Shinn等人，2023）聚焦于单一问题实例的学习闭环，但LearnAct的创新之处在于实现了向通用场景的迁移能力。

5 实验  

5.1 任务  

机器人规划（Ma等，2024）涵盖四大挑战性任务：夹爪操作、积木世界、调酒师和轮胎世界。我们沿用了AgentBoard（Ma等，2024）与$\mathrm{LLA+P}$（Liu等，2023）的环境配置方案。这些任务考验智能体的长周期规划能力，比如根据订单调配鸡尾酒、用双机械臂跨房间搬运物品，或是重组积木达成目标布局。  

AlfWorld（Shridhar等，2020）则模拟了六类家居任务。例如：智能体需在房屋中搜寻苹果，加热后精准投放至目标区。由于环境状态仅在智能体探查后才可知晓，这些任务倒逼智能体建立系统性探索策略。

5.2 基准方案




Act & ReAct（Yao等，2023a）：基础版Act智能体通过环境中的迭代动作与观察来执行任务。进阶版ReAct则新增"思考"动作模块，实现更智能的决策流程。

Reflexion（Shinn等，2023）：该语言学习框架将自然语言转化为可执行策略。原版针对单实例策略设计，我们改进为通用训练-测试模式，增强策略的跨场景迁移能力（详见附录）。

Code As Policy（Liang等，2023）：这种开环方案直接生成完整代码解决方案。由于缺乏环境交互机制，仅适用于机器人策略任务——ALfWorld等需要动态环境探索的任务无法支持其运行。

Voyager（Wang等，2023a）：采用函数化动作的闭环智能体系统。原版专为Minedojo（Fan等，2022）的分层技能学习设计，我们参照Wong等（2023）的方案重构，通过代码生成并验证基础技能模块。

5.3 实验配置  

我们选用GPT-4和GPT-3.5 Turbo作为测试阶段的LLM模型，并全程采用GPT-4完成动作生成、优化以及Reflexion和Voyager的学习过程。实验温度参数固定为0.0以确保结果可复现，采样次数设为4次。每个任务随机选取3个训练样本，其余数据用于测试。测试集结果涵盖LearnAct及所有基线模型，以任务成功率作为核心评估指标。每组实验重复三次并取均值，所有模型均使用相同的上下文示例以保证公平性，其中Code As Policy和Voyager的动作生成代码与我们的完全一致。  

表2：机器人规划任务中LearnAct与基线模型表现对比  

![](images/2b2b0076c421948b74534e1ecef4b3682b6c7611b62c98e1bea3796fc97c3356.jpg)  

表3：Alfworld任务中LearnAct与基线模型表现对比  

![](images/47dabb823f77efc736d581432fe6f9a7a9e997ea0f5c779d42cd7558969df51f.jpg)

5.4 核心发现

表2和表3的数据对比显示，无论基于GPT-3.5 Turbo还是GPT-4架构，LearnAct在多任务场景中均全面领先基线模型。我们将深入解读这些性能差异背后的启示。

精心打磨的动作空间是智能体突破性能瓶颈的关键。LearnAct对Act基线的显著优势验证了动作空间设计的重要性——经过学习优化的LearnAct虽保留Act类智能体特性，但其进化的动作空间有力印证了我们的核心观点：智能体的潜力释放取决于规划能力与动作空间的有机融合。在机器人规划和AlfWorld任务中，LearnAct对ReAct的全面超越更揭示出：脱离动作空间支撑的"思考"如同无本之木。

动作学习展现降维打击优势。在机器人相关任务中，LearnAct以压倒性优势战胜各类Reflexion变体。尽管Reflexion通过动态生成策略提示带来改进，但其效果远不及LearnAct在动作空间的深耕细作。这证明：语言策略如同导航地图，而精熟的动作技能才是智能体驰骋疆域的战马。

代码生成任务进一步验证了动作学习的不可替代性。相比开环生成的Code As Policy，LearnAct的闭环学习机制优势尽显；而与算法相似的Voyager相比，其缺失的迭代学习环节恰恰成为性能差距的注脚——没有持续优化的动作就像未经打磨的兵器，终难展露锋芒。

5.5 学习机制解析

学习通过提升动作可靠性与实用性来优化性能。为探究学习效果，我们对比了训练前后动作的使用频率与准确率（图3左）。数据显示，经过学习后的动作正确率显著高于初始值，印证了其增强的实用价值——这正是我们纠错式学习方法的直接成效。值得注意的是，动作调用频率也同步提升，这可能源于智能体在验证动作有效性后更主动地加以运用。

表4：学习方式占比
（图片占位符）

表5：新习得动作均值
（图片占位符）

LearnAct采用三管齐下的学习策略：函数迭代更新、注释编写与新动作生成。分析表明，模型90%情况下选择函数更新（表4），这得益于代码调优能有效规避执行错误。学习过程还会催生新动作：如表5所示，两大测试场景中平均新增3-4个动作，且总量持续增长，体现"温故知新"的创新机制。

迭代次数直接影响模型表现。图3右显示，LearnAct在两次迭代内即可实现性能飞跃，效果远超ReAct+Reflexion——后者虽有效能提升，但稳定性欠佳且副作用明显。

为何持续学习反成掣肘？研究发现，过度修正智能体的执行错误会导致对特定案例的过拟合，进而误解任务本质（失败案例详见附录）。

5.6 消融实验

通过消融实验，我们验证了方法中各模块的关键作用。首先测试了动作描述与使用示例两种格式的效果（以GPT-4为智能体在机器人规划与Alfworld中的表现见表6）。结果显示：虽然使用示例常含错误，但其对性能的促进作用远超动作描述。值得注意的是，描述文本在机器人规划任务中尤为重要——这可能是因为复杂的前置条件使得动作描述能有效指导智能体正确操作。

（表7：学习方法对比 表6：动作格式分析）

![](images/cbd3bfb9c85158e257face074061e0965519a7763a43b93b8c81fd19e797d9bb.jpg)

进一步分析学习算法的核心组件：函数更新、动作笔记记录以及采样机制（结果见表7）。数据显示：90%的学习过程都伴随着函数更新，说明智能体更倾向直接优化函数而非简单调整参数标记。特别值得注意的是，采样机制对学习效果至关重要——缺乏采样时，单次迭代产生的动作方案极易出错；但单纯采样（不结合学习）也无法达到LearnAct的效果，这印证了我们算法的独特价值。

6 研究结论

本研究通过赋予大型语言模型（LLM）智能体与环境直接交互学习的能力，实现了智能体技术的重大突破。我们提出的LearnAct框架开创性地支持开放动作学习，使智能体性能获得显著提升——这种学习机制与人类技能习得过程高度吻合。在机器人规划与Alfworld环境中的实验成果，充分验证了动作学习对开发更强大LLM智能体的重要价值。