我们正式发布MCP-Bench基准测试平台，专为评估大语言模型（LLM）在真实场景中的复杂任务表现而设计。该平台基于模型上下文协议（MCP），将LLM与28个实时MCP服务器对接，整合金融、旅游、科研等八大领域的250种工具。与传统API测试不同，每个MCP服务器都配备协同工具组，可构建输入输出深度耦合的多步骤任务。MCP-Bench创新性地测试智能体三大核心能力：通过模糊指令检索工具、规划复杂任务的多跳执行路径、协调跨领域工作流——这些正是现有基准测试的盲区，因其过度依赖明确工具说明、简单工作流和单一领域操作。我们建立了涵盖工具理解、任务规划等维度的评估体系，对20款前沿LLM的测试结果揭示了诸多待解难题。项目开源地址：https://github.com/Accenture/mcp-bench。

1. 开篇

大语言模型（LLM）的突破性进展孕育出新一代工具型智能体，它们能够理解自然语言指令、设计多步骤方案，并通过调用外部工具完成复杂任务（详见OpenAI等机构2025年研究成果）。这类智能体已在旅游规划、医疗诊断和金融分析等实际场景大显身手，在这些需要串联多工具、解析结构化数据并协调关联操作的领域展现出强大潜力。

然而当前工具使用评测体系存在明显短板。早期基准如ToolBench虽汇集海量API，但功能彼此割裂，导致任务简化为单步调用或依赖人工拼接流程。进阶版τ-Bench精选兼容API实现流畅协作，却受限于狭窄的领域覆盖。最新MCP协议基准虽引入跨服务器标准化调用，但任务类型仍显单一，且普遍存在三大盲区：模糊指令应变（任务说明过于直白）、多目标协同（如行程规划需联动交通住宿）和跨领域推理（如结合财经数据与新闻分析股价）能力评估缺失。

为此，我们构建了MCP-Bench评测体系。该平台对接28个真实服务器、250个专业工具，覆盖科研金融等多领域。独创的LLM合成管道能自动生成贴近实际的多跳任务，并通过"去工具化"改写创建模糊指令变体。智能体通过多轮对话执行任务后，将接受双重检验：既核查工具调用的合规性，又通过AI裁判评估策略合理性。研究团队对20个主流模型进行104项任务测试，首次系统揭示了复杂场景下智能体能力的薄弱环节。

MCP-Bench的三大创新在于：真实生态对接（支持跨服务器协作）、模糊任务生成（模拟实际应用场景）和立体评估体系（结合规则检查与AI评分）。这为衡量语言模型的工具使用智能提供了前所未有的标准化平台。

2. 研究进展

大语言模型评测演进。评测体系已从静态评估逐步升级为交互式现实任务测试。早期MMLU（Hendrycks等，2021）和BIG-bench（Srivastava等，2023）采用单轮问答形式考察基础知识，HELM（Liang等，2023）则构建多维度评估框架。最新研究聚焦智能体协同与复杂推理（Koh等，2024；Zhang等，2025），MMLU-Pro（Wang等，2024）通过AI生成的烧脑题提升难度，MT-Bench（Zheng等，2023）专注多轮对话质量，AgentBench（Liu等，2024）模拟工具决策场景。尽管WebArena（Zhou等，2024）的网页导航和REALM-Bench（Geng & Chang，2025）的动态规划取得突破，现有评测仍难以还原需要灵活调度多工具的真实工作流。

工具化能力测评前沿。当前评估重点转向跨平台推理与执行：Mind2Web（Deng等，2023）固定浏览器操作，WebArena（Zhou等，2024）引入嵌入式工具，但均受限于预设工具。新一代评测各辟蹊径——τ-Bench（Yao等，2025）模拟用户终态验证，BFCL v3（Patil等，2025b）解析API工作流，C³-Bench（Yu等，2025）强化工具链推理，ComplexFuncBench（Zhong等，2025）采用执行验证评分。为突破定制工具局限，MCP系列评测应运而生：MCPRADAR（Gao等，2025）实现标准化工具交互，MCPEval（Liu等，2025b）更通过五服务器集群实现全自动测评。如何在真实MCP生态中构建可扩展的复杂任务评估体系，正是本研究着力突破的方向。

3. MCP-Bench框架：形式化定义与设计准则

3.1 工具型LLM智能体的形式化建模

基于Yao等人(2023)的研究框架，我们将基准测试构建为经典POMDP模型的扩展版本，专门适配跨多服务器协同工作的工具型智能体。该模型支持两种执行模式：全局单次规划与渐进式多轮决策。每个任务被定义为POMDP七元组$(S,\mathcal{A},O,T,R,\mathcal{U},\Sigma)$，其中：
- $s$：全局状态空间
- $\mathcal{A}$：融合规划指令与工具调用的复合动作空间
- $O$：包含工具反馈与内部信号的观测空间
- $T$：状态转移与观测生成函数
- $R$：状态奖励函数
- $\mathcal{U}$：任务指令空间
- $\Sigma$：MCP服务器集群，各节点$\sigma_i$提供专属工具集$\mathcal{T}_i$，共同构成全局工具库$\mathcal{T}=\bigcup_{i}\mathcal{T}_{i}$

工具调用采用结构化格式$a_{\mathrm{tool}}=\langle\sigma_{i},$工具名,参数⟩，完整动作空间为规划与工具调用的并集，观测空间则包含工具反馈与系统状态。

智能体采用多轮渐进决策机制：每轮生成基于历史观测的行动计划，执行工具调用后更新内部状态，循环直至达到最大轮数$T_{\mathrm{max}}=20$或触发终止条件。完整执行轨迹将用于最终推理，具体流程如算法1所示：
1. 初始化阶段（4-6行）：创建执行层栈$L$、轨迹记录器trajectory，并从任务指令$u$推导初始状态$s_0$
2. 决策循环（7-13行）：
   - $\pi_{\mathrm{plan}}$生成工具调用方案
   - $\pi_{\mathrm{exc}}$执行具体操作
   - $\pi_{\mathrm{compress}}$对冗长工具输出进行摘要处理（避免上下文窗口溢出）
3. 终止检测（14-15行）：实时监控continue信号
4. 结果生成（16-17行）：通过$\pi_{\mathrm{final}}$解析完整轨迹输出最终答案

（智能体执行使用的具体提示模板详见附录A.2节）

3.2 工具型LLM智能体的核心能力矩阵与MCP-Bench测评体系

在工具增强环境中游刃有余，LLM智能体需要突破传统语言建模的边界，掌握以下关键能力：

工具契约解析力。智能体需精准驾驭嵌套JSON、枚举类型、参数阈值等复杂调用契约，实现自然语言思维与机器语法的无缝对接。MCP-Bench设置250种梯度化工具契约（从基础标量到十层嵌套结构），连最细微的契约偏差都无所遁形。典型契约案例详见附录A.5。

迷雾导航能力。当任务描述如同雾里看花时，智能体要能在数百工具组成的迷宫中精准定位目标。MCP-Bench设置双重考验：每个任务配备10个干扰服务器（单实例工具库扩容100+），同时模糊任务变体（4.2节）刻意隐去工具名称，训练智能体凭借语义雷达在迷雾中穿行。

多线作战智慧。真实世界需要智能体像交响乐指挥家那样，统筹跨域多轮工作流，平衡并行目标，协调异构输出。MCP-Bench设置20轮超长任务链，不仅考察基础流程执行，更通过多服务器多目标复杂编排任务，检验智能体的时空规划能力（评估维度详见第5节）。

证据链思维。为杜绝信口开河，智能体必须建立完整证据链：每项结论都锚定工具输出，跨调用保持事实一致性。MCP-Bench采用执行溯源+LLM双盲评审机制，让真凭实据说话，让无源之水现形（详见第5节评估方案）。

现实变形记。最终考验在于智能体能否像变形金刚般适应千行百业。MCP-Bench搭建28个行业沙盘（从金融风控到文物修复），让智能体在逼近真实战场的复杂环境中，展现知识迁移与系统整合的终极实力。

4. MCP-Bench平台搭建

4.1. MCP服务器全景图

我们精选了28个代表性MCP服务器，覆盖11大功能领域（图2a）。媒体娱乐和知识研究并驾齐驱（各占14.3%），金融、科学和软件开发紧随其后（各占10.7%）。地理旅行、社交智能、数学和健康平分秋色（各7.1%），天气、占卜等小众领域（各3.6%）则点缀其间。这些服务器共提供250种工具，规模悬殊（图2b）：既有征稿系统、水果百科等单一工具服务，也不乏BioMCP（35工具）、科学计算平台（26工具）这样的多面手。从科研计算到金融分析，从内容推荐到地理服务，MCP-Bench构建了功能完备的生态系统。完整服务器清单和工具说明详见附表8。

4.2 任务合成

打造工具型智能体基准测试的核心挑战，在于如何将分散的MCP服务器转化为自然语言描述的高质量任务。面对跨服务器的工具网络，我们创新性地通过三阶段流程实现规模化任务构建：依赖链挖掘、智能质量筛选和任务描述泛化。典型任务示例见表2和表9。所有任务均通过人工校验确保真实性、可执行性与逻辑合理性，合成引擎采用OpenAI的o4-mini模型（2025c），完整提示模板见附录A.3。最终产出包含56个单服务器任务、30个双服务器任务及18个三服务器任务组合，多服务器配置方案详见表10。

依赖网络构建。我们从工具间的输入输出流入手，建立两类依赖关系：自然形成的工具链与人工设计的工作流。特别关注跨服务器协作场景，由此衍生出线性流程、并行组合作业等多元结构。这些依赖网络既指导LLM生成具体任务（提示模板见A.3），又作为后续评估的黄金标准（参见A.4）。

智能质检。采用双维度严格筛选：9分以上的可完成性确保技术可行性，5分以上的实用价值杜绝"纸上谈兵"。未达标任务直接淘汰（细则见A.3），宁缺毋滥保障基准质量。

语义泛化。通过剥离操作细节、保留核心目标，将机械指令转化为自然业务需求。对于数值敏感领域，在保持参数精确性的前提下进行语言风格转换，既考验智能体从用户意图反推技术方案的能力，又不失数学严谨性。泛化算法详见附录A.3。

5. 评估方法与指标




我们采用融合规则指标与LLM评分的综合评估体系。规则模块通过执行轨迹分析工具使用的健壮性，涵盖名称合规性、模式匹配度、运行成功率及依赖遵循度四大维度。智能裁判模块则从任务达成度、工具选用合理性、规划效能等战略层面进行评判，通过结构化量规结合提示轮换与分数均化来保证公平性。

5.1 规则化评估体系

我们从三大维度评估智能体的工具使用表现，检验其对接口模式的理解能力和执行稳定性。定义执行轨迹$E=\left\{e_{1},...,e_{k}\right\}$为所有工具调用的集合。

工具识别准确率：
$R_{\mathrm{valid}}=\frac{|\{e\in E:\mathrm{tool}(e)\in\mathcal{T}_{\mathrm{available}}\}|}{|E|}$
通过统计有效工具引用占比，反映智能体对工具集的认知准确度，杜绝幻觉调用。

接口规范遵循度：
$C_{\mathrm{schema}}=\frac{|\{e\in E:\mathrm{valid\_tool}(e)\wedge\mathrm{valid\_schema}(e)\}|}{|\{e\in E:\mathrm{valid\_tool}(e)\}|}$
双重验证工具存在性及参数合规性，确保每次调用都符合API契约要求。

运行时成功率：
$R_{\mathrm{success}}=\frac{|\{e\in E:\mathrm{success}(e)\}|}{|E|}$
直接衡量工具调用的执行效果，高数值代表智能体具备稳健的外部系统集成能力。

5.2 大语言模型裁判评估体系

为全面评估智能体的策略水平，我们创新性地采用"大语言模型裁判"框架，从三个维度精准打分：任务完成质量、工具使用合理性及规划有效性。所有评估均严格基于任务定义、最终方案和执行轨迹等客观证据，默认采用o4-mini（OpenAI，2025c）作为裁判模型。

智能裁判工作流程：
裁判系统接收完整评估资料包，包括：
- 原始及模糊化任务描述
- 依赖关系分析（对被评估智能体保密）
- 最终解决方案
- 执行轮次统计
- 精简版执行日志
- 可用工具清单

裁判需保持绝对中立，严格遵循结构化评分标准（详见附录A.4），将每个维度细分为多个子项进行1-10分制评分，最终归一化为[0,1]区间标准化得分。

三大评估维度解析：
1. 任务完成质量
- 目标达成度：是否完美实现任务目标
- 信息完备性：子任务覆盖与证据支持
- 响应精准度：解决方案的相关性与聚焦程度

2. 工具使用质量
- 工具匹配度：子任务与工具的契合程度
- 参数准确度：工具参数设置的专业性

3. 规划有效性
- 依赖处理：工具间约束关系的把控
- 执行优化：并行处理与冗余控制能力

抗干扰评估方案：
针对提示顺序敏感性（Li et al., 2025），我们创新采用：
1. 五轮随机洗牌：打乱维度和子项顺序但保持内容不变
2. 独立评分机制：每次洗牌后重新获取裁判评分
3. 科学统计算法：
   - 单次评分：子项均分→维度归一化
   - 最终得分：五轮维度均分的平均值

实证表明（6.4节），该方案显著降低方差，使评估结果更稳定可靠，极大提升了基于大语言模型的裁判系统的公平性与鲁棒性。

6. 性能基准数据

6.1 核心发现

本次实验评估了20款主流大语言模型，包括Meta的llama系列（8B/70B/90B参数）、OpenAI的GPT系列（4o-mini/4o/5）、Google的Gemini系列等（完整列表见原文）。表3数据显示：顶级模型如GPT-5、O3等在模式理解任务中表现卓越，合规性指标均突破98%大关；但在高阶推理维度，各模型差异显著——GPT-5以0.749总分领跑，而小规模模型如llama-3-8B仅获0.428分，尤其在依赖关系处理和并行计算环节暴露短板。

对比单机与分布式环境（表4-5）发现：弱模型在服务器扩容时性能明显滑坡。例如llama-3-8B总分从0.438跌至0.415，其依赖追踪能力在分布式场景中急剧退化。值得注意的是，性能衰减呈现非线性特征，反映出工作流编排对模型架构的严苛要求。反观第一梯队模型，GPT-5在两种环境下均稳定保持0.75左右的高分，彰显出处理跨服务器长周期任务的绝对优势。

这些发现揭示：当基础执行能力趋于普及时，系统扩展鲁棒性正成为区分模型层级的关键指标。表3完整展示了各模型在MCP-Bench基准测试中的综合表现（单机/集群环境均值）。

6.2 智能体能力全景：MCP-Bench深度解析

六大能力维度表现亮眼。表4与表5的细粒度数据显示：在任务达成、信息锚定、工具匹配、参数精度、依赖感知和并行效率等维度上，顶尖模型（如gpt-5、o3）展现出碾压性优势——任务完成度突破0.63，工具匹配度稳居0.7+高地。相比之下，小规模模型（如llama-3-1-8b）各项指标均不足0.45，暴露出明显的语义断层。最惊人的差距来自规划能力：gpt-5以0.76的依赖感知度一骑绝尘，而同类小模型普遍卡在0.3瓶颈线下，这成为区分智能体等级的核心标尺。

MCP-Bench三大发现：
1）基础能力趋于饱和：各模型在模式合规等底层能力上已达成95%+的共识，执行保真度不再是关键瓶颈
2）多服务器适应性分野：当任务规模扩展时，强者恒强（o3多服务器环境仍保持稳定），弱者愈弱（llama-3性能骤降30%+并伴随异常波动）
3）高阶推理鸿沟：顶级模型在结构化推理（0.72+）上的表现是普通模型的2.4倍，揭示长程规划仍是待攻克的圣杯

（表4/5可视化数据从略）

这项研究清晰划定了当前LLM的能力边界：虽然基础任务执行已臻完善，但面对跨服务器协同等复杂场景时，现有系统仍显捉襟见肘。MCP-Bench如同精密CT扫描仪，为智能体进化提供了精准的导航图。

6.3 各模型的任务执行轮次与工具调用对比

表6统计了各模型在MCP-Bench中完成任务所需的平均交互轮次和工具调用量。数据既揭示了基准测试的复杂度，也凸显了模型间的效能差异。该基准任务天然具有多步骤特性，常需跨服务器协调异构工具，要求模型兼具串行推理与并行调度能力。因此即便是顶尖模型，通常也需要经历多轮交互（见表6）并频繁调用工具，足见任务设计之精妙。

模型表现泾渭分明：轻量级系统如llama-3-1-8b-instruct资源消耗最大，单任务平均需17.3轮交互及155+次工具调用；gemini-2.5-flash-lite等模型也表现出对工具的重度依赖（平均86.8次）。反观gpt-4o、o3等强者，仅用30-40次调用和6-8轮交互就能斩获同等甚至更高的成功率。前沿模型如gpt-5则采取平衡策略：虽进行7-9轮深度推理，但将工具调用精准控制在48-79次范围内。

6.4. 大语言模型评估流程的消融实验

本节通过消融研究，验证提示词随机化和分数平均化对评估流程的优化效果。

模型间评分稳定性分析。我们基于50个基准任务计算了变异系数(CV)来量化评估流程的稳定性。这些任务由两个真实场景的MCP服务器自动生成：具备检索摘要功能的WebSearch 1和提供时间推理服务的Time 2。三个大语言模型(o4-mini、gpt-4o和gpt-4o-mini)采用统一流程对每个任务进行0-10分制评分。具体计算公式为：CV𝑡=σ𝑡/μ𝑡×100%，其中μ𝑡和σ𝑡分别表示各模型评分的均值和标准差。如表1所示，启用提示词随机化和分数平均化后，整体CV从16.8%降至15.1%，显著提升了模型间评分一致性。

人工评估一致性验证。我们邀请三位标注者对不同流程的评分结果进行独立评估，采用三级一致性评分标准(0-2分)。结果显示，基础流程的平均一致性得分为1.24分，而引入提示词扰动策略后提升至1.43分，证实该优化策略能有效改善人工评估体验。

7. 总结

本文提出了MCP-Bench基准测试平台，用于在真实生态系统的工具使用场景中评估大语言模型智能体。该平台基于MCP架构，将智能体与28个生产服务器无缝对接，提供250种工具支持，可实现复杂的多跳工作流和跨领域协同。我们通过自动化流程生成了104个模糊指令的挑战性任务，这些任务对智能体的能力提出了极高要求。结合规则校验和LLM评分的评估体系显示，即便是顶尖模型在依赖链遵循、噪声环境工具选择及长程规划等关键能力上仍面临挑战。