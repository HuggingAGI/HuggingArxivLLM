数学推理过程奖励模型的开发启示




张振如 郑楚杰 吴阳珍 张北辰 林润基 余博文* 刘大一恒* 周靖人 林俊旸*  





阿里巴巴集团Qwen团队  





https://hf.co/Qwen/Qwen2.5-Math-PRM-7B https://hf.co/Qwen/Qwen2.5-Math-PRM-72B

摘要

过程奖励模型（PRMs）为大型语言模型的数学推理过程监督开辟了新路径，能有效捕捉和修正推理链中的中间错误。但构建高效PRMs仍面临数据标注和评估方法两大核心挑战。实验表明，当前主流的蒙特卡洛估计法合成的PRM数据，其性能与泛化能力均逊色于LLM评判和人工标注方案。这是因为MC方法依赖完成模型验证步骤正确性，容易出现"错误步骤得正解"或"正确步骤出错答"的验证失真问题。研究还揭示了传统BoN评估策略的三重偏差：1）策略模型可能产出答案正确但过程漏洞百出的响应，使评估标准与过程验证目标背离；2）PRMs对此类响应的宽容导致BoN得分虚高；3）现有PRMs在最终答案步骤出现低分扎堆现象，暴露了BoN优化过程中从过程监督向结果评估的偏移。为此，我们创新性地提出共识过滤机制，有机融合MC估计与LLM评判，并构建响应级-步骤级双维度评估体系。这些突破使BoN评估和逐步纠错任务的模型性能与数据效率获得显著提升。最终推出的新一代PRM模型性能超越现有开源方案，为过程监督模型的研发提供了重要实践指引。

图1：Qwen2.5-Math-7BInstruct策略模型采用Best-of-8策略在PROCESS BENCH基准（Zheng等，2024）上的评估结果全景（细节参见表6、表7）。

1 引言

近年来，大型语言模型（LLMs）在数学推理领域突飞猛进（OpenAI，2023；Dubey等，2024；Shao等，2024；Zhu等，2024；Yang等，2024a；c；b），却仍难逃计算失误与逻辑漏洞，最终导致错误结论。更棘手的是，即便得出正确答案，这些"数学天才"也常会虚构看似合理的推导步骤——让正确结论建立在错误根基之上，严重动摇LLM推理的可信度。为此，过程奖励模型（PRMs；Lightman等，2023；Wang等，2024b）应运而生，成为近期研究焦点，专门揪出推理过程中的"作弊行为"，实现更精细的思维监督。

PRM研发面临两大难关：首先是推理过程标注这个"烧钱"工程。虽然Lightman等（2023）不惜重金聘请专业标注团队，但学界已开始探索自动标注捷径。主流方案是通过蒙特卡洛（MC）模拟计算"蒙对答案"的概率来反推过程正确性（Xiong等，2024；Wang等，2024b；Luo等，2024）。其次是评估困境——现有研究多采用Best-of-N（BoN）评估法，简单粗暴地从N个答案中挑选PRM打分最高者。直到PROCESS BENCH（Zheng等，2024）横空出世，才填补了逐步评估的空白。

我们在传统MC+BoN框架下训练PRM时，却发现了"皇帝的新衣"：MC估计训练出的PRM，性能竟远逊于LLM评委和人工标注。究其根源，MC方法存在先天缺陷——试图用未来结果倒推当前步骤，就像用考试分数反推每道题的解题思路。更糟的是，补全模型常会"歪打正着"（错误步骤得正解）或"功亏一篑"（正确步骤得错答），给评估注入大量噪声。而BoN评估更是漏洞百出：策略模型常交出"答案全对、过程全错"的"作弊卷"，PRM却因验证能力不足而"放水"，导致评分虚高。最讽刺的是，现有PRMs的打分分布显示，它们早已把"过程评估"的初心抛诸脑后，变成只认结果的"势利眼"。

为此，我们打造了"双保险"共识过滤机制：只有LLM评委和MC估计共同认定的错误步骤才会被采纳。这套方案在BoN评估中不仅数据利用率更高，还碾压现有开源PRMs。我们还提倡用PROCESS BENCH逐步评估为BoN补位。最终训练的PRMs展现出惊人优势——无论是专业PRMs还是通用模型，在错误识别能力上都望尘莫及，真正实现了"透视推理过程"的初衷。

核心突破包括：
• 撕下MC估计的"高效伪装"，实证其性能远逊于LLM评委与人工标注
• 打破BoN评估的"唯结果论"，开创响应级+步骤级双轨评估体系
• 设计"双判官"共识过滤机制，让PRM训练事半功倍
• 开源经实证检验的PRMs，为推理监督研究树立新标杆

本节记录了我们在基于蒙特卡洛（MC）估计的推理标注训练PRMs上的初步探索。虽然我们扩充了训练数据并精心优化目标函数，但基于MC估计的PRMs相较人工标注数据训练的模型（Lightman等人，2023）并未显现优势，尤其在识别特定错误推理步骤时表现明显逊色。

2.1 训练方案

数据合成 采用Math-Shepherd（Wang等，2024b）的MC估计方法构建PRM训练数据。我们整合了约50万条含标准答案的查询数据，通过混合Qwen2系列数学模型（7B/72B参数版本）生成6-8个多样化响应，并使用$\prime\prime\backslash\mathfrak{n}\backslash\mathfrak{n}^{\prime\prime}$分隔符拆解为独立步骤。基于Qwen2.5模型的八次并行推理，通过统计各步骤推导出正确答案的频次来标注步骤可靠性。

模型训练 以Qwen2.5数学指令模型为基座，将原始输出头替换为双线性标量头。采用两种标注策略：硬标签（八次推理中成功即标为正确）和软标签（正确率作为连续值）。分别在步骤终止标记处计算交叉熵损失（分类任务）和均方误差损失（回归任务）。特别地，我们剔除了错误步骤后的所有后续步骤，以消除错误累积对模型训练的干扰。

2.2 评估方案

我们从两个维度评估训练好的过程奖励模型（PRMs）：一是直接提升下游任务表现的效果，二是精准定位推理链条中错误步骤的能力。

最优采样（Best-of-N） 沿用学界主流方法（Lightman等，2023；Wang等，2024b），我们采用BoN策略进行评估：从$N$个候选答案中选取PRM评分最高的响应，记作$\mathrm{\Sigma^{\prime\prime}}\widetilde{\ p r m\@N^{\prime\prime}}$指标。具体实施中，基于Qwen2.5-Math-7B-Instruct模型生成8组响应（$N=8$），覆盖GSM8K数学题集、MATH基准等七大测试平台。每个响应的最终得分采用步骤得分连乘法计算（Lightman等，2023）。我们同步汇报了八次采样的多数投票结果$\scriptstyle({\bar{\mathsf{m a j@8}}})$作为基线，并以$@8$正确率（八次采样中出现正确答案的样本占比）作为理论上限。

过程评测（PROCESS BENCH） 该专项测试（Zheng等，2024）重点考察模型识别数学推导错误的能力。评估时要求模型要么定位首个错误步骤，要么判定全流程正确。我们沿用该基准的PRM评估方案，通过模型输出的预测分数序列来锁定首个错误步骤。

2.3 评估结果分析

表1和表2展示了三种模型的对比表现：基于MC估计数据集训练的硬标签模型（Qwen2.5-Math-7B-PRM-MC-hard）、软标签模型（Qwen2.5-Math-7B-PRM-MC-soft），以及PRM800K基准模型（Qwen2.5-Math-7B-PRM800K）。实验数据显示，在Best-of-8评估中，所有模型的prm$@8$得分均未突破ma$@8$基准线。更值得注意的是，在PROCESS BENCH测试中，两个MC估计模型的错误定位能力显著弱于人工标注的基准模型。这些结果促使我们重新审视当前主流的数据合成与评估方法，后续的优化过程也为我们带来了宝贵的实践洞见。

表1：不同训练策略模型在Best-of-8评估中的性能对比（含MC硬标签/软标签模型与人工标注基准模型）

表2：不同训练策略模型在PROCESS BENCH评估中的性能对比（含MC硬标签/软标签模型与人工标注基准模型）

本节将分享我们在PRM训练中收获的核心洞见，重点探讨两大关键问题：一是传统MC估计方法在PRM训练中的固有缺陷，二是仅凭BoN指标优化PRM可能带来的评估偏差。

3.1 PRM训练中蒙特卡洛估计的局限性分析

3.1.1 价值模型与PRM的本质差异

在数学推理任务中，奖励模型担任结果验证者角色，而PRM则通过对中间推理步骤的精准评估提供细粒度监督。与预测未来解题潜力的价值模型不同，PRM的核心功能在于对当前步骤正确性进行确定性评判。

蒙特卡洛估计方法试图预测从当前步骤最终解题的可能性。这种将价值评估理念融入PRM训练的做法，本质上模糊了两类模型的边界，可能带来模型性能与泛化能力的隐忧。

3.1.2 三大数据构建方法对比

实验数据显示（参见2.3节），传统蒙特卡洛方法在错误识别方面存在明显短板。我们系统比较了三种数据构建方案：
1）蒙特卡洛估计：基于44.5万Math-shepherd数据集和自建86万数据集
2）LLM裁判机制：采用Qwen2.5-72B验证86万数据
3）人工标注：使用去重后的26.5万PRM800K数据集

表3-4的对比结果揭示有趣现象：
- Best-of-8测试中，蒙特卡洛方案表现最佳
- PROCESS BENCH场景下，人工标注以最少数据量拔得头筹
- 数据规模效应：86万蒙特卡洛数据较44万基准提升显著

特别值得注意的是，人工标注展现惊人泛化能力，在Olympiad等复杂任务中优势明显；而LLM裁判在难题处理上优于蒙特卡洛方法。

3.1.3 共识过滤机制创新

针对蒙特卡洛方法噪声率高的问题，我们创新性地引入LLM裁判双重验证机制。如图2所示，经严格共识过滤后仅保留40%高质量数据，但模型性能却实现显著提升——在PROCESS BENCH评估中，过滤后数据表现媲美LLM裁判方案，而数据量仅为后者40%。

3.1.4 硬标签的显著优势

在300万原始数据实验中，我们发现：
- 未经过滤时软硬标签差异不大（噪声掩盖）
- 经150万数据过滤后，硬标签方案全面胜出

软标签的固有缺陷包括：
1）违背步骤评判的确定性原则
2）8次采样带来的高方差问题
3）正负样本区分度下降

阈值实验（图5）进一步证实：采用0值作为唯一负标签标准时，模型性能达到最优。

3.1.5 核心结论

综合实验表明：
1）蒙特卡洛方法整体弱于LLM裁判和人工方案
2）共识过滤机制可提升40%数据效率
3）硬标签+零阈值是最佳实践方案

3.2 PRM性能评估中的BoN采样偏差

尽管BoN评估广泛应用于PRM优化，但将其作为单一优化指标时需警惕其潜在局限性。

3.2.1 策略模型不可靠引发评估错位

理想情况下，模型响应应保持答案与解题步骤的一致性。但现有策略模型常产生"答案正确但过程错误"的响应，而BoN仅关注答案准确性，导致评估标准与PRM过程验证目标产生偏差。我们通过Qwen2.5-Math-7B-Instruct模型在四大数学数据集上的实验发现（图6），随着题目难度提升，这种"答案正确过程错误"的现象愈发显著。这说明优质PRM可能因严格评判过程错误而影响BoN得分表现。

3.2.2 过程验证缺陷导致分数虚高

当PRM无法识别"答案正确过程错误"的响应时，会造成BoN评估结果虚高。如图8所示，使用MC估计数据训练的PRM在BoN和PROCESS BENCH评估中呈现截然不同的表现趋势。测试数据显示（表5），除我们发布的Qwen系列PRM外，其他开源模型对这类案例的识别准确率均不足50%，凸显了现有PRM在过程验证上的重大缺陷。

3.2.3 优化目标偏移引发评估退化

BoN导向的优化使PRM逐渐偏离过程验证的初衷。如图7所示，分析多个开源PRM的评分模式发现，Eurus等模型超过40%的最低分出现在最终答案步骤，而我们的Qwen系列这一比例显著更低。这表明BoN优化正使PRM退化为结果导向的评估器，亟需引入步骤级评估（如PROCESS BENCH）来矫正这种偏移。

3.2.4 评分策略需因"模"制宜

研究发现（图9），对于MC估计训练的PRM，采用最终步骤分数策略效果最佳；而基于人工标注和LLM评判的模型则更适合乘积或最小分策略。这种差异提醒我们：评分策略的选择必须与模型训练方式相匹配。

3.2.5 核心发现总结

本研究揭示BoN评估存在三重局限：
1）诱发答案与过程的评估错位
2）掩盖过程验证能力缺陷
3）导致模型优化目标偏移

因此，必须引入步骤级评估作为必要补充。同时，不同训练方式的PRM需要匹配相应的最优评分策略，这对未来PRM研发具有重要指导意义。

4 方法论精要




本节将详细阐述我们突破现有局限的技术方案，并揭秘训练出的PRM模型如何实现业界顶尖性能。同时，我们也将呈现实验配置方案、对比基线模型以及详实的评估结果。

4.1 训练细节

数据构建分为两大步骤：数据扩充与数据筛选。在扩充阶段，我们采用第2.1节所述的MC估计算法生成数据，并设定硬性标准——只有当8个生成结果均未得出正确答案时，才判定为负面样本。在筛选阶段，我们使用Qwen2.5-Instruct-72B大模型（Yang等人，2024b）作为"AI裁判"，对每个推理步骤进行严格校验。通过创新的共识过滤机制，自动剔除LLM标注与MC估计存在分歧的样本，确保最终数据在推理标注上兼具高质量与一致性。训练时，我们在每个推理步骤末端采用交叉熵损失函数，基于硬标签进行二分类任务训练。实验分别使用Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct初始化了7B和72B参数的PRM模型。

4.2 实验配置

为验证Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B的训练效果，我们分别开展响应级BoN评估和步骤级过程错误检测任务PROCESS BENCH（Zheng等，2024）。

最佳N选择实验沿用2.2节设定。在rm@8评估中，我们对比了结果奖励模型（ORMs）和过程奖励模型（PRMs）。其中ORMs采用Qwen2.5-Math-RM-72B（Yang等，2024c）进行整体响应评分，PRMs则以各步骤得分的乘积作为最终评分。

对比的PRMs包括：
• Math-Shepherd-PRM-7B（Wang等，2024b）：通过最终答案正确率估算步骤标签
• RLHFlow双模型（Xiong等，2024）：基于LLaMA3.1架构，改进Math-Shepherd训练方案
• Skywork双PRM（2024）：最新发布的Qwen2.5-Math系列
• EurusPRM两阶段模型（Cui等，2025）：采用隐式PRM训练法
• 自研双模型：基于PRM800K和Math-Shepherd数据集微调

PROCESS BENCH测试中，评判模型（LLM-as-a-judge）对比了GPT-4o-0806、o1-mini等商业模型，以及Llama-3.3-70B、Qwen2.5系列等开源模型。我们将多步响应拆解为单步实例，供Qwen2.5-Math-RM-72B独立评分。

表6展示Qwen2.5-Math-7B-Instruct在Best-of-8策略的表现，⋆标注为自研模型。

![](images/deb279703c4838ee21ea993842e5c3185c2b80b23de925f81964cb6ef929d895.jpg)

表7为PROCESS BENCH测试结果，采用相同计算标准，⋆标注自研模型。

![](images/d4770506d572e004b0b0c75ed9c030de5dd934ec31db1196f7105fb2821d5ec6.jpg)

4.3 实验结果  

**最优N选测评**  
Qwen2.5-Math-7b-Instruct策略模型的评估数据（表6）显示，该模型在同等规模PRM中性能领先——不仅全面超越maj@8（平均提升1.4%），其72B版本在Minerva Math和MMLU STEM任务中的表现尤为亮眼。完整实验数据（含Qwen2.5-Math72b-Instruct的BoN测试、多维度评分及中文基准评估）详见附录A。  

**流程基准分析**  
如表7所示，Qwen2.5-Math-PRM-7B在开源模型中一骑绝尘：相较LLM-as-judge优势显著，对专有模型GPT4o-0806形成压制，但与o1-mini尚有距离。两大版本PRM均大幅刷新同类记录，而Qwen2.5-Math-RM-72B展现意外之喜——其步骤错误检测能力甚至优于部分开源PRM，印证了规则机制外延奖励模型的巨大潜力。

5 相关研究

数学推理的奖励机制
在提升数学推理精度方面，奖励模型对最优答案的筛选至关重要。现有两种主流模型：(1) 结果奖励模型（ORM）——针对解题全过程（尤其是最终答案）评分；(2) 过程奖励模型（PRM）（Uesato等，2022；Lightman等，2023）——逐步评估推理链条。研究表明（Lightman等，2023；Wang等，2024b），尽管PRM需要更优质训练数据，但其性能超越ORM，潜力更为显著。

推理步骤验证体系
步骤验证存在两大路径：其一是人工标注（Lightman等，2023），数据精准但代价高昂；其二是自动化评估，又分为：(1) 逆向推导法——通过结果反推步骤正确性，涵盖MC估计（Wang等，2024b；Luo等，2024；Chen等，2024）、渐进式ORM标记（Xi等，2024）及信用分配（Wang等，2024a；Cui等，2025；Yuan等，2024）等技术；(2) 提示法——直接调用大语言模型担任裁判（即LLM-as-a-judge）（Zhang等，2024；Gao等，2024；Xia等，2024）。本研究创新性地融合了MC估计与LLM裁判双轨机制。

6 结论

本文深入研究了过程奖励模型（PRM），并推出了一款性能卓越的实用模型。首先，我们揭示了蒙特卡洛（MC）估计中的关键缺陷。通过大量实验证明：相较于大语言模型评审和人工标注，基于MC估计构建的数据在性能与泛化能力上均相形见绌。更重要的是，我们发现传统BoN评估会扭曲PRM的能力判断，诱发优化偏差——使验证重心从过程追踪滑向结果论成败。为此，我们创新性地融合MC估计与大语言模型评审，提出"共识过滤"策略突破MC局限。评估环节则双管齐下：既保留响应级BoN评估，又新增步骤级过程错误诊断任务PROCESS BENCH，彻底规避单一评估的偏颇。实验数据表明，该策略使数据效用与模型表现获得双重提升。展望未来，PRM在数据构建与评估体系上仍蕴藏巨大潜能，必将催生更稳健可靠的下一代模型。

局限性
当前研究存在两大待解课题：其一，PRM实际表现与BoN理论上限（pass@8）仍存显著落差，昭示着广阔的优化空间；其二，虽然我们实现了大语言模型评审与MC估计的协同过滤，但对海量优质人工标注数据的深度挖掘仍属空白。例如，如何通过弱监督方法实现高质量数据集的渐进式扩展，便是极具前景的探索方向。