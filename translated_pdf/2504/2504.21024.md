WebEvolver：用协同进化世界模型赋能网络智能体自主进化

田庆芳†，张宏明等 腾讯AI实验室

![](images/325d30e25727ed595b586382410442223aa63f5a1eb292ee2293461e57860a54.jpg)

图1：WebEvolver架构——融合世界模型前瞻机制的智能进化系统。该框架通过实时轨迹数据，让智能体与世界模型协同训练：世界模型根据当前状态预测后续观察，既可作为虚拟网络引擎生成训练用的合成轨迹，又能在决策时进行多步前瞻推演，为动作选择提供最优评估。

摘要

智能体自我进化技术——通过基于自身策略的轨迹采样来训练核心大语言模型（LLM）——正成为提升性能的新范式。然而当前网络环境中的最新研究遭遇瓶颈：在自主学习周期中性能会陷入停滞。究其原因，在于对网络环境的探索不足，以及未能充分挖掘LLM预训练的网页知识。为此，我们创新性地引入协同进化的世界模型LLM：它能根据当前观察和操作预测网络环境的下一个状态。依托LLM海量的网页知识储备，这个世界模型身兼双职：既是生成自指导训练数据的虚拟服务器，持续优化智能体策略；又能在推理时充当想象引擎，通过前瞻模拟指导智能体LLM的决策。在Mind2Web-Live、WebVoyager和GAIA-web等真实网络环境中的实验表明，该方法较现有自进化智能体性能提升10%，且无需借助闭源大模型蒸馏，充分验证了方案的有效性与泛化能力。本研究证实：整合世界模型是实现智能体持续进化的关键所在。

1 引言

自主智能体，特别是活跃在网络环境中的Web智能体，正成为实现复杂任务自动化、推进通用人工智能发展的核心力量（OpenAI等，2025）。这类智能体的能力源自两大支柱：支撑海量网络信息获取处理的系统架构，以及作为核心的（多模态）大语言模型——它能基于上下文智能生成操作指令。

当前智能体自我进化研究主要通过"行动-环境交互-轨迹收集-模型微调"的闭环实现迭代优化（Yin等，2024）。虽然这种自举方案降低了人工标注依赖，但存在天然天花板：随着策略过度适应已知轨迹，探索多样性衰减，性能最终停滞（Zeng等，2024）。现有解决方案如搜索算法变体（Koh等，2024b）虽能扩展动作空间，但需要耗费大量现实交互成本；模拟推演方法（Gu等，2024）又往往缺乏多步连贯性。

我们创新性地引入协同进化世界模型，通过预测"当前操作→网页状态变化"的语言模型，构建起智能数字孪生。基于Llama-3等预训练模型对网络生态的隐式认知，世界模型与智能体同步进化，实现两大突破：
• 虚拟服务器功能：自主生成涵盖更广场景的模拟轨迹，突破现实交互的数据局限
• 想象引擎作用：支持零成本多步推演，预评估策略可行性（Zhang等，2025a）

在Mind2WebLive等真实网络环境测试中，新框架性能超越Open Web Voyager基线10%，尤其在处理未知任务时优势显著。本研究的主要启示在于：通过世界模型构建的"虚实双轨"训练机制，为智能体突破数据瓶颈提供了新范式——既保持现实 grounding，又获得低成本想象力，在最小人工干预下实现持续进化。

2 相关研究

网络智能体 当前网络智能体研究以（多模态）大语言模型为核心（Dubey等，2024；Jia等，2024），通过ReAct（Yao等，2023）、MCP（Anthropic，2024）等框架实现推理。这些智能体在WebShop（Yao等，2022）、Mind2Web（Deng等，2023）等基准测试中表现优异。除直接使用现成模型外，Explorer（Pahuja等，2025）等数据增强方案显著提升了模型训练效果。智能体树搜索（Koh等，2024b）、蒙特卡洛树搜索（Putta等，2024）等推理优化技术，进一步提升了决策质量。

智能体进化 研究呈现两大方向：一是基于现成LLM的策略模型微调，二是利用开源模型提升智能体能力（Aksitov等，2023）。BAGEL（Murty等，2024）等系统通过"探索-反馈-优化"的迭代循环，使智能体能在真实网络环境中持续进化。AgentQ（Putta等，2024）等方案结合强化学习，让智能体从成败经验中学习。Godel智能体（Yin等，2024）更实现了跨任务技能积累的突破。

世界模型 源自强化学习的世界模型（Ha等，2018），现已发展为智能体推理的利器（Valevski等，2024）。RAP（Hao等，2023）创新性地让LLM兼具世界模拟与推理决策双重功能。WebDreamer（Gu等，2024）等方案虽能预测行动结果，但仍受限于现有LLM的推理深度。

本研究突破性地在智能体进化过程中同步训练专用世界模型，实现了真正的多步轨迹推演，为交互决策提供了超越现有提示方法的可靠基础。

3 方法论框架




本节我们将揭秘WebEvolver——世界模型与智能体策略模型协同进化的共学框架，整体架构如图1所示。

3.1 任务建模

我们将网页智能体的任务建模为部分可观测马尔可夫决策过程（POMDP）$(\mathcal{S},\mathcal{A},\mathcal{O},\check{T,}\mathcal{R})$。智能体接收自然语言指令$q$后，需在网页环境中执行多步交互：状态空间$\mathcal{S}$表征完整环境，观测空间$\mathcal{O}$则仅包含可见元素。每个时刻t的观测$o_t=\Omega(s_t)$由提取函数$\Omega$从当前状态$s_t$中获取（含URL、网页元素等）。动作空间$\mathcal{A}$涵盖点击、输入、返回、滚动和停止等基础操作。转移函数$\mathcal{T}$执行浏览器操作，策略函数$\pi(o_t,q)\to a_t$生成动作序列$\tau=\{(o_1,a_1),...,(o_t,a_t)\}$，最终奖励通过自评估函数$\hat{r}(\tau,q)\in[0,1]$计算。

给定任务指令$q$和目标网站$w$，系统初始化环境并获取首帧观测$o_1\in\mathcal{O}$。参照Cognitive Kernel（Zhang等，2024a）的方案，我们采用可访问性树表示观测$o_t$中的元素。基于参数$\theta$的LLM策略模型会在每个时刻t生成思维链$h_t$和动作$a_t$：

$$(h_t,a_t)\sim\pi_\theta(\cdot|I,q,o_{1:t},h_{1:t-1},a_{1:t-1})$$

其中$I$为系统指令集。环境状态通过转移函数更新：

$$s_{t+1}=\mathcal{T}(s_t,a_t),\ o_{t+1}=\Omega(s_{t+1})$$

完整交互轨迹记为$\tau=(o_1,h_1,a_1,...,o_T,h_T,a_T)$，$T$为总导航步数。

3.2 智能体自我进化

本节阐述骨干智能体基础模型$\mathcal{M}$的自我进化机制，其策略函数表示为$\pi_{\mathcal{M}}$。

轨迹采集 基于输入查询$q$，模型$\mathcal{M}$生成动作序列，用于收集网页导航轨迹。作为认知内核的核心，M智能体通过与网页环境交互，依据最近k步的网页可访问树信息决策行动。

对每个查询$q\in{\mathcal{Q}}$，从策略$\pi_{\theta_{M}}(\tau\mid I,q)$采样轨迹$\tau_{i}$。为防止长上下文导致的性能衰减，当$t-1>k$时截断历史$c_{t}$，仅保留最新观测。保留思考与动作以维持历史信息的压缩表达。

$$
c_{t}^{\mathrm{clip}}=\big(h_{1},a_{1},\ldots,h_{t-k},a_{t-k},o_{t-k+1},h_{t-k+1},a_{t-k+1},\ldots,o_{t-1}\big),
$$

新动作生成函数：

$$
\big(h_{t},a_{t}\big)\sim\pi_{\theta_{M}}\big(\cdot\mid I,q,c_{t}^{\mathrm{clip}}\big).
$$

![](images/10b5d24cdf354c4d8cd62283dccd7de39998503b5a57ffa8b93f7177d3c8c59a.jpg)

图2：世界模型的轨迹合成与前瞻决策示意图

关键设计在于保留每步思考与动作，既维持完整推理链，又避免上下文爆炸。通过自动评估$\hat{r}(\tau,q)$进行拒绝采样，筛选成功轨迹。

迭代精进 第i轮进化时，筛选后的轨迹集记为$D_{i}$。优化目标函数：

$$
\mathcal{I}(\theta)=\mathbb{E}_{(q,\tau)\sim D_{\mathrm{i}}}\sum_{t=1}^{T}\left[\log\pi_{\theta}\big(a_{t}|q,c_{t}^{\mathrm{clip}^{\prime}},h_{t}\big)+\log\pi_{\theta}\big(h_{t}|q,c_{t}^{\mathrm{clip}^{\prime}}\big)\right],
$$

获得新策略${\mathcal{M}}_{i}$后，重新从查询集$\mathcal{Q}$采样，将成功轨迹扩充至$D_{i+1}$，开启下一轮进化。

3.3 WebEvolver：世界模型与智能体的共舞

本节将揭秘协同学习的世界模型，以及它如何赋能轨迹合成与前瞻推理。图2生动展现了这一机制。

世界模型养成记 这个语言模型高手能根据当前网页结构（o_t）和操作指令（a_{t-1}），精准预测下一个页面状态（o_{t+1}）。我们通过智能体自我进化过程中积累的实战经验来训练这个世界模型M_w。

将原始轨迹τ转化为世界模型专用格式τ_w后，模型就能根据历史操作预测未来状态。为避免长文本干扰，我们只保留最近的关键记忆，并让基础大模型M提炼状态转移的逻辑推理h_t^w，就像给模型配备了战略参谋。

世界模型的运作流程：
1. 构建记忆上下文c_t^w = (历史操作+推理笔记+最新状态)
2. 基于当前认知I_w和记忆上下文，生成下一页预测o_t
3. 通过轨迹数据持续优化模型参数θ_w

虚拟训练场 用策略模型M_i和世界模型M_w搭建的"数字孪生"系统，能无限生成合成轨迹：
- 世界模型扮演环境：ô_t = 模拟下一页(I_w, c_t^w)
- 策略模型做出决策：(ĥ_t,â_t) = 智能响应(I, q, ĉ_t^clip)

这些虚拟经验经过严格筛选后，与真实数据融合，共同训练出更强大的WebEvolver。

先知决策术 WMLA机制让智能体拥有"走一步看三步"的能力：
1. 对每个候选动作，用世界模型推演后续d步发展
2. 请评估专家LLM给每个推演剧本打分（0/0.5/1三档）
3. 选择最有前途的动作方案

这种虚实结合的训练方式，既避免了真实环境的高昂试错成本，又让智能体获得了超强的预见能力。

4 实验验证

4.1 实验配置

我们以认知内核（Zhang等，2024a）为底层框架，重点调用其Web智能体模块实现自主网页交互。其中，状态空间$s$覆盖整个互联网，通过Playwright驱动认知内核的Web容器。支持六类基础操作：输入、点击、滚动、返回、停止及重启。每个时间步$t$的观测值$o_t$来自虚拟浏览器当前视窗的可访问组件树，精准模拟人类浏览时的视觉感知。转移函数$\mathcal{T}$会执行原子级网页操作并更新状态，将执行异常实时反馈至推理系统，循环直至任务达成或触发终止条件。针对评估体系$\mathcal{R}$，我们采用GPT-4o进行端到端任务验证（参照He等2024a方案），有效规避人工逐步标注可能产生的漏判（Pan等2024）。该设计认可任务达成路径的多样性——成功轨迹不必与人工标注样本重合。GPT-4o将基于完整操作序列判断初始查询$q$的完成状态，输出0/1二元评估。

自优化方面，主干模型选用Llama-3.3-70b，所有自改进实验均基于该模型展开。拒绝采样阶段同样由Llama-3.3-70b担任任务完成度裁判。完整系统设计（含原子操作规范、提示词工程等）详见附录A。

实验选取两大实时网页导航基准：WebVoyager（He等2024a）与Mind2Web-Live（Pan等2024）。智能体需在真实网络环境中完成任务。受地域/IP限制影响，我们对不可访问站点进行了过滤。为保障结果可靠性，所有实验均在相同时段重复两次并取均值报告。

4.2 自我优化

我们以Llama3.3-70B为核心大语言模型$\mathcal{M}$，开展采样与自我优化。训练查询集采用Mind2web原始训练数据及WebVoyager、Mind2web网站的自生成查询，共计1,516条（参照Open Web Voyager方法）。首轮采样以Llama3.3-70B为策略模型，并以其自身为评估函数${\hat{r}}^{4}$进行拒绝采样（评估模板见附录A）。所得轨迹用于微调得到初代优化模型self-improve（iter 1）。随后用优化模型进行次轮采样，将新轨迹与首轮数据合并训练，得到二代模型self-improve（iter 2）。同时将轨迹数据重构为世界模型训练格式，即根据历史观察$a_{t-1}$预测下一状态$o_{t}$。

世界模型构建 同步使用Llama3.3-70B微调获得两代世界模型（iter 1/2）。合成轨迹生成阶段，联合使用二代世界模型$M_{w}$与性能更优的一代策略模型$M_{1}$。对每个查询$q$，从初始状态$\left(o_{0},a_{0}\right)$出发，通过世界模型预测（式9）与策略决策（式10）的交替执行，生成7步长的合成轨迹$\hat{\tau}$（遇终止状态则提前结束，示例见图4）。为增强数据多样性，仅采集优化迭代中失败的查询生成合成轨迹。最后采用零样本Llama3.3-70B为评估骨干，通过${\dot{\mathcal{R}}}$协议进行拒绝采样，将世界模型合成数据与监督微调数据融合，最终训练得到WebEvolver模型。

表1：纯文本WebVoyager测试集（473条）与Mind2Web-Live筛选集（53条）任务成功率对比。WebEvolver与WMLH为本研究方案，推理前瞻实验以WebEvolver为策略模型，更多策略模型对比见图3。

图3：WebVoyager与Mind2Web-Live平台成功率演进趋势可视化。

4.3 推理阶段世界模型前瞻（WMLA）

执行前瞻推理时，我们通过策略模型$\mathcal{M}$最多采样3个候选动作。在时刻$t$获取观测$o_{t}$后，首先采用温度参数为0的原始策略模型生成首个动作$a_{t}^{(1)}$。考虑到微调后的策略模型输出分布过于集中，为增加动作多样性，我们在保持解码温度0.7的同时，追加提示语："请生成与$\{a_{t}^{(j)},j\in\{1,\dots,k-1\}\}$不同的第$k$个动作"。随后，基于最终版世界模型（第2次迭代）和策略智能体，运用公式(11)分别进行1步、2步和3步的前瞻轨迹推演。最后参照WebDreamer框架，采用GPT-4o作为评分器，根据前瞻结果择优执行最高分动作。

表2：世界建模能力评估。结构正确率（STR）检验可访问性树的语法有效性，相似度（Sim.）比对真实网页内容，综合评分（O/A）衡量功能语义一致性。所有指标均为0-100%区间值，具体评估标准详见4.4节。

4.4 结果分析

本节呈现四大核心发现：自我优化效果、WMLA机制表现、世界模型内在评估及GAIA平台扩展实验。

核心性能表现（表1）：
- 基线模型（GPT-4o系列）建立基准参照
- 首轮迭代即实现6%成功率跃升（WebVoyager场景）
- 世界模型数据注入带来额外4%增益，显著优于传统合成轨迹方案

智能决策优化：
WMLH算法在预测深度d=2时达到最佳平衡点（表2），深度增至d=3时呈现边际效益递减规律，印证模型自身特性。

世界模型三维评估：
1. 结构完整性(STR)：生成DOM树语法验证
2. 内容相似度(Sim.)：与真实网页对齐度
3. 整体可用性(O/A)：功能语义连贯性
（评分体系：GPT-4o自动评估，0-1标准化）

图4典型案例显示，未经专门训练的世界模型能准确还原GitHub"排序"菜单选项，展现LLM内生的网络结构知识储备。

跨领域验证（GAIA-web）：
- L1级查询：持续优化趋势明显
- L2级复杂任务：提升有限（受当前训练范围制约）
- 采用bing.com测试成功突破训练数据边界（表3）

重要发现：
世界模型生成的"幻觉"内容客观上丰富了轨迹多样性，其正向效应已通过下游任务增益得到实证。

5 总结

本文提出的WebEvolver框架，通过智能体与世界模型的协同学习实现自我进化，显著提升了模型迭代优化的效率。该世界模型不仅能辅助训练，还能在推理时进行前瞻预判，智能筛选最优行动方案。在WebVoyager、Mind2Web-Live和GAIA-web三大平台上的实验，充分验证了该框架对智能体自我提升的卓越成效。