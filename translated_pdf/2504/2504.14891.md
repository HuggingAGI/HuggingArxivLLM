大语言模型时代的检索增强生成评估：全面综述  

  

Aoran $\mathbf{G}\mathbf{A}\mathbf{N}^{1}$、Hao $\mathbf{Y}\mathbf{U}^{2}$、Kai ZHANG$^1$、Qi $\mathbf{LU}(\boxtimes)^{1}$、Wenyu $\mathbf{Y}\mathbf{A}\mathbf{N}^{1}$、Zhenya HUANG$^1$、Shiwei TONG$^3$、Enhong CHEN$^1$、Guoping $\mathbf{H}\mathbf{U}^{1,4}$  

  

$^1$中国科学技术大学认知智能国家重点实验室，合肥  
$^2$麦吉尔大学，蒙特利尔，加拿大  
$^3$腾讯公司，深圳  
$^4$科大讯飞人工智能研究院，合肥  

  

$\copyright$高等教育出版社 2025  

  

摘要  
检索增强生成（RAG）技术通过将大语言模型（LLM）与外部信息检索相结合，实现了跨领域应用中准确、即时且可验证的文本生成，为自然语言处理带来了革命性突破。然而，这种融合检索与生成组件的混合架构，以及其对LLM时代动态知识源的依赖性，使得RAG系统评估面临独特挑战。为此，本文系统梳理了LLM时代下的RAG评估方法与框架，涵盖系统性能、事实准确性、安全性和计算效率等维度，既回顾传统方法，又剖析新兴范式。我们首次对RAG专用数据集与评估框架进行体系化归类，并对高影响力研究中的评估实践开展元分析。据我们所知，这是目前最全面的RAG评估综述，不仅贯通传统方法与LLM驱动的新范式，更为推动RAG技术发展提供了关键路线图。  

  

关键词  
检索增强生成，系统评估，大语言模型

1 引言

检索增强生成（RAG）作为一种突破性方法论，通过整合外部知识显著提升了自然语言生成能力。该技术通过非参数学习、多源知识融合和垂直领域适配三大核心机制[1,2]，使大语言模型能够基于权威实时数据生成既符合语境又准确可靠的响应，推动了自然语言处理系统的重大革新[3,4]。

从宏观架构来看，这个融合语言模型与检索技术的复杂系统可划分为检索与生成两大模块。检索模块涵盖预处理、稠密/稀疏检索、重排序等核心操作[5,6]；生成模块则包含检索规划、多源知识融合及逻辑推理等组件[7,8]。系统还集成文档分块、向量嵌入、安全验证等上下游环节[9]，其整体效能既取决于各组件性能，更依赖于系统级的协同优化。

面对如此复杂的系统架构，如何建立兼顾整体与组件的评估体系成为关键课题。RAG系统评估尤其面临三重挑战：应用场景的广泛性、内部组件的异构性以及技术迭代的动态性，这使得建立统一评估范式成为当前研究前沿。为此，我们系统梳理了近年来的RAG评估方法，其全面性体现在：1）体系完整性——涵盖组件级与系统级评估；2）方法多样性——包含传统统计指标与LLM时代的新型评估；3）来源广泛性——整合结构化框架与前沿论文方法；4）实践指导性——聚焦可量化指标与实际应用。通过这种多维视角，为研究者提供评估优化RAG系统的完整工具箱。

本文结构如下：第2章概述RAG系统基础架构；核心评估分为内部评估（第3章）与外部评估（第4章）——前者聚焦组件级技术指标，后者关注安全性、效率等系统级属性。我们特别分析了LLM赋能的评估新范式，这种时代特有的创新方法正在重塑评估体系。第5章整合现有评估框架、数据集与方法库；第6章则从评估视角对近年高水平研究进行多维度解析。通过这种系统化的知识梳理，旨在推动RAG技术向更精准、可靠的方向发展。

2 背景

2.1 大型语言模型（LLM）

步骤1直译：
具有数十亿参数的大型语言模型是一类基于海量自然语言数据训练的生成式神经语言模型[10,11]。由于训练语料覆盖面广，LLM被认为隐式整合了世界知识[12]。通过指令微调，LLM能够遵循人类指令或请求，从而有效理解并生成类人文本[13]。其泛化能力为NLP、信号处理、推荐系统等应用开辟了广阔前景[14,15]。然而LLM的能力仍受限于训练数据，在处理训练数据之外的新信息时，有时会产生事实性矛盾的输出（幻觉）[16]。尽管通过后训练或特定数据集微调可使LLM适应不同下游任务，但这些方法在算术性、时效性、灵活性或可用性（针对闭源模型）方面仍存在挑战。因此，LLM推理阶段的优化技术备受关注。代表性技术之一是提示工程，通过人工构建任务描述和指令来增强LLM对任务目标的理解。上下文学习旨在使LLM能够分析模式并从任务样本中泛化，在少样本场景中具有显著优势[17,18]。与这些方法不同，RAG通过引入外部知识来解决LLM固有的知识局限问题。LLM与RAG具有互补优势：RAG能有效结合LLM的卓越推理能力与外部数据的广阔知识范围，更广泛地探索LLM应用潜力[19]；而LLM可作为RAG的关键组件，充当决策者、推理器、生成器甚至评估者[20,21]。

步骤2优化：
2.1 大型语言模型（LLM）

拥有数十亿参数的大型语言模型，是基于海量自然语言数据训练的生成式神经网络[10,11]。得益于广泛的训练语料覆盖，这类模型被认为隐含着世界知识的编码[12]。通过指令微调技术，LLM不仅能理解人类指令，还能生成自然流畅的文本响应[13]。其强大的泛化能力为自然语言处理、信号分析和推荐系统等领域开辟了新天地[14,15]。但LLM的能力边界仍受限于训练数据，在处理未见信息时可能出现事实性谬误（即"幻觉"现象）[16]。

虽然通过后训练或特定数据微调能让LLM适配不同任务，但这些方法在计算精度、实时性、灵活性及闭源模型可用性等方面仍存在局限。因此，LLM推理阶段的优化技术成为研究热点。其中，提示工程通过精心设计任务指令来提升模型理解能力；上下文学习则让模型从少量样本中举一反三，在数据稀缺场景表现突出[17,18]。而检索增强生成（RAG）另辟蹊径，通过引入外部知识库来突破LLM的知识瓶颈。

LLM与RAG形成完美互补：RAG能充分发挥LLM的推理优势，结合外部数据的广博知识，极大拓展应用边界[19]；同时，LLM又可作为RAG系统的智能中枢，承担决策推理、内容生成甚至效果评估等关键角色[20,21]。

2.2 检索增强生成（RAG）

结果1：
RAG是一种通过整合外部知识检索来增强NLP系统的技术框架，其核心创新在于能够对参数固定的神经语言模型进行训练后的额外非参数优化，有效扩展其操作领域同时保持架构稳定性[22]。在LLM广泛应用之前，学术研究已确立了通过外部知识注入来增强NLP任务的方法[23]。早期关于RAG的研究遵循基本的索引和阅读范式[24,25]。后来的研究明确了两个核心组件：(1)检索器，从外部数据源识别、索引、筛选和结构化相关知识片段；(2)生成器，通过分析和逻辑推理综合整理后的片段以生成输出[9]。图1展示了当前使用LLM实现组件推荐的RAG系统工作流程。下面我们简要描述每个模块的处理过程。

RAG系统的检索组件受到多个领域检索技术的启发，如信息检索[26]、开放域问答[27]和推荐系统[28,29]。在检索之前，首先需要为检索组件构建合适的语料库。数据来源多样，如特定领域数据集（维基百科）、专业语料（科学文章、财务报告）[30]，或通过网络爬虫或搜索引擎收集的实时数据[31]。随后通过离线分块和嵌入对语料进行过滤和预处理，使其符合检索友好结构。分块涉及根据原始结构或上下文信息将大文档分割为更小、更易管理的单元[32-34]。嵌入（或文本向量化）旨在将文本内容表示为高维密集语义空间，以实现高效检索计算[5,35]。

通常，RAG评估将任务转化为问答对话形式，包含问题、真实答案和文档候选[36,37]。在在线RAG工作流中，检索前会引入一些额外组件，如意图识别、查询重写和路由[38]。检索器随后从数据源索引文档集合。在这个核心步骤中，可采用多种检索策略，包括稀疏检索、密集检索、图检索或混合方法[6,39]。某些系统通过搜索引擎进行额外动态搜索，这在商业化产品中较为常见。一些系统可能引入检索后步骤对文档重新排序或融合不同来源数据[7,40]。在生成流程中，基于相关文档的响应过程分配给LLM，LLM主要作为决策者或推理者[8]。LLM不是独立生成知识，而是综合检索信息形成连贯响应，从而降低内部幻觉风险。此外，还可使用多种提示工程方法，如CoT[18]、ToT[41]、SelfNote[42]和RaR[43]等。根据具体任务和预期输出，在知识导向响应后可能需要后处理步骤，如多选题或分类任务的实体识别，以及多语言任务的翻译组件。此外，模型应用的实用性也是一个关注点，特别是安全性和效率[44]。

结果2：
检索增强生成（RAG）是一种通过融合外部知识来提升NLP系统性能的技术框架。其创新核心在于：能在固定参数的神经语言模型训练完成后，进行额外的非参数优化，既拓展模型能力边界，又保持架构稳定[22]。早在LLM普及前，学界就探索出通过外部知识注入增强NLP任务的方法[23]。早期RAG研究采用基础索引-读取模式[24,25]，后续研究则明确了双模块架构：(1)检索器：从外部数据源定位、索引、筛选并结构化知识片段；(2)生成器：通过逻辑推理整合知识片段生成输出[9]。图1展示了当前基于LLM的RAG系统工作流及组件实现方案，下文将简述各模块运作机制。

RAG检索组件融合了多领域技术精髓，包括信息检索[26]、开放域问答[27]和推荐系统[28,29]。实施时需先构建专用语料库，数据源可选用维基百科等领域数据集、科研文献/财报等专业语料[30]，或搜索引擎/爬虫获取的实时数据[31]。通过离线分块（依文档结构切分为可管理单元[32-34]）和嵌入（将文本映射为高维语义向量[5,35]），将原始语料转化为检索友好格式。

典型RAG评估采用QA对话形式，包含问题、标准答案及候选文档[36,37]。在线流程中，检索前常增设意图识别、查询优化等预处理模块[38]。核心检索阶段可采用稀疏/密集/图检索等多元策略[6,39]，商业系统还会联动搜索引擎动态补充。部分方案增设检索后处理，如多源数据融合或结果重排序[7,40]。生成阶段由LLM担任"决策大脑"，基于检索内容合成响应而非凭空生成，有效规避幻觉风险[8]。配合CoT[18]、ToT[41]等提示工程技术，可进一步优化输出。根据任务需求，后续可添加实体识别（多选题/分类任务）或翻译组件（多语言场景）。实际应用中还需重点考量系统安全性及运行效率[44]。

（注：配图说明文字保持原样：图1 LLM时代的RAG系统工作流程及组件实现）

2.3 相关综述研究

结果1：
Li等人[23]对RAG的核心定义进行了系统归纳与形式化描述，同时梳理了早期方法论与实践应用。Zhao等人[45]则将视野拓展至自然语言处理领域之外，勾勒出多模态RAG在AIGC全景中的发展轨迹。随着大语言模型(LLM)的兴起，RAG方法迎来爆发式发展，催生了大量记录该领域进展的综述论文[1,9,19,20,46]。当前研究多聚焦方法收集或应用案例，但缺乏对系统化评估机制的实质性探讨。虽然Yu等人[21]开创性地综述了RAG评估的概念性方法，但其分析主要局限于主流框架，对适用于多元场景的新兴评估方法着墨有限。本综述在既有研究基础上突破这些局限，为新兴评估方法提供更深入的洞察。

本研究在系统论框架下扩展了文献[21]的工作，纳入更丰富的RAG评估方法体系。我们区分内部评估与外部评估：前者聚焦系统架构中RAG组件的测评及其交互过程，后者侧重整体系统评估与环境考量（此处环境特指外部任务或特定评估场景）。研究视野不再局限于评估方法的概念定义，而是深入探究其在实际RAG研究中的应用。同时我们重点关注LLM语境下的RAG评估，以非结构化文本检索作为主流范式。由于基础架构差异，领域特异性变体（如知识图谱、多模态检索）不在讨论范围内。除非特别说明，后文所有"RAG"均特指采用非结构化文档作为外部知识源的无训练操作框架。

结果2：
2.3 相关研究综述

Li团队[23]首次系统梳理了RAG的核心定义与早期技术脉络，而Zhao团队[45]则将研究视野拓展至多模态领域，揭示RAG在AIGC生态中的演进路径。随着大语言模型(LLM)的崛起，RAG技术迎来井喷式发展，相关综述文献如雨后春笋[1,9,19,20,46]。现有研究多停留在技术罗列层面，对评估体系的系统性探讨尚属空白。Yu团队[21]虽开创性地概述了评估框架，但其研究仍囿于传统范式，对新兴场景的适用性分析较为有限。本文突破这些桎梏，着力构建更全面的评估方法论体系。

我们在系统论视角下深化了文献[21]的研究，建立包含内外双维度的评估体系：内部评估聚焦组件级指标与系统交互，外部评估则关注整体性能与环境适配性（环境特指具体应用场景）。不同于既往研究停留于概念探讨，本文重点剖析评估方法在真实研究中的实践应用。研究以LLM为背景，以非结构化文本检索为基准范式，排除知识图谱等多模态变体（因架构差异显著）。如无特别说明，后文"RAG"均指基于非结构化文档的无训练架构。

请步骤 1 和步骤 2  结果：

结果1：在本节中，我们总结并整理了先前研究中RAG系统内部组件及其相互作用的评估。我们解构了整个RAG系统的评估，重点关注内部组件的相互作用。然后介绍了一系列评估方法，从传统方法到新方法。所提到的元素和内部评估的含义指向了一个评估RAG系统核心功能优势的框架，即生成准确和可信的输出。

结果2：本节系统梳理了现有研究中RAG系统内部组件的交互评估。通过解构整体系统评估，聚焦内部组件协同机制，我们系统介绍了从传统到新兴的评估方法体系。这些评估要素共同构成了衡量RAG系统核心能力——生成准确可靠输出的评估框架。

3.1 评估目标

RAG系统的各个组件可以归结为解决两个核心问题：真实信息的检索，以及生成与标准答案高度契合的响应。这两个问题分别对应检索模块和生成模块的评估目标。

图2总结了检索组件和生成组件的评估目标。检索组件包含召回和排序两个主要阶段，二者的输出（相关文档）具有相似的评估方式。我们可以通过以下定义的目标，为检索组件构建若干成对关系：

相关性（相关文档↔查询）：评估检索到的文档与查询所需信息的匹配程度，衡量检索过程的精确性和针对性。

全面性（相关文档↔相关文档）：评估检索文档的多样性和覆盖范围，衡量系统是否全面捕捉了与主题相关的各类信息，确保检索结果能根据查询提供完整的视角。

准确性（相关文档↔候选文档）：对比候选文档集评估检索结果的精确度，衡量系统对相关文档的识别能力，以及能否给予高相关性文档更高评分。

生成组件的成对关系和评估目标如下：

相关性（响应↔查询）：衡量生成响应与初始查询意图及内容的契合度，确保响应内容切题且满足特定需求。

忠实度（响应↔相关文档）：评估生成响应是否准确反映相关文档的信息，衡量生成内容与源文档的一致性。

正确性（响应↔示例响应）：类似于检索组件的准确性指标，通过对比标准答案评估生成响应的准确度，检验响应内容的事实正确性和语境适配性。

3.2 传统评估方法

RAG系统植根于信息检索(IR)与自然语言生成(NLG)两大传统领域，其评估体系沿袭了这两个领域的经典指标，主要从检索和生成两个维度进行评测。

3.2.1 信息检索相关指标

这类指标源自传统检索系统，根据是否考虑排序可分为两类：

• 非排序类指标
这类指标仅评估二元相关性（是否相关），不考虑项目在排序列表中的位置。

准确率/Hit@K：考察结果中真正例与真负例的比例
$$Accuracy=\frac{TP+TN}{TotalNumber}$$
其中TP为真正例数，TN为真负例数。

召回率@K：在前k个结果中，检索到的相关实例占全部相关实例的比例
$$Recall=\frac{|RD\cap Top_{kd}|}{|RD|}$$
RD表示相关文档集，Top_{kd}表示前k个检索文档。

精确率@K：在前k个结果中，相关实例占检索实例的比例
$$Precision=\frac{TP}{TP+FP}$$
TP为真正例，FP为假正例。

F1分数：精确率与召回率的调和平均数
$$F1=\frac{2\times Precision\times Recall}{Precision+Recall}$$

• 排序类指标
这类指标关注相关项在排序列表中的位置分布。

平均倒数排名(MRR)：首个正确答案排名的倒数的平均值
$$MRR=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_i}$$
|Q|为查询总数，rank_i表示第i个查询首个相关文档的排名。

归一化折损累积增益(NDCG)：对低位相关文档进行折损计算
$$NDCG@k=\frac{DCG@k}{IDCG@k}$$
其中DCG@k为折损累积增益，IDCG@k是理想增益值。

平均精确率(MAP)：各查询平均精确率的均值
$$MAP=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{\sum_{k=1}^n(P(k)\times rel(k))}{|relevant\ documents_q|}$$
P(k)为位置k的精确率，rel(k)为相关性指示函数。

3.2.2 自然语言生成相关指标

这类指标着重评估文本输出的内容质量。

精确匹配(EM)：严格比对生成答案与标准答案的完全一致性，匹配得1分否则0分。通常需对答案进行标准化预处理（如转小写、去标点等）。

ROUGE：通过n-gram重叠度评估摘要质量，含ROUGE-N（n元语法）、ROUGE-L（最长公共子序列）等变体。

BLEU：基于n-gram精确率的机器翻译评估指标，会施加简短惩罚。虽广泛使用，但无法评估文本流畅度。

METEOR：改进版BLEU，引入同义词匹配和词序惩罚机制：
$$METEOR=(1-p)\frac{(\alpha^2+1)Precision\times Recall}{Recall+\alpha Precision}$$
α为平衡因子，p为词序惩罚因子。

BertScore：利用BERT等模型的上下文嵌入计算语义相似度，生成精确率、召回率和F1分数，对语义等价更敏感。

文本相似度：评估检索文档间的语义差异，可通过文档内相似度或文档间相似度计算：
$$Minimize=\frac{1}{|D|^2}\sum_{i=1}^{|D|}\sum_{j=1}^{|D|}sim(d_i,d_j)$$
D为文档集，sim()为相似度度量（如余弦相似度）。

覆盖率：检索到的相关文档占全部相关文档的比例：
$$Coverage=\frac{|RD\cap Retrieved|}{|RD|}$$
也可按主题/类别分组计算。

困惑度(PPL)：衡量语言模型预测能力，基于交叉熵的指数形式：
$$Perplexity=\exp\left(-\frac{1}{N}\sum_{i=1}^N\log p(w_i|w_1,...,w_{i-1})\right)$$

需注意，IR与NLG指标并非严格对应RAG的检索与生成评估。例如SCARF用BLEU/ROUGE评估检索器查询相关性，Blagojevic等则用余弦相似度评估检索多样性。

3.2.3 上游预处理评估

随着RAG发展，语料预处理（分块和嵌入）的评估日趋重要。

分块评估分为两个层面：
1）内在指标：如关键词全覆盖率（要求关键词至少出现在一个检索块中）、完整上下文所需令牌数等
2）外在指标：分析不同分块方法对下游任务检索性能的影响，如比较ROUGE、BLEU等指标

嵌入模型评估方面，MTEB和MMTEB已成为行业标准。MTEB涵盖58个数据集的8类任务，证明没有万能嵌入方案；MMTEB进一步扩展至250+语言、500+任务，新增指令遵循、长文档检索等挑战场景。

虽然分块与嵌入模型应用广泛，但其核心价值仍体现在对检索器评估指标的提升上。

3.3 基于大语言模型的评估方法

随着大语言模型（LLM）的发展，RAG系统架构研究迎来了精细化探索的新阶段。当前研究越来越多地采用LLM驱动的评估指标，这些指标为不同RAG模块的迭代优化提供了可量化的基准。这些方法主要可分为基于输出和基于表征的两大类。

3.3.1 基于LLM输出的评估方法

这类方法通过对LLM生成的文本格式输出进行内容识别或统计分析，其流程简洁直观，且不受LLM开源/闭源属性的限制。

最直接的方式是通过提示工程让LLM对组件输出进行显式评分。例如RAGAS[61]和Databricks Eval[62]会向GPT裁判发出"检查回答是否得到检索上下文支持"或"评估回答对用户查询的完整度"等指令。张等人[63]采用小样本提示设计，利用GPT-4判断生成答案与标准答案的匹配程度。Finsås团队[64]构建多智能体LLM框架评估检索性能，其相关性判断比传统方法更符合人类偏好。Patil等人[65]提出基于抽象语法树(AST)的方法来量化RAG系统中的幻觉现象，该方法能有效监测外部API调用的准确性。这些方法通常受益于思维链推理技术。

此外，学者们还从LLM输出中提炼出新颖的统计指标定义，为多维度评估RAG组件提供了可能：

戴等人[66]提出语义困惑度(SePer)指标，通过聚类实体目标捕捉LLM对生成答案正确性的内部置信度。给定查询q和参考答案a*，其计算公式为：
$$SePer_M(q,a^*)=P_M(a^*|q)\approx\sum_{C_i\in C}k(C_i,a^*)p_M(C_i|q)$$
其中M代表特定LLM，C为聚类模型生成的响应簇集，k()为衡量语义簇与参考答案距离的核函数。

齐团队[67]将关键点提取引入RAG评估，设计KPR指标量化LLM将检索文档关键点融入回答的程度：
$$KPR(\cdot)=\frac{1}{|Q|}\sum_{q\in Q}\frac{\sum_{x\in\mathbf{x^q}}I(x,\mathcal{M}(q||d^q))}{|\mathbf{x^q}|}$$
其中I()函数判断LLM输出是否包含预定义关键点。

为评估不同检索器的表现差异，李等人[68]提出相对胜率比(MRWR/MRLR)指标。给定M个检索器在N个问答样本上的表现，首先计算各检索器在样本上的正确性标识I^m(n)，继而定义检索器r_i相对于r_j的相对胜率：
$$RWR(i,j)=\frac{\sum_{n=1}^N\mathbf{I}^i(n)*(1-\mathbf{I}^j(n))}{\sum_{n=1}^N1-\mathbf{I}^j(n)}$$
MRWR和MRLR则分别通过对行列方向取平均获得。

Min团队[69]提出的FactScore通过将生成内容分解为原子事实来验证其与知识源的匹配度。Chiang等人[70]进一步考虑同义表达，提出进阶版D-FActScore。其核心公式分别为：
$$FS(y)=\frac{1}{|\mathcal{A}_y|}\sum_{a\in\mathcal{A}_y}\mathbb{I}_{[a\,\mathrm{is\,supported\,by}\,C]}$$
$$DFS(y)=\frac{1}{|\mathcal{A}_y|}\sum_{\mathcal{A}_{y_i}\in\mathcal{A}_y}\sum_{a\in\mathcal{A}_{y_i}}\mathbb{I}_{[a\,\mathrm{is\,supported\,by}\,C_i^*]}$$

陈等人[71]从风险管控角度提出四维评估体系：
1) 风险度：保留样本中风险案例占比
$$Risk=\frac{|\mathbf{UK}|}{|\mathbf{AK}|+|\mathbf{UK}|}$$
2) 谨慎度：对不可答样本的识别率
$$Carefulness=\frac{\lvert\mathrm{UD}\rvert}{\lvert\mathrm{UK}\rvert+\lvert\mathrm{UD}\rvert}$$
3) 对齐度：系统判断与标注的一致性
$$Alignment={\frac{\left|\mathbf{AK}\right|+\left|\mathbf{UD}\right|}{\left|\mathbf{AK}\right|+\left|\mathbf{AD}\right|+\left|\mathbf{UK}\right|+\left|\mathbf{UD}\right|}}$$
4) 覆盖率：样本保留比例
$$Coverage=\frac{\left|\mathbf{AK}\right|+\left|\mathbf{UK}\right|}{\left|\mathbf{AK}\right|+\left|\mathbf{AD}\right|+\left|\mathbf{UK}\right|+\left|\mathbf{UD}\right|}$$

3.3.2 基于LLM表征的评估方法

这类方法通过建模LLM中间层或最终层的向量表征来获取评估指标，其优势在于能减轻对表面词汇模式的过度依赖，但可能因数值相似度与事实正确性的非必然关联而损失可解释性。

部分方法受传统指标启发，如GPTScore[72]基于BertScore思想构建LLM评分体系；ARES[73]结合分类器与LLM嵌入来验证生成答案与证据的语义对齐；RAGAS[61]则采用余弦相似度衡量答案相关性。

更多研究者开发了具有指导意义的新型表征指标：
赵团队[74]提出Thrust指标，通过分析LLM隐藏状态下的样本聚类效果来评估知识掌握程度：
$$s_{thrust}(q)=\left\|\frac{1}{N\cdot K}\sum_{l=1}^N\sum_{k=1}^K\frac{|C_{kl}|}{\|d_{kl}(q)\|^2}\cdot\frac{d_{kl}(q)}{\|d_{kl}(q)\|}\right\|$$

朱等人[75]将信息瓶颈理论引入检索组件评估，其目标函数为：
$$IB(\tilde{x})=P_{LLM}(x|[q,\tilde{x},y])-\alpha P_{LLM}(y|[q,\tilde{x}])$$

李团队[76]基于METEOR提出GECE指标量化生成文本的长尾特性：
$$GECE=\frac{|\mathrm{METEOR}(pred,ref)-\frac{1}{n}\sum_{i=1}^nP_{LLM}(t_i)|}{\alpha\cdot[E(\nabla_{ins})\cdot\nabla_{ins}]}$$

孙等人[77]设计的外部语境评分ε通过注意力机制量化知识利用程度：
$$\mathcal{E}_{\mathbf{r}}^{l,h}=\frac{1}{|\mathbf{r}|}\sum_{t\in\mathbf{r}}\frac{e\cdot x_t^L}{\Vert e\Vert\Vert x_t^L\Vert}$$
其中e向量由中间层注意力权重筛选的关键表征聚合而成。

值得注意的是，部分LLM评估指标虽非直接针对RAG系统设计，但其方法论创新对本领域研究具有重要启示意义。

4 外部评估




我们已经剖析了RAG的各个组件，并全面阐述了其内部评估机制。本节将重点转向RAG作为完整系统所面临的外部效用问题。我们将外部效用归纳为两大领域：安全性与效率，其具体评估方法如下所述。

4.1 安全评估

安全性能关乎RAG系统在动态、嘈杂甚至危险环境中生成稳定无害内容的能力。随着RAG系统广泛应用，其安全隐患已超越独立大语言模型。外部知识源的引入带来了独特漏洞，需要专门评估框架[20]。

【鲁棒性】评估聚焦系统处理误导性检索结果时的表现。RECALL基准[78]通过BLEU、ROUGE-L和误导率等指标测试系统区分可靠与虚假知识的能力。Wu等[79]采用误表征率和不确定率量化系统对语义相关但无关信息的敏感性。SafeRAG[80]针对"上下文冲突"等挑战设计专项指标，C-RAG[81]则通过保形风险分析和ROUGE-L提供理论风险保障。Cheng等[82]提出两大评估指标：1）韧性率——衡量检索增强前后系统保持响应准确的比例，体现稳定性；2）提升率——统计初始错误答案经检索文档修正的比例，评估RAG实效性。

【事实性】确保生成信息准确，避免看似合理实则错误的陈述（幻觉），尤其在检索结果存在噪声或冲突时[78,83,84]。核心指标包括：事实准确率（在误导性语境下采用EM/F1等标准QA指标[78]）、幻觉率（生成内容与检索文档矛盾的比例，常用LLM评判或人工评估[85]）、引证准确度（通过引证精确率/召回率评估来源标注[20,85]）以及忠实度指标（衡量输出与检索信息的吻合程度[83]）。

【对抗攻击】针对RAG流程特定环节：知识库投毒（Poisoned RAG[86]）通过注入恶意文本诱导预设输出，采用攻击成功率(ASR)及检索精度/召回率评估；检索劫持（HijackRAG[87]）操纵排序算法优先返回恶意内容，重点评估跨模型攻击迁移能力；幻影攻击[88]通过检索失败率(Ret-FR)评估触发文档效果；阻塞攻击[89]则插入强制拒答的"拦截"文档，采用预言指标评估。

【隐私性】评估检索库或用户查询的信息泄露风险[90]，常通过模拟攻击测试[91,92]。关键指标包括：提取成功率（从知识库获取特定隐私信息的频次[90]）、PII泄露率（生成输出中个人身份信息暴露比例[93]）、成员推断攻击成功率（判断特定数据是否存在于知识库的能力）。

【公平性】检测系统是否放大检索文档或训练数据中的偏见[94]。偏见指标量化不同人群的性能差异（如错误率、情感分数）；刻板印象检测统计有害陈规的出现频率；反事实公平性检验敏感属性变更时输出的合理性变化。

【透明度/问责性】评估系统推理过程的可理解性与可追溯性[95,96]，采用定性化指标：解释质量（人工评估说明信息的清晰度与完整性[96]）、可追溯性（输出与源文档的关联便捷度）、引证准确率（精确率/召回率）[20]。

综合安全基准需多维度标准化评估。SafeRAG[80]将攻击任务分为四类并配套数据集；VERA框架[97]采用自助采样计算安全指标置信区间；DeepTeam红队测试[93]通过系统化检测识别漏洞。当前研究表明，现有防御机制仍难以应对复杂攻击[86-88]。评估显示现有RAG系统存在显著脆弱性[87,88]，亟需建立针对检索-生成交互特性的评估体系。RAG安全性能评估仍需持续探索。

4.2 效率评估

效率是RAG实用性的另一关键维度，直接关系到系统普及度、成本效益与实际价值。

延迟评估通常关注两大核心指标。首词响应时间（TTFT）[98]衡量系统接收查询后生成首个输出词元所需时长，这对用户体验至关重要——它直接决定了用户感知的响应速度。该指标在需要即时反馈的交互式应用中尤为关键。完整响应时间（总延迟）则统计从提交查询到生成完整响应的全过程耗时，涵盖检索时长、处理时长及所有词元的生成时长。Hofstatte等人[99]提出的单查询延迟特指处理单个查询端到端的总耗时，包含完整的检索与生成阶段。

资源与资金成本评估是衡量RAG效率的另一核心要素。成本评估方法通常聚焦于量化直接影响系统经济性的直接支出与效率指标。RAG系统的总成本可分为以下关键组成部分[126]：

• 基础设施成本：本地计算资源（用于嵌入生成、向量数据库维护）及开源模型的LLM推理开销  
• 按量计费成本：基于输入输出词元使用量的外部LLM服务API费用  
• 存储成本：随语料库规模增长的向量数据库托管与维护支出  

表1 RAG基准测试及其评估数据集概览。来源领域标注数据出处（如实时新闻、专业语料库），特性标注则突出独特创新点（如领域特定任务、动态变化或错误前提数据）。

![](images/c9288c481b612883791990a70e559c75958ebded8b3a4f2e9423215f4388eb52.jpg)  

• 运维开销：人工监管、系统维护及知识库定期更新  
• 开发成本：系统初建、集成与定制化费用  

针对按量计费成本，OpenAI等LLM服务商提供词元用量统计功能。通过将词元数量乘以对应单价[127]即可计算成本。研究者已开发出多项经济性评估指标：

• 性价比比率：衡量单位成本带来的性能提升，实现不同RAG配置的标准化对比[127]  
• 检索精度投资回报率：通过量化减少无关上下文处理所节约的成本，反映提升检索精度的经济收益[127]。该指标显示优化检索可使LLM推理的词元消耗降低约50%，显著提升成本效益  
• 用户可控的成本-精度权衡：Su等人[128]提出采用可解释控制参数(α)的系统评估方法，能全面分析检索成本与精度的关系。该方法支持在动态成本约束下（而非固定操作点）评估RAG系统  
• 对比成本分析：针对特定用例评估不同RAG方案相对效率的方法论，兼顾直接成本与长期经济可持续性[129]

5 大资源




先前探讨的评估方法虽然全面，但数量上并不算丰富。本节系统性地整理、分类并呈现了大型语言模型时代涌现的RAG评估框架、基准测试、分析工具和数据集。据我们所知，这是目前文献中记载的最详尽的RAG评估框架合集。





数据集。我们汇总了近年来的基准测试及相关数据集。早期研究主要使用静态通用QA数据集（如NQ[100]、HotpotQA[101]），虽提供了成熟基线，但缺乏时效性和领域特异性。最新基准测试通过以下方式突破这些局限：1）采用实时新闻或快速更新的在线文档（如RGB[85]、MultiHop-RAG[7]）测试时效性能力；2）构建法律、医疗或金融等领域的专业语料库（如MedRAG[107]、OmniEval[120]、LegalBench-RAG[114]）；3）生成含错误前提或反事实元素的合成数据/专业QA对（如FreshLLMs[56]、RAGEval[115]）以评估鲁棒性和错误信息处理能力。如表1所示，我们根据原始资料对各数据集的来源领域和特征进行了简要说明。需注意，仅收录包含真实检索文档的数据集，这体现了对系统组件深度评估的关注。





含评估方法的框架。如表2所示，我们汇总了现有框架设计的评估方法。这些成果从早期的单点研究[40,99]发展到后期的多组件评估工具与基准测试[73,131]，构成了极其全面的评估框架集合。所用方法既包含传统指标[78,132]也涵盖基于LLM的度量[106,110]，还包括专注安全性的评估框架[85,116]，以及面向文档[55,125]、电信[113]、医疗[107]等特定领域的定制方案。参照3.1节提出的组件评估目标，我们对评估要素和具体指标进行了分类标注。





![](images/a18d4a32fdb5b7fb7e706953a9e5b3b8b4ef8bfdc1bb282829941baaa4e2fe9a.jpg)  

表2 RAG评估框架，突出核心评估目标与方法。检索侧重点主要在相关性(R)、正确性(C)或全面性，生成侧（右侧）则聚焦忠实度(F)、正确性(C)或相关性(R)。外部评估目标（安全性、效率）或其他声明以斜体标示。





![](images/63fb268265dbf6a91e6f4ffaadb26b2d065b9905374a3363a4c8d8477027b764.jpg)  

图3 RAG研究在检索、生成、安全性和效率四大领域的分布统计。单篇论文可能涉及多个领域的评估方法。





![](images/a0c64eb95ca0b9bc0cdadbf89cbe6c0e56d9cf795e99bcbf78a31edcfa367882.jpg)  

图4 RAG研究评估指标的词云频率统计。基于LLM的方法按目标分类并标注"-LLM"后缀，F-score指扩展版的F1分数。

6 讨论

6.1 RAG评估的统计与分析

随着大语言模型（LLM）的普及，RAG评估方法呈现出显著的多元化趋势。当前研究虽然全面覆盖了RAG评估维度，但各研究对其效用陈述往往带有主观性。为评估这些方法的流行度，我们从文献调研视角对现有方法进行了统计分析——这亦可视为一种研究导向的简易元评估。我们爬取了2022年秋季以来NLP与AI领域顶级会议录用论文中涉及RAG关键词的文献，提取其研究重点与采用的评估指标，最终收集到582篇PDF稿件。所有入选论文均经过严格同行评审，具有完整的实验方法与逻辑严密的评估流程，具备学术代表性。

研究焦点分布 图3展示了RAG研究中四个不同环节（检索/生成/安全性/效率）评估方法的统计分布。数据显示当前研究主要聚焦RAG系统的内部评估，这体现在检索与生成过程的高覆盖率上；而外部评估（特别是安全性相关）则关注较少。

指标偏好 我们对论文提及的评估指标进行词频统计（词云见图4）。当某指标在正文被正式引用或在实验结果表格中出现时，其词频计数+1。我们对同章节出现的同义指标进行人工归并映射，并剔除全局出现次数低于两次的词汇。观察发现：传统指标仍占据评估主导地位，基于LLM的方法尚未获得研究者广泛认可。这一现象源于传统指标的简便可靠特性，而LLM方法通常需要更多投入，且涉及LLM版本、提示设计等难以跨研究保持一致的变量。

LLM应用趋势 尽管存在上述问题，图5显示LLM方法的应用呈现明显增长趋势：2024下半年与2025上半年（截至3月31日）的应用量分列前两位。LLM评估者最终能处理更复杂的设计，更贴近实际应用场景。此外，LLM自身性能持续提升，支持功能不断扩展，也推动了这一趋势的发展。

6.2 挑战与未来方向



本节探讨当前RAG评估中存在的若干核心挑战。



基于大语言模型方法的局限性  
现有评估方案未能充分解决大语言模型固有的时效性与黑箱特性问题。采用大语言模型进行测评（特别是通过直接提示的方式）会引发稳定性与安全性的潜在风险。未来研究应着力提升评估流程的鲁棒性，并降低RAG系统中大语言模型的出错概率。



评估成本问题  
RAG系统的运行成本已引发关注。然而由于涉及工具与数据集的庞大规模，进行全面评估仍代价高昂。探索高效的系统评估方法，或在成本与效能间取得平衡，将成为未来重点研究方向。



进阶评估方法  
随着大语言模型持续进化，RAG系统组件日趋多元化。当前多数组件仍采用端到端的RAG本体指标进行评估，缺乏全面的功能解构评估或理论分析。与此同时，大语言模型自身功能尚存未开发潜力——例如针对深度思考模型（如openai-o1[135]）的评估，以及大语言模型思维流程与RAG检索生成过程的协同评估仍不完善。这些深度评估策略亟待未来研发突破。



评估框架的完备性  
尽管现有评估框架数量可观，但单个框架在指标体系和评估方法上均存在局限。且当前绝大多数框架仅聚焦英语、中文等主流语言。我们亟需兼具方法论创新与语言多样性的新型评估框架。

7 结论  

本文首次对LLM时代的RAG评估方法进行了全面梳理。通过系统化分析，我们为研究者和从业者揭示了这些日益普及的系统的重要洞见。在RAG内部性能评估方面，我们剖析了系统内部组件、明确了评估目标，并汇集了从传统到创新的各类方法与指标。此外，我们还探究了涉及系统完整性的外部评估维度（如安全性与效率）——根据我们的统计分析，这些在现有RAG研究中尚未得到充分探索。我们同时对当前评估数据集与框架进行了归类整理，以阐明这些资源的独特属性与评估侧重点。最后，我们分析了现有评估方法的实施现状，并综合提出了LLM时代RAG评估面临的挑战与未来方向。

步骤1：
致谢




利益冲突 作者声明他们没有需要披露的竞争利益或财务冲突。

步骤2：
致谢




利益声明 作者声明不存在任何竞争利益或财务冲突需要披露。