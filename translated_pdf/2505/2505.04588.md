零搜索：不依赖搜索即可释放大语言模型（LLMs）的检索潜能




郝孙 齐乐乔 郭佳岩* 范宣博 侯英燕 姜勇 谢鹏军 黄飞 张岩* 
同义实验室$\circledast$ 阿里巴巴集团$\mathcal{S}$ 
主页｜模型｜数据集｜$\Game$代码

摘要

高效信息检索是增强大语言模型（LLM）推理与生成能力的关键。最新研究尝试通过强化学习（RL）让LLM与实时搜索引擎互动来优化搜索性能，虽成效显著却存在两大痛点：其一，文档质量参差不齐——搜索引擎返回结果的质量波动会干扰训练稳定性；其二，天价API成本——RL训练需执行数十万次搜索请求，高昂费用严重制约模型扩展。为此，我们推出ZERO SEARCH框架，无需调用真实搜索引擎即可提升LLM的智能检索能力。该方法先对LLM进行轻量微调，将其转化为能同时生成相关文档与噪声文档的检索模块；在RL训练中采用渐进式课程策略，通过逐步劣化生成文档质量，让模型在递进式挑战中锤炼推理能力。实验表明：仅用3B参数的LLM作为检索模块，ZERO SEARCH就能显著提升搜索效能；7B参数版本性能比肩真实搜索引擎，14B版本更实现反超。该框架不仅适配不同规模的基座模型与指令微调模型，还能兼容各类RL算法，展现出卓越的泛化能力。

1 引言  

大型语言模型（LLMs）[36, 3, 45]在数学推理、问答和代码生成等任务中表现卓越[38, 39, 11, 22]，但其知识受限于预训练数据，存在生成幻觉或过时信息的问题[13, 34, 44]。为此，让LLMs接入外部信息源成为提升回答准确性的关键。  

检索增强生成（RAG）[29, 33, 30, 6, 2, 27]是当前主流解决方案。早期研究通过提示策略引导LLMs完成查询生成与多轮检索[43, 28, 42, 16, 33, 23]，但这类方法依赖精细的提示工程。后续工作转向监督微调（SFT）提升小模型性能[1, 24, 12]，并探索蒙特卡洛树搜索（MCTS）等动态扩展技术[25, 15, 46, 14]，但计算开销较大。  

近期，强化学习（RL）通过增强推理能力展现出巨大潜力[7, 9]。OpenAI-o1等模型仅凭奖励机制就实现了逻辑推理的突破。Search-R1等研究进一步将RL应用于信息检索，Deep Researcher更开创了与谷歌搜索的实时交互训练。然而，真实搜索环境带来两大挑战：文档质量不可控和高昂API成本。  

为此，我们提出ZERO SEARCH框架，其核心创新在于：  
1. 利用LLMs自身知识模拟搜索引擎，通过轻量微调即可生成风格匹配的文档；  
2. 设计课程rollout机制，逐步增加生成文档的噪声强度，让策略模型渐进适应复杂场景；  
3. 支持3B到14B不同规模的模型，7B版本即可媲美谷歌搜索，且兼容PPO、GRPO[32,7]等主流RL算法。  

实验表明，该方法在零API成本下超越真实搜索引擎方案，且泛化性强，为LLMs的搜索能力训练开辟了新路径。

2 研究背景

2.1 检索增强生成

检索增强生成（RAG）通过融合外部知识显著提升生成效果。早期研究多采用提示工程方案，借助查询生成、分解及多轮检索等流程驱动大模型[43,28,42,16,33,23]，但这类方法既需精巧的提示设计，又对模型推理能力要求苛刻。为提升效率并降低对黑盒大模型的依赖，新近研究转向小模型监督微调：如Self-RAG引入自省机制，通过预测反思标记迭代优化输出；RetroLLM采用约束解码技术，使模型能直接从语料生成细粒度证据。前沿进展还包括测试时扩展技术[25,15,46,14]，尤以蒙特卡洛树搜索（MCTS）为代表，可动态拓展推理搜索空间——RAG-star将其与树状推理结合，AirRAG则用以激活模型内在推理潜能。虽然效果突出，但这些方法计算成本高昂，实际落地仍存挑战。

2.2 强化学习驱动的搜索探索  

近年来，强化学习（RL）为大型语言模型（LLM）的推理能力提升开辟了新路径[7,9]。OpenAI-o1、DeepSeekR1等RL模型仅凭奖励信号驱动，无需分步指导，便在逻辑推理与迭代思考中展现出惊人潜力。研究还涌现出专攻信息检索的RL技术：Search-R1让模型在推理中自主生成搜索词，R1-Searcher设计了两阶段结果导向型RL框架，而ReSearch则实现了无监督的搜索式推理。但这些方法依赖维基百科等静态文本库，难以模拟真实交互的复杂性。Deep Researcher率先接入谷歌等商业搜索引擎，构建逼真训练环境，却受限于文档质量波动、天价API费用等现实瓶颈。为此，我们提出ZERO SEARCH——通过LLM模拟实时搜索，摆脱昂贵API束缚。该方案采用轻量化监督微调，精准控制文档质量，配合渐进式课程机制，显著提升训练稳定性与鲁棒性。

3ZEROSEARCH  

本节首先明确了无搜索引擎的强化学习目标，随后详解ZERO SEARCH的设计方案，包括训练模板、搜索模拟调优、渐进式部署策略、奖励机制设计以及训练算法。

3.1 告别搜索引擎的强化学习新范式




我们创新性地提出了一个强化学习框架，通过调用大语言模型（LLM）来模拟搜索引擎功能，从而摆脱对实体搜索引擎的依赖。该框架的优化目标定义为：  





' ' ' 





 ' 


$$


\operatorname*{max}_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},\,y\sim\pi_{\theta}(\cdot\mid x;\pi_{\psi})}\big[\,r_{\phi}(x,y)\big]\ -\ \beta\,\mathrm{D}_{\mathrm{KL}}\big[\pi_{\theta}(y\mid x;\pi_{\psi})\bigparallel\pi_{\mathrm{ref}}(y\mid x;\pi_{\psi})\big],


$$


 θ  





式中    $\pi_{\theta}$   为待优化的策略模型，  $\pi_{\mathrm{ref}}$   作为参考基准模型，  $r_{\phi}$   为奖励函数。  $\pi_{\psi}$   是承担搜索引擎模拟任务的LLM，其参数在训练全程保持冻结状态。  





![](images/98903521c2a1ff50ce6c1c7c13d66595274682cf7fccadb7b0ae9bc8933710bf.jpg)  


图1：PPO与GRPO训练方案示意图（无需真实搜索引擎支持）。

3.2 训练模板




回答问题时，需先在<think>和</think>标签内完成推理。若发现知识不足，可通过<search>查询</search>调用搜索引擎获取<information>返回的搜索结果。搜索次数不限，若无外部知识需求，则直接在<answer>内简洁作答，例如<answer>北京</answer>。问题：





表1：训练模板（问题在训练和推理时置于末尾）





ZERO SEARCH摒弃了监督微调方案，采用提出的多轮交互模板，通过持续推理与信息检索引导模型逐步逼近最终答案。





如表1所示，交互流程分为三阶段：先于<think>标签内完成逻辑推演；如需佐证则通过<search>发起查询；最终在<answer>标签输出结论。这种"推理-检索-应答"的模块化设计，既规范了决策流程，又提升了模型的可解释性与可靠性。

3.3 搜索模拟调优

在推演阶段，我们通过大语言模型（LLM）生成响应查询的文档来模拟真实搜索引擎。虽然直接让LLM生成文档看似简单，但其输出风格往往与真实搜索引擎存在明显差异。

为此，我们设计了一套轻量级监督微调（SFT）方案：首先收集LLM与真实搜索引擎的多轮交互轨迹，将获得正确答案的标记为正面样本（有效检索），错误答案的标记为负面样本（噪声检索）。随后提取查询-文档对进行微调，显著提升了LLM的模拟能力。如表2所示，仅需调整提示中的关键词即可区分文档质量，同时融入问题与答案对还能扩展模型的知识边界。最终，微调后的LLM能动态生成不同质量的文档。

"请模拟谷歌搜索引擎。根据给定查询生成五个[有效/噪声]文档（每个约30词），用于回答：[问题]（正确答案：[真实答案]）。查询：[查询] [有效/噪声] 输出："

表2的模板设计通过关键词控制文档质量，并借助问题-答案对增强知识覆盖。推演时，策略模型生成的查询会输入模拟LLM获取文档。我们采用课程学习机制逐步提升难度：通过动态概率函数$p_{i}=p_{s}+\frac{b^{i/m}-1}{b-1}(p_{e}-p_{s})$（默认$b=4$）控制噪声文档生成比例，让模型先掌握基础能力，再逐步适应复杂场景。随着训练步数$i$增加，噪声概率$p_{i}$从初始值$p_{s}$向终值$p_{e}$递增，实现渐进式学习。

3.5 奖励机制设计

奖励信号是强化学习的核心监督机制。本研究采用基于规则的奖励函数，仅以答案准确性为评判标准。初期实验发现，若采用精确匹配（EM）作为奖励指标，容易引发策略模型通过生成冗长答案来"骗取"奖励的问题。为此，我们改用F1分数作为奖励基准，其兼顾精确率与召回率，计算公式为：

$$
r_{\phi}(x,y)=\frac{2\times I N}{P N+R N},
$$

式中IN代表预测与标准答案的重合词数，PN为预测答案词数，RN为标准答案词数。鉴于模型无需额外监督即可输出格式规范的响应，我们未对输出格式设置专门奖励。

3.6 训练算法




我们的方案适配多种强化学习算法，无论是近端策略优化（PPO）、组相对策略优化（GRPO）[32,7]，还是强化升级版Reinforce++，都能为检索增强推理提供独特优化优势。




在ZERO SEARCH框架中，推演序列融合了策略模型生成的令牌和模拟大模型返回的文档令牌。由于检索内容来自外部系统而非策略模型直接控制，若采用统一优化方案易导致训练波动。




为此我们创新性地引入检索令牌损失掩蔽机制，确保梯度计算仅作用于模型自身输出。该方案在维持检索增强效果的同时，显著提升了强化学习训练的稳定性。

四大核心发现

4.1 数据集与评估体系




我们在多类问答基准上测试ZERO SEARCH性能：(1) 单跳问答数据集含NQ、TriviaQA及PopQA；(2) 多跳问答数据集涵盖HotpotQA、2 Wiki Multi Hop QA、Musique与Bamboogle。




参照方案，采用精确匹配(EM)评估标准：预测答案经规范化处理后与任一标准答案完全一致，即视为正确。

4.2 基线对比  

为验证ZERO SEARCH的效能，我们选取以下基线方法：(1) 基础提示法：含直接提问、思维链（CoT）及标准检索增强生成（RAG）；(2) 进阶RAG方案：如RAgent与Search-o1，二者通过多轮迭代检索信息；(3) RL优化模型：包括纯推理的R1和可交互搜索的Search-R1，后者支持策略模型在推理时动态调用真实搜索引擎。  

所有RL方法均以F1分数为统一评估指标。特别说明：基于RL的搜索基线中，我们仅对比Search-R1——因其无需复杂奖励机制或冗余训练步骤，能更纯粹地对比真实搜索与模拟搜索的效能差异。

4.3 实验配置

本次实验采用三大模型家族：Qwen-2.5-7B（基础/指导版）与Qwen-2.5-3B（基础/指导版），以及LLaMA-3.2-3B（基础/指导版）。为模拟真实检索场景，我们通过SerpAPI接入谷歌搜索作为外部引擎，并统一限定各方法检索五篇文档以保证对比公平性。

数据集方面，参照方案，合并NQ与HotpotQA训练集构建统一微调数据集。我们在七个数据集上开展评估，全面检验模型在域内外的表现。提示学习基线采用指导版模型（基础版难以遵循指令），而强化学习方法则同时测试基础版与指导版以验证模型泛化能力。

模拟LLM训练采用Qwen-2.5系列三款模型进行轻量监督微调，学习率设为1e-6。ZERO SEARCH训练采用GRPO与PPO双算法：GRPO模式下策略LLM以1e-6学习率训练，每提示生成五个响应；PPO模式则区分策略LLM（1e-6）与价值模型（1e-5）的学习率。全程使用λ=1、γ=1的广义优势估计(GAE)，默认采用GRPO算法与Qwen-2.5-14B作为模拟LLM。

4.4 性能表现

表3对比了ZERO SEARCH与多种基线方法在七个数据集上的表现，我们可以得出以下重要发现：

ZERO SEARCH全面碾压所有基线方法。无论是在领域内数据集（NQ和HotpotQA）还是领域外数据集（TriviaQA、PopQA等）上，都展现出卓越的鲁棒性。

ZERO SEARCH甚至超越了真实搜索引擎方案。相比采用真实搜索引擎的Search-R1，ZERO SEARCH表现更优，这为大规模强化学习提供了更高效的替代方案。

表3：不同LLM主干下的性能对比（最优结果加粗显示）

ZERO SEARCH展现出强大的泛化能力。无论模型架构、参数量级或类型（基础/指令调优）如何变化，其性能始终领先。更令人惊喜的是，模型规模越大，优势越明显，充分体现了其出色的可扩展性。

5 深入解析

5.1 ZERO SEARCH与真实搜索引擎性能对比

我们在LLaMA-3.2-3B模型上对比了ZERO SEARCH与真实搜索引擎Search-R1的奖励曲线（图2a、2b），主要发现如下：

首先，两者奖励趋势高度一致。随着训练推进，两种方法的得分均持续攀升，证明策略模型成功掌握了搜索引擎交互与答案生成能力。

其次，ZERO SEARCH学习曲线更为平稳。虽然初期表现稍逊（图2b），但凭借渐进式课程设计，最终以更稳定的优势反超Search-R1。

更重要的是，该方案展现出强大的模型适应性。无论是基础模型还是指令调优版本，ZERO SEARCH均能持续提升奖励表现，验证了其卓越的泛化能力。

图2：(a-b)LLaMA-3.2-3B上的奖励曲线对比 (c)基础模型训练中的交互轮次与奖励进展

表4：不同规模LLM（3B-14B）构建的模拟搜索引擎与谷歌搜索的对比结果

5.2 模拟大语言模型选型  

本节探讨不同模拟引擎配置对性能的影响，涵盖基于提示词和微调的3B至14B参数规模模型。表4数据揭示以下关键发现：  

其一，7B微调版模拟引擎（SFT-7B）性能比肩谷歌搜索，14B版本（SFT-14B）更胜一筹。这印证了在强化学习场景中，优质大模型可有效替代真实搜索引擎。  

其二，微调方案显著碾压提示词方案。尽管后者被明确要求模仿搜索引擎响应模式，但存在明显的分布偏差，致使效果逊色。  

其三，模型规模与性能呈正相关。参数量更大的模拟模型不仅模拟能力更强，还能精准识别文档相关性，为训练过程中的课程学习提供更优支持。

5.3 交互轮次分析

本研究采用LLaMA3.2-3BBase模型，通过追踪训练过程中的奖励曲线与交互轮次，深入解析了ZERO SEARCH的学习轨迹（详见图2c）。

训练初期呈现"双速现象"：交互轮次骤降而奖励缓升。这是因为初始策略不谙搜索调用之道，产生大量冗余交互，但模型很快掌握了正确范式，开始高效精简步骤。

随着训练深入，交互轮次与奖励均经历快速攀升后趋于平稳。这表明策略已能精准检索文档并给出正确答案，从而斩获更高奖励。值得注意的是，尽管后期奖励曲线看似平稳，但课程机制持续提升任务难度，迫使策略必须不断精进推理能力以维持稳定表现。

表5：强化学习算法对比（基于Qwen2.5-3B-Base和LLaMA-3.2-3B-Base模型的PPO与GRPO效果对比）

表6：课程设置对比（基于Qwen-2.5-3B系列模型的标准课程与逆向课程效果分析）

5.4 强化学习算法对决：PPO大战GRPO  

我们在ZERO SEARCH框架下，基于Qwen2.5-3B-Base和LLaMA-3.2-3B-Base模型，对PPO和GRPO两大主流强化学习算法进行了性能比拼（详见表5）。  

结果显示，两大算法都能显著增强框架的搜索能力，印证了方案的普适性。但GRPO表现更亮眼——双模型测试中稳定性更胜一筹，训练过程稳如磐石。不过要注意，GRPO的重复推演机制会推高真实搜索引擎的API调用成本，这也反衬出模拟搜索环境的设计智慧。  

（说明：结果2通过以下手法增强可读性：  
1. 用"对决/大战"替代"对比"，增加画面感  
2. "性能比拼"比"评估性能"更生动  
3. "表现更亮眼/稳如磐石"等比喻提升感染力  
4. "反衬出...设计智慧"比原句更突出结论价值  
5. 保留专业术语（PPO/GRPO/API）的同时，通过短句结构和口语化表达降低阅读负担）

5.5 逆向课程实验  

本节通过对比正向课程与逆向课程的效果展开分析——后者通过逐步提升检索文档质量来降低训练难度，具体数据见表6。  

实验结果表明，标准的"由易到难"课程在两种模型上均稳定优于"由难到易"方案，印证了课程学习机制的有效性。优质初始数据能让策略模型快速掌握搜索引擎调用和基础输出格式解析，随着训练深入，逐步增加的挑战性场景持续强化模型的推理能力。

6 总结  

本文创新性提出ZERO SEARCH框架——通过强化学习赋予大语言模型自主搜索能力，完全脱离真实搜索引擎依赖。我们采用监督微调将LLM改造为智能检索模块，既能生成目标文档也能模拟干扰项，并通过递进式训练策略让模型在渐增难度的检索场景中持续进化。实验显示，该框架不仅超越传统搜索模型性能，更能灵活适配各类LLM架构，且对主流强化学习算法表现出极强兼容性。  

需说明的是，当前方案需依托GPU服务器部署。虽较商业API更具成本优势，但基础设施投入仍需纳入考量（详见附录成本分析）。