精准控制误报率：基于多尺度共形预测的零样本机器文本检测框架




作者：朱晓伟1,2，任宇斌，曹亚男，林夕寻1,2，方芳，李阳溪3  





单位：1中科院信工所（北京）；2国科大网安学院（北京）；3国家互联网应急中心（北京）

摘要

大型语言模型的迅猛发展，引发了人们对其可能遭恶意滥用的深切忧虑。为此，开发高效检测工具以防范风险刻不容缓。然而现有检测方法大多过度追求准确率，却忽视了高误报率（FPR）潜藏的社会危害。本研究创新性地采用符合性预测（CP）技术，精准控制误报率上限。但单纯应用CP虽能约束误报，却会大幅降低检测效能。为此，我们提出基于多尺度符合性预测（MCP）的零样本机器文本检测框架，在严格管控误报率的同时提升检测性能。研究还构建了跨领域高质量数据集RealDet，通过与MCP结合，既确保校准真实性，又实现卓越的检测表现。大量实验证实，MCP不仅能有效控制误报率，显著增强检测能力，更能提升模型对抗多种检测器与数据集的攻击鲁棒性。

1 引言

大型语言模型（LLM）的迅猛发展，使其生成的文本愈发流畅自然，质量直逼人类创作。LLM正赋能新闻报道、文学创作、学术研究等多个领域（Alshater，2022；Yuan等，2022；Christian，2023），为产业与学界注入强劲动力。然而，这项技术也被恶意利用，催生了虚假新闻（Ahmed等，2021）、垃圾信息（Guo等，2021）、恶意评价（Adelani等，2019）等危害社会的内容，使得开发高效的机器生成文本（MGT）检测技术迫在眉睫。

现有检测方案主要分两类：基于统计指标的零样本检测器（Bao等，2024；Hans等，2024；Mitchell等，2023）和预训练微调的有监督检测器（Solaiman等，2019b；Conneau等，2019）。但这些方法过度追求检测精度，忽视了高误报率（FPR）的社会风险。Dugan等（2024）的研究证实，现有检测器在默认阈值下常出现危险的高FPR，导致实际应用价值大打折扣。

本文创新性地引入共形预测（CP）（Vovk等，1999）来解决MGT检测的高FPR难题。CP能为FPR设定理论上限，但直接应用会降低检测灵敏度。为此，我们提出多尺度共形预测框架（MCP），在严格限制FPR的同时提升检测性能，且无需额外训练。

MCP框架的工作流程如下：首先从目标数据中采样校准集与测试集；选定基线检测器计算非共形分数；通过多尺度分位数确定检测阈值；最终应用于新数据检测。我们还构建了RealDet基准数据集，其覆盖15个领域、22个主流LLM和两种对抗攻击，包含84.7万条双语文本，能更真实地反映人类文本分布。

实验表明，MCP框架既能有效控制FPR上限，又可提升检测性能，在对抗攻击下表现尤为稳健。本研究的主要贡献包括：
• 首创将CP引入MGT检测领域，并深入探索优化机制
• 提出MCP框架，实现FPR控制与检测性能的双重提升
• 发布目前规模最大、场景最全的双语检测基准RealDet

2 理论基础

保形预测框架。保形预测（Vovk等，2005；Papadopoulos等，2002；Lei和Wasserman，2014）是一种无需训练的统计学习方法，能提供可靠的预测结果。该框架仅需满足数据可交换性，即可为预测结果提供统计保证。其核心流程包括：

1. 数据划分：将数据集划分为校准集$D_{cal}$（含$n$个样本）和测试集$D_{test}$。

2. 非保形度定义：对于输入$x$输出$y$的模型，定义非保形分数$s(x,y)\in\mathbb{R}$，分数越高表示预测结果与输入越不匹配。

3. 分位数计算：基于校准集计算临界分位数$\hat{q}$：

$$
\hat{q}=\mathrm{quantile}\left(s_{1},...,s_{n};\frac{\left[(n+1)(1-\alpha)\right]}{n}\right).
$$

4. 预测决策：以$\hat{q}$为阈值构建预测区间：

$$
\mathcal{C}\left(X_{\mathrm{test}}\right)=\left\{y:s\left(X_{\mathrm{test}},y\right)\leq\hat{q}\right\}.
$$

定理1. 保形覆盖定理（Vovk等，1999）。若校准集$(X_{i},Y_{i})_{i=1,\dots,n}$与测试样本$(X_{test},Y_{test})$满足独立同分布，则：

$$
P\left(Y_{test}\in\mathcal{C}\left(X_{test}\right)\right)\geq1-\alpha.
$$

文本生成检测应用。给定$n$篇人工文本$\left(X_{1},...,X_{n}\right)$作为校准集，需判定新文本$X_{test}$的来源。基于检测器$Det$输出定义非保形分数$s\in[0,1]$（分数越高表示机器生成概率越大），按公式1计算阈值$\hat{q}$后：

$$
\mathcal{C}(X_{test})=\left\{\begin{array}{ll}
人工文本, & s\leq\hat{q}\\
机器生成, & s>\hat{q}.
\end{array}\right.
$$

该方案确保误判率$\mathrm{FPR}\leq\alpha$，即人工文本被误判为机器生成的概率不超过$\alpha$。

3 多尺度共形预测

图2展示了MCP预测的全过程：先从目标数据集中抽取校准集和测试集；然后选定基础检测器并定义其非共形分数；接着基于校准集分数计算多尺度分位数；最终将这些分位数作为阈值，实现对新样本的MGT检测。

3.1 数据准备  

我们从目标数据集中抽取样本，构建了校准集$D_{cal}$和测试集$D_{test}$。校准集仅含人类撰写的文本，测试集则混合了人类与机器生成的文本。这种同源采样方式保证了两个集合中人类文本的独立同分布性（i.i.d.）。  

图2：MCP框架。整个预测流程分为四个步骤：数据准备→定义非一致性分数→计算多尺度分位数→执行MGT检测，依次推进。

3.2 异常评分定义




我们首先选定基础检测器$Det$，该检测器选择灵活，可兼容多数MGT检测方案。随后定义异常评分函数$s(\cdot)$，将$Det(x)$的输出转化为预测用的异常值$s$：





$$


s=(1+e^{-k(Det(x)-\tau)})^{-1},


$$  





式中$\tau$为基础检测器默认阈值，$k$取值±1。$s$值越高，表明文本由人类撰写的可能性越低。

3.3 多尺度分位数计算

传统CP方法的局限性。图3揭示了一个关键矛盾：虽然传统方法能精准控制假阳性率(FPR)，却严重削弱了检测能力，导致大量机器文本漏网。数据分析表明：

核心发现：文本越长，非一致性评分越高。图3清晰展示了文本长度对分位数的决定性影响。经测算，长度与评分的皮尔逊系数ρₗₛ接近1，证实二者存在强正相关。这正是短文本检测失效的根源——低分机器文本成为漏网之鱼。

MCP的创新方案。我们创造性地将长度-评分关联纳入预测体系，对校准集D_cal实施智能分箱：按文本长度划分为K个区间（K=⌊L_max/w⌋），构建子集{D_cal¹,..., D_calᴷ}。

多尺度分位数q̂_M的精妙之处在于：
q̂_M = {q̂ⁱ | q̂ⁱ = 分位数(s₁ⁱ,..., sₙₜⁱ; [(n_i+1)(1-α)]/n_i)}
其中n_i为子集样本量，sⁱ为对应评分，α为FPR阈值。这套动态校准机制，让不同长度的文本都能获得量身定制的检测标准。

3.4 机器生成文本检测




针对测试集 $D_{test}$ 中的样本 $X_{test}$，我们通过其非一致性分数 $s_t$ 和文本长度 $l_t$ 进行判定。MCP框架的检测机制如下：


$$


s_t=(1+e^{-k(Det(X_{test})-\tau)})^{-1},


$$


$$


\mathcal{C}(X_{test})=\mathbb{I}(s_t>\hat{q}^{\lfloor\frac{l_t}{w}\rfloor}),


$$


式中 $\hat{q}^{\lfloor\frac{l_t}{w}\rfloor}$ 为对应文本长度区间的分位阈值，$\mathcal{C}(X_{test})$ 为判定结果：0表示人工创作，1表示机器生成。具体案例分析参见附录A。




推论1. MCP框架下机器文本检测的误判率上限严格控制在 $\alpha$ 内，完整证明过程详见附录B。

4 RealDet数据集

现有数据集不仅覆盖面有限，还存在领域偏差问题（Wu等人，2024a，b），难以全面反映跨领域的人类书写文本。我们在表1中推出RealDet数据集，其三大核心优势尤为突出：（1）领域全覆盖：涵盖15个文本领域，多样性远超同类数据集；（2）模型广覆盖：整合22个主流大语言模型，创下当前基模型覆盖之最；（3）海量语料库：包含84.7万条原始文本（不含对抗文本），其中11.3万条为人类书写，规模优势显著。此外，数据集还囊括中英双语文本及改写攻击等对抗样本。

数据来源方面，我们精选15个代表性数据源，覆盖6类写作任务：（1）问答：ELI5、WiKiQA等；（2）新闻写作：XSum、BBC News等；（3）故事创作：Writing Prompt等；（4）评论表达：Yelp、IMDB等；（5）学术写作；（6）知识解释：Wikipedia等。详见附录C.1。

模型配置包含22个黑盒/白盒模型：黑盒组含GPT-4、文心一言等商用API；白盒组集齐LLaMA2、ChatGLM2等开源模型。完整清单见附录C.2。

提示设计采用三大策略：（1）续写：基于首句延伸；（2）主题写作：生成特定类型文本；（3）问答：根据问题生成答案。具体模板详见附录C.3。

我们通过系列实验全面评估MCP：检验其控制误报率的能力、检测性能表现、抗现实攻击的稳健性、校准数据影响，并与其他校准方法进行对比。

5.1 实验配置

数据集。我们选用RealDet及三大标杆数据集——M4（Wang等，2024）、RAID（Dugan等，2024）和MAGE（Li等，2024）进行MCP评估，这些数据集均具备多样性、高质量和大规模特性。每个数据集随机选取5000篇人工文本作为校准集，2500篇人工文本与2500篇机器文本构成测试集。

评估指标。以误判率（FPR，即人工文本被误判为机器生成的比例）作为核心指标，同时采用检出率（TPR）和F1分数（F1@FPR）综合评估检测性能。

检测器选型。基准检测器包括：零样本检测领域的SOTA方案Fast-DetectGPT（Bao等，2024）和Binoculars（Hans等，2024），以及DetectGPT（Mitchell等，2023）、似然法、对数秩次和熵值法（Gehrmann等，2019；Su等，2023；Ippolito等，2020）等零样本方案；监督方案则选用基于GPT-2数据微调RoBERTa的OpenAI-D（Solaiman等，2019b）。

表2：跨检测器与数据集的核心实验结果。"原始"表示检测器默认配置，"MCP"代表经框架优化的版本。

超参数配置：具体参数设置及分析详见附录F。

5.2 误报率约束  

图4呈现了MCP框架下各检测器在不同数据集中的误报率表现，α取值集合为{0.2,0.1,0.05,0.02,0.01,0.005}。每个子图对应特定α值，并以颜色区分数据集。结果表明，所有误报率均稳定低于α设定的理论上限，验证了MCP精准控制误报的能力。尤其值得注意的是，各检测器间高度一致的误报率，彰显了该框架强大的泛化性，堪称严控误报需求场景的理想解决方案。  

（说明：第二版优化了术语统一性，如将"FPR"统一译为"误报率"；调整了长句结构使其更符合中文表达习惯；用"呈现/彰显/堪称"等动词提升文本生动性；通过"严控/精准/理想"等修饰词增强专业场景下的语言张力。）

5.3 核心发现

表2汇总了四大数据集和七种检测器的测试结果。MCP框架相较基础检测器展现出持续稳定的性能提升，其泛化能力尤为突出。虽然在较高误报率（20%/10%/5%）下增益有限，但在严苛的2%/1%/0.5%低误报阈值区间，MCP改进效果显著。以RealDet数据集为例，MCP使真阳性率（TPR）平均提升11%，F1分数提升8%；跨数据集整体来看，TPR和F1分数分别实现10%和6%的平均增幅。该框架通过多尺度共形分位数动态平衡检测精度与误报控制，在严苛误报约束场景中表现尤为出色。

MCP在低误报环境下的优势更为明显。例如在MAGE数据集上，TP@0.5%和F1@0.5%分别实现157%和91%的惊人提升。而在较高误报区间，由于多尺度分位数趋于接近，性能增益相对平缓。特别值得注意的是，整合MCP的先进检测器在极端低误报条件下仍保持卓越表现：Fast-DetectGPT在RealDet上的TP@0.5%达69.32%，F1@0.5%达81.59%；Binoculars更是分别达到84.34%和91.29%。这种通过多尺度分位数智能调节检测阈值的能力，使MCP既能精准控制误报率，又不损失检测效能，堪称高精度检测任务的理想解决方案。

5.4 模块效能分析

需要明确的是，MCP是一个整体框架，本次分析聚焦于框架内部各组件的作用。如表3所示，我们对比了采用多尺度分位数计算模块前后的检测效果差异。其中"w/o $\hat{q}_{M}$"表示仅使用全局分位数的基准方案。实验数据清晰表明：引入多尺度分位数后，系统检测性能获得显著提升。当移除该模块时，真正例率平均骤降$\mathbf{22\%}$，F1分数也下滑$15\%$。这验证了通过对校准集分段计算区间分位数的设计优势——该机制能实现更精准的模型校准，充分体现了多尺度分位数的重要价值。

5.5 实战攻击下的稳健表现  

图5呈现了MCP框架抵御两类对抗攻击的能力：文本改写与词元级篡改（插入、删除及替换）。改写攻击由DIPPER工具（Krishna等，2023）实施，对机器生成文本进行语义重构；词元篡改则以1%或3%的比例随机增删改词元。  

实验数据显示，MCP在所有攻击情境下均展现出优于传统检测器的真正例率（TPR）。其抗干扰能力尤为突出：无论是何种攻击形式或强度，TPR始终稳居高位。当攻击强度从1%升至3%时，传统检测器TPR大幅滑坡，而MCP的检测性能依然坚挺。以插入和删除攻击为例，MCP以显著优势胜出，印证了其有效化解对抗性篡改的能力。这些发现有力证明了MCP框架的独特优势——即便面对高强度对抗干扰，仍能保持精准检测，其稳健性远超基线检测器。

5.6 校准数据的影响




为探究校准数据对MCP性能的影响，我们评估了多领域数据集（M4、RAID、MAGE、RealDet）及新闻（BBC News）、学术（Abstracts）、社交媒体（ELI5）等垂直领域数据集。如图6所示，采用单数据集校准，其余数据依次作为测试集。




多样化校准数据通常能显著提升模型性能与泛化能力。图6显示，特定领域数据（BBC News等）校准后FPR较高，表明其校准精度不足；而多领域数据（RAID等）表现更优，但仍受数据偏差限制。值得注意的是，M4与RealDet校准效果突出，其中RealDet表现更稳定——校准时TP@1%平均提升13%。不过M4同样表现优异，这说明虽然RealDet略胜一筹，但跨领域数据融合对性能提升至关重要，可避免单一数据依赖。




![](images/122c2eb02f8388ffee8d4bd3d77f48ab08fe1c0118c10e9e05e78a08d486b41a.jpg)  




图6：不同数据集校准下，MCP中Binoculars在α=1%时的假阳性率。

5.7 校准方法大比拼




我们在附录G中将MCP与主流校准方案进行了全面对比，包括基于F1优化的度量法（Lipton等，2014）、概率校准的Platt Scaling（Platt，1999）和等渗回归（Brunk等，1973）。虽然这些方法都能小幅提升检测效果，但在误报率控制上都力有不逮。反观MCP，不仅分类准确率问鼎业界标杆，更能将误报率维持在极低水平，可靠性更胜一筹。

6 研究背景

MGT检测技术。现有检测方法主要分为两类：(1)零样本检测器通过大语言模型的统计特征识别异常文本，例如基于"机器文本倾向处于概率极值"假设的DetectGPT，其改进版FastDetectGPT优化了文本扰动策略；Binoculars则创新性地采用双模型交叉评估来解决复杂文本检测难题。(2)监督式检测器通过构建人工与机器文本的分类模型实现，典型如OpenAI-D对RoBERTa模型进行针对性微调。

MGT基准数据集。早期Turing Bench数据集因模型迭代逐渐失效，后续研究转向构建垂直领域数据集，如HC3涵盖多领域问答对。当前最前沿的MGTBench、M4等基准则致力于建立跨模型、跨场景的大规模评估体系。

7 结论

本文提出了一种基于多尺度共形预测（MCP）的可靠机器文本检测框架，通过严格控制误报率（FPR）来降低社会风险，同时提升检测准确率。经过七个检测器和四个数据集的广泛验证，MCP不仅效果显著，更能增强系统鲁棒性。未来我们将持续完善RealDet高质量数据集，应对大语言模型快速迭代的挑战，并研发新一代检测器，确保在MCP严苛的误报率标准下仍保持顶尖检测水准。

实验表明，虽然我们采用了基于CP的固定宽度分箱多尺度优化策略，但不同分箱宽度总会影响检测性能。若能采用更灵活的分箱方案，或将获得更优的检测效果，这值得进一步研究。

伦理声明




检测技术天然带有指控属性。尽管本研究严格设定了误报率上限，力求为用户提供更可靠的判断依据，但我们坚决反对将该框架的检测结果直接作为惩罚依据。无论检测精度多高，此类滥用都可能带来严重后果。同时，我们严格遵守伦理规范，数据集构建全程未涉及任何私人或非公开信息。