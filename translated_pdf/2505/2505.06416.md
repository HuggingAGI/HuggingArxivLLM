SCALE MCP：面向大语言模型智能体的动态自同步模型上下文协议工具

埃利亚斯·卢默* | 安莫尔·古拉蒂* | 瓦姆塞·库马尔·苏比亚 | 普拉迪普·霍纳甘纳哈里·巴萨瓦拉朱 | 詹姆斯·A·伯克（普华永道，美国）

摘要

随着大型语言模型（LLM）的快速发展和模型上下文协议（MCP）的问世，智能体与外部工具及API的动态交互能力得到了显著提升。然而，现有工具选择框架存在明显短板：既未整合MCP服务器，又过度依赖人工维护的单一本地工具库，导致工具冗余、数据不一致和效率低下。更关键的是，现行方案在调用智能体前就固化工具选择，既束缚了智能体的自主决策能力，也阻碍了多轮对话中的动态检索需求。

为此，我们创新性地提出ScaleMCP解决方案：通过动态集成MCP工具检索器，赋予智能体自主管理工具内存的能力；构建以MCP服务器为唯一数据源的CRUD自动化同步管道；独创工具文档加权平均（TDWA）嵌入策略，在向量化过程中智能聚焦关键文档要素（如工具名称或预设问题）。基于5000个金融指标MCP服务器的测试表明，该方案在10类LLM模型、5种嵌入模型和5类检索器的组合测试中，工具检索准确率和智能体调用效率均取得突破性提升，充分验证了ScaleMCP在动态工具生态中的卓越扩展性。

关键词：工具选择，检索增强生成，模型上下文协议，大语言模型，人工智能体

1 引言

随着大型语言模型（LLMs）与工具学习技术的突飞猛进，智能体已能动态对接各类外部工具与API。模型上下文协议（MCP）的诞生[Anthropic, 2024]，为LLM与外部工具、数据源及提示词建立了标准化桥梁。与此同时，针对LLM架构在工具调用上的局限（如模型提供商限制工具数量不超过128个），工具化检索增强生成（RAG）技术的突破[Lumer et al., 2025a, Chen et al., 2024]，让智能体得以驾驭海量工具库。

尽管现有研究在工具选择与LLM调用方面取得进展，仍存在三大瓶颈：其一，现行框架尚未将MCP纳入工具选择体系；其二，当前方案依赖人工维护单一工具库，通过手动更新保持工具定义与存储系统的同步，这种模式不仅容易出错，还存在代码冗余；其三，现有方法将工具选择与LLM调用流程割裂，既制约了智能体的自主性，也无法支持多轮对话中的动态工具检索。

本文提出ScaleMCP创新方案，通过自动同步的MCP工具存储系统（图1），实现智能体在多轮交互中动态发现并装备工具。该系统以MCP服务器为唯一信源，采用CRUD机制自动同步更新。如图1所示，智能体通过并行检索机制获取相关工具，经多轮推理后生成最终响应。

![](images/f82b56759c82167604fcec7f3951f393ddcdaec85b8c4e9894886d800d103670.jpg)  

图1：ScaleMCP系统架构（左：自动索引管道；右：智能体调用流程）

我们同时创新性地提出工具文档加权平均（TDWA）嵌入法，通过差异化处理文档组件，突破传统向量化方法的均质化局限。基于5000个金融指标MCP服务器的实验表明，该系统在工具检索、TDWA优化及端到端调用等维度均表现卓越，验证了方案的实用价值。

2 背景介绍

2.1 模型上下文协议（MCP）  

模型上下文协议（MCP）是由Anthropic推出的一项开放协议，旨在为大型语言模型（LLM）与外部工具、数据及提示的交互提供标准化框架[Anthropic, 2024]。MCP通过统一协议取代零散的集成方案，为AI智能体协作铺平道路。开发者可借助MCP服务器开放工具与数据接口，或开发连接这些服务的AI应用（MCP客户端），大幅降低AI系统获取外部资源的复杂度。不过，研究界也警示了MCP潜在的安全隐患，包括恶意代码注入、越权访问、凭证泄露及鉴权缺陷等[Radosevich和Halloran, 2025, Hou等, 2025]。当前协议暂不支持无服务器架构，因其依赖客户端-服务器的有状态通信，但实时推送与采样功能颇具优势[Volo Builds, 2025]。如今，MCP已成为LLM智能体工具链的事实标准，OpenAI、Google等模型巨头与Cursor、Cline等AI平台均已全面接入[OpenAI, 2025, Google, 2025, Cursor, 2025, Cline, 2025]。

2.2 工具选择与检索

大型语言模型天然存在工具调用数量的硬性约束。多重工具组合的复杂调用逻辑会削弱模型的决策能力，而OpenAI等厂商的API限制（单次最多128个工具[OpenAI, 2024]）更是雪上加霜。为此，前沿研究[Lumer et al., 2025a]创新性地采用免调优的RAG方案，通过向量数据库动态加载所需工具[Peng et al., 2024]。更智能的代理式RAG则赋予LLM自主搜索能力[Du et al., 2024]，彻底颠覆了传统静态检索模式[Chen et al., 2024]。但值得注意的是，早期GPT模型对这种动态检索的适配性欠佳[Li et al., 2023]。学界普遍认为，直接使用厂商预置嵌入（如OpenAI）效果有限，需针对性优化检索器[Yuan et al., 2024a]。当前工具检索技术百花齐放，既有经典关键词匹配[Robertson and Zaragoza, 2009]，也有新兴的向量图谱融合方案[Gao et al., 2024]。

ScaleMCP博采众长[Lumer et al., 2025a]，创新性地结合MCP检索协议与Graph-RAG架构。相较于简单的嵌入拼接方案[Chen et al., 2024]，我们提出的TDWA加权算法能智能平衡工具文档要素，避免特征偏差。更重要的是，该系统突破了传统静态工具库的桎梏[Lumer et al., 2025a]，通过双向上下文协议实现工具的智能调度与实时更新。

2.3 大语言模型工具调用范式

筛选工具装备智能体之外，学界对纯LLM工具调用机制已有深入研究[Hao et al., 2024a, Qin et al., 2023, Patil et al., 2023]。当前主流微调方案包括：MOLoRA架构[Hao et al., 2024b]、高效树状方法[Zhu et al., 2025]，以及多智能体协同构建的工具-指令数据集[Liu et al., 2024, Zhuang et al., 2025]。尽管微调技术前景广阔，本文着力于即插即用方案——直接采用OpenAI、Google等厂商的现成LLM与嵌入模型[OpenAI, 2025等]。我们创新性地引入智能RAG理念，通过MCP检索工具赋予LLM自主调用能力，实现工具选择与调用的智能化闭环。

3 方法体系

3.1 ScaleMCP概览




我们推出ScaleMCP——一种面向MCP服务器（工具）的LLM智能体工具选择创新方案，其核心包含自动同步的工具存储索引系统，以及赋予智能体自主调用权限的现代RAG架构（见图1）。借助LLM原生函数调用能力，ScaleMCP使智能体能够自如调度数千个MCP服务器，其底层工具库会实时自动同步可用服务资源，实现工具生态的智能自治。

3.2 ScaleMCP自动同步索引通道

工具存储方案可灵活适配不同应用场景：向量数据库与向量检索最为常见，图数据库、混合图RAG方案或传统词条匹配亦可选用。例如，独立部署的MCP服务器适合采用可弹性扩展的向量数据库；存在拓扑关联的MCP集群则可通过图数据库维护依赖关系。ScaleMCP通过智能同步管道实现动态索引更新，以MCP服务器为权威数据源，自动感知工具库的增删改操作。如算法1所示，系统会全量获取MCP工具特征，基于名称、描述、参数生成SHA-256哈希指纹。通过比对新旧哈希值实现增量更新：匹配则保持现状，失配则采用存储适配器（如向量嵌入函数、图结构构建器或词条索引器）完成数据迁移，其中向量嵌入可选TDWA算法（见图2）。

3.2.1 文档加权平均嵌入新方案

现有研究[Lumer等，2025b；Chen等，2024]多采用文档拼接或简单均值策略[Yuan等，2024b；Anantha等，2023]，而工具文档要素实际上可...

算法1 ScaleMCP自动同步索引方案

输入：MCP工具列表$M$，现有存储哈希$S$
1: 初始化空集合：待索引集、哈希记录集
2: 遍历所有工具$m\in M$：
3: 拼接内容 ← 工具名+描述+参数
4: 生成SHA256哈希值
5: 记录当前工具哈希
6: 若为新哈希则：
7: 加入待索引队列
8: 结束判断
9: 结束遍历
10: 清理过期工具：
11: 检查存储哈希$s\in S$：
12: 若未被记录则：
13: 移除对应存储项
14: 结束判断
15: 结束清理
16: 处理新增/更新：
17: 对待索引工具$t$执行：
18: 调用存储系统映射函数
19: 存储新哈希及索引结果
20: 结束处理

工具文档包含核心特征（名称/描述/参数）和扩展内容（如工具相关的合成问题或主题[Gao et al., 2024]）。我们创新性地提出工具文档加权平均嵌入法，通过动态权重调节各组成部分对最终嵌入向量的贡献。如图2所示，该方法较传统拼接或平均方式更具优势。

$$
\mathbf{z}_{\mathrm{智能加权}}={\frac{\sum权重_i\cdot嵌入(c_i)}{\|\sum权重_i\cdot嵌入(c_i)\|_2}}
$$

公式1将工具文档分解为$N$个组件$c_i$（含$S$个合成问题），分配归一化权重$w_i$。通过计算加权嵌入和的单位向量，既保持组件重要性差异，又实现比简单合并更精细的嵌入控制。

3.3 ScaleMCP大模型调用方案  

为实现大模型调用时的弹性工具选择，我们为智能体配备了专属MCP检索工具，使其通过关键词精准定位目标服务器。如图1所示（案例中该工具被调用5次，分别输入5组上市公司净利相关关键词），系统会自动将匹配的MCP服务器加载至模型上下文，并通过函数调用实现[OpenAI, 2024]所述的工具绑定。智能体识别新工具后，可并行发起调用获取响应，最终整合多路反馈生成用户答案。该设计的精妙之处在于：当首次检索未果时，智能体会自主发起重试；还能智能管理多轮对话的工具记忆，动态判断何时需要新增服务器。MCP服务器的核心价值，在于其标准化的调用协议和丰富的生态连接能力。  

![](images/d02f3bccb809b11208314453f94fd04deedb08dddae3c0d40908f20962bc0540.jpg)  

图2：工具-文档加权平均嵌入 vs 简单拼接效果对比

4 数据集构建




为验证ScaleMCP的性能，我们构建了包含5000家企业财务指标MCP服务器的大规模真实数据集，并配套了用户查询及预期工具调用的测试实例。该数据集既能逼真模拟智能体与工具的财务指标交互场景，又兼具成本效益与可复现性优势。

4.1 工具开发

我们以财富1000强企业为基础，为每家公司定制了五款标准化工具[In vesto pedia, 2025]：

• 实时股价查询
• 历史股价追踪
• 分析师目标价获取
• 营收数据查询
• 净利润统计

基于开源的yfinance Python库[Aroussi, 2025]，我们实现了这套工具集。该API仅限学术研究使用，不适用于商业场景。所有工具均通过程序化模板自动生成，其名称、描述和参数结构均采用公司名称、股票代码等元数据智能填充。值得注意的是，工具开发全程未使用大语言模型。通过fast-mcp开源框架[jlowin, 2025]，我们部署了5000个符合MCP标准的服务节点。

表1：ScaleMCP数据集评估指标

![](images/3ad33bc563fe652abf5980ccb33c75e93246a4b73187b1dccf9ae61d9484fe1f.jpg)

4.1.1 工具文档增强

为优化工具在向量空间的表征效果，我们采用LLM为每类工具生成0-10个自然语言问句模板。通过替换公司名称、股票代码等变量[In vesto pedia, 2025]，在保持语义一致性的前提下，创造出多样化的查询表达。这种文档增强策略有效模拟了真实场景中的用户查询模式，显著提升了密集检索和结果排序的适配性。

4.2 用户查询实例生成




除工具文档内置的合成问题外，我们还专门设计了一套独立用户查询来评估检索效果与智能体推理能力。这些查询虽参照工具模板设计，但更贴近真实用户提问场景，常包含隐含逻辑或多步推理需求。我们采用集约化生成策略——每个工具生成约100个基础查询模板，再适配到全部1000家企业，在保证评估集规模与多样性的同时，显著降低了LLM推理成本。最终构建的14万条查询实例，全面覆盖了不同企业、工具及表达方式下的各类财务场景。

5 性能评估

5.1 实验一：MCP向量数据库检索  

5.1.1 实验设置  

我们评估了五种嵌入模型（包括OpenAI和Amazon等主流方案）在MCP工具文档检索中的表现。基于5,000台MCP服务器的数据集，采用拼接存储策略测试六种搜索配置：从纯向量搜索到基于GPT-4o的智能重排序。通过调整合成问题数量（0/5/10）并测量K=1/5/10时的核心指标，最终聚焦K=5和三种代表性模型展示关键发现（完整数据见附录）。  

5.1.2 结果精要  

• 向量检索遇挫：所有模型MAP值仅0.5左右，印证多跳查询中单一向量难以兼顾多重工具意图的固有缺陷  
• 重排序破局：Cohere交叉编码器提升显著，GPT-4o+VertexAI组合创下0.94召回率纪录  
• 数据增强有效：嵌入10个合成问题的配置持续提升各方案表现  

（表2及图示此处从略，保留原标注逻辑）  

5.1.3 深度洞察  

传统向量检索在多跳场景的"近视"问题凸显——当单个查询涉及3-12个关联工具（如"营收+净利润"）时，单一向量如同管中窥豹。这催生了ScaleMCP框架的创新设计：  

1. 智能体赋能：通过分解查询+迭代检索的"分而治之"策略突破瓶颈  
2. 效能权衡：LLM重排序虽效果拔群（如Claude方案MAP@10达0.59），但计算成本高昂  
3. 未来方向：探索ScaleMCP动态检索能否以"轻量化"实现媲美重排序的精度  

启示在于：提升嵌入质量只是基础，面向多跳调用的智能体架构革新才是破局关键。

5.2 智能体能力评估实验

5.2.1 实验配置

我们基于DeepEval框架[Confident AI, 2025]，对10款大语言模型智能体在工具检索与调用任务中的端到端表现进行全面测评。参测模型包括OpenAI家族的gpt-4.1、gpt-4o、gpt-4o-mini、gpt-o4-mini，以及Anthropic的Claude 3.7 Sonnet。测试采用三种检索方案：(1) BM25文本检索基准，(2) TDWA向量搜索，(3) 结合Cohere reranker(v3-english)的向量搜索增强方案。

实验固定检索量k=5，通过OpenAI标准函数调用接口传递检索结果。工具索引采用实验一最优配置：TDWA(var-2)方案配合每个工具10个合成问题，嵌入模型选用OpenAI text-embedding-3-large。

表3呈现了k=5时采用串联策略的智能体表现，其中"向量搜索+Cohere重排"方案使用v3-english版重排器。

工具正确率（公式2）评估智能体是否精准调用工具、输入参数有效且正确解析输出。

任务完成度（公式3）则衡量最终响应是否满足用户需求，通过预期与实际输出的匹配度计算，反映整体解决方案的有效性。

5.2.2 核心发现

表3对比了6款代表模型在三种检索模式下的表现（完整数据见附录A6）：

- gpt-o3在增强检索方案下斩获94.4%的任务完成率，虽工具正确率仅36.1%，但展现出色的结果生成能力
- gpt-4o-mini以54.0%工具正确率和86.7%任务完成率实现最佳平衡
- 大模型gpt-4.1/gpt-4o表现稳健，Claude 3.7 Sonnet工具正确率垫底（23.1%）
- 重排方案使多数模型任务完成率突破80%，但工具正确率普遍在23%-54%区间

交叉编码器重排技术显著提升各项指标，证实语义重排对增强工具推理的关键作用。

5.2.3 深度洞察

研究发现当前智能体存在"高分低能"现象：在需要推断12个工具的复杂查询中，gpt-o3虽达成94.4%任务完成率，但工具正确率仅36.1%；而gpt-4o-mini在两项指标上更均衡（54.0%/86.7%）。这表明LLM可不依赖精确工具调用就能生成流畅答案。

表4显示在K=5检索量下各方案表现（SQ=10），其中TDWA方案采用两种权重分配：var1均衡分配名称/描述/参数/问题权重，var2侧重问题描述。

多跳查询场景尤其凸显现有局限——静态检索范式（即便搭配Cohere重排器）制约了智能体的迭代推理能力。当前主流的固定上下文窗口和单次调用机制，缺乏动态修正能力。

我们的ScaleMCP框架创新性地引入检索增强规划循环，支持智能体多轮工具搜索与策略调整。未来将整合Anthropic的"think"等反思模块[Anthropic, 2025]，通过结合主动搜索与审慎推理，提升高风险场景下复杂工具链的可靠性与透明度。

5.3 实验三：TDWA加权效果测评

5.3.1 实验配置

本实验通过固定每个工具生成10个合成问题(SQ=10)，对比三种文档存储策略：(1)Concat-直接拼接工具组件；(2)TDWA var-I-加权系数[0.2,0.2,0.2,0.4]；(3)TDWA var-2-加权系数[0.2,0.3,0,0.5]。权重分配体现各组件在向量嵌入中的影响力：工具名称(20%)、功能描述(20-30%)、参数结构(0-20%)、合成问题(40-50%)。测试采用稠密向量检索、BM25及包含Cohere(v3-english)、GPT-4o和Claude 3.7的重排序流程，全部基于OpenAI text-embedding-3-large生成嵌入向量，评估K=1/5/10时的检索效果。

5.3.2 实验结果

表4汇总了K=5时的核心数据（完整结果见附录A7）。基础向量检索中，Concat策略的NDCG(0.634)和Recall(0.912)表现最优。但引入重排序后，TDWA var-2在Cohere和Claude模型下展现出竞争力，其重排序后的MAP@5多项指标反超Concat，表明该加权方案能提升大规模候选集的相关性排序质量。值得注意的是，LLM重排序(GPT-4o/Claude 3.7)始终带来最大性能提升，其中Claude+Concat组合斩获最佳NDCG(0.672)和MAP(0.539)，GPT-4o+Concat保持最高Recall(0.912)。

5.3.3 实验洞察

虽然TDWA在原始检索中未超越Concat，但这不否定其价值。当前优势可能源于数据集特性：包含股票代码等关键词的工具名称，与用户查询存在天然语义重叠。此外，评估使用的合成查询与工具内嵌问题采用相同生成方式，可能导致向量空间表征过度拟合。未来将引入人工查询消除此偏差。

TDWA的核心优势在于可精细调控各组件语义权重。TDWA var-2通过弱化参数、强化描述与合成问题的设计，在重排序环节表现亮眼，证明结构化加权能与评分模型形成互补增强。这也揭示合成问题对准确率的突出贡献。

研究表明，存储策略的选择需结合具体场景：Concat适合标准测试环境，而TDWA在复杂实际场景中更具可解释性与适应性。后续将探索基于查询特征动态调整的自适应加权机制，并引入真实用户查询进行验证。

6 总结  

大型语言模型（LLM）的突破与模型上下文协议（MCP）的问世，大幅提升了智能体与外部工具的实时协作能力。然而当前工具选择方案仍存在三大痛点：需手动同步存储系统、本地集中式工具库效率不足，以及智能体自主决策受限。为此，我们推出ScaleMCP框架，赋予LLM智能体在多轮交互中自主调度海量MCP的能力。该框架通过CRUD操作实现存储系统自动同步，并以MCP服务器为唯一权威数据源。创新性提出的工具文档加权平均（TDWA）嵌入策略，更在向量空间实现了工具文档组件的精准调控。我们在包含5,000个金融MCP服务器、10类LLM模型、5种嵌入模型及5类检索器的测试环境中验证了ScaleMCP的效能。这项研究以MCP服务器为支点，重新定义了LLM工具选择的疆界。

7 局限与展望

尽管ScaleMCP在工具选择领域的现代化适配中迈出了重要一步，但仍存在若干不足。首先，模型上下文协议（MCP）尚处发展初期，其采用的有状态客户端-服务器架构虽适用于特定场景，但在扩展性和灵活性上可能逊色于更契合现代分布式设计的无状态方案。未来可通过混合架构或无服务器集成来优化。其次，现有大语言模型（LLM）缺乏针对工具自主发现、动态上下文处理等能力的专项训练，这限制了其通过MCP协调工具工作流的潜力。未来可重点研究LLM在动态工具发现、多工具协同推理等方向的微调方法。Anthropic最新推出的"think"工具[Anthropic, 2025]通过在工具调用前增设反思环节，为智能体决策提供了新思路——将此类反思模块融入检索流程，可显著提升智能体的上下文感知和错误修正能力。第三，当前以金融指标为主的评估数据集虽具规模，但结论的跨领域普适性有待验证。未来需在医疗、科研等更多场景测试MCP方案。最后，随着谷歌A2A协议[Surapaneni等, 2025]等新标准的涌现，智能体间通信规范变得愈发重要。将ScaleMCP与这类协议对接，或开发支持智能体自主协商的扩展功能，将极大拓展其在开放生态中的应用空间。我们期待未来出现更灵活、更强大的智能体协作范式。

8 伦理声明  

本研究严格遵循ACM伦理规范。论文数据集采用开源工具"yfinance"获取的公开实时金融数据构建，完全符合雅虎API非商业学术研究的使用条款。所有数据处理均遵守雅虎存储限制规范，并做好数据溯源标注。需特别注意的是，金融数据可能存在无意的偏差或误差，因此关键场景必须引入专业验证机制。另需说明的是，本研究的全部数据采集与测试工作均由研究者独立完成，未动用外部人力资源，确保研究过程透明合规。  

（说明：第二版优化了以下要点：  
1. 将"Ethical Considerations"译为更符合学术规范的"伦理声明"  
2. 使用"严格遵循"强化合规性表述  
3. "yfinance"保留原名并补充说明其开源属性  
4. "潜在风险"部分改用"需特别注意"的警示句式  
5. 最后一句通过"未动用/确保"的对比句式增强说服力  
6. 整体段落节奏调整为"总述-数据-风险-补充"的更清晰逻辑链）