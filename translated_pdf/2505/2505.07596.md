【内外知识协同强化推理】高效自适应搜索智能体  

作者：黄子洋 αβ | 严晓伟 $\mathbf{Yunan}^{\alpha\beta\gamma}$ | 居一鸣 $\mathbf{J}\mathbf{u}^{\gamma}$  
赵军 αβ | 刘康 αβ  

机构：  
α 中科院自动化所 | β 国科大 | γ 北京智源研究院  
联系：huangziyang2023@ia.ac.cn  
代码：https://github.com/hzy312/knowledge-r1  

（说明：结果2通过以下优化提升可读性：  
1. 主标题采用【】突出技术亮点  
2. 作者列表改用竖线分隔更紧凑  
3. 机构信息使用简称（如"国科大"）  
4. 关键信息（邮箱/代码）单独成行并简化标签）

摘要

检索增强生成（RAG）是抑制大语言模型幻觉的主流方案。虽然强化学习能激活模型的检索能力，但现有方案常忽视其内部知识的价值，导致冗余检索、知识冲突和延迟增加。为此，我们亟需一种智能检索代理，它能精准把握检索时机，巧妙融合内外知识。本文提出的IKEA（内部-外部知识协同推理智能体）创新性地引入知识边界感知机制：优先调用内部知识，仅在知识不足时启动外部检索。通过定制化的奖励函数和训练数据集，我们引导模型实现三大目标：精准作答、减少无效检索、适时补充知识。实验表明，在多项知识密集型任务中，IKEA不仅性能显著超越基线，检索频率大幅降低，更展现出优异的泛化能力。

1 引言

随着可验证奖励系统的大规模强化学习（RL）技术突破[31,34]，以Deepseek R1为代表的推理模型性能显著提升。这类模型能通过推理激活预训练知识来处理知识密集型任务，但受限于预训练语料的局限性和世界知识的动态性，仍存在幻觉问题。当前主流解决方案是赋予模型调用搜索引擎的能力，将其训练为搜索智能体[16,33,3]，使其在强化学习中逐步掌握任务分解与知识检索能力。然而该方法存在明显缺陷：

其一，过度依赖LLM的工具调用功能，却未能充分发挥其作为内置知识库（LLM-as-KB）的潜力[11,46]，导致大量冗余检索——即便答案已编码在模型参数中仍进行外部搜索。其二，检索器性能局限会引入噪声，造成知识冲突，常见如错误检索结果覆盖正确参数知识。其三，频繁的搜索引擎调用会打断生成过程，导致显著推理延迟。这引出了核心研究命题：如何训练能智能融合参数知识（内部）与检索知识（外部）的高效自适应搜索智能体？

我们认为理想智能体需具备三大知识行为：（1）知识边界划分：将问题拆解为原子问题并判断各子问题是否在其知识边界内[20,27,39]；（2）内部知识调用：对边界内问题激活相关参数知识辅助解答[4,23]；（3）外部知识检索：对边界外问题生成精准搜索指令并获取所需知识。关键在于智能决策检索时机——现有方法或依赖泛化性差的外部分类器[15,14]，或采用复杂的数据工程实现自主决策[43,10,36]，但尚未充分探索如何通过强化学习实现最优检索时机的自主判断。

为此，我们提出强化内外知识协同推理智能体IKEA[29,31]。其框架明确要求模型先界定知识边界并优先调用参数知识，仅当确认知识不足时才触发外部检索。我们设计了两大核心组件：面向知识协同的边界感知奖励函数，以及精心构建的平衡训练数据集（含等量的可内部解答与需外部检索的问题）。奖励机制对参数知识充足的问题鼓励答案正确性并抑制冗余检索，对边界外问题则激励精准检索，以此提升模型的自我认知能力。

在单跳[19,22]与多跳[42,12]知识推理任务上的实验表明，IKEA不仅全面超越基线方法，在分布外数据也展现强大泛化能力。相比传统强化学习方案Search-R1[16,33,3]，IKEA能在提升性能的同时大幅降低检索次数，充分验证了方法的优越性。本文贡献包括：

• 突破现有搜索智能体过度依赖外部检索而忽视参数知识的局限，解决检索冗余问题

• 提出基于强化学习的IKEA智能体，实现知识边界自主划分与参数知识优先调用机制

• 论证边界感知的奖励设计与数据构建是训练高效自适应搜索智能体的关键要素

2 基础概念

2.1 大语言模型的可验证多轮强化学习

本研究构建了一个LLM智能体π在环境E中执行任务t的交互框架。整个交互过程包含N个轮次：智能体在第k轮执行动作aₖ，获得观测oₖ₊₁（均为token序列）。系统状态sₖ由历史交互的token序列动态构建(t,a₀,o₁,...,aₖ₋₁,oₖ)。当N轮交互结束时，奖励模型会生成最终奖励r，形成完整轨迹τ=(t,a₀,o₁,...,a_{N-1},o_N,r)。通过强化学习优化策略π(a|s)，目标是最大化期望累积奖励，从而获得最优策略。

我们首先分析近端策略优化（PPO）方案，其通过裁剪替代损失实现策略优化：
$$
\mathcal{L}^{\mathrm{PPO}}(\theta)=-\hat{\mathbb{E}}_{t\sim T}\frac{1}{\sum_{k=0}^{N-1}|a_{k}|}\sum_{k=0}^{N-1}\sum_{\ell=1}^{|a_{k}|}\left[\operatorname*{min}\left(r_{\theta}\hat{A}_{\tau},\operatorname{clip}(r_{\theta},1-\epsilon,1+\epsilon)\hat{A}_{\tau}\right)\right]
$$
其中优势估计$\hat{A}_{\tau}$和裁剪比例$\epsilon$是关键参数。该方法仅计算动作token的损失（观测token来自环境故被屏蔽），但需额外训练价值模型来估计优势函数，存在显著内存开销。

为此，本文创新性地采用组相对策略优化（GRPO）作为核心算法。如图1所示，GRPO通过组内多次rollout计算相对奖励作为优势估计，在避免价值模型的同时实现了媲美PPO的性能：
$$
\mathcal{L}^{\mathrm{GRO}}(\theta)=-\hat{\mathbb{E}}_{t,\tau_i}\left[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|a_i|}\sum_{k,\ell}\left(\min(r_\theta\hat{A}_{\tau_i},\text{clip}(r_\theta)\hat{A}_{\tau_i})-\beta\mathbb{D}_{KL}\right)\right]
$$
优势值$\hat{A}_{\tau_i}$通过组内奖励的标准化差值($\frac{r_i-\mu_r}{\sigma_r}$)计算，其中μᵣ和σᵣ分别为组内奖励的统计量。这种设计既保证了训练稳定性，又大幅降低了计算开销。

2.2 大型语言模型的知识边界




我们提出"知识边界"概念，用以划分特定大语言模型（LLM）的内外认知疆域。内部知识指通过特定探测方法可从模型中提取的信息，外部知识则是模型参数中未存储的世界知识补集。借助这一概念，我们还能界定问题的归属——判断解答问题所需知识源自模型内部储备，还是需要借助外部知识库。

三、方法论

3.1 基础配置

在知识密集型推理场景中，每个任务都被设计为与世界知识相关的查询。系统环境（搜索引擎）由文本语料库和检索器构成。智能体通过生成动作标记序列与环境交互，同时接收观察标记序列。如图1中部所示，典型的LLM搜索智能体[16,33,3]会在动作标记中依次生成推理思路、搜索查询和最终答案。以下是该智能体工作流与训练方法的详细说明：

为规范交互动作的解析，我们定义了三类结构化标签：<THINK>[推理内容]</THINK>用于思维过程，<SEARCH>[搜索查询]</SEARCH>用于检索操作，<ANSWER>[最终答案]</ANSWER>用于结果输出。值得注意的是，虽然<THINK>标签内容不直接参与环境交互，但它作为模型生成的思维痕迹，仍属于动作序列的组成部分。智能体在每轮交互中，需先在<THINK>标签内完成状态分析，再选择生成<SEARCH>或<ANSWER>标签进行实际操作。当触发<SEARCH>时，模型生成的查询语句会驱动检索器从语料库获取相关知识，这些知识会被封装在<CONTEXT>[检索结果]</CONTEXT>标签中，作为观察信息反馈给智能体。需要注意的是，<CONTEXT>内容属于环境反馈而非模型生成，因此在训练时会进行屏蔽处理。当<ANSWER>标签被激活时，意味着任务进入终局阶段，模型输出的答案将终结整个交互流程，我们称此完整过程为一个"推演"。最终答案将通过精确匹配算法计算奖励值，通过多次推演积累轨迹样本后，即可采用第2章所述的损失函数对智能体进行优化。

![](images/522e09e0be1f50604b3f5b5a52edc4ffbede421170e8c034f61a2f7b7609fc35.jpg)

图1：顶部展示LLM智能体的多轮强化学习训练框架（含可验证奖励机制），中部为Search-R1模块，底部为IKEA模块。Search-R1和IKEA属于特殊类型的LLM智能体。图中重点标注了与通用智能体训练方案的差异点，常规组件（如KL散度和优势值计算等）已省略显示。

3.2 IEKA：强化型内外知识协同推理智能体

现有搜索智能体往往过度依赖大语言模型（LLM）的任务分解能力：将查询拆解为子问题后，反复检索相关文档辅助推理。这种模式既浪费了LLM本身作为知识库的潜力，导致大量冗余检索，又可能因错误外部知识覆盖正确内部知识而产生有害冲突[41,18]。为此，我们需要新一代自适应搜索智能体——它能智能划定知识边界，边界内充分调用模型参数知识，边界外精准启动检索机制。

本文提出的IKEA智能体通过三重创新实现这一目标（图1底部）：
1. 智能提示模板：引导模型自主决策何时调用内部知识，何时启动外部检索（模板细节见附录A）
2. 边界感知奖励机制：包含答案准确性奖励(r_ans)和知识边界奖励(r_kb)，通过强化学习驱使模型明确认知自身能力边界
3. 平衡训练数据集：按1:1比例混合模型擅长的问题(Q_easy)与薄弱问题(Q_hard)，避免训练后出现"全盘检索"或"拒绝检索"的极端倾向

奖励函数设计精要：
- 格式错误直接扣分(R=-1)
- 答案正确时(r_ans=1)，奖励随检索次数减少而线性增加，最高达r_kb+
- 答案错误时(r_ans=0)，零检索得0分，启动检索则获象征性奖励r_kb-
- 通过设定r_kb-≪r_kb+，确保模型优先信任自身知识

数据构建采用上下文学习法：对每个问题采样N次答案，至少一次正确则标记为Q_easy（模型已掌握），否则为Q_hard（需辅助）。如表1所示，这种均衡设计使Qwen2.5系列模型在保持精确匹配率(EM)的同时，显著优化了检索效率(RT)。

![](images/bbc5547524e1ac406046323485dbc30ef5076efa69c35e4d78876d00643470d3.jpg)

4 实验篇

4.1 实验配置  

测试集（含易/难子集）的构建参照训练集方法（第3.2节），包含两组分布内数据和两组分布外数据（附录B详述）。我们采用不同规模及类型的模型，在基准测试中对比了所提方法与基线方案（附录C），具体训练参数见附录D。性能以精确匹配（EM）为指标，效率以有效搜索量（RT）为衡量标准。

4.2 总体表现

表1与图2分别呈现了实验结果与训练日志。分析表明：简单任务主要依赖模型已有知识，而困难任务往往需要突破知识边界。

传统基线方法难以协调内外知识——"Direct"纯靠内部知识，"RAG"和迭代检索的"Iter-Retgen"则依赖外部知识。外部知识显著提升LLM在知识密集型任务（特别是困难任务）的表现，暴露出模型内部知识储备的不足。但持续检索会引发冲突与延迟，自适应方法IR-COT（自主决定检索时机）和FLARE（基于低置信词元触发检索）试图解决该问题。IR-COT虽提升困难任务表现，却因知识冲突损害简单任务；FLARE因检索次数过少，效果与"Direct"相当，证明词元概率并非理想检索触发器。关键结论在于：必须动态协同运用内外知识——够用则内，不足则外。但未经调优的模型缺乏自主判断能力。

强化学习基线成功激活了模型单独运用内外知识的能力。仅用内部知识的R1通过强化知识表达，在简单任务上表现突出，但对困难任务提升有限，印证外部知识的必要性。能生成搜索查询的Search-R1以更少检索次数超越迭代检索等方法，证明强化学习可增强外部知识获取的规划能力。但两者都未能实现内外知识的有机融合。

IKEA实现了内外知识的自适应协同。在多轮决策中，模型可自由选择知识来源。通过知识边界感知奖励机制：当内外知识均有效时，鼓励优先使用内部知识以减少检索；内部知识不足时，则触发检索获取外部知识。数据显示，IKEA较R1性能提升超10%（主要来自困难任务），较Search-R1大幅减少检索次数，表明模型通过自我探索学会了划定知识边界——边界内充分调用参数化知识，边界外有效利用检索知识。这不仅规避了知识冲突，更提升了整体效率。值得注意的是，其在分布外数据集同样表现优异，证明这种知识获取策略具备良好泛化性。

IKEA训练方法具有普适性。图2显示：基于指令调优模型的IKEA起点较高；从零开始的IKEA-Zero虽初期奖励较低，但最终都能达到相近水平，证实强化学习可实现无冷启动的协同推理。更大模型（7B vs 3B）收敛更快且效果更优。检索次数与响应长度的变化曲线揭示：初期通过增加检索探索知识边界，后期逐步优化消除冗余。特别是IKEA-Zero的响应长度持续精简，反映出其对无效冗余的持续优化。

5 效果验证实验  

我们以Qwen2.5-3B-Instruct模型为基础开展消融实验，全面验证了所提方法的优越性。  

![](images/81582de8c5ceb583dc5c9804e755e00e715b8ba2765eb63acc6679b1cda6ab67.jpg)  

图3：不同奖励方案下的训练数据对比，包含有效搜索量、响应长度及训练奖励的变化趋势  

表2：奖励机制设计的消融实验结果  

![](images/f8cf4901fb2b274d086e032f06d3442e6aeaa32d63ca00b0020b18d845f25580.jpg)

5.1 奖励机制的设计影响

图3展示了采用不同奖励策略的训练过程，表2则呈现了最终测试结果。当移除知识边界感知奖励（"w/o r_{kb}"）时，有效检索次数和响应长度持续攀升，远超基准模型。这是由于训练初期，检索行为比调用内部知识更容易获得奖励，梯度更新自然偏向抑制后者，最终形成"检索至上"的决策偏好，与Search-R1策略如出一辙。而在仅保留正向奖励（"w/o r_{kb}-"）时，模型检索频率和响应长度骤减——因为奖励机制过度鼓励依赖内部知识，导致模型错误地将R1策略泛化到所有问题。测试数据显示，"w/o r_{kb}"版本虽然EM分数相当，但检索开销激增；而"w/o r_{kb}-"版本则因检索不足导致性能明显下滑。由此可见，优秀的边界感知奖励函数必须精巧平衡内外知识的调用，才能实现协同增效。

5.2 数据集难度的影响分析

图4清晰呈现了不同难度训练数据集的动态变化：有效搜索次数、响应长度和训练奖励的演变曲线。通过对比简单、混合、困难三种难度数据集的训练过程（详见表3测试结果），我们发现一个稳定规律：无论是有效搜索次数还是响应长度，困难数据集的表现均优于混合数据集，而混合数据集又优于简单数据集。这是因为模型会智能地根据问题难度选择知识调用方式——对熟悉领域使用参数化知识，对陌生领域则启用检索机制。特别值得注意的是，使用简单数据集训练时，模型的检索频率和响应长度会持续衰减，最终完全适配训练数据的难度特征。测试结果显示，经过难度调整的IKEA模型（无论是简化版还是困难版），其精确匹配率都明显逊色于原始模型。其中简化版的检索频次骤降，而困难版则检索激增。这一现象生动说明：过度依赖单一知识类型会制约模型潜能，唯有让参数化记忆与检索知识珠联璧合，才能实现最优的推理效果。

6 相关研究

基于大语言模型的强化学习（RL）是LLM后期训练的核心技术，既能校准预训练模型的价值观，又能提升其在下游任务中的表现。研究社区已推出PPO、DPO等特色RL算法。通过设计差异化环境与奖励机制，可将LLM训练成具备自主决策能力的智能体。典型代表是搜索智能体[16,3,33]，它能通过与搜索引擎的持续交互获取环境知识，最终完成知识密集型推理任务。

LLM的知识疆界 大语言模型同时具备参数化内部知识和外部知识调用能力。知识疆界理论[20,40]对此作出明确区分：既可通过提示模板法（分析特定问题的响应）探测，也能借助隐状态或SAEs等模型特征进行判定。准确把握这一疆界，对检索增强生成模型（RAG）规避幻觉、实现精准问答至关重要。

7 总结与局限

本文提出的强化内外知识协同推理智能体（IKEA），开创性地解决了现有基于强化学习的搜索智能体三大痛点：内部知识利用率低导致的冗余检索、知识冲突和推理延迟。该系统的核心优势在于能智能判断知识边界——优先调用内部参数化知识，仅在知识储备不足时启动外部检索。这一突破性能力源于两大创新设计：知识边界感知奖励函数与定制化训练数据集。实验证明，IKEA在知识密集型任务中实现了效率与准确率的双重提升。但需注意的是，其通用性受限于特定数据集构建要求，奖励函数需网格搜索调参，且强化训练计算开销较大。未来研究方向包括：开发动态知识边界识别算法、拓展多任务适用场景、优化训练资源消耗。