Graph-Reward-SQL：基于图匹配与渐进式奖励的无执行文本转SQL强化学习方案

韩文 宋元峰 曾盾 刘博一 杨映翔 詹毅 崔龙杰 尹晓明 孙阳 | 字节跳动团队

摘要

强化学习（RL）已成为提升大语言模型（LLM）在文本转SQL任务表现的主流方案。但现有方法依赖的基于执行或LLM的Bradley-Terry奖励模型（BTRM）各有局限：前者因频繁调用数据库导致延迟飙升，后者则带来沉重的GPU内存负担，严重制约了RL流程的效率和扩展性。为此，我们创新性地提出GRAPH-REWARD-SQL框架，采用GMNScore结果奖励模型，通过SQL图表示精准捕捉奖励信号，同时大幅降低推理耗时和显存占用。更进一步，我们开发了StepRTM逐步奖励模型，对CTE子查询实施渐进式监督，在确保SQL功能正确的同时优化结构可读性。在Spider、BIRD等标准测试集上的大量对比实验表明，我们的方案全面超越了现有奖励模型。

1 引言

文本转SQL技术（Tai等，2023；Li等，2024b；Shi等，2025）致力于将自然语言转换为结构化数据库查询，通过赋能非技术人员高效操作关系数据库，在数据普惠领域扮演关键角色。当前研究聚焦于基础模型微调，其中强化学习（RL）被证实能显著提升模型表现（Pourreza等，2025b；Berdnyk与Collery，2025；Ma等，2025）。而奖励模型（RM）的精心设计尤为关键，其信号质量直接影响微调效果。

现有RL方案中，执行准确率仍是核心指标（Nguyen等，2025；Ma等，2025；Pourreza等，2025b；Berdnyk与Collery，2025），通过查询正确性提供直观反馈。基于大模型的Bradley-Terry奖励模型（BTRM）（Christiano等，2017）通过执行结果构建偏好对，已成功应用于代码生成（Zeng等，2025a）。另有研究采用抽象语法树（AST）结构奖励来捕捉语法相似性（Shojaee等，2023）。但这些方法各存缺陷：执行奖励因实时数据库访问导致延迟；BTRM计算内存开销大；AST匹配易误判语法相异但语义相同的查询，产生噪声信号。这些痛点凸显了文本转SQL领域的关键挑战：如何设计高效奖励模型，在保持性能前提下替代执行验证。

为此，我们提出Graph-Reward-SQL创新框架，包含两大互补奖励模型：图匹配网络评分（GMNScore）和渐进式关系运算符树匹配（StepRTM）。GMNScore通过图匹配网络（GMN）直接评估SQL功能等价性（Zhan等，2025），无需执行即可捕捉深层语义。相较执行验证，其速度提升显著；相比BTRM，GMN轻量架构大幅降低GPU内存占用。StepRTM则通过渐进奖励机制评估公共表表达式（CTE）子查询生成，与GMNScore形成优势互补。

本方案具备三大优势：（1）训练高效：较现有方案显著降低时间成本与显存占用；（2）渐进反馈：突破传统结果导向评估，通过CTE结构实现训练过程精细指导；（3）性能卓越：在Spider（Yu等，2018）和BIRD（Li等，2024b）基准测试中，全面超越主流奖励模型。

主要创新点包括：
• 首创GMNScore奖励模型，以图匹配网络替代执行验证，实现效率性能双提升
• 设计StepRTM渐进奖励机制，通过CTE子查询匹配提供精准训练指导
• 实验证实方案在保持高效推理与低内存消耗的同时，持续提升模型表现

2 研究背景

文本转SQL技术。作为自然语言处理的核心任务，文本转SQL旨在将自然语言查询转换为可执行的数据库指令（Tai等，2023；Li等，2024b；Shi等，2025）。随着大语言模型的普及，研究者们开发了智能体框架（Wang等，2025；Pourreza等，2025a；Lei等，2024），通过迭代推理和外部工具增强模型与数据库的交互能力。以DeepSeek-Coder（Guo等，2024）和Qwen2.5-Coder（Hui等，2024）为代表的代码大模型为这些系统提供了核心支持。现有优化方案包括模型微调（Li等，2024a；Yang等，2024）、提示工程（Pourreza等，2023）和模式链接技术（Guo等，2019）等。

强化学习范式。在代码模型优化领域，强化学习已成为重要手段。近端策略优化（Schulman等，2017）等方法的有效性高度依赖奖励模型质量（Trella等，2023）。该领域涌现了诸多创新：CodeRL（Le等，2022）采用执行反馈机制，PPOCoder（Shojaee等，2023）引入语法树匹配，AceCoder（Zeng等，2025a）则创新性地应用了基于大模型的评分体系。

奖励模型演进。自Zhong等（2017）开创执行奖励机制以来，该领域持续创新：Nguyen等（2025）提出关键词匹配评分，Berdnyk等（2025）利用大模型迭代优化设计。DeepSeekR1（Guo等，2025）等推理模型的突破，催生了更精细的奖励方案：SQL-R1（Ma等，2025）引入格式约束，Reasoning-SQL（Pourreza等，2025b）则整合了多维度评估体系。尽管技术迭代，执行奖励仍是当前研究的核心。

现存问题与创新。现有方法存在两大局限：一是忽视计算成本，二是未能深度挖掘SQL语义特征。此外，仅关注最终结果而忽略中间过程的监督信号。为此，我们创新性地提出无执行奖励机制和渐进式评分体系，在保证效果的同时大幅提升计算效率。

表1显示，我们提出的GMNScore和StepRTM方案在性能提升的同时，显著降低了时间和内存消耗。

3 基础知识

3.1 问题建模  

在文本转SQL任务中，设$x$为自然语言输入，$\hat{q}$为模型生成的SQL语句，$q^{\star}$为标注的标准SQL。本研究采用近端策略优化（PPO）算法（Schulman等，2017），通过最大化以下目标函数训练策略模型$\pi_{\theta}$：  

$$  
\mathcal{I}(\boldsymbol{\theta})=\mathbb{E}[r(\hat{\boldsymbol{q}},\boldsymbol{q}^{\star})] - \beta\mathbb{D}_{\mathrm{KL}}(\pi_{\boldsymbol{\theta}}\parallel\pi_{\mathrm{ref}})  
$$  

式中$\pi_{\mathrm{ref}}$为参考模型，$\beta$为PPO超参数，$r(\cdot)$为奖励函数。该方法可无缝扩展至组相对策略优化（GRPO）（Shao等，2024），具体实现参见附录D。

3.2 现有奖励模型纵览

鉴于奖励模型在强化学习中的核心地位，本节重点剖析三类主流模型。如表1所示，我们从推理耗时和GPU显存占用维度，将现有模型与我们的方案进行对比，并在6.1节给出最终性能评估排名（详见附录F）。

执行准确率（EX）。在Text-to-SQL任务中，EX通过判断SQL执行结果正确性提供离散奖励。我们采用融合语法错误分析（Pourreza等，2025b）和运行时诊断（Shojaee等，2023）的改进模型，其计算公式为：

$$
r_{\mathrm{EN}}(\hat{q},q^{\star})=R_{执行}+R_{语法}+R_{运行时}
$$

但EX存在明显缺陷：当数据库存在数据质量问题（如缺失、不一致）或结构异常时，不同查询可能输出相同结果（Zhong等，2020）。虽然测试套件TS（Zhong等，2020）尝试解决此问题，但假阳性/假阴性仍难以避免（Zhan等，2025），且频繁执行SQL会导致计算开销激增（详见附录E）。

Bradley-Terry模型（BTRM）。该模型架构为$r_{\psi}(x,y)=h_{r}(\mathcal{M}_{\theta}(x,y))$，通过预训练语言模型$\mathcal{M}_{\theta}$和奖励头$h_{r}$，基于正确执行的查询对$\mathcal{D}=\{(x_{i},y_{i}^{+},y_{i}^{-})\}_{i=1}^{N}$（Zeng等，2025b），最小化Bradley-Terry似然函数：

$$
-\sum_{i=1}^{N}\log\frac{e^{r_{\psi}(x_i,y_i^+)}}{e^{r_{\psi}(x_i,y_i^+)}+e^{r_{\psi}(x_i,y_i^-)}}
$$

相较于EX，BTRM无需实时查询数据库即可提供密集奖励信号（Christiano等，2017），但大模型参数导致显存占用较高。

![](images/96c0d708f7fef55300473a0bb9387625dabce108fab00756b8d4716d60d3ae71.jpg)

图1展示本框架创新：1) 采用GMNScore解决EX的语义盲区（如WHERE age>34与>=34得分相同的问题）；2) 为CTE-SQL设计渐进式奖励模型StepRTM，填补中间奖励空白（详见示意图2）。

匹配式奖励。现有方法包括SQL关键词匹配（Nguyen等，2025）和n-gram重叠检测（Pourreza等，2025b），虽响应迅速但易误判语法差异的语义等价查询。PPOCoder（Shojaee等，2023）虽引入语法树匹配，仍局限于浅层结构分析。

4 方法  

我们推出G RAPH -R EWARD -SQL框架，通过两大创新优化SQL生成：其一，GMNScore取代传统EX方法，无需执行数据库即可高效计算奖励信号，兼顾速度与精度；其二，基于CTE SQL关系操作树（ROT）的StepRTM逐步奖励模型，实现分阶段反馈指导。

4.1 关系操作符树（ROT）  

精准建模SQL的结构与语义是查询分析与比对的核心。虽然抽象语法树（AST）能捕捉SQL的句法特征，但SQL天然缺失控制流图（CFGs）和数据流图（DFGs）等关键逻辑表征——这些恰是呈现程序逻辑与数据依赖的基石。  

为此，我们创新引入关系操作符树（ROT），将SQL查询解构为关系代数操作符的层次化树结构。每个树节点代表特定逻辑操作（如连接、投影），而枝干走向则暗含查询的依赖关系与执行脉络。基于Apache Calcite框架，SQL被编译为标准化中间表示RelNode，其内置操作符优化与子句精简能力，可生成抗语法干扰的规范逻辑计划。更突破的是，RelNode能像CFGs/DFGs般，通过边结构融合控制流与数据流（Zhan等，2025），为深度查询分析提供全景式的图模型支撑。

4.2 功能等价性评估图匹配网络

获取SQL图表示后，我们使用基于SQL语句对训练的图匹配网络(GMN)来评估功能等价性。该模型创新性地融合了全局位置编码与跨图注意力机制，通过对比学习预训练结合监督学习，深度捕捉SQL查询的语义关联。相似度计算采用最终图嵌入的负欧氏距离：$s(h_{G_{1}},h_{G_{2}})=$$\left|\left|h_{G_{1}}-h_{G_{2}}\right|\right|_{2}$，其中$h_{G_{1}}$和$h_{G_{2}}$由GMN联合计算两图的表征得出。该技术方案首见于Fun cE val GM N研究，详见附录M的完整说明。

4.3 ROT/RelNode局部匹配（RelPM）

与抽象语法树（AST）类似，RelNode同样能通过图匹配评估SQL相似性。RelPM（Zhan等人，2025）作为基于规则的匹配算法，通过SQL的RelNode表示（记为$\mathcal{G}_{\hat{q}}$和$\mathcal{G}_{q^{\star}}$）来衡量相似度。其AST版本称为AstPM（Zhan等人，2025）。二者均采用分层局部匹配策略，依据节点匹配的精确率与召回率得出全局相似度评分。具体而言，算法将生成节点$n^{\prime}\in\mathcal G_{\hat{q}}$与参考树候选节点$n\,\in\,\mathcal{G}_{q^{\star}}$逐一比对：当运算符类型和值相同时判定匹配，并通过子图比对计算匹配分数，最终选取最高分节点作为匹配结果。详见附录L。

4.4 奖励函数设计

图1展示了我们的奖励机制设计，包含最终结果评估模型GMNScore和分步评估模型StepRTM。对于生成的SQL查询$\hat{q}$和标准答案$q^{\star}$，系统在时间步$t$对长度为$T$的序列计算奖励值：

$$
\begin{array}{r l}&{\mathcal{R}_{t}(\hat{q},q^{*})=\,\,\mathbb{1}(cond_{\mathrm{eos}})\cdot\left[R_{\mathrm{GNNScore}}(\hat{q},q^{*})-\beta R_{\mathrm{bl}}(\hat{q}_{<t})\right]}\\ &{\quad+\,\mathbb{1}(cond_{\mathrm{sub}})\cdot\left[R_{\mathrm{StepRTM}}(\hat{q}_{\leq t},q^{*})-\beta R_{\mathrm{bl}}(\hat{q}_{<t})\right]}\\ &{\quad+\,\,\mathbb{1}(\neg cond_{\mathrm{eos}})\cdot\mathbb{1}(\neg cond_{\mathrm{sub}})\cdot\left[-\beta R_{\mathrm{bl}}(\hat{q}_{<t})\right],}\end{array}
$$

其中：$cond_{\mathrm{eos}}$标记生成过程结束，触发最终结果评估；$cond_{\mathrm{sub}}$表示子查询完成，启动分步评估机制将当前子查询与标准答案中的对应结构比对。$\neg$为逻辑非运算符。KL散度惩罚项$R_{\mathrm{kl}}(\hat{q}_{<t})$实时监控策略与预训练模型的偏差，确保策略更新的稳定性。超参数$\beta$用于调节奖励权重与策略规范化强度。

4.5 结果奖励：GMN评分机制

如4.2节所述，我们采用Fun cEval GMN指标$\mathcal{M}_{\mathrm{GMD}}$来评估生成SQL的功能正确性，这与强化学习奖励模型的目标高度契合。具体奖励模型设计如下：

$$
R_{\mathrm{GMN评分}}(\hat{q},q^{\star})=\left\{\!\!\begin{array}{l l}{-1,}&{\text{语法错误}}\\ {-0.6,}&{\text{ROT解析错误}}\\ {\max(0,\mathcal{M}_{\mathrm{GMN}}+1)}\end{array}\!\!\right.
$$

该评分机制对SQL查询中的语法错误和ROT解析错误实施分级惩罚。对于正常查询，我们通过仿射变换将原本在$(-\infty,0]$区间的$\mathcal{M}_{\mathrm{GMD}}$相似度评分线性映射到$[0,1)$区间，并将负值归零处理。

4.6 分步奖励机制：StepRTM  

现代ETL（数据抽取-转换-加载）流程通常不会一蹴而就，而是由分析师拆解为循序渐进的子查询计划。公用表表达式（CTE）正是实现这一计划的利器：  

```sql
WITH 步骤1 AS (/* 子查询1 */), 步骤2 AS (/* 子查询2 */) SELECT ... FROM 步骤2;  
```  

CTE不仅能提升复杂SQL的可读性，清晰展示ETL的中间转换步骤，更天然适配分步评估的需求。  

借鉴子图匹配技术（Lou等，2020；Roy等，2022），我们创新性地提出分步关系运算符树匹配（StepRTM），通过分步奖励机制提供渐进式反馈。如图2所示，系统将参考SQL$q^{*}$与生成的CTE式SQL$\hat{q}_{\mathrm{cte}}$进行比对：  

![](images/76cf6eec655ae439f2cfec5906120140094751f954231d394fc02a67ddf89e41.jpg)  

图2：StepRTM运作流程。(a) 将生成SQL拆解为带位置标记的子查询链；(b) 将参考查询与子查询均解析为运算符树；(c) 通过树结构逐级匹配，动态计算增量奖励。  

具体实现上，系统通过公式：  
$$  
\mathcal{R}_{\mathrm{StepRTM}}^{(i)}=\frac{\text{新增匹配节点数}}{\text{参考查询总节点数}}  
$$  
确保奖励机制既避免重复计算，又能准确反映语义覆盖的渐进过程。这种"小步快跑"的监督策略，通过密集的中间反馈显著提升了SQL生成的准确性。

5 实验配置

数据集。实验主要基于Spider和BIRD两大基准展开。Spider数据集（Yu等，2018）囊括10,181条自然语言问询与5,693个跨越138个领域的复杂SQL语句。BIRD数据集（Li等，2024b）则包含12,751道专业问题，覆盖37+个垂直领域。

采用Spider训练集进行模型训练，并选取两个数据集的开发集进行评估。PPO训练前使用200k-Text2SQL数据集进行预热，完整数据说明详见附录A。

基线模型。我们对比了多种典型基线方案：1）主流方案EX（Nguyen等，2025）；2）基于DeepSeek-Coder-1.3B-Ins架构的BTRM奖励模型（详见附录K）；3）最新匹配式奖励模型AstPM与RelPM（Zhan等，2025），相关研究参见（Shojaee等，2023）。

评估体系。采用测试套件TS（Zhong等，2020）作为核心指标，通过多数据库验证提升评估鲁棒性（评估细则见附录B）。

表2展示了Deepseek-Coder双模型在各类基线及GMNScore奖励机制下的TS表现。

实施细节。PPO训练前分两阶段进行监督微调：1）使用与Spider等量的200k-Text2SQL子集训练1.3B/6.7B模型2个epoch；2）将BIRD数据转为CTE格式构建CTE-SFT预热集，以优化逐步奖励实验中的CTE生成能力。超参数配置详见附录C。

6 实验结果  

6.1 奖励机制性能对比  
GMNScore成功取代了传统EX方案，彻底摆脱了对SQL执行和数据库环境的依赖。如表8所示，在1.3B和6.7B模型上，GMNScore均取得了最优的平均TS得分，充分验证了强化学习中奖励信号设计的关键作用。值得注意的是，RelPM表现显著优于AstPM，两种模型规模下分别实现2.53%和1.71%的性能提升。这一优势源于ROT采用归一化逻辑计划进行SQL解析，有效规避了表层语法差异的干扰，为两种奖励模型提供了更鲁棒的表示。GMNScore通过图嵌入技术捕捉深层语义特征，不仅省去了执行结果对比环节，更大幅降低了误报噪声。此外，该方案无需构建和维护数据库，为大规模Text-to-SQL强化学习提供了轻量化解决方案（案例详见附录Q）。  

引入StepRTM逐步奖励机制后，模型性能得到进一步提升。如表3所示，CTE-SFT与StepRTM的组合在各种奖励模型中均展现出稳定增益。特别地，集成GMNScore与StepRTM的框架实现了最佳综合表现：在BIRD数据集上提升5.87%，Spider数据集上提升0.97%。这表明由于BIRD数据集具备更复杂的数据库结构和查询逻辑，其从逐步奖励机制中的获益更为显著。

6.2 GMNScore在GRPO中的卓越表现




我们研发的GMNScore不仅与PPO配合默契，在GRPO框架下同样大放异彩。通过使用PPO和GRPO对Qwen2.5-Coder-7B/14B-Ins模型进行训练，图3清晰显示：在这两种强化学习方案中，GMNScore始终力压EX，充分彰显了其稳定可靠的优越性能。





![](images/5dd8a8f7df5e15932685d270ce70f46452ce112af92d0e08f9420fed791cbea6.jpg)  


表4：通过对比参考SQL、失败SQL和CTE SQL，有力验证了StepRTM的实用价值。  





![](images/cea59dd82e11fd2f9b320db166bbf8e35ddbbf33000ae4d5edb96503e91a332d.jpg)  





图4：训练过程中奖励分与执行结果的AUC曲线显示，GMNScore以97.6%的超高一致率展现了惊人的稳定性。

6.3 案例研究：StepRTM赋能CTE SQL  

表4通过两组对比，揭示了逐步奖励模型对SQL语句正确性与结构优化的双重提升。案例一中，传统模型误检索评论表数据，而采用StepRTM的CTE SQL通过"用户定位→帖子评分聚合"的分步子查询精准解决问题。案例二中，当传统SQL因硬编码性别标识而失效时，CTE SQL则以"男性英雄筛选→超能力提取"的模块化设计完美实现需求。

6.4 GMNScore准确性分析




实验证明，GMNScore作为PPO的奖励模型表现优异，远超BTRM。通过分析训练过程中两种奖励信号与实际结果的关联性（图4可见），GMNScore始终保持着与执行结果的高度同步2。这说明GMNScore在训练时能提供更稳定精准的反馈信号，这正是其性能出众的关键所在。

7 讨论

本文提出的GMNScore不仅完美兼容各类奖励模型，还为EX提供了全新替代方案。如附录G所示，我们突破StepRTM集成（第6.1节）的框架，引入混合结果奖励模型，使性能更上层楼。这一发现与先前采用多奖励模型的研究（Pourreza等，2025b；Ma等，2025）不谋而合。

我们刻意规避了BIRD数据集中的年龄计算等提示信息，因其在Spider等主流Text-to-SQL数据集中并不常见。这导致本研究的性能表现略逊于那些利用BIRD提示信息的研究成果。

8 结论  

我们提出G RAPH -R EWARD -SQL——一种创新的Text-to-SQL强化学习框架，采用结果奖励模型GMNScore和渐进式奖励模型StepRTM替代传统执行奖励机制。该方案通过解除训练过程中的执行依赖，大幅提升训练效率。实验证明，新型奖励模型性能卓越，为强化学习驱动的Text-to-SQL技术开辟了更高效、更可扩展的发展路径。

9 局限与展望

尽管我们研发的奖励模型在文本转SQL任务中表现优异（基于执行的版本尤为突出），但其专为SQL设计的架构特性，目前难以直接应用于Python、C++等其他编程语言的代码生成场景。要实现跨语言支持，需重构程序语言表示机制，特别是解决不同编程范式下功能等价性判定的难题。

这一局限恰恰孕育着新的机遇。未来我们将重点探索免执行奖励模型在通用代码生成中的应用，突破传统强化学习对执行结果的依赖，推动构建更普适、高效且灵活的代码生成评估体系。