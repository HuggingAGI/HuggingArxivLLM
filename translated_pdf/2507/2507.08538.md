AI语言能力监测仪——大型语言模型多语言基准测试进展追踪




David Pomerenke 1 Jonas Nothnagel 2 Simon Ostermann 3 
1 德国联邦经济合作与发展部(BMZ) 
2 德国国际合作机构(GIZ) 
3 德国人工智能研究中心(DFKI) 
david.pomerenke@bmz.bund.de 
jonas.nothnagel@giz.de 
simon.ostermann@dfki.de

摘要  

为让大型语言模型（LLM）的技术红利惠及全球，需对其跨语言能力进行系统评估。我们推出「AI语言能力监测器」——首个覆盖200种语言（尤其关注低资源语言）的多维度评测体系，集成了翻译、问答、数学及逻辑推理等任务，采用$\mathrm{FLOPES+}$、MMLU等权威数据集。通过开源动态排行榜与交互看板，研究者可直观发现模型强项与短板，平台还提供全球能力热力图、演进趋势等深度洞察。这一基准不仅填补了现有多语言评估空白，更致力于推动AI技术的公平性与包容性发展。访问地址：https://huggingface.co/spaces/fair-forward/evals-for-every-language。

1 引言

要让大型语言模型（LLM）真正普惠大众，就必须让它们掌握全人类的语言。尽管多语言LLM如雨后春笋般涌现，但主流研发仍以英语为中心。然而现实是，全球近80%的人口并不以英语为母语（Eberhard等，2025）。为弥合这一鸿沟，学界已为众多语言开发了丰富的多语种评估与训练资源，涵盖预训练（Mayer与Cysouw，2014；Conneau等，2020a；Abadji等，2022；Weber等，2024）和模型微调（Fu等，2022；Chiang等，2023；Asai等，2024）。我们在此推出"AI语言能力监测器"，通过整合多语种评估数据集，构建了一个动态评估AI模型语言能力的全球基准，尤其关注资源匮乏语种。

本基准配备开源仪表盘，主要服务于三大场景：
• 研究者可用"AI语言能力分数"这一多任务指标进行模型对标
• 从业者可快速匹配语种与任务的最佳模型
• 决策机构能精准识别亟待语言能力建设的区域

我们依托FLORES⁺（Goyal等，2022）的翻译与分类能力，以及多语言版MMLU（Hendrycks等，2020）、ARC（Clark等，2018）、GSM8K（Cobbe等，2021b）和TruthfulQA（Lin等，2022）等基准，每日自动评估100-200种语言（覆盖全球80-95%人口），既包含商业闭源模型，也纳入开源方案。

除实时排行榜外，我们还提供：
- 全球AI语言能力热力图
- 高低资源语种发展对比
- 人口规模/GDP/数据量对AI语言能力的影响分析

相比FLORES⁺等静态基准，我们的创新在于：
1）动态追踪LLM进化历程
2）借鉴OpenLLM排行榜架构但专注多语种评估
3）融合非洲/欧洲区域榜单经验，扩展为全球覆盖
4）提供语种专属可视化看板

当前虽已涵盖主流模型与语种，但我们更期待：
• 各地开发者提交定制化模型
• 社区共建更多语种评估数据
让这项研究真正赋能全球数字包容进程。

2 研究背景与相关工作

2.1 多语言评估基准




当前多语言大模型评估基准存在明显局限：要么聚焦特定区域，要么对低资源语言覆盖不足。以欧洲LLM排行榜2（Thellmann等，2024b）为例，该基准基于OpenGPT-X评估框架3，涵盖21种欧洲语言（缺爱尔兰语等3种）的NLU和机器翻译能力测试，数据源包括MMLU（Hendrycks等，2020）等公开数据集，但仅限欧洲语系。




Cohere.AI推出的多语言LLM评估4基准集合虽收录了Global-MMLU等丰富数据集，却缺乏统一的评估接口和可视化功能。




MEGA基准（Ahuja等，2023）覆盖70种语言的16个NLP数据集，其升级版MEGAVERSE（Ahuja等，2024）扩展至83种语言22个数据集，并加入多模态评估。这两个基准在语言覆盖量上仍不及我们（仅达我们基准语言数的一半），且未提供排行榜或可视化工具，对非技术人员不够友好。




区域性基准的代表作AfroBench（Ojo等，2025）专注非洲语言，包含64种语言的九类NLU数据集、六类文本生成任务等，但适用范围具有明显地域局限性。

2.2 多语言与非英语语言模型

多语言大模型（MLLMs）具备跨语言文本处理与生成能力，可支撑翻译、检索、推理等跨语言应用。早期代表如mBERT（Devlin等，2019）和XLM-R（Conneau等，2020a）这类编码器模型，以当今标准来看规模较小。针对低资源语言的嵌入方法也有探索（Gurgurov等，2025）。在LLM领域，开放权重的LLama 3系列（Grattafiori等，2024）和Gemma 3（Team等，2025）最为知名。

Qin等（2025）系统梳理了MLLMs的四大跨语言技术路径：（1）共享词表/标记对齐：采用Sentence Piece等跨语言子词切分器，但可能造成低资源语言的标记化偏差（Rust等，2021）；（2）架构级对齐：共享多语言参数，辅以语言适配器平衡效率与专精（Pfeiffer等，2020）；（3）表示层对齐：通过对比学习或平行语料直接对齐多语言嵌入（Artetxe和Schwenk，2018）；（4）事后对齐：将单语模型映射至共享空间，无需显式多语言训练即可实现对齐（Conneau等，2020b）。各方案在扩展性、公平性及算力成本间存在权衡。

3 系统设计与实现

3.1 数据采集与多语言转化

我们系统收集了翻译、文本分类、问答及数学等NLP核心任务的多语言基准数据集，优先选择具备多语言平行数据的资源，确保跨语言效果可比性。表1详细列出了所选数据集及其翻译方式（人工/机器）。

核心数据源FLOPES+涵盖200种语言，其衍生数据集SIB-200则对相同语句进行主题分类。针对问答和数学任务，我们将原英文基准数据通过人工或机器翻译（见表1标注）转化为多语言版本，优先采用人工翻译结果。当同一语言存在不同文字变体时，依据Unicode CLDR⁶的用户基数数据，仅保留主流文字版本。为确保数据平行性，Masakhane系列数据集（AfriMMLU等）未覆盖的原文内容均作对应剔除。

基于CLDR全球语言使用排名，我们对前100种语言中缺失基准数据的语种采用机器翻译补全。尽管机器翻译质量不及人工（已有GlobalMMLU等研究先例），但对资源稀缺语种仍具实用价值。通过FLOPES+翻译评估，我们最终选用表现优于通用大模型的Google神经机器翻译系统（v2版）完成所有缺失语种的基准数据转化。

3.2 任务实现与效果评估

本排行榜采用小样本学习模式进行评估。由于所选数据集未提供翻译提示，且人工大规模翻译超出项目范畴，我们放弃了机器翻译方案——避免翻译误差对结果造成过度干扰。最终采用语言无关的最小化提示配合示例演示，让模型通过观察而非说明理解任务。集成模型中表现最全面的...

表1展示了各任务基准数据集概况，包含语言覆盖数量及翻译方式标注（Ξ表示人工，Φ表示自动）。

数学任务特别设置了"<推理>####<答案>"的响应格式，为所有模型提供推理空间。具体提示示例详见Github仓库。

分类与问答任务均采用单选题形式（单选正确答案），以准确率作为评估标准。数学任务则依据最终答案的正确率评分。翻译评估更为复杂：传统BLEU指标（Papineni等，2002）可能低估分词粒度大的语言；BertScore（Zhang等，2019）及其多语言变体则可能高估训练见过的语言。因此我们选用SpBLEU指标（Goyal等，2022），其基于在FLOPES+数据集所有语言上均衡训练的SentencePiece分词器。评估时特别区分两种方向：从目标语言译出（按语言使用人口抽样）与译入目标语言。后者易受评估指标的语言偏好影响，而前者因评估对象（译文）在不同语言间保持稳定，更具可比性。

3.3 模型与推理




为最大化排行榜价值，我们持续收录最新模型，聚焦实践者社区最关注的方案，同时涵盖开源模型与纯API模型。借助OpenRouter平台，我们既能追踪历史与当红模型动态，又能高效执行推理任务。通过Github Actions实现的自动化流程，每日都会抓取新模型并完成评估。





![](images/80c3344c65d3d8bc0db12ac293c6e598835fd1c5acc64fed5c90bf9f5116b025.jpg)  


图2：各模型语言能力得分时序图。趋势线展示每日最高得分记录。





OpenRouter的核心机制确保仅接入禁止数据训练的API端点，这在API基准测试中有效杜绝了数据污染风险。  





针对非商业开源模型——这类模型因专项优化和低成本特性，往往更受地域开发者青睐——我们采用Hugging Face推理API。该平台允许用户自由发布模型端点进行基准测试，现有提交机制只需填写在线表单并分享模型接口即可加入自动评估。未来版本将实现全自动化提交与集成功能。

3.4 语言能力评分  

为了提升排行榜的可比性，我们采用最小-最大归一化方法对所有任务得分进行标准化处理。  

我们创新性地提出了"语言能力评分"这一综合指标，通过计算各任务得分的均值，全面评估AI模型对目标语言的理解水平，为衡量新模型的多语言能力提供统一标准。在排行榜中，您可以看到两种维度的评分：按模型统计（跨语言平均）和按语言统计（跨模型平均）。  

![](images/a12166a7c3a0c9068612ac24552688e18bce3adbe58b6632461b466f5847db29.jpg)  

图3：各语言能力评分对比

3.5 网络应用




我们的网络应用部署在Hugging Face平台，与现有模型和数据集无缝对接。该应用包含四大功能模块：





•  模型排行榜：根据语言能力对LLM进行智能排序，展示综合语言能力评分及各任务得分。支持按模型类型（开源/API）、单次调用成本筛选，开源模型还支持参数规模筛选（数据源自OpenRouter和Hugging Face）。默认展示多语言综合评分，用户可通过筛选器切换至特定语言榜单。





•  语言看板：动态呈现各语言数据及AI模型表现，按使用人口排序并支持语系筛选。展示语言能力评分与细分任务得分，用户选择目标语言后，模型榜单将实时聚焦显示。





•  数据集面板：系统评测采用的基准数据集一览，支持按任务类型、支持语种数量、数据来源（人工/机器翻译）等多维度筛选。





•  可视化中心：提供交互式图表展示，包括全球语言分布地图（图1）、模型表现趋势图（图3）及性价比对比图（图2）等。





系统界面截图详见附录A。全部代码及评估流程遵循MIT开源协议，项目地址：https://github.com/datenlabor-bmz/evals-for-every-language。

4 性能评估

4.1 模型与语言表现

图3清晰呈现了各语言的能力评分：圆点大小对应使用者规模，X轴位置反映语言资源丰度。数据印证了既有认知——资源最充沛的英语稳居榜首；法语、西班牙语等欧洲主流语言虽使用者较少，表现依然亮眼；而斯瓦希里语等资源匮乏但使用者众多的语言则表现欠佳。图2的时序对比更直观显示，谷歌模型当前保持领先优势。

4.2 利益相关者反馈精要

我们开展了一项小规模定性调研，邀请来自企业、中小企业、公共机构及非政府组织的代表，将我们的评估工具与主流免费基准（包括AfroBench、欧洲LLM榜单等）进行对比。调研显示：用户特别青睐该工具简洁的操作界面和对非欧洲语种的专注，但也指出当前仅采用学术基准的局限性——这些指标与实际场景中的语言应用表现未必相关。虽然仪表盘的语言支持广度和易用性获得好评，但用户仍期待看到跨语言趋势分析及更丰富的任务覆盖，这些建议将纳入我们下一版本的优化计划。

5 总

5.1 局限性  

我们研究的主要局限在于基准测试评估大型语言模型的固有缺陷。如近期研究（Raji et al., 2021; Asai et al., 2024）所述，基准测试往往只能捕捉能力的片面表现，难以体现真实场景中的实用性，尤其是在多元低资源环境下的应用。我们的监测工具因整合多类基准为统一榜单，同样受此制约。  

此外，资源限制与系统原型特性暂未支持全量基准测试——若覆盖200种语言及多任务评估，需消耗巨额算力。当前阶段，我们优先验证方法可行性，故采用模型-任务-语言组合的10例抽样测试。未来将通过增强算力与社区协作，逐步扩展评估规模。

5.2 未来展望




虽然当前监测系统已涵盖多语言大模型（尤其是小语种）的性能评估，但仍有拓展空间。我们将持续纳入更多本土化模型、测试基准及弱势语种，通过开源框架与各地研究社区协同推进，构建更全面、更具影响力的评估体系。

5.3 总结  

我们推出了AI语言能力监测器——一个持续更新、覆盖200种语言的大型语言模型多语言能力评估基准。通过整合多样化评测数据集，并借助开源可视化面板呈现结果，该系统帮助开发者、研究者和政策制定者洞察性能差异，推动语言公平。这一方案为包容、透明且可落地的多语言AI评估奠定了可扩展的基础。诚邀各界贡献新模型、新任务与新语种，共同完善这一开放资源。