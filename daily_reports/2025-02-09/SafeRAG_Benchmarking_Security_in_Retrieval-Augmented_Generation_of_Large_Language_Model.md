# SafeRAG: 大型语言模型检索增强生成的安全性评估
发布时间：2025年01月28日

`RAG`
> SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model
>
> 检索增强生成（RAG）的索引-检索-生成范式通过整合外部知识到大型语言模型（LLMs）中，在解决知识密集型任务上取得了显著成功。然而，引入外部和未经验证的知识增加了LLMs的脆弱性，攻击者可通过操纵知识进行攻击。本文提出了一个名为SafeRAG的基准，用于评估RAG的安全性。我们首先将攻击任务分为银噪声、上下文间冲突、软广告和白拒绝服务四类。接着，我们主要为每类任务手动构建了RAG安全评估数据集（即SafeRAG数据集），并利用该数据集模拟RAG可能遇到的各种攻击场景。在14个代表性RAG组件上的实验表明，RAG对所有攻击任务均表现出显著脆弱性，即使是最明显的攻击任务也能轻松绕过现有检索器、过滤器或高级LLMs，导致RAG服务质量下降。代码可在以下网址获取：https://github.com/IAAR-Shanghai/SafeRAG。
>
> https://arxiv.org/abs/2501.18636

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2501.18636](https://arxiv.org/abs/2501.18636)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)