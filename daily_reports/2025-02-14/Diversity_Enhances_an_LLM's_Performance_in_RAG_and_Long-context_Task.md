# 多样化策略显著提升LLM在RAG与长上下文任务中的表现。
发布时间：2025年02月13日

`RAG`
> Diversity Enhances an LLM's Performance in RAG and Long-context Task
>
> 大型语言模型（LLMs）的快速发展凸显了上下文窗口限制的挑战，这主要是由于自注意力机制的二次时间复杂度（\(O(N^2)\)，其中 \(N\) 表示上下文窗口长度）。这一限制影响了问答（Q\&A）中的检索增强生成（RAG）和长上下文摘要等任务。一种常见的方法是选择与查询最相似的内容；然而，这通常会导致冗余，并排除多样化的相关信息。基于最大边缘相关性（MMR）和最远点采样（FPS）的原则，我们在内容选择过程中引入了多样性。我们的研究发现，在基于LLM的问答和摘要之前，将多样性融入内容选择，可以显著提高相关句子或块的召回率。这些结果强调了在未来的LLM应用中保持多样性的重要性，以进一步提升摘要和问答的效果。
>
> https://arxiv.org/abs/2502.09017

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.09017](https://arxiv.org/abs/2502.09017)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)