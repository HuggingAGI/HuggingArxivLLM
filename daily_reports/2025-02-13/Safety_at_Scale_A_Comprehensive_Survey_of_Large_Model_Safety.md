# # 安全规模化：大型模型安全的全面综述
发布时间：2025年02月12日


> Safety at Scale: A Comprehensive Survey of Large Model Safety
>
> 大型模型的快速发展，得益于其通过大规模预训练展现出的卓越学习与泛化能力，已经重塑了人工智能（AI）领域的格局。这些模型现已成为多种应用场景的基础，包括对话式AI、推荐系统、自动驾驶、内容生成、医学诊断以及科学发现。然而，这些模型的大规模部署也带来了显著的安全风险，引发了关于鲁棒性、可靠性和伦理影响的担忧。

本研究综述了目前针对大型模型的安全研究现状，涵盖视觉基础模型（VFMs）、大型语言模型（LLMs）、视觉语言预训练（VLP）模型、视觉语言模型（VLMs）、扩散模型（DMs）以及基于大型模型的智能体。我们的贡献总结如下：
（1）我们提出了针对这些模型的安全威胁的全面分类，包括对抗攻击、数据投毒、后门攻击、越狱与提示注入攻击、能量-延迟攻击、数据与模型提取攻击，以及新兴的智能体特定威胁。
（2）我们回顾了针对每种攻击类型所提出的防御策略（如有），并总结了安全研究中常用的数据集与基准测试。
（3）在此基础上，我们识别并探讨了大型模型安全领域的开放挑战，强调了全面安全评估、可扩展且有效的防御机制，以及可持续数据实践的必要性。更重要的是，我们强调了研究界与国际合作的集体努力的必要性。

我们的工作可作为研究人员与从业者的有用参考，助力全面防御系统与平台的持续发展，从而保护AI模型的安全。
>
> https://arxiv.org/abs/2502.05206

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.05206](https://arxiv.org/abs/2502.05206)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)