# Blended RAG：融合语义搜索与混合型查询检索，精进RAG技术的准确度
`RAG`
> 检索增强生成（RAG）广泛应用于结合大型语言模型（LLM）和私有文档库，以打造高效的生成式问答系统。但随着文档库规模的扩大，提高RAG的准确率变得愈发困难，检索器通过从众多文档中筛选出最相关的信息来为LLM提供必要上下文，对RAG的准确性起着至关重要的作用。本文提出了一种新型的“混合RAG”策略，结合了密集向量索引、稀疏编码器索引等语义搜索技术及混合查询策略。通过这种方法，我们在NQ和TREC-COVID等信息检索数据集上取得了更佳的检索成效，并刷新了基准记录。此外，我们将“混合检索器”应用于RAG系统，使得在SQUAD等生成式问答数据集上的表现大幅超越了传统的细调方法。

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image7.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image9.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image12.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image10.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07220/image11.png)

[https://wx.zsxq.com/dweb2/index/topic_detail/8855251241481122](https://wx.zsxq.com/dweb2/index/topic_detail/8855251241481122)

[https://arxiv.org/abs/2404.07220](https://arxiv.org/abs/2404.07220)