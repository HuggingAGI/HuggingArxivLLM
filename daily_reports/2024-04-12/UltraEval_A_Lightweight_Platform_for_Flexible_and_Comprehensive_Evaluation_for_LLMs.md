# UltraEval：为大型语言模型（LLMs）提供了一个轻巧、灵活且全面的评估平台。
`动手训练`
> 评估对提升大型语言模型（LLMs）的性能至关重要，它能帮助我们精确把握模型的能力并指导其优化。随着LLMs的迅猛发展，我们急需一个轻巧、易于操作的评估框架，以便快速进行评估。然而，鉴于众多实施细节的考量，打造一个全面的评估平台并非易事。现有的评估平台往往过于复杂，模块化程度低，难以与研究人员的工作流程无缝对接。本文推出了UltraEval，这是一个用户友好的评估框架，它以轻量、全面、模块化和高效著称。我们筛选并重新构建了模型评估的三大核心要素（模型、数据和评价指标）。这种组合的灵活性使得在同一个评估流程中可以自由搭配不同的模型、任务、提示和评价指标。此外，UltraEval得益于其统一的HTTP服务，能够支持多种模型，同时提供强大的推理加速能力。目前，UltraEval已经向研究界公开发布。

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07584/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07584/ultraeval_pipeline_white.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07584/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.07584/screenshot.png)

[https://wx.zsxq.com/dweb2/index/topic_detail/5122528542488884](https://wx.zsxq.com/dweb2/index/topic_detail/5122528542488884)

[https://arxiv.org/abs/2404.07584](https://arxiv.org/abs/2404.07584)