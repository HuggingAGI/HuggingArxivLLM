# # **ReZero：再试一次，助力大语言模型搜索能力升级**
发布时间：2025年04月15日


> ReZero: Enhancing LLM search ability by trying one-more-time
>
> 检索增强生成（RAG）在知识密集型任务上显著提升了大型语言模型（LLM）的性能，但其效果高度依赖于初始搜索查询的质量。现有方法通常采用强化学习（RL），主要关注查询构建或结果推理，却忽视了在搜索失败后鼓励重试的重要性。为此，我们提出ReZero（Retry-Zero），一种全新的RL框架，通过直接奖励在初次搜索失败后重新尝试查询的行为，激励LLM探索更多替代方案，而非过早终止。实验结果表明，ReZero在准确率上实现了46.88%的显著提升，远超25%的基线水平。通过奖励坚持不懈的精神，ReZero在复杂信息检索场景中显著增强了LLM的鲁棒性，尤其是在初始查询可能无法满足需求的情况下。
>
> https://arxiv.org/abs/2504.11001

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2504.11001](https://arxiv.org/abs/2504.11001)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)