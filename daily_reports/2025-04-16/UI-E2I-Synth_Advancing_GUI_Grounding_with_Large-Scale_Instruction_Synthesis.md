# 9.9M指令合成！微软亚洲研究院用GPT-4o打造最强GUI视觉定位模型
发布时间：2025年04月15日



![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2025/02/12/1739367812022-81912e8f-5f91-4b9d-b4b2-52b0e322d137.png)
**添加请注明**

**如遇无法添加，请+ vx: iamxxn886**

<hr />



## 为什么需要GUI视觉定位技术？

### 1.1 数字时代的效率革命
图形用户界面(Graphical User Interface, GUI)代理正在重塑人机交互方式。这类代理通过模仿人类的视觉感知能力，可以直接"看懂"屏幕内容并执行操作指令。微软亚洲研究院团队的研究表明，相比依赖HTML等GUI元数据的传统方法（存在平台依赖性和实现差异问题），基于视觉的方法具有更广泛的适用性。例如，在跨平台操作场景中，视觉代理可以统一处理Windows、Web和移动端界面，而无需针对每个平台开发特定解析器。

### 1.2 现有技术的三大瓶颈
研究团队在分析现有GUI视觉定位技术时发现了三个关键挑战：

**元素屏幕比失衡**：现有基准测试中的按钮等元素尺寸通常占据屏幕的1%-5%，而真实桌面环境（如1080p/1440p分辨率）中元素占比往往小于0.5%。这种差异导致模型在测试环境表现良好，但在实际应用中定位小元素时准确率骤降。例如，在1440p分辨率下，"保存"按钮可能仅占屏幕面积的0.2%。

**元素类型不平衡**：现有数据集过度集中于文本按钮（占比超过60%），而复选框、单选框等长尾元素占比不足5%。这种不平衡导致模型难以识别功能性图标，如Photoshop工具栏中的"魔棒工具"图标，尽管这些图标在专业软件中至关重要。

**隐含指令理解困难**：用户常基于元素功能而非可见文本发出指令。例如说"保存文档"而非"点击左上角第三个图标"，现有模型对此类指令的理解准确率不足40%。这种认知鸿沟严重限制了GUI代理的实际应用价值。

### 1.3 数据标注的成本困局
构建高质量的GUI定位数据集面临双重挑战：标注人员需要同时理解界面元素功能（如区分"提交"按钮和"保存"按钮）和用户操作意图（如理解"完成注册"对应的具体操作）。这种复合型标注任务使得单个样本标注成本高达2-3美元，而训练一个实用模型通常需要百万级样本量。例如，标注一个电商结账页面可能需要识别10-15个交互元素，并生成20种以上的自然语言指令变体。




## 二、UI-E2I-Synth技术解析

### 2.1 三步合成流水线

UI-E2I-Synth（User Interface Element-to-Instruction Synthesis，用户界面元素到指令合成）采用分阶段处理策略，将复杂的指令生成任务分解为三个关键步骤：

1. **原始数据收集与解析**
系统从网页、Windows和Android平台采集截图-元数据对，通过启发式解析器提取元素的三个核心属性：类型（如按钮/输入框）、内容（如文本标签）和边界框坐标。这种结构化处理为后续步骤提供了可靠的基础数据，例如从网页DOM（Document Object Model）中解析出的搜索框元素会被标记为"Inputfield"类型。

2. **指代表达生成**
利用GPT-4o生成两种元素描述方式：
- 显式表达：直接描述可见特征（如"蓝色搜索按钮"）
- 隐含表达：通过功能或上下文关系间接描述（如"页面顶部的返回箭头"）
实验数据显示，这种方法使非文本元素的识别准确率提升23%，特别适用于图标等无文字元素。

3. **指令合成**
将用户操作分解为动作类型（点击/输入）、动作内容（输入文本）和元素对象三个参数。通过参数化组合生成自然的第一人称指令，如"在用户名输入框填写'admin'"。相比直接生成，这种方法使指令准确率提高15.6%。

### 2.2 技术突破点

该方案创新性地解决了两个关键问题：

**多模态幻觉抑制**
通过先提取确定性的元素属性再生成描述，有效降低了模型"虚构"元素的概率。在测试中，这种方法将元素定位错误率从传统方法的34%降至12%。

**用户视角模拟**
采用参数化指令合成克服了LLM（Large Language Model）默认以助手视角生成指令的倾向。例如将"请点击登录按钮"转换为"我要登录这个网站"的第一人称表达，使指令自然度提升28%。

典型案例显示，对于包含10个交互元素的界面，传统方法平均需要5次修正才能准确定位，而UI-E2I-Synth首次尝试成功率可达82%。这种提升主要得益于分阶段处理策略和精准的属性控制机制。




## 3.1 新基准测试UI-I2E-Bench

研究团队构建的UI-I2E-Bench基准测试包含1,477条指令，具有三大创新特性。首先，该基准采用更接近真实场景的元素屏幕比（element-to-screen ratio），平均比现有基准小37%。这意味着测试元素在屏幕中的占比更接近实际使用场景，如图1所示，现有基准中的元素比例明显大于常见的1080p和1440p桌面显示器标准。这种设计能更准确地评估模型在真实环境中的表现。

其次，基准实现了元素类型的平衡分布，非文本元素（如图标、输入框等）占比达到23%。如图2右侧所示，现有基准中文本按钮占据主导地位，而UI-I2E-Bench通过精心设计的数据采样策略，确保了各类GUI元素的均衡覆盖。例如，对于复选框这类依赖周边元素定义功能的组件，基准中给予了合理权重。

第三项创新是显式/隐含指令分类标注，其中隐含指令占比达42%。如图1示例所示，当用户说"返回顶部"时，这属于需要理解功能语义的隐含指令；而"点击蓝色返回按钮"则是直接描述视觉特征的显式指令。这种区分有助于评估模型不同层次的认知能力。

## 3.2 模型性能对比

在跨平台测试中，使用合成数据训练的UI-I2E-VLM-7B模型展现出显著优势。在ScreenSpot基准上达到89.2%的准确率，比之前最佳模型OS-Atlas-7B提升9.7个百分点。特别值得注意的是，模型对隐含指令的理解准确率提升12.1%，这得益于合成数据中对功能语义的强化训练。

模型在资源效率方面表现突出，仅使用OS-Atlas 72%的训练数据量就取得更好效果。如表2所示，这种优势在长尾元素识别上尤为明显：图标识别准确率提升18.3%，输入框定位精度提升14.6%。图4的对比曲线显示，随着元素屏幕比减小（即元素更小），模型性能优势逐渐扩大，在最小比例区间（<0.03）仍保持65%以上的准确率。

## 3.3 实际应用验证

将模型集成到OSWorld实时GUI代理测试环境后，任务成功率比纯GPT-4o方案提升23.4%。如表5所示，在专业软件操作等高难度场景中优势更显著。例如在Adobe Photoshop的"将图层混合模式改为正片叠底"任务中，模型能准确识别隐含的功能图标，而基线方法常误选相邻按钮。

这种提升源于两方面：一是合成数据包含各类专业软件的界面样本，二是指令生成时模拟了真实用户的操作逻辑。例如在"登录邮箱"任务中，模型会先定位登录模块再寻找输入框，展现出层级理解能力。

## 3.4 失败案例分析

研究团队对典型错误进行了系统归类（如图5所示）：
1. 无文本图标误识别占错误样本31%，主要发生在专业软件的特殊图标上；
2. 行列定位偏差（如点错同排相邻按钮）占28%，反映空间关系理解的不足；
3. 层级关系误解（如未先定位父容器就直接操作子元素）占22%，显示UI结构认知的局限。

为促进社区发展，团队开源了三大核心资源：
1. 合成数据集：包含990万指令的大规模训练数据，涵盖Web/Windows/Android多平台；
2. 基准测试：UI-I2E-Bench的详细标注数据与评估工具；
3. 模型代码：基于InternVL2和Qwen2-VL的改进实现（项目地址见GitHub）。



<hr />

- 论文原文: [https://arxiv.org/abs/2504.11257](https://arxiv.org/abs/2504.11257)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)