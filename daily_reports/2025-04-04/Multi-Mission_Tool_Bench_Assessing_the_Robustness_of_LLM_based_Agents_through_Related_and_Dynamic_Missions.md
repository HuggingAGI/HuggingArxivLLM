# 多任务工具基准平台：借助相关动态任务评估LLM代理的鲁棒性表现
发布时间：2025年04月03日

`Agent应用`
> Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions
>
> 大型语言模型（LLMs）凭借其卓越的理解与规划能力，展现出作为工具调用代理的巨大潜力。越来越多的用户依赖这些基于LLMs的代理，通过反复交互解决复杂问题。然而，现有的基准测试大多局限于单一任务场景，难以反映现实中的复杂性。为解决这一问题，我们提出了Multi-Mission Tool Bench。在这个基准中，每个测试案例包含多个相互关联的任务，要求代理能够动态适应不断变化的需求。此外，该基准在固定任务数量内探索所有可能的任务切换模式。具体而言，我们提出了一种多代理数据生成框架来构建该基准，并引入了一种新的方法，利用动态决策树来评估代理决策的准确性和效率。在多种开源和闭源LLMs上的实验结果揭示了影响代理鲁棒性的关键因素，并为工具调用领域提供了宝贵的实践见解。
>
> https://arxiv.org/abs/2504.02623

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2504.02623](https://arxiv.org/abs/2504.02623)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)