# 大型语言模型的推理能力解析：基于3-SAT相变的特性分析
发布时间：2025年04月04日


> Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition
>
> 大型语言模型（LLMs）被誉为拥有高级推理能力的AI模型。理论上，具备思维链（CoT）能力的自回归LLMs能够通过更多串行计算来解决复杂的推理任务。然而， recent studies suggest that, despite this capacity, LLMs do not truly learn to reason but instead fit on statistical features. 为了系统地研究推理能力，我们从计算理论的视角出发，提出了一种以3-SAT为核心的实验方案——这是逻辑推理和约束满足任务中最具代表性的NP完全问题。具体而言，我们通过改变问题实例的内在难度，研究随机3-SAT的相变现象，并表征当前最先进的LLMs的推理能力。通过比较DeepSeek R1与其他LLMs，我们的研究发现两个关键点：（1）LLM的准确性在更难的实例上显著下降，表明所有当前模型在统计捷径不可用时都会遇到困难；（2）与其他LLMs不同，R1显示出学习了底层推理机制的迹象。遵循系统化的实验方案，我们的研究超越了LLM推理研究中常见的基于基准的证据。我们的发现揭示了重要差距，并为未来研究指明了明确方向。
>
> https://arxiv.org/abs/2504.03930

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2504.03930](https://arxiv.org/abs/2504.03930)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)