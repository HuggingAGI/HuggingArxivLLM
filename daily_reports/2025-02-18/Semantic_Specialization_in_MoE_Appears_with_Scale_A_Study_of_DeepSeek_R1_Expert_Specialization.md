# 混合专家模型（MoE）的语义专业化随着模型规模的扩大而显现：对DeepSeek R1专家专业化的研究
发布时间：2025年02月15日

`DEEPSEEK`
> Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek R1 Expert Specialization
>
> DeepSeek-R1 是目前最大开源的专家混合模型（MoE），其推理能力可与专有前沿模型相媲美。尽管先前研究探讨了 MoE 模型中的专家路由机制，但发现专家选择往往依赖于标记而非语义驱动。鉴于 DeepSeek-R1 强化的推理能力，我们研究其路由机制是否相较于以往 MoE 模型展现出更高的语义专业性。为此，我们进行了两项关键实验：首先，在词汇语义消歧任务中，我们分析了不同语义下词汇的专家激活模式；其次，在 DiscoveryWorld 的交互任务环境中，我们评估了 DeepSeek-R1 的结构化思维过程。最终，我们得出结论：DeepSeek-R1 的路由机制不仅具有更高的语义感知能力，还能够参与结构化的认知过程。
>
> https://arxiv.org/abs/2502.10928

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.10928](https://arxiv.org/abs/2502.10928)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)