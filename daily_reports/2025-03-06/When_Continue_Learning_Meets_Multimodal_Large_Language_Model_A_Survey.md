# 当持续学习邂逅多模态大型语言模型：综述
发布时间：2025年02月26日


> When Continue Learning Meets Multimodal Large Language Model: A Survey
>
> 人工智能领域的突破性进展催生了多模态大型语言模型（MLLMs）。然而，如何高效地将这些预训练模型适应动态数据分布和多样化任务仍是一个挑战。针对特定任务对MLLMs进行微调时，模型往往会在原有知识领域内出现性能下降，这一现象被称为“灾难性遗忘”。尽管这一问题在持续学习（CL）领域已得到深入研究，但在MLLMs中仍带来了新的挑战。本文作为首篇针对MLLMs持续学习的综述，对这一领域的440篇研究论文进行了全面概述与分析。综述分为四个部分：第一部分聚焦MLLMs的最新研究进展，涵盖模型创新、基准测试及跨领域应用；第二部分对持续学习的最新研究进行了分类概述，包括非大型语言模型单模态持续学习（Non-LLM Unimodal CL）、非大型语言模型多模态持续学习（Non-LLM Multimodal CL）以及大型语言模型中的持续学习（CL in LLM）；第三部分深入分析了MLLMs持续学习研究的现状，包括基准评估、架构创新以及理论与实证研究的总结。最后一部分探讨了MLLMs持续学习的挑战与未来方向，旨在激发该领域的未来研究与开发。本文将持续学习在多模态大模型中的基础概念、理论见解、方法创新与实际应用相连接，全面阐述了该领域的研究进展与挑战，旨在启发研究人员并推动相关技术的发展。
>
> https://arxiv.org/abs/2503.01887

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2503.01887](https://arxiv.org/abs/2503.01887)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)