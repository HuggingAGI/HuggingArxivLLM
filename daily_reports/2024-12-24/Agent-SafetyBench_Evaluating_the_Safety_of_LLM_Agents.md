# Agent-SafetyBench：对 LLM 代理的安全性进行评估
发布时间：2024年12月18日

`模型安全`
> Agent-SafetyBench: Evaluating the Safety of LLM Agents
>
> 随着大型语言模型（LLMs）愈发多地充当代理，它们融入交互环境和工具使用带来了新的安全挑战，这些挑战超出了模型自身相关的范畴。然而，缺乏评估代理安全性的综合基准，成为有效评估和进一步改进的重大阻碍。在本文中，我们推出了Agent-SafetyBench，这是一个专为评估LLM代理安全性而设的综合基准。Agent-SafetyBench涵盖349个交互环境和2000个测试案例，评估8类安全风险，包含了在不安全交互中常见的10种故障模式。我们对16个热门LLM代理的评估得出了令人忧心的结果：没有一个代理的安全分数能超过60%。这凸显了LLM代理存在显著的安全挑战，也强调了极大的改进需求。通过定量分析，我们明确了关键的故障模式，并总结出当前LLM代理的两个基本安全缺陷：缺乏稳健性和缺乏风险意识。此外，我们的发现表明，仅依赖防御提示无法解决这些安全问题，强调需要更先进、更强大的策略。我们在url{https://github.com/thu-coai/Agent-SafetyBench}发布了Agent-SafetyBench，以推动代理安全评估和改进方面的进一步研究与创新。
>
> https://arxiv.org/abs/2412.14470

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2412.14470](https://arxiv.org/abs/2412.14470)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)