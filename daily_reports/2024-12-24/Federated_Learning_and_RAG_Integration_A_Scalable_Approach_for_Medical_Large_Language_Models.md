# 联邦学习与 RAG 集成：医疗大型语言模型的可扩展之法
发布时间：2024年12月18日

`RAG`
> Federated Learning and RAG Integration: A Scalable Approach for Medical Large Language Models
>
> 本研究在联邦学习框架中融入检索增强生成（RAG）系统，对医疗领域的特定大型语言模型（LLMs）性能展开分析。借助联邦学习在保护数据隐私、支持分布式计算等方面的固有优势，本研究探索将 RAG 系统与不同客户端配置下训练的模型相整合，以优化性能。实验结果显示，基于联邦学习且与 RAG 系统集成的模型在所有评估指标上均始终优于未集成的同类模型。本研究凸显了将联邦学习与 RAG 系统相结合用于开发医疗领域特定 LLMs 的潜力，为提升文本生成能力提供了一种可扩展且能保护隐私的解决方案。
>
> https://arxiv.org/abs/2412.13720

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2412.13720](https://arxiv.org/abs/2412.13720)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)