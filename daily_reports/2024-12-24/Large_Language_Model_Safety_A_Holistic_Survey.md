# 大型语言模型的安全：全面综述
发布时间：2024年12月23日


> Large Language Model Safety: A Holistic Survey
>
> 大型语言模型（LLMs）的迅猛发展与部署，在人工智能领域开拓了新前沿，展现出自然语言理解与生成方面前所未有的能力。然而，这些模型在关键应用中的不断融合，引发了重大安全忧虑，故而有必要对其潜在风险及相关缓解策略进行深入探究。
  此次调研对 LLM 安全的现状进行了全面梳理，涵盖四大类别：价值偏差、对抗攻击的稳健性、滥用以及自主人工智能风险。除了对这四个方面的缓解办法和评估资源予以全面审视外，我们还进一步探讨了与 LLM 安全相关的四个主题：LLM 代理的安全影响、可解释性在提升 LLM 安全方面的作用、一系列人工智能公司和机构为 LLM 安全提出并遵循的技术路线图，以及针对 LLM 安全的人工智能治理，包括国际合作、政策提议和未来监管方向的探讨。
  我们的发现凸显了对 LLM 安全采取积极、多面策略的必要性，强调技术解决方案、伦理考量和强大治理框架的融合。本次调研意在为学术研究人员、行业从业者和政策制定者提供基础参考，就 LLMs 安全融入社会所涉及的挑战与机遇提供见解。最终，致力于为 LLMs 的安全、有益发展贡献力量，与利用人工智能推动社会进步和福祉的总体目标相契合。相关论文的精选列表已在 https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers 公开。
>
> https://arxiv.org/abs/2412.17686

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2412.17686](https://arxiv.org/abs/2412.17686)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)