# 大型语言模型综述：能力与局限性的深入探讨
发布时间：2025年02月09日


> A Survey on Large Language Models with some Insights on their Capabilities and Limitations
>
> # 摘要
人工智能的迅猛发展，尤其是基于Transformer架构的大型语言模型（LLMs）的崛起，彻底改变了自然语言处理的能力。这些模型在文本生成、问答、翻译和摘要等语言任务中表现出色，甚至能与人类的理解力相媲美。更令人惊叹的是，LLMs还展现出超越其核心功能的涌现能力，如常识推理、代码生成和算术等。本文深入探讨了这些能力背后的基础组件、扩展机制和架构策略，重点分析了GPT和LLaMA等模型。我们研究了数据和计算资源的指数级增长对LLM性能的影响，并探讨了扩展带来的权衡。此外，我们还展示了LLM在医疗、金融、教育和法律等领域的广泛应用，凸显了其解决特定领域挑战的潜力。本文的核心问题在于：LLMs如何在多样化任务中泛化？它们如何展示规划和推理能力？这些涌现能力能否被系统性地激发或增强？我们特别关注了LLMs中的CoT（思维链）和PoT（思维计划）能力，探讨了预训练数据对其涌现的影响。同时，我们还研究了LLM-modulo框架，这些框架通过集成外部系统，使LLMs能够处理复杂、动态的任务。通过分析这些因素，本文旨在推动关于LLMs能力和局限性的讨论，促进其在复杂环境中的负责任开发与应用。
>
> https://arxiv.org/abs/2501.04040

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2501.04040](https://arxiv.org/abs/2501.04040)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)