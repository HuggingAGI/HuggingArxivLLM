# 通过正确答案自我学习，无需额外提示，大型语言模型（LLM）展现出高效的推理能力。
`提示工程`
> 大型语言模型（LLMs）在众多任务上展现了卓越能力，但仍受限于幻觉、偏颇推理和不良内容等问题。本文提出了一种新颖的自我修正推理框架，无需依赖人工反馈或外部辅助工具，也无需手工设计提示。这一框架名为“从正确性学习”（LeCo），通过多步推理方法，避免了从错误中学习的需要，转而强调从正确的推理过程中吸取经验。它还独创性地利用生成logits来衡量每一步推理的信心水平。多项多步推理任务的实验结果显示，LeCo框架能有效提升推理效果，同时降低token的使用量。

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2403.19094/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2403.19094/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2403.19094/x3.png)

[https://wx.zsxq.com/dweb2/index/topic_detail/2855811854518511](https://wx.zsxq.com/dweb2/index/topic_detail/2855811854518511)

[https://arxiv.org/abs/2403.19094](https://arxiv.org/abs/2403.19094)