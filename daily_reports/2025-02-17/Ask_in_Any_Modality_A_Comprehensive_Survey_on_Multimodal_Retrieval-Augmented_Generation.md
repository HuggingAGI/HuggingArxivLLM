# 以任意模态提问：多模态生成增强的综合性研究综述
发布时间：2025年02月12日

`RAG`
> Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation
>
> 大型语言模型（LLMs）面临着幻觉和知识过时的挑战，这主要源于其对静态训练数据的依赖。检索增强生成（RAG）通过引入外部动态信息，有效提升了生成内容的事实准确性和时效性。近年来，多模态学习的突破性进展催生了多模态RAG，它通过整合文本、图像、音频和视频等多种模态，显著提升了生成效果。然而，跨模态对齐与推理为多模态RAG带来了独特挑战，使其与传统的单模态RAG有所不同。本综述对多模态RAG系统进行了全面而系统的分析，涵盖了数据集、评估指标、基准测试、评估方法、技术路线，以及检索、融合、增强和生成等关键环节的创新。我们详细探讨了训练策略、鲁棒性提升方法和损失函数设计，同时深入分析了多模态RAG在不同场景下的应用潜力。此外，我们还总结了当前面临的开放性挑战，并展望了未来的研究方向，以推动这一快速发展的领域不断向前。本综述为构建更强大、更可靠的AI系统奠定了基础，这些系统能够充分利用多模态动态外部知识库。相关资源可在https://github.com/llm-lab-org/Multimodal-RAG-Survey获取。
>
> https://arxiv.org/abs/2502.08826

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.08826](https://arxiv.org/abs/2502.08826)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)