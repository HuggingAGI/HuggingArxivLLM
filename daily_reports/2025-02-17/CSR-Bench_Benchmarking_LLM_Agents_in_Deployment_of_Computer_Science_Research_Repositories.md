# # CSR-Bench：评估 LLM 代理在计算机科学研究存储库部署中的基准测试
发布时间：2025年02月11日


> CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories
>
> 计算机科学研究项目的复杂性日益增加，对更高效的代码仓库部署工具的需求也愈发迫切。大型语言模型（LLM）如Anthropic Claude和Meta Llama，在计算机科学研究的多个领域都展现出了显著的进展，特别是在多种软件工程任务的自动化方面。为了评估LLM在处理研究项目中复杂代码开发任务的效能，特别是针对NLP/CV/AI/ML/DM等主题，我们推出了CSR-Bench——专为计算机科学研究项目设计的基准测试。该基准测试从准确性、效率和部署脚本质量等多个维度评估LLM，旨在探索它们在自主进行计算机科学研究方面的潜力。我们还推出了创新性框架——CSR-Agents，通过多个LLM代理实现计算机科学研究项目GitHub代码仓库的自动化部署。具体而言，该模型通过解析Markdown文件中的指令和仓库结构，自动生成并持续优化bash命令，从而完成实验环境的搭建和代码的部署以执行研究任务。来自CSR-Bench的初步结果显示，LLM代理能够显著提升仓库部署的工作流程，从而提高开发者的生产力并优化开发工作流程的管理。
>
> https://arxiv.org/abs/2502.06111

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.06111](https://arxiv.org/abs/2502.06111)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)