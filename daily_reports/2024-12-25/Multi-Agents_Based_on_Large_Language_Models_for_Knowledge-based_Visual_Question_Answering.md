# 基于大型语言模型的多智能体应用于基于知识的视觉问答
发布时间：2024年12月24日

`Agent应用`
> Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering
>
> 大型语言模型（LLMs）在基于知识的视觉问答（VQA）领域成果斐然。但现有方法仍面临挑战：无法自主运用外部工具，也难以团队协作。人类在碰到新问题时，通常清楚是否需用外部工具，比如面对熟悉的问题，往往能直接作答，而遇到陌生问题时，则倾向使用搜索引擎之类的工具。另外，人类还倾向与他人合作探讨以获取更优答案。受此启发，我们提出了多智能体投票框架。我们设计了三个基于LLM的智能体，模拟团队中不同层级的人员，并按层级分配可用工具。每个智能体给出相应答案，最后对所有智能体提供的答案进行投票，得出最终答案。在OK-VQA和A-OKVQA上的实验显示，我们的方法分别比其他基线高出2.2和1.0。
>
> https://arxiv.org/abs/2412.18351

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2412.18351](https://arxiv.org/abs/2412.18351)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)