# 在大型语言模型中探讨公平性问题：一项系统性综述
`模型安全`
> 大型语言模型（LLMs）在众多领域取得了令人瞩目的成就。尽管它们在现实世界的多种应用中展现出强大的潜力，但大多数模型在公平性方面尚显不足，有时甚至可能导致对特定群体，尤其是那些处于社会边缘的群体产生歧视性的结果。这一问题引发了对构建公平LLMs的深入研究。与传统机器学习不同，LLMs的公平性问题需要我们考虑其特有的背景、分类体系和实现方法。本文综述了近期关于公平LLMs的研究进展，首先简要介绍了LLMs的基本概念，然后分析了造成LLMs偏见的各种原因。文章还系统地探讨了LLMs公平性的定义，总结了评估LLMs偏见的指标和目前用于提升公平性的算法。同时，文中汇总了用于评估LLMs偏见的工具和数据集资源，并在最后讨论了当前研究面临的挑战和未来的研究方向。
