# DeepSeek-R1 思维之道：聊聊 LLM 推理那些事儿
发布时间：2025年04月01日


> DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning
>
> 大型推理模型（Large Reasoning Models）如 DeepSeek-R1 标志着 LLM 在处理复杂问题方式上的重大转变。与传统模型直接输出答案不同，DeepSeek-R1 会生成详细的多步骤推理链，仿佛在“思考”问题后再给出答案。这一公开的推理过程为研究模型的推理行为提供了无限可能，并开拓了“思维学”这一新领域。我们从 DeepSeek-R1 推理的基本构建块分类法入手，分析了推理链长度的影响与可控性、长篇或复杂上下文的管理、文化与安全问题，以及 DeepSeek-R1 在认知现象中的地位，如类人语言处理和世界建模。研究发现揭示了复杂的推理图景。值得注意的是，我们发现 DeepSeek-R1 存在推理的“最佳区间”，额外的推理时间可能损害模型性能。此外，我们发现 DeepSeek-R1 存在持续沉溺于先前探索的问题表述的倾向，阻碍进一步探索。与非推理版本相比，我们还发现 DeepSeek-R1 存在显著的安全漏洞，这也可能危及与安全对齐的 LLM。
>
> https://arxiv.org/abs/2504.07128

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2504.07128](https://arxiv.org/abs/2504.07128)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)