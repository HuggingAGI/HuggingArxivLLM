# S1-Bench：简单易用的基准测试，专为评估大型推理模型的直觉思维能力而设计
发布时间：2025年04月14日


> S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models
>
> 我们推出 S1-Bench，一个全新的基准测试，专注于评估大型推理模型（LRMs）在简单任务中的表现，这些任务更依赖直觉的系统1思维，而非深思熟虑的系统2推理。尽管LRMs在复杂推理任务中通过明确的思维链取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维的能力。目前，评估LRMs在需要此类能力的任务中的表现缺乏合适的基准。为此，S1-Bench提供了一系列简单、多样化且自然清晰的问题，涵盖多个领域和语言，特别设计用于评估LRMs在这些任务中的表现。我们对22个LRMs的全面评估显示了显著的效率偏低趋势，其输出平均比传统小型LLMs长15.5倍。此外，LRMs通常在早期就能识别出正确答案，但会继续不必要的思考，有些模型甚至产生大量错误。这些发现凸显了当前LRMs推理模式的僵化，并强调了实现平衡的双系统思维能力以适应任务复杂度所需的重大发展。
>
> https://arxiv.org/abs/2504.10368

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2504.10368](https://arxiv.org/abs/2504.10368)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)