# 对抗性提示评估：系统性基准测试护栏以抵御针对大型语言模型的提示输入攻击
发布时间：2025年02月21日

`模型安全`
> Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs
>
> 随着大型语言模型逐步融入日常应用，确保其稳健性和安全性变得日益关键。尤其是，这些模型可能受到被称为'越狱提示'的操控，从而产生不安全行为。随着越狱手法的不断演变，部署外部防御措施'护栏'变得尤为重要。尽管已提出诸多防御方案，但受限于训练数据的局限性，现有防护措施往往难以应对新型攻击。防御方案的碎片化也严重制约了其实际应用效果。本研究对15种防御方案进行了系统性评估，涵盖了恶意与良性数据集。结果表明，防御方案的表现因越狱手法的不同而存在显著差异。此外，基于现有评估数据，简单基线方案在某些场景下甚至能与先进方案相媲美。代码已开源，地址为：https://github.com/IBM/Adversarial-Prompt-Evaluation。
>
> https://arxiv.org/abs/2502.15427

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.15427](https://arxiv.org/abs/2502.15427)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)