# 从 RAG 到记忆：大型语言模型的持续学习新突破
发布时间：2025年02月20日

`RAG`
> From RAG to Memory: Non-Parametric Continual Learning for Large Language Models
>
> 持续获取、组织和利用知识的能力是人类智能的显著特征，也是 AI 系统发挥潜力的关键。面对大型语言模型（LLMs）在持续学习中的挑战，检索增强生成（RAG）成为引入新信息的主要方式。然而，传统 RAG 对向量检索的依赖限制了其模拟人类长期记忆动态特性的能力。

近期研究通过结合知识图谱等结构增强向量嵌入，改进了 RAG 的意义理解和联想能力。然而，这些改进在基础事实记忆任务上的表现却显著低于标准 RAG。针对这一问题，我们推出了 HippoRAG 2，一个在事实、意义理解和联想记忆任务上全面超越标准 RAG 的框架。

HippoRAG 2 在原有个性化 PageRank 算法基础上，通过更深入的段落整合和更高效的在线 LLM 使用进行了全面优化。这一创新使 RAG 系统更接近人类长期记忆的效果，在联想记忆任务上比现有模型提升了 7% 的表现，同时在事实知识和意义理解能力上也实现了突破。这项研究为 LLM 的非参数持续学习奠定了基础。

我们的代码和数据已在 https://github.com/OSU-NLP-Group/HippoRAG 上开源，欢迎交流与合作。
>
> https://arxiv.org/abs/2502.14802

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.14802](https://arxiv.org/abs/2502.14802)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)