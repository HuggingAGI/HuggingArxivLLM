# 基于多阶段质量混合的人类反馈的多智能体强化学习
发布时间：2025年03月03日

`Agent应用`
> : Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
>
> 在多智能体强化学习（MARL）中设计有效的奖励函数是一个重大挑战，常常导致在复杂且需要协调的环境中出现次优或不一致的行为。我们提出了一种名为多智能体多阶段混合质量人类反馈强化学习（$	ext{M}^3	ext{HF}$）的新框架，该框架将多阶段混合质量的人类反馈整合到 MARL 的训练过程中。通过让不同专业水平的人类提供迭代指导，$	ext{M}^3	ext{HF}$ 能够结合专家与非专家的反馈，持续优化智能体的策略。在训练过程中，我们通过暂停智能体学习来获取人类评估，利用大型语言模型解析反馈并进行合理分配，结合预定义模板和自适应权重（采用权重衰减和基于性能的调整）来更新奖励函数。这种方法不仅能够整合不同质量层次的人类见解，还显著提升了多智能体协作的可解释性和鲁棒性。在具有挑战性的环境中，实证结果表明 $	ext{M}^3	ext{HF}$ 显著优于现有最先进的方法，成功解决了 MARL 中奖励设计的复杂性，并为人类在训练过程中的广泛参与提供了可能性。
>
> https://arxiv.org/abs/2503.02077

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2503.02077](https://arxiv.org/abs/2503.02077)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)