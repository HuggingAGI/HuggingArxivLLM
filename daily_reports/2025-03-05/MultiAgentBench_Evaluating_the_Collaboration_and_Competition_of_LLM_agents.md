# MultiAgentBench：评估LLM智能体间的协作与竞争
发布时间：2025年03月03日

`Agent应用`
> MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents
>
> 大型语言模型（LLMs）作为自主代理展现出了非凡的能力，但现有基准测试或专注于单智能体任务，或局限于狭窄领域，未能捕捉多智能体协调与竞争的动态。本文引入MultiAgentBench，一个全面的基准测试框架，旨在评估基于LLMs的多智能体系统在各种交互场景中的表现。我们的框架不仅衡量任务完成情况，还通过基于里程碑的关键性能指标评估协作与竞争的质量。此外，我们评估了多种协调协议（包括星型、链式、树状和图结构）以及创新策略，如群体讨论和认知规划。值得注意的是，gpt-4o-mini在任务得分中表现最佳，在研究场景中，图结构在协调协议中表现最优，而认知规划将里程碑达成率提升了3%。代码和数据集可在https://github.com/MultiagentBench/MARBLE公开获取。
>
> https://arxiv.org/abs/2503.01935

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2503.01935](https://arxiv.org/abs/2503.01935)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)