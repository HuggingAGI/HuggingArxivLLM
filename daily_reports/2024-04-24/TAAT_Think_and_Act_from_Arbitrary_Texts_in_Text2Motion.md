# TAAT：在 Text2Motion 技术中，任意文本触发思考与行动
发布时间：2024年04月23日
`多模态大模型`
> Text2Motion 致力于根据文本描述生成人体动作。目前的数据集通常假设文本中包含动作标签，如“行走”、“弯腰”和“拾起”，但这种设定在现实应用中并不实用。本研究以更符合实际的假设为基础，即文本内容不受限制。具体而言，这些任意文本既包括由动作标签构成的动作描述（如“一个人行走并弯腰拾物”），也包括没有明确动作标签的场景描述（如“一个人发现前方地上有他的钱包”）。为解决现实场景与现有数据集之间的差异，我们对 HumanML3D 数据集中的动作文本进行了扩展，增加了更多场景文本，创建了新的 HumanML3D++ 数据集，涵盖了任意文本。在这个充满挑战的新数据集上，我们不仅对现有的顶尖方法进行了评估，还提出了一个创新的两阶段框架：首先利用大型语言模型（LLM）从任意文本中提取动作标签，然后基于这些标签生成动作。我们在多种应用场景下进行了广泛的实验，以验证所提框架在现有和新数据集上的有效性。实验结果表明，在现实条件下进行 Text2Motion 任务极具挑战性，这为未来研究方向提供了新的启示。我们将公开我们的数据集和相关代码。


[https://wx.zsxq.com/dweb2/index/topic_detail/2855284821441481](https://wx.zsxq.com/dweb2/index/topic_detail/2855284821441481)

[https://arxiv.org/abs/2404.14745](https://arxiv.org/abs/2404.14745)