# # **通过分层合成数据生成扩展指令微调的 LLM 至百万级上下文**
通过分层合成数据生成扩展指令微调的 LLM 至百万级上下文
发布时间：2025年04月17日


> Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation
>
> 大语言模型（LLMs）在长上下文推理上表现欠佳，这不仅源于计算复杂度随序列长度呈二次增长，还因长上下文数据的标注稀缺且成本高昂。目前几乎没有开源项目系统性研究长上下文数据，也缺乏公开的长上下文指令微调数据集。为解决这一问题，我们提出了一种创新的后训练合成数据生成策略，旨在高效扩展LLMs的上下文窗口，同时保持其通用任务性能。我们的方法突破了真实数据长度限制，能够支持任意长度的上下文扩展，有效缓解了长上下文数据稀缺的问题。通过逐步旋转位置嵌入（RoPE）缩放训练策略，我们证明了上下文长度达100万token的模型在RULER基准和InfiniteBench上表现优异，同时在通用语言任务中保持了稳健性能。
>
> https://arxiv.org/abs/2504.12637

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2504.12637](https://arxiv.org/abs/2504.12637)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)