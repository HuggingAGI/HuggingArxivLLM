# 每个计算都至关重要：无需高端 GPU，轻松扩展 3000 亿参数专家混合 LING 大语言模型
发布时间：2025年03月06日


> Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs
>
> 本技术报告聚焦于大规模专家混合模型（MoE）训练中的挑战，致力于突破成本低效与资源限制的瓶颈。我们推出了两种不同规模的MoE大型语言模型（LLMs）：Ling-Lite（168亿参数，27.5亿激活参数）与Ling-Plus（2900亿参数，288亿激活参数），中文名为“百灵”。这两个模型的性能均与行业领先基准相当。报告为资源受限环境下的AI开发提供实用见解，推动技术的可扩展与可持续发展。我们提出了三项创新方法：优化模型架构与训练流程、完善训练异常处理、提升模型评估效率。借助知识图谱生成的高质量数据，我们的模型在工具使用能力上表现更优。实验结果显示，3000亿参数的MoE LLM可在低性能设备上训练，性能与同规模模型相当，且计算成本降低约20%。模型访问地址：https://huggingface.co/inclusionAI。
>
> https://arxiv.org/abs/2503.05139

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2503.05139](https://arxiv.org/abs/2503.05139)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)