# 多模态环境下的检索增强生成基准测试
发布时间：2025年02月24日

`RAG`
> Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts
>
> 本文提出Multi-Modal Retrieval-Augmented Generation (M²RAG)基准测试，旨在评估多模态大语言模型（MLLMs）在多模态检索文档中提取知识的能力。该基准包含图像描述生成、多模态问答、多模态事实核查及图像重新排序四大任务，均在开放领域环境下运行，要求RAG模型从多模态文档中检索相关信息并作为上下文输入。为提升MLLMs的上下文利用能力，我们还提出了Multi-Modal Retrieval-Augmented Instruction Tuning（MM-RAIT），一种在多模态上下文中优化MLLMs的指令微调方法。实验表明，MM-RAIT显著提升了RAG系统的性能，使其能够有效学习多模态上下文。所有数据和代码均可在GitHub上获取。
>
> https://arxiv.org/abs/2502.17297

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.17297](https://arxiv.org/abs/2502.17297)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)