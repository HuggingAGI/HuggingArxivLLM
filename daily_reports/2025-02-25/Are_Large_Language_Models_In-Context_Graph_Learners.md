# 大型语言模型是否具备上下文图学习能力？
发布时间：2025年02月19日


> Are Large Language Models In-Context Graph Learners?
>
> 大型语言模型（LLMs）在处理语言或图像等非结构化输入时展现了卓越的上下文推理能力。然而，由于缺乏对非欧几里得结构的理解，LLMs在处理图等结构化数据时表现挣扎，其性能显著落后于图神经网络（GNNs）。本文提出了一种全新的视角：将图数据的学习过程视为检索辅助生成（RAG）过程，其中节点或边等特定实例作为查询，图本身则作为检索到的上下文。基于这一见解，我们提出了一系列RAG框架，旨在提升LLMs在图学习任务中的上下文学习能力。实验结果表明，我们的RAG框架显著提升了LLMs在基于图的任务中的性能，尤其是在必须使用预训练好的LLMs且无法修改或只能通过API访问的情况下。
>
> https://arxiv.org/abs/2502.13562

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2502.13562](https://arxiv.org/abs/2502.13562)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)