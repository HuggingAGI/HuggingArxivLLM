# # 遥感基础模型综述：从视觉迈向多模态
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年03月27日

`其他` `地理信息`

> A Survey on Remote Sensing Foundation Models: From Vision to Multimodality

# 摘要

> 视觉和多模态模型的快速发展显著提升了遥感数据分析的能力。通过整合光学、雷达、LiDAR 图像等多源数据，并结合文本和地理信息，这些模型实现了对遥感数据的全面分析与理解。多模态结合在目标检测、土地覆盖分类和变化检测等任务中表现出色，但遥感数据的复杂性仍带来诸多挑战。尽管如此，数据类型的多样性、大规模标注数据集的需求，以及多模态融合技术的复杂性，仍是模型部署的主要障碍。此外，训练和微调多模态模型的高计算需求进一步限制了其实际应用。本文全面综述了用于遥感的视觉和多模态基础模型的最新进展，聚焦于模型架构、训练方法、数据集和应用场景。我们探讨了数据对齐、跨模态迁移学习和可扩展性等关键挑战，并指出了克服这些限制的新兴研究方向。本文旨在清晰呈现遥感基础模型的当前发展状况，并激发未来研究，推动这些模型在实际应用中的更广泛应用。资源列表可在 https://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models 查看。

> The rapid advancement of remote sensing foundation models, particularly vision and multimodal models, has significantly enhanced the capabilities of intelligent geospatial data interpretation. These models combine various data modalities, such as optical, radar, and LiDAR imagery, with textual and geographic information, enabling more comprehensive analysis and understanding of remote sensing data. The integration of multiple modalities allows for improved performance in tasks like object detection, land cover classification, and change detection, which are often challenged by the complex and heterogeneous nature of remote sensing data. However, despite these advancements, several challenges remain. The diversity in data types, the need for large-scale annotated datasets, and the complexity of multimodal fusion techniques pose significant obstacles to the effective deployment of these models. Moreover, the computational demands of training and fine-tuning multimodal models require significant resources, further complicating their practical application in remote sensing image interpretation tasks. This paper provides a comprehensive review of the state-of-the-art in vision and multimodal foundation models for remote sensing, focusing on their architecture, training methods, datasets and application scenarios. We discuss the key challenges these models face, such as data alignment, cross-modal transfer learning, and scalability, while also identifying emerging research directions aimed at overcoming these limitations. Our goal is to provide a clear understanding of the current landscape of remote sensing foundation models and inspire future research that can push the boundaries of what these models can achieve in real-world applications. The list of resources collected by the paper can be found in the https://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.

[Arxiv](https://arxiv.org/abs/2503.22081)