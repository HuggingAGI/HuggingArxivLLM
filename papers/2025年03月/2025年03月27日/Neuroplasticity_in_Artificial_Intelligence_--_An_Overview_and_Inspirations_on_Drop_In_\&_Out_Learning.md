# 人工智能中的神经可塑性——Drop-In-Out学习的概述与启发

发布时间：2025年03月27日

`其他` `人工智能` `生物学`

> Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In \& Out Learning

# 摘要

> 人工智能（AI）随着深度神经网络（DNNs）的兴起，在性能和应用范围上取得了显著提升。最初受人类神经元及其连接启发的神经网络（NNs），已成为众多先进AI架构的基础。然而，人类大脑中至关重要的神经发生和神经可塑性，以及更为普遍的神经凋亡过程，在DNN设计中却鲜受关注。相反，当前AI研究主要致力于构建如大型语言模型等先进框架，这些模型在训练与推理过程中保持神经连接的静态结构。基于此，我们探讨神经发生、神经凋亡和神经可塑性如何启发未来的AI发展。具体而言，我们研究人工神经网络中类似的过程，引入“神经发生滴入”（dropin）的概念，并重新审视“神经凋亡滴出”（dropout）和结构剪枝。此外，我们建议将神经可塑性与这两种机制结合，为未来“终身学习”环境中的大型神经网络提供生物学启发。最后，我们呼吁在这一跨学科领域投入更多研究，并指出了未来探索的有前景方向。

> Artificial Intelligence (AI) has achieved new levels of performance and spread in public usage with the rise of deep neural networks (DNNs). Initially inspired by human neurons and their connections, NNs have become the foundation of AI models for many advanced architectures. However, some of the most integral processes in the human brain, particularly neurogenesis and neuroplasticity in addition to the more spread neuroapoptosis have largely been ignored in DNN architecture design. Instead, contemporary AI development predominantly focuses on constructing advanced frameworks, such as large language models, which retain a static structure of neural connections during training and inference. In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances. Specifically, we examine analogous activities in artificial NNs, introducing the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and structural pruning for neuroapoptosis. We additionally suggest neuroplasticity combining the two for future large NNs in ``life-long learning'' settings following the biological inspiration. We conclude by advocating for greater research efforts in this interdisciplinary domain and identifying promising directions for future exploration.

[Arxiv](https://arxiv.org/abs/2503.21419)