# 小样本医疗数据集上的 LLM 微调：文本分类与规范化在心脏病报告和出院记录数据集中的效果研究

发布时间：2025年03月27日

`LLM应用`

> Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records

# 摘要

> 我们研究了在小规模医疗数据集上微调大型语言模型（LLMs）在文本分类和命名实体识别任务中的有效性。通过使用德国心脏病报告数据集和i2b2吸烟挑战数据集，我们发现即使在有限的训练数据下，对小规模LLM进行本地微调也能显著提升性能，达到与大规模模型相当的水平。实验结果表明，微调在两个任务上均表现优异，尤其是仅需200-300个训练样本就能取得显著的性能提升。这项研究充分展示了任务特定微调在自动化临床工作流程和高效提取结构化数据方面的巨大潜力，特别是在处理非结构化医疗文本时。

> We investigate the effectiveness of fine-tuning large language models (LLMs) on small medical datasets for text classification and named entity recognition tasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge dataset, we demonstrate that fine-tuning small LLMs locally on limited training data can improve performance achieving comparable results to larger models. Our experiments show that fine-tuning improves performance on both tasks, with notable gains observed with as few as 200-300 training examples. Overall, the study highlights the potential of task-specific fine-tuning of LLMs for automating clinical workflows and efficiently extracting structured data from unstructured medical text.

[Arxiv](https://arxiv.org/abs/2503.21349)