# 挑战推理边界：面向大型语言模型的数学奥林匹克水平基准测试

发布时间：2025年03月27日

`LLM应用`

> Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models

# 摘要

> 近年来，大型推理模型的快速发展使得现有数学推理评估基准已无法满足需求，亟需更具挑战性和严格性的评估框架。为此，我们推出了OlymMATH——一个全新的奥林匹克级别数学基准测试，专注于严格考验LLMs的复杂推理能力。OlymMATH包含200个精心策划的问题，每个问题均经过人工验证，并提供英文和中文双语版本。这些问题系统地分为两个难度等级：AIME级别问题（简单）作为数学推理评估的基准，以及更具挑战性的问题（困难）以突破当前最先进模型的边界。在我们的基准测试中，这些问题涵盖了四个核心数学领域，并为每个问题提供了可验证的数值解，以实现客观、基于规则的评估。实证结果显示，OlymMATH极具挑战性，包括DeepSeek-R1和OpenAI的o3-mini在内的最先进模型在困难子集上的准确度表现有限。此外，该基准测试支持全面的双语数学推理能力评估——这一关键维度在主流数学推理基准测试中仍 largely 未得到充分关注。我们已在STILL项目中发布了OlymMATH基准测试：https://github.com/RUCAIBox/Slow_Thinking_with_LLMs。

> In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.

[Arxiv](https://arxiv.org/abs/2503.21380)