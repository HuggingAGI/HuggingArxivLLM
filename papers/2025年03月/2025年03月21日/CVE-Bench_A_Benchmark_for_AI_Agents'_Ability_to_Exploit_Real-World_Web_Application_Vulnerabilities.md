# CVE-Bench：评估AI代理利用真实世界Web漏洞能力的基准测试

发布时间：2025年03月21日

`LLM应用` `网络安全` `漏洞利用`

> CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities

# 摘要

> LLM代理自主执行网络攻击的能力日益增强，对现有应用构成严重威胁。这一风险凸显了迫切需要一个现实世界基准来评估LLM代理利用网络应用漏洞的能力。然而，现有基准存在局限，要么局限于抽象的夺旗竞赛，要么缺乏全面覆盖。构建此类基准需要专业技能来重现漏洞利用，并系统性评估不可预测威胁。为应对这一挑战，我们推出了CVE-Bench，这是一个基于关键严重性常见漏洞和暴露（CVE）的现实世界网络安全基准测试。在CVE-Bench中，我们设计了一个沙盒框架，使LLM代理能够在模拟现实条件的场景中利用易受攻击的网络应用程序，同时还能有效评估其漏洞利用效果。我们的评估显示，最先进的代理框架能够解决高达13%的漏洞。

> Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities.

[Arxiv](https://arxiv.org/abs/2503.17332)