# Kaiwu：一个多模态操作数据集与框架，专为机器人学习与人机交互设计

发布时间：2025年03月07日

`其他` `机器人` `多模态数据`

> Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction

# 摘要

> 前沿的机器人学习技术，包括基础模型和人类模仿学习，都需要大规模、高质量的数据集支持，这成为通用智能机器人领域的重要瓶颈。针对复杂装配场景中真实世界同步多模态数据缺失的问题，特别是动态信息及其细粒度标注的不足，本文提出了开物多模态数据集。该数据集整合了人类、环境和机器人数据采集框架，涵盖20名受试者和30个交互对象，共生成11,664个综合动作实例。每个演示都记录了手部动作、操作压力、装配声音、多视角视频、高精度运动捕捉信息、带有第一人称视角视频的注视点以及肌电图信号。基于绝对时间戳的细粒度多级标注和语义分割标注也已完成。开物数据集旨在助力机器人学习、灵巧操作、人类意图探究及人机协作研究。

> Cutting-edge robot learning techniques including foundation models and imitation learning from humans all pose huge demands on large-scale and high-quality datasets which constitute one of the bottleneck in the general intelligent robot fields. This paper presents the Kaiwu multimodal dataset to address the missing real-world synchronized multimodal data problems in the sophisticated assembling scenario,especially with dynamics information and its fine-grained labelling. The dataset first provides an integration of human,environment and robot data collection framework with 20 subjects and 30 interaction objects resulting in totally 11,664 instances of integrated actions. For each of the demonstration,hand motions,operation pressures,sounds of the assembling process,multi-view videos, high-precision motion capture information,eye gaze with first-person videos,electromyography signals are all recorded. Fine-grained multi-level annotation based on absolute timestamp,and semantic segmentation labelling are performed. Kaiwu dataset aims to facilitate robot learning,dexterous manipulation,human intention investigation and human-robot collaboration research.

[Arxiv](https://arxiv.org/abs/2503.05231)