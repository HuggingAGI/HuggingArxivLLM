# 稀疏自动编码器综述：深入解析大型语言模型的内在机制

发布时间：2025年03月07日

`LLM理论` `人工智能`

> A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models

# 摘要

> 大型语言模型（LLMs）彻底革新了自然语言处理领域，但其内部机制依然神秘。近期，机制可解释性成为理解LLMs工作原理的热门研究方向。在众多方法中，稀疏自动编码器（SAEs）因其能够将LLMs复杂叠加的特征分解为更易理解的组件而备受关注。本文全面探讨了SAEs在解释和理解LLMs中的潜力。我们系统性地介绍了专门针对LLM分析的SAE原则、架构和应用，涵盖理论基础、实现策略以及稀疏机制的最新进展。同时，我们还深入探讨了如何利用SAEs来揭示LLMs的内部运作机制、引导模型行为向期望方向发展，以及为未来模型开发更透明的训练方法。尽管在SAE的实现和扩展方面仍面临挑战，但它们为理解大型语言模型的内部机制提供了有力工具。

> Large Language Models (LLMs) have revolutionized natural language processing, yet their internal mechanisms remain largely opaque. Recently, mechanistic interpretability has attracted significant attention from the research community as a means to understand the inner workings of LLMs. Among various mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have emerged as a particularly promising method due to their ability to disentangle the complex, superimposed features within LLMs into more interpretable components. This paper presents a comprehensive examination of SAEs as a promising approach to interpreting and understanding LLMs. We provide a systematic overview of SAE principles, architectures, and applications specifically tailored for LLM analysis, covering theoretical foundations, implementation strategies, and recent developments in sparsity mechanisms. We also explore how SAEs can be leveraged to explain the internal workings of LLMs, steer model behaviors in desired directions, and develop more transparent training methodologies for future models. Despite the challenges that remain around SAE implementation and scaling, they continue to provide valuable tools for understanding the internal mechanisms of large language models.

[Arxiv](https://arxiv.org/abs/2503.05613)