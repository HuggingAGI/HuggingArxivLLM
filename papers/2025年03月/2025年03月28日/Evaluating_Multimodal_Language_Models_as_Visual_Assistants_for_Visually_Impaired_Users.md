# 评估多模态语言模型作为视障用户视觉助手的潜力

发布时间：2025年03月28日

`LLM应用` `无障碍技术` `视觉辅助技术`

> Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users

# 摘要

> 本研究探讨了多模态大型语言模型（MLLMs）作为视觉障碍人士辅助技术的有效性。通过用户调查，我们识别出技术采用模式及用户面临的挑战。尽管MLLMs的采用率较高，但研究发现，视觉障碍者在上下文理解、文化敏感性及复杂场景识别方面存在显著担忧，尤其当他们完全依赖此类模型进行视觉解析时。基于研究结果，我们设计了五个以用户为中心的任务，涵盖图像与视频输入，其中包括创新性的光学盲文识别任务。对十二款MLLMs的系统评估显示，需进一步发展以克服文化背景、多语言支持、盲文理解、辅助对象识别及幻觉生成等方面的限制。本研究为多模态AI在无障碍技术领域的未来发展提供了重要见解，强调了开发更具包容性、可靠性和可信度的视觉辅助技术的迫切需求。

> This paper explores the effectiveness of Multimodal Large Language models (MLLMs) as assistive technologies for visually impaired individuals. We conduct a user survey to identify adoption patterns and key challenges users face with such technologies. Despite a high adoption rate of these models, our findings highlight concerns related to contextual understanding, cultural sensitivity, and complex scene understanding, particularly for individuals who may rely solely on them for visual interpretation. Informed by these results, we collate five user-centred tasks with image and video inputs, including a novel task on Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals that further advancements are necessary to overcome limitations related to cultural context, multilingual support, Braille reading comprehension, assistive object recognition, and hallucinations. This work provides critical insights into the future direction of multimodal AI for accessibility, underscoring the need for more inclusive, robust, and trustworthy visual assistance technologies.

[Arxiv](https://arxiv.org/abs/2503.22610)