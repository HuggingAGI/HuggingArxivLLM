# 基于 LLM 的多轮对话代理研究综述

发布时间：2025年03月28日

`LLM应用` `人机交互` `对话系统`

> Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey

# 摘要

> 本次调研聚焦于大型语言模型（LLM）智能体在多轮对话场景下的评估方法。我们基于PRISMA框架，系统性地梳理了近250篇学术文献，涵盖了各出版平台的前沿进展，为后续分析奠定了坚实基础。本研究通过构建两个相互关联的分类体系，提供了一种结构化的研究路径：其一定义了\emph{评估内容}，其二阐述了\emph{评估方法}。第一个分类体系明确了多轮对话中LLM智能体的关键组成要素及其评估维度，包括任务完成度、响应质量、用户体验、记忆与上下文保持能力，以及规划与工具整合能力。这些要素确保了对话智能体的性能能够得到全面且有意义的评估。第二个分类体系则聚焦于评估方法论，将评估手段划分为基于标注的评估、自动化指标、结合人工评估与定量指标的混合策略，以及利用LLM的自评估方法。该框架不仅涵盖了源自语言理解的传统指标（如BLEU和ROUGE分数），还纳入了反映多轮对话动态交互特性的先进评估技术。

> This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \emph{what to evaluate} and another that explains \emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.

[Arxiv](https://arxiv.org/abs/2503.22458)