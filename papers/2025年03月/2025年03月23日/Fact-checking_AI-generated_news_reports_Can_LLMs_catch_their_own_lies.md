# AI 生成新闻的把关人：LLMs 能否揪出自己的“谎言”？

发布时间：2025年03月23日

`LLM应用` `事实核查`

> Fact-checking AI-generated news reports: Can LLMs catch their own lies?

# 摘要

> 本文研究了大型语言模型（LLMs）对自己或其它LLMs生成的新闻报道中声明真实性的评估能力。我们发现，LLMs在评估国内外新闻时表现优于本地新闻，在验证静态信息和真实声明时也更具优势。这种差异可能源于训练数据中对前一类声明的更好覆盖。此外，通过检索增强生成（RAG）整合搜索引擎结果，虽然能显著减少无法评估的声明数量，但也可能因低质量检索结果而增加错误评估。未来研究应优先提升检索信息的精准度和相关性，并在动态事件和地方新闻的事实核查中引入人机协作机制，以确保更高准确性和可靠性。

> In this paper, we evaluate the ability of Large Language Models (LLMs) to assess the veracity of claims in ''news reports'' generated by themselves or other LLMs. Our goal is to determine whether LLMs can effectively fact-check their own content, using methods similar to those used to verify claims made by humans. Our findings indicate that LLMs are more effective at assessing claims in national or international news stories than in local news stories, better at evaluating static information than dynamic information, and better at verifying true claims compared to false ones. We hypothesize that this disparity arises because the former types of claims are better represented in the training data. Additionally, we find that incorporating retrieved results from a search engine in a Retrieval-Augmented Generation (RAG) setting significantly reduces the number of claims an LLM cannot assess. However, this approach also increases the occurrence of incorrect assessments, partly due to irrelevant or low-quality search results. This diagnostic study highlights the need for future research on fact-checking machine-generated reports to prioritize improving the precision and relevance of retrieved information to better support fact-checking efforts. Furthermore, claims about dynamic events and local news may require human-in-the-loop fact-checking systems to ensure accuracy and reliability.

[Arxiv](https://arxiv.org/abs/2503.18293)