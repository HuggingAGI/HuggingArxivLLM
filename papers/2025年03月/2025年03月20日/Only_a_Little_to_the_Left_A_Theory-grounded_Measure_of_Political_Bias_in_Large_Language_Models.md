# 左倾少许：大型语言模型中的政治偏见度量研究

发布时间：2025年03月20日

`LLM应用` `政治科学` `社会学`

> Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models

# 摘要

> 基于提示的语言模型（如GPT4和LLaMa）在模拟智能体、信息搜索和内容分析等多种场景中得到了广泛应用。然而，这些模型中的政治偏见可能会影响其性能。尽管一些研究者尝试通过政治罗盘测试（PCT）等基于调查的评估方法来研究模型的政治倾向，但结果往往存在分歧。此外，PCT并非科学上有效的工具。本研究基于政治科学理论，结合调查设计原则，测试了多种输入提示，分析了11个开源和商业模型（包括经过指令微调和未经过微调的模型）的88,110个响应，揭示了不同提示变体下的政治偏见分布。结果显示，PCT可能夸大了部分模型（如GPT3.5）的偏见，但总体而言，经过指令微调的模型通常表现出更左的政治倾向。

> Prompt-based language models like GPT4 and LLaMa have been used for a wide variety of use cases such as simulating agents, searching for information, or for content analysis. For all of these applications and others, political biases in these models can affect their performance. Several researchers have attempted to study political bias in language models using evaluation suites based on surveys, such as the Political Compass Test (PCT), often finding a particular leaning favored by these models. However, there is some variation in the exact prompting techniques, leading to diverging findings and most research relies on constrained-answer settings to extract model responses. Moreover, the Political Compass Test is not a scientifically valid survey instrument. In this work, we contribute a political bias measured informed by political science theory, building on survey design principles to test a wide variety of input prompts, while taking into account prompt sensitivity. We then prompt 11 different open and commercial models, differentiating between instruction-tuned and non-instruction-tuned models, and automatically classify their political stances from 88,110 responses. Leveraging this dataset, we compute political bias profiles across different prompt variations and find that while PCT exaggerates bias in certain models like GPT3.5, measures of political bias are often unstable, but generally more left-leaning for instruction-tuned models.

[Arxiv](https://arxiv.org/abs/2503.16148)