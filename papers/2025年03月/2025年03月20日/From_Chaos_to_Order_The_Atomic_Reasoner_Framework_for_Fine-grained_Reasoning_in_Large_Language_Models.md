# 从混沌到秩序：大型语言模型中细粒度推理的原子推理框架

发布时间：2025年03月20日

`LLM理论` `认知科学` `人工智能`

> From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models

# 摘要

> 尽管大型语言模型 (LLMs) 取得了显著进展，但其在逻辑 ``慢思考'' 推理方面的能力仍是研究前沿。当前推理扩展范式面临两大挑战：零散的思维流程影响逻辑连贯性，以及计算复杂度随搜索空间维度激增。为突破这些限制，我们提出了 	extbf{Atomic Reasoner} (	extbf{AR})，一种通过系统性原子级操作实现精细推理的认知策略。AR 将推理过程分解为原子认知单元，借助认知路由机制动态构建推理表示并编排推理路径。这种方法实现了逐步、结构化的认知，不仅确保逻辑连贯，更显著降低了认知负荷，成功模拟了人类深度思考的认知模式。实验结果表明，AR 在无需穷举搜索的情况下展现出卓越推理能力，尤其在语言逻辑谜题中表现突出。这些发现证实了 AR 在增强 LLMs 进行稳健、长序列逻辑推理和深思熟虑方面的能力。

> Recent advances in large language models (LLMs) have shown remarkable progress, yet their capacity for logical ``slow-thinking'' reasoning persists as a critical research frontier. Current inference scaling paradigms suffer from two fundamental constraints: fragmented thought flows compromising logical coherence, and intensively computational complexity that escalates with search space dimensions. To overcome these limitations, we present \textbf{Atomic Reasoner} (\textbf{AR}), a cognitive inference strategy that enables fine-grained reasoning through systematic atomic-level operations. AR decomposes the reasoning process into atomic cognitive units, employing a cognitive routing mechanism to dynamically construct reasoning representations and orchestrate inference pathways. This systematic methodology implements stepwise, structured cognition, which ensures logical coherence while significantly reducing cognitive load, effectively simulating the cognitive patterns observed in human deep thinking processes. Extensive experimental results demonstrate AR's superior reasoning capabilities without the computational burden of exhaustive solution searches, particularly excelling in linguistic logic puzzles. These findings substantiate AR's effectiveness in enhancing LLMs' capacity for robust, long-sequence logical reasoning and deliberation.

[Arxiv](https://arxiv.org/abs/2503.15944)