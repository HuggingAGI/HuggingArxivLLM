# FutureGen——基于LLM-RAG方法探索科学论文的未来研究方向

发布时间：2025年03月20日

`LLM应用

理由：这篇论文主要探讨了如何利用大型语言模型（LLMs）和RAG方法来生成科学文章的未来工作建议，并分析这些趋势的演变。研究中结合了LLM反馈机制，优化了生成流程，并提出了“LLM即裁判”的评估方法。虽然提到了RAG方法，但论文的核心目标是应用LLMs来生成和评估未来工作建议，属于LLM的应用领域。` `学术研究` `学术出版`

> FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article

# 摘要

> 科学文章的未来工作部分通过识别现有研究的空白和局限性，为潜在研究方向提供了指引。这一部分不仅是早期职业研究人员寻找未开发领域的宝贵资源，也为资深研究人员提供了新项目或合作的灵感。在本研究中，我们从科学文章的关键部分和相关论文中生成未来工作建议，并分析了这些趋势的演变。我们尝试了多种大型语言模型（LLMs），并结合增强生成（RAG）方法优化了生成流程。通过引入LLM反馈机制，我们提升了生成内容的质量，并提出了一种“LLM即裁判”的评估方法。实验结果表明，结合LLM反馈的RAG方法在定性和定量指标上均优于其他方法。此外，我们还进行了人工评估，以测试LLM作为提取器和裁判的能力。本项目的代码和数据集已开源，代码地址为：HuggingFace

> The future work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from key sections of a scientific article alongside related papers and analyze how the trends have evolved. We experimented with various Large Language Models (LLMs) and integrated Retrieval-Augmented Generation (RAG) to enhance the generation process. We incorporate a LLM feedback mechanism to improve the quality of the generated content and propose an LLM-as-a-judge approach for evaluation. Our results demonstrated that the RAG-based approach with LLM feedback outperforms other methods evaluated through qualitative and quantitative metrics. Moreover, we conduct a human evaluation to assess the LLM as an extractor and judge. The code and dataset for this project are here, code: HuggingFace

[Arxiv](https://arxiv.org/abs/2503.16561)