# 跨模态状态空间图推理的结构化摘要方法

发布时间：2025年03月26日

`其他` `图神经网络`

> Cross-Modal State-Space Graph Reasoning for Structured Summarization

# 摘要

> 从大规模多模态数据中提取简洁且有意义的摘要对视频分析、医疗报告等众多应用至关重要。然而，传统跨模态摘要方法往往面临高计算开销和有限的可解释性。为此，我们提出了一种结合状态空间模型与图结构信息传递的	extit{跨模态状态空间图推理}（	extbf{CSS-GR}）框架，灵感源自于高效状态空间模型的先前工作。与现有依赖纯序列模型的方法不同，我们的方法构建了一个捕获跨模态和单模态关系的图结构，从而能够对文本和视觉流进行更全面的推理。实验结果表明，与现有方法相比，我们的方法在保持计算效率的同时，显著提升了摘要质量和可解释性，并在标准多模态摘要基准上进行了验证。此外，我们还进行了彻底的消融实验，以突出每个组件的贡献。

> The ability to extract compact, meaningful summaries from large-scale and multimodal data is critical for numerous applications, ranging from video analytics to medical reports. Prior methods in cross-modal summarization have often suffered from high computational overheads and limited interpretability. In this paper, we propose a \textit{Cross-Modal State-Space Graph Reasoning} (\textbf{CSS-GR}) framework that incorporates a state-space model with graph-based message passing, inspired by prior work on efficient state-space models. Unlike existing approaches relying on purely sequential models, our method constructs a graph that captures inter- and intra-modal relationships, allowing more holistic reasoning over both textual and visual streams. We demonstrate that our approach significantly improves summarization quality and interpretability while maintaining computational efficiency, as validated on standard multimodal summarization benchmarks. We also provide a thorough ablation study to highlight the contributions of each component.

[Arxiv](https://arxiv.org/abs/2503.20988)