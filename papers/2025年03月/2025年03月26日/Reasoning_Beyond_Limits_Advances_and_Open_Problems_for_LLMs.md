# # **Reasoning Beyond Limits: Advances and Open Problems for LLMs**  
# **LLMs的推理突破：进展与挑战**

发布时间：2025年03月26日

`LLM理论` `生成式推理`

> Reasoning Beyond Limits: Advances and Open Problems for LLMs

# 摘要

> 生成式推理的突破重新定义了大语言模型（LLMs）解决复杂问题的方式，使其在生成连贯的多步骤思考过程中能够动态检索和优化信息。推理时扩展、强化学习、监督微调和蒸馏等技术的成功应用，显著提升了DeepSeek-R1、OpenAI的o1 & o3、GPT-4o、Qwen-32B以及各种Llama变体等模型的推理能力。本文对2023年至2025年间发布的27款顶级LLM模型进行了全面分析，包括Mistral AI Small 3 24B、DeepSeek-R1、Search-o1、QwQ-32B和phi-4等。我们还对训练方法进行了详尽概述，涵盖通用训练方法、专家混合（MoE）与架构创新、增强型检索生成（RAG）、链式思维与自我改进技术，以及推理时计算扩展、蒸馏和强化学习（RL）方法。最后，我们探讨了提升LLM能力的关键挑战，包括在无人监督下改进多步推理、克服链式任务的限制、平衡结构化提示与灵活性，以及增强长上下文检索和外部工具集成。


> Recent generative reasoning breakthroughs have transformed how large language models (LLMs) tackle complex problems by dynamically retrieving and refining information while generating coherent, multi-step thought processes. Techniques such as inference-time scaling, reinforcement learning, supervised fine-tuning, and distillation have been successfully applied to models like DeepSeek-R1, OpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in enhanced reasoning capabilities. In this paper, we provide a comprehensive analysis of the top 27 LLM models released between 2023 and 2025 (including models such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and phi-4). Then, we present an extensive overview of training methodologies that spans general training approaches, mixture-of-experts (MoE) and architectural innovations, retrieval-augmented generation (RAG), chain-of-thought and self-improvement techniques, as well as test-time compute scaling, distillation, and reinforcement learning (RL) methods. Finally, we discuss the key challenges in advancing LLM capabilities, including improving multi-step reasoning without human supervision, overcoming limitations in chained tasks, balancing structured prompts with flexibility, and enhancing long-context retrieval and external tool integration.

[Arxiv](https://arxiv.org/abs/2503.22732)