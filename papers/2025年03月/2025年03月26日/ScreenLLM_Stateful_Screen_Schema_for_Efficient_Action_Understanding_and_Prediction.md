# ScreenLLM: 基于带状态屏幕架构的高效动作理解与预测

发布时间：2025年03月26日

`LLM应用` `人工智能` `图形用户界面`

> ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Prediction

# 摘要

> # 摘要
图形用户界面（GUI）代理是能够解释和生成动作的自主系统，实现智能用户协助与自动化。这些代理的有效训练面临独特挑战，包括监督信号稀疏性、大数据集的可扩展性以及对用户理解的细微需求。我们提出了一种状态屏幕架构，这是一种高效表示 GUI 交互的方法，能够捕捉用户随时间的关键动作和意图。在此基础上，我们引入了 ScreenLLM，一套专为高级 UI 理解和动作预测设计的多模态大型语言模型（MLLMs）。在开源和专有模型上的广泛实验表明，ScreenLLM 能够准确建模用户行为并预测动作。我们的工作为构建可扩展、健壮且智能的 GUI 代理奠定了基础，这些代理能够提升用户在各种软件环境中的交互体验。


> Graphical User Interface (GUI) agents are autonomous systems that interpret and generate actions, enabling intelligent user assistance and automation. Effective training of these agent presents unique challenges, such as sparsity in supervision signals, scalability for large datasets, and the need for nuanced user understanding. We propose stateful screen schema, an efficient representation of GUI interactions that captures key user actions and intentions over time. Building on this foundation, we introduce ScreenLLM, a set of multimodal large language models (MLLMs) tailored for advanced UI understanding and action prediction. Extensive experiments on both open-source and proprietary models show that ScreenLLM accurately models user behavior and predicts actions. Our work lays the foundation for scalable, robust, and intelligent GUI agents that enhance user interaction in diverse software environments.

[Arxiv](https://arxiv.org/abs/2503.20978)