# # **UrbanVideo-Bench**：基于城市空间视频数据，研究视觉语言模型在具身智能领域的基准测试方法

发布时间：2025年03月08日

`LLM应用` `城市科学` `城市规划`

> UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces

# 摘要

> 大型多模态模型虽然展现出卓越的智能，但它们在开放城市三维空间中的具身认知能力仍有待探索。我们引入了一个基准测试，旨在评估视频-LLMs是否能像人类一样自然地处理连续的第一人称视觉观察，从而实现记忆、感知、推理和导航功能。我们手动控制无人机，从真实城市和模拟环境中收集了3D具身运动视频数据，最终收集到1,500个视频片段。随后，我们设计了一个流程，生成了5,200个多项选择题。对17个广泛使用的视频-LLMs的评估揭示了当前在城市具身认知方面的局限性。相关性分析显示，因果推理与记忆、感知和导航有很强的相关性，而反事实和联想推理能力与其他任务的相关性较低。我们还通过微调验证了在城市具身认知中从模拟到现实迁移的潜力。

> Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation. We have manually control drones to collect 3D embodied motion video data from real-world cities and simulated environments, resulting in 1.5k video clips. Then we design a pipeline to generate 5.2k multiple-choice questions. Evaluations of 17 widely-used Video-LLMs reveal current limitations in urban embodied cognition. Correlation analysis provides insight into the relationships between different tasks, showing that causal reasoning has a strong correlation with recall, perception, and navigation, while the abilities for counterfactual and associative reasoning exhibit lower correlation with other tasks. We also validate the potential for Sim-to-Real transfer in urban embodiment through fine-tuning.

[Arxiv](https://arxiv.org/abs/2503.06157)