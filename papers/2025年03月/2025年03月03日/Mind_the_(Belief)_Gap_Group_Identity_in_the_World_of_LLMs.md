# 警惕信仰鸿沟：大型语言模型中的群体认同

发布时间：2025年03月03日

`LLM应用

摘要中提到论文探讨了大型语言模型在多智能体系统中的应用，特别是在模拟群体心理学中的信念一致性方面。论文提出了一种多智能体框架，并研究了LLMs在信念一致性上的表现及其对错误信息传播和学习的影响，提出了缓解策略。因此，这篇论文属于LLM应用类别。` `多智能体系统` `社会心理学`

> Mind the (Belief) Gap: Group Identity in the World of LLMs

# 摘要

> 社会偏见和社会信念驱动的行为显著影响大型语言模型（LLMs）在多个任务上的决策。随着LLMs在多智能体系统中被越来越多地用于社会模拟，它们在模拟基本群体心理特征方面的能力虽关键却仍待深入探索。本研究提出了一种多智能体框架，用于模拟信念一致性——一种在塑造社会互动和偏好方面起关键作用的经典群体心理学理论。研究发现，LLMs在各类情境下展现出比人类更强的信念一致性。我们进一步探讨了这一特性在两个下游任务中的影响：（1）错误信息传播和（2）LLM学习，发现信念一致性使错误信息传播加剧并阻碍学习。为缓解这些负面影响，我们提出了三种策略，分别受到以下启发：（1）接触假设，（2）准确性提示和（3）全球公民框架。结果显示，最佳策略可将错误信息传播减少高达37%，并将学习效果提升11%。本研究融合社会心理学与人工智能，为利用LLMs应对现实世界互动中的信念驱动偏见提供了新见解。


> Social biases and belief-driven behaviors can significantly impact Large Language Models (LLMs) decisions on several tasks. As LLMs are increasingly used in multi-agent systems for societal simulations, their ability to model fundamental group psychological characteristics remains critical yet under-explored. In this study, we present a multi-agent framework that simulates belief congruence, a classical group psychology theory that plays a crucial role in shaping societal interactions and preferences. Our findings reveal that LLMs exhibit amplified belief congruence compared to humans, across diverse contexts. We further investigate the implications of this behavior on two downstream tasks: (1) misinformation dissemination and (2) LLM learning, finding that belief congruence in LLMs increases misinformation dissemination and impedes learning. To mitigate these negative impacts, we propose strategies inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global citizenship framework. Our results show that the best strategies reduce misinformation dissemination by up to 37% and enhance learning by 11%. Bridging social psychology and AI, our work provides insights to navigate real-world interactions using LLMs while addressing belief-driven biases.

[Arxiv](https://arxiv.org/abs/2503.02016)