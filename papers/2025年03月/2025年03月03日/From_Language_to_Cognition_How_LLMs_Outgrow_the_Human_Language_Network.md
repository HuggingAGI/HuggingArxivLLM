# 从语言到认知：LLMs超越人类语言网络的能力

发布时间：2025年03月03日

`LLM理论

摘要分析：这篇论文探讨了大型语言模型（LLMs）与人类语言网络神经活动的相似性，分析了模型在不同训练阶段的表现和模型大小对对齐的影响。研究属于对LLM内部机制的理论分析，而非具体应用，因此归类为LLM理论。` `人工智能` `认知科学`

> From Language to Cognition: How LLMs Outgrow the Human Language Network

# 摘要

> 大型语言模型 (LLMs) 与人类语言网络的神经活动高度相似。然而，语言如何塑造大脑类表征的关键特性，以及这些特性在不同任务训练过程中如何演变仍不明确。本研究对跨越 300B 个代币的 34 个训练检查点进行了基准测试，涵盖 8 种不同模型大小，旨在分析大脑对齐与语言能力之间的关系。研究发现，大脑对齐更紧密地追踪正式语言能力的发展，即对语言规则的掌握，而非功能性语言能力。功能性能力涉及世界知识和推理，虽在整个训练过程中不断发展，但与大脑对齐的关系较弱，表明人类语言网络主要编码正式语言结构，而非更广泛的认知功能。此外，我们发现模型大小并非大脑对齐的可靠预测器（控制特征大小后），且一旦模型超越人类语言熟练度，下一个单词预测、行为对齐和大脑对齐之间的相关性会减弱。最后，借助目前最大的严格神经语言基准测试集，我们发现语言大脑对齐基准测试仍未饱和，为未来模型改进提供了重要机遇。综合来看，研究结果表明，人类语言网络更应通过语言的形式方面来建模，而非功能性方面。

> Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.

[Arxiv](https://arxiv.org/abs/2503.01830)