# # AI说服AI与人类：LLMs在促进亲环境行为中的不同效果

发布时间：2025年03月03日

`LLM应用` `人工智能`

> AI persuading AI vs AI persuading Humans: LLMs' Differential Effectiveness in Promoting Pro-Environmental Behavior

# 摘要

> 促进环保行为（PEB）对于应对气候变化至关重要，但如何将环保意识转化为实际行动仍是一个难题。我们研究了大型语言模型（LLMs）作为推动PEB的工具，比较了其对3,200名参与者的影响：真实人类（n=1,200）、基于实际参与者数据的模拟人类（n=1,200）以及完全合成的人格（n=1,200）。所有参与者都面临个性化或标准聊天机器人，或静态声明，并采用了四种劝说策略（道德基础、未来自我连续性、行动导向，或由LLM选择的“自由风格”）。结果显示了一个“合成劝说悖论”：合成和模拟代理显著改变了他们在干预后的PEB立场，而人类的反应则几乎没有变化。模拟参与者更贴近人类趋势，但仍高估了效果。这种脱节凸显了LLM在预评估PEB干预方面的潜力，但也揭示了其在预测真实世界行为方面的局限性。我们呼吁进一步优化合成建模，并进行持续和扩展的人类试验，以实现对话式AI的承诺与切实的可持续性成果相一致。

> Pro-environmental behavior (PEB) is vital to combat climate change, yet turning awareness into intention and action remains elusive. We explore large language models (LLMs) as tools to promote PEB, comparing their impact across 3,200 participants: real humans (n=1,200), simulated humans based on actual participant data (n=1,200), and fully synthetic personas (n=1,200). All three participant groups faced personalized or standard chatbots, or static statements, employing four persuasion strategies (moral foundations, future self-continuity, action orientation, or "freestyle" chosen by the LLM). Results reveal a "synthetic persuasion paradox": synthetic and simulated agents significantly affect their post-intervention PEB stance, while human responses barely shift. Simulated participants better approximate human trends but still overestimate effects. This disconnect underscores LLM's potential for pre-evaluating PEB interventions but warns of its limits in predicting real-world behavior. We call for refined synthetic modeling and sustained and extended human trials to align conversational AI's promise with tangible sustainability outcomes.

[Arxiv](https://arxiv.org/abs/2503.02067)