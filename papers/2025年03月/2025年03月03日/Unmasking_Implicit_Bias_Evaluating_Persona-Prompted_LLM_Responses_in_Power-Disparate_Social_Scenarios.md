# # 摘要
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年03月03日

`LLM应用

理由：这篇论文探讨了大型语言模型在实际应用中表现出的偏见问题，特别是涉及人口统计信息时。研究者分析了模型在不同社会场景和人口统计轴上的表现，揭示了模型中的偏见，并提出了改进方向。这些内容属于模型的实际应用和影响分析，因此归类为LLM应用。` `社会学` `人工智能`

> Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios

# 摘要

> 大型语言模型（LLMs）在模拟人类行为和社会智能方面表现非凡，但它们可能加剧社会偏见，特别是在涉及人口统计信息时。我们提出了一种基于余弦距离的新框架来衡量语义变化，并采用大型语言模型评估的偏好胜率（WR）来分析人口统计提示如何影响权力失衡社会场景中的响应质量。通过对五种大型语言模型在100种不同社会场景和九种人口统计轴上的评估，我们发现了一种倾向于中年、健全、本土出生、白人、无神论且持中间立场的男性的“默认角色”偏见。此外，涉及特定人口群体的互动通常会产生质量较低的回应。最后，权力差异的存在增加了不同人口群体在语义和质量上的回应变异性，暗示在权力失衡条件下，隐性偏见可能更为突出。这些发现揭示了大型语言模型中固有的人口统计偏见，并为未来减少偏见的努力提供了潜在方向。

> Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a "default persona" bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover, interactions involving specific demographics are associated with lower-quality responses. Lastly, the presence of power disparities increases variability in response semantics and quality across demographic groups, suggesting that implicit biases may be heightened under power-imbalanced conditions. These insights expose the demographic biases inherent in LLMs and offer potential paths toward future bias mitigation efforts in LLMs.

[Arxiv](https://arxiv.org/abs/2503.01532)