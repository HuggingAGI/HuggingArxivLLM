# 构建安全的生成式AI应用：大型语言模型的红队测试端到端概述

发布时间：2025年03月03日

`LLM应用` `人工智能`

> Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models

# 摘要

> 大型语言模型（LLMs）的快速发展带来了隐私、安全和伦理等多重挑战。尽管已有不少研究提出防御LLM系统免受恶意攻击的方法，但近期研究通过一种更具进攻性的红队攻击方法进一步完善了这一领域——即主动攻击LLMs以发现其漏洞。本文旨在为LLM红队攻击文献提供一份简洁实用的综述，并以端到端多组件系统的结构进行阐述。我们从知名LLMs的安全需求出发，探讨红队攻击系统的各个组件及其实现软件包。内容涵盖多种攻击方法、评估攻击成功的策略、衡量实验结果的指标，以及其他诸多考量因素。本综述将帮助读者快速掌握红队攻击的核心概念，并将其应用于实际场景。

> The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications.

[Arxiv](https://arxiv.org/abs/2503.01742)