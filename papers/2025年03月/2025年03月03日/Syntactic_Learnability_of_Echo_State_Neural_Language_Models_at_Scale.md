# 大规模回声状态神经语言模型的句法可学性研究

发布时间：2025年03月03日

`LLM理论` `神经网络`

> Syntactic Learnability of Echo State Neural Language Models at Scale

# 摘要

> 什么样的神经模型既结构简单又具备良好的语言学习能力？为寻找这样一个简单而有效的神经语言模型，我们重新审视了一种基本的水库计算（RC）模型——回声状态网络（ESN），这是一种简单的循环神经网络的限制类。实验结果表明，配备大型隐藏状态的ESN在约1亿词训练后，在语法判断任务中的表现可与Transformer相媲美，甚至更优。这表明，像Transformer那样复杂的架构可能并非句法学习的唯一选择。

> What is a neural model with minimum architectural complexity that exhibits reasonable language learning capability? To explore such a simple but sufficient neural language model, we revisit a basic reservoir computing (RC) model, Echo State Network (ESN), a restricted class of simple Recurrent Neural Networks. Our experiments showed that ESN with a large hidden state is comparable or superior to Transformer in grammaticality judgment tasks when trained with about 100M words, suggesting that architectures as complex as that of Transformer may not always be necessary for syntactic learning.

[Arxiv](https://arxiv.org/abs/2503.01724)