# 机器学习从业者应正视大型语言模型作为个人数据的法律层面影响。

发布时间：2025年03月03日

`其他

这篇论文探讨了大型语言模型（LLMs）在处理个人数据时的法律和社会影响，特别是与数据保护法规相关的问题。它讨论了LLMs如何记忆和处理个人数据，以及这些处理方式对数据主体权利的影响。论文还提出机器学习研究人员需要在整个开发周期中考虑这些法律影响，并提出了应对这些影响的方法。因此，这篇论文主要关注的是LLMs的法律和社会影响，而不是直接的技术应用或理论，因此归类为“其他”。` `数据保护`

> Machine Learners Should Acknowledge the Legal Implications of Large Language Models as Personal Data

# 摘要

> GPT 是否了解你？这个问题的答案取决于你的公开知名度；但如果你的信息曾出现在某个网站上，答案很可能就是肯定的。所有大型语言模型（LLMs）都会在一定程度上记住训练数据。如果 LLM 的训练语料库包含个人数据，它也会记住这些信息。开发 LLM 通常涉及处理个人数据，这直接落入数据保护法的范畴。如果一个人可以被识别或识别出来，影响将是深远的：即使在训练阶段结束后，AI 系统也必须遵守欧盟通用数据保护条例的要求。为了支持我们的论点：（1）我们重申，LLMs 在推理时会输出训练数据，无论是逐字还是以概括的形式。 （2）我们表明，一些 LLMs 可以被视为个人数据本身。这会引发一系列数据保护影响，如数据主体权利，包括访问、纠正或删除的权利。这些权利也适用于嵌入 AI 模型中的信息。 （3）本文认为，机器学习研究人员必须在整个 ML 开发生命周期中承认 LLMs 作为个人数据的法律影响，从数据收集和整理到模型在 GitHub 或 Hugging Face 等平台上的提供。 （4）我们提出 ML 研究界应对这些法律影响的不同方法。本文为改进数据保护法与 LLM 技术能力的结合提供了起点。我们的研究结果强调了法律领域与 ML 社区之间需要更多的互动。

> Does GPT know you? The answer depends on your level of public recognition; however, if your information was available on a website, the answer is probably yes. All Large Language Models (LLMs) memorize training data to some extent. If an LLM training corpus includes personal data, it also memorizes personal data. Developing an LLM typically involves processing personal data, which falls directly within the scope of data protection laws. If a person is identified or identifiable, the implications are far-reaching: the AI system is subject to EU General Data Protection Regulation requirements even after the training phase is concluded. To back our arguments: (1.) We reiterate that LLMs output training data at inference time, be it verbatim or in generalized form. (2.) We show that some LLMs can thus be considered personal data on their own. This triggers a cascade of data protection implications such as data subject rights, including rights to access, rectification, or erasure. These rights extend to the information embedded with-in the AI model. (3.) This paper argues that machine learning researchers must acknowledge the legal implications of LLMs as personal data throughout the full ML development lifecycle, from data collection and curation to model provision on, e.g., GitHub or Hugging Face. (4.) We propose different ways for the ML research community to deal with these legal implications. Our paper serves as a starting point for improving the alignment between data protection law and the technical capabilities of LLMs. Our findings underscore the need for more interaction between the legal domain and the ML community.

[Arxiv](https://arxiv.org/abs/2503.01630)