# MRCEval：一个全面、具挑战性且易于获取的机器阅读理解基准

发布时间：2025年03月10日

`LLM应用` `人工智能`

> MRCEval: A Comprehensive, Challenging and Accessible Machine Reading Comprehension Benchmark

# 摘要

> 机器阅读理解（MRC）是评估自然语言理解能力的重要任务。然而，现有的MRC数据集主要针对阅读理解的特定方面，缺乏一个全面的评估基准。为了解决这一问题，我们首先提出了一种新的分类法，用于系统地划分阅读理解所需的关键能力。基于这一分类法，我们构建了MRCEval，这是一个全新的MRC基准测试，它创新性地使用先进大型语言模型（LLMs）作为样本生成器和选择评委。MRCEval是一个全面、具有挑战性且易于使用的基准，旨在全面评估LLMs的阅读理解能力。该基准涵盖13种不同的阅读理解技能，包含2.1K个高质量的多选题。我们对28个广泛使用的开源和专有模型进行了全面评估，结果表明，即使在LLMs时代，MRC仍然是一个极具挑战性的任务。

> Machine Reading Comprehension (MRC) is an essential task in evaluating natural language understanding. Existing MRC datasets primarily assess specific aspects of reading comprehension (RC), lacking a comprehensive MRC benchmark. To fill this gap, we first introduce a novel taxonomy that categorizes the key capabilities required for RC. Based on this taxonomy, we construct MRCEval, an MRC benchmark that leverages advanced Large Language Models (LLMs) as both sample generators and selection judges. MRCEval is a comprehensive, challenging and accessible benchmark designed to assess the RC capabilities of LLMs thoroughly, covering 13 distinct RC skills with a total of 2.1K high-quality multi-choice questions. We perform an extensive evaluation of 28 widely used open-source and proprietary models, highlighting that MRC continues to present significant challenges even in the era of LLMs.

[Arxiv](https://arxiv.org/abs/2503.07144)