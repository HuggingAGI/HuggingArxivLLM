# EgoDTM：探索3D感知自我中心视角的视频语言预训练

发布时间：2025年03月19日

`LLM应用` `计算机视觉` `视频处理`

> EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining

# 摘要

> 以自我为中心的视频-语言预训练推动了视频表示学习的发展。人类在三维世界中感知与互动，形成了超越文本理解的空间意识。然而，以往研究多依赖1D文本或2D视觉线索（如边界框），缺乏三维理解能力。为突破这一限制，我们提出了EgoDTM——一种基于自我中心的深度与文本感知模型，通过大规模三维感知视频预训练和视频-文本对比学习实现联合训练。EgoDTM搭载轻量级三维感知解码器，能够从深度估计模型生成的伪深度图中高效获取三维感知能力。为了进一步提升三维感知视频预训练效果，我们通过结合多个基础模型，将原始简短字幕与手-物体视觉线索有机融合。实验结果表明，EgoDTM在各类下游任务中表现出色，彰显了其卓越的三维感知视觉理解能力。我们的代码即将在https://github.com/xuboshen/EgoDTM开放。

> Egocentric video-language pretraining has significantly advanced video representation learning. Humans perceive and interact with a fully 3D world, developing spatial awareness that extends beyond text-based understanding. However, most previous works learn from 1D text or 2D visual cues, such as bounding boxes, which inherently lack 3D understanding. To bridge this gap, we introduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained through large-scale 3D-aware video pretraining and video-text contrastive learning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently learn 3D-awareness from pseudo depth maps generated by depth estimation models. To further facilitate 3D-aware video pretraining, we enrich the original brief captions with hand-object visual cues by organically combining several foundation models. Extensive experiments demonstrate EgoDTM's superior performance across diverse downstream tasks, highlighting its superior 3D-aware visual understanding. Our code will be released at https://github.com/xuboshen/EgoDTM.

[Arxiv](https://arxiv.org/abs/2503.15470)