# # 大型语言模型的思维链于贝叶斯推理中是否存在幻觉、认知偏差或恐惧？

发布时间：2025年03月19日

`LLM应用` `认知科学`

> Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?

# 摘要

> 推理与论点阐述是培养学生认知、数学及计算思维的关键。特别是在涉及不确定性和贝叶斯推理的问题中，这一过程更具挑战性。借助新一代具备链式思维（CoT）推理能力的大语言模型（LLMs），我们迎来了一次难得的学习贝叶斯推理的机遇。这些模型通过与内部人工语音的对话，清晰地解释其推理过程，为学习者提供了生动的学习场景。此外，由于不同LLMs有时会得出截然相反的解决方案，CoT通过详细比较推理过程，为深度学习开辟了新的可能。然而，与人类不同，我们发现这些模型并未自主采用生态有效的策略，如自然频率、整体对象和具身启发式。这着实令人惋惜，因为这些策略不仅帮助人类避免重大错误，还在贝叶斯推理教学中展现出显著价值。为了克服这些偏见并促进理解和学习，我们设计了特定的提示，引导LLMs运用这些策略。研究发现，具备CoT能力的LLMs能够融入这些策略，但表现并不一致。它们往往倾向于符号推理，对生态有效策略则表现出回避或抗拒的态度。

> Learning to reason and carefully explain arguments is central to students' cognitive, mathematical, and computational thinking development. This is particularly challenging in problems under uncertainty and in Bayesian reasoning. With the new generation of large language models (LLMs) capable of reasoning using Chain-of-Thought (CoT), there is an excellent opportunity to learn with them as they explain their reasoning through a dialogue with their artificial internal voice. It is an engaging and excellent opportunity to learn Bayesian reasoning. Furthermore, given that different LLMs sometimes arrive at opposite solutions, CoT generates opportunities for deep learning by detailed comparisons of reasonings. However, unlike humans, we found that they do not autonomously explain using ecologically valid strategies like natural frequencies, whole objects, and embodied heuristics. This is unfortunate, as these strategies help humans avoid critical mistakes and have proven pedagogical value in Bayesian reasoning. In order to overcome these biases and aid understanding and learning, we included prompts that induce LLMs to use these strategies. We found that LLMs with CoT incorporate them but not consistently. They show persistent biases towards symbolic reasoning and avoidance or phobia of ecologically valid strategies.

[Arxiv](https://arxiv.org/abs/2503.15268)