# 塑造共享语言：人类与大型语言模型在涌现式通信中的归纳倾向

发布时间：2025年03月06日

`LLM理论` `语言学` `人工智能`

> Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication

# 摘要

> 语言由其使用者的归纳偏见所塑造。通过经典指称游戏，我们研究了人工语言在针对人类和大型语言模型（LLMs）的归纳偏见优化时的演化过程。这一研究通过Human-Human、LLM-LLM以及Human-LLM实验得以实现。我们发现，具有指称基础的词汇表在所有条件下都能实现可靠沟通，即使人类与LLMs协作也是如此。不同条件下的比较表明，针对LLMs优化的语言与针对人类优化的语言存在微妙差异。有趣的是，人类与LLMs之间的互动缓解了这些差异，并产生了更贴近人类而非LLM的词汇表。这些发现加深了我们对LLMs归纳偏见在人类语言动态特性中所扮演角色的理解，并有助于维持人类与机器沟通的一致性。尤为重要的是，我们的研究强调了在LLMs训练过程中纳入人类互动的新方法的必要性，并表明将沟通成功作为奖励信号可以成为富有成效且新颖的方向。

> Languages are shaped by the inductive biases of their users. Using a classical referential game, we investigate how artificial languages evolve when optimised for inductive biases in humans and large language models (LLMs) via Human-Human, LLM-LLM and Human-LLM experiments. We show that referentially grounded vocabularies emerge that enable reliable communication in all conditions, even when humans and LLMs collaborate. Comparisons between conditions reveal that languages optimised for LLMs subtly differ from those optimised for humans. Interestingly, interactions between humans and LLMs alleviate these differences and result in vocabularies which are more human-like than LLM-like. These findings advance our understanding of how inductive biases in LLMs play a role in the dynamic nature of human language and contribute to maintaining alignment in human and machine communication. In particular, our work underscores the need to think of new methods that include human interaction in the training processes of LLMs, and shows that using communicative success as a reward signal can be a fruitful, novel direction.

[Arxiv](https://arxiv.org/abs/2503.04395)