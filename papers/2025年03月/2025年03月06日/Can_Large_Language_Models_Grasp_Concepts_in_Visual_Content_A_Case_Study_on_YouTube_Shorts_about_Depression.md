# 大型语言模型能否理解视觉内容中的概念？以 YouTube 短视频中的抑郁症为例

发布时间：2025年03月06日

`LLM应用` `计算社会科学` `视频分析`

> Can Large Language Models Grasp Concepts in Visual Content? A Case Study on YouTube Shorts about Depression

# 摘要

> 大型语言模型（LLMs）在计算社会科学领域的应用日益广泛。尽管先前的研究主要集中在文本分析上，但多模态大型语言模型（MLLMs）在在线视频研究中的潜力仍有待深入挖掘。我们开展了一项关于MLLM辅助视频内容分析的开创性案例研究，对比了AI与人类对抽象概念的理解差异。利用LLaVA-1.6 Mistral 7B模型，我们解析了与视频中介自我披露相关的四个抽象概念，分析了142个抑郁症相关YouTube短视频中的725个关键帧。通过对MLLM自动生成解释的定性研究，我们发现操作化的程度显著影响MLLM的理解效果。令人意外的是，细节的增加并不必然提升人机一致性。此外，我们还发现概念复杂性和视频类型多样性等因素也会影响AI与人类理解的契合度。本研究强调了为特定概念量身定制提示的重要性，并呼吁研究人员在多模态环境中与AI协作时，采用更多以人类为中心的评估方法。

> Large language models (LLMs) are increasingly used to assist computational social science research. While prior efforts have focused on text, the potential of leveraging multimodal LLMs (MLLMs) for online video studies remains underexplored. We conduct one of the first case studies on MLLM-assisted video content analysis, comparing AI's interpretations to human understanding of abstract concepts. We leverage LLaVA-1.6 Mistral 7B to interpret four abstract concepts regarding video-mediated self-disclosure, analyzing 725 keyframes from 142 depression-related YouTube short videos. We perform a qualitative analysis of MLLM's self-generated explanations and found that the degree of operationalization can influence MLLM's interpretations. Interestingly, greater detail does not necessarily increase human-AI alignment. We also identify other factors affecting AI alignment with human understanding, such as concept complexity and versatility of video genres. Our exploratory study highlights the need to customize prompts for specific concepts and calls for researchers to incorporate more human-centered evaluations when working with AI systems in a multimodal context.

[Arxiv](https://arxiv.org/abs/2503.05109)