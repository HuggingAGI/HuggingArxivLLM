# 标签分布学习增强型双KNN用于文本分类

发布时间：2025年03月06日

`LLM理论` `机器学习`

> Label Distribution Learning-Enhanced Dual-KNN for Text Classification

# 摘要

> 文本分类方法通常借助外部信息（如标签描述和知识库）提升性能，但对模型自身生成的内部信息（如文本嵌入和预测标签概率分布）利用不足。本文提出双k近邻（DkNN）框架，通过两个kNN模块从训练集中检索近邻并增强标签分布。然而，kNN模块在噪声数据集或相似数据集中检索时易混淆并导致错误预测。为此，我们引入标签分布学习模块，通过学习标签相似性生成更优分布，帮助模型更高效地区分文本。该模块缓解过拟合问题，提升分类性能，并在推理过程中优化kNN模块检索质量。基准数据集的实验验证了方法的有效性。

> Many text classification methods usually introduce external information (e.g., label descriptions and knowledge bases) to improve the classification performance. Compared to external information, some internal information generated by the model itself during training, like text embeddings and predicted label probability distributions, are exploited poorly when predicting the outcomes of some texts. In this paper, we focus on leveraging this internal information, proposing a dual $k$ nearest neighbor (D$k$NN) framework with two $k$NN modules, to retrieve several neighbors from the training set and augment the distribution of labels. For the $k$NN module, it is easily confused and may cause incorrect predictions when retrieving some nearest neighbors from noisy datasets (datasets with labeling errors) or similar datasets (datasets with similar labels). To address this issue, we also introduce a label distribution learning module that can learn label similarity, and generate a better label distribution to help models distinguish texts more effectively. This module eases model overfitting and improves final classification performance, hence enhancing the quality of the retrieved neighbors by $k$NN modules during inference. Extensive experiments on the benchmark datasets verify the effectiveness of our method.

[Arxiv](https://arxiv.org/abs/2503.04869)