# START: 自带工具的自学者

发布时间：2025年03月06日

`LLM应用` `学术领域` `软件工程`

> START: Self-taught Reasoner with Tools

# 摘要

> 大型推理模型（LRMs）如 OpenAI-o1 和 DeepSeek-R1 凭借长链式思维（CoT）在复杂推理任务中表现突出，但因仅依赖内部推理，常出现幻觉和低效问题。本文提出了一种名为 START 的创新工具集成式长 CoT 推理 LLM，通过引入外部工具显著提升推理能力。START 通过代码执行实现复杂计算、自我验证、方法探索及自我修复，有效克服传统 LRMs 的局限。其核心技术包括：1）Hint-infer：在推理过程中嵌入人工设计的提示（如 ``等等，也许在这里使用 Python 是个好主意''），激发模型主动调用外部工具的能力，无需额外演示数据；2）Hint-RFT：结合 Hint-infer 和 RFT，通过对 LRM 生成的带工具调用推理轨迹进行评分、筛选和优化，随后进行微调。基于此框架，我们对 QwQ-32B 模型进行了优化，得到 START。在博士级科学问答（GPQA）、竞赛级数学基准（AMC23、AIME24、AIME25）及代码竞赛基准（LiveCodeBench）中，START 的准确率分别达到 63.6%、95.0%、66.7%、47.1% 和 47.3%，显著超越基础 QwQ-32B，性能比肩开源领先模型 R1-Distill-Qwen-32B 和专有模型 o1-Preview。

> Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.

[Arxiv](https://arxiv.org/abs/2503.04625)