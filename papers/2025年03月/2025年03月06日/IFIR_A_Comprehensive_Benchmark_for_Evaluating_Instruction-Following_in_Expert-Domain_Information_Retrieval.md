# IFIR：全面评估专家领域信息检索中遵循指令能力的基准测试

发布时间：2025年03月06日

`LLM应用` `信息检索`

> IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval

# 摘要

> 我们推出IFIR，首个专为评估专家领域中指令遵循信息检索能力设计的全面基准测试。IFIR包含2,426个高质量示例，涵盖金融、法律、医疗和科学文献四大领域的八个子集。每个子集针对特定领域的检索任务，模拟了定制指令至关重要的真实场景。通过在不同复杂度级别中融入指令，IFIR能够细致分析指令遵循检索能力。我们还提出了一种基于LLM的新评估方法，以更精确、可靠地评估模型的指令遵循能力。通过对15个前沿检索模型的广泛实验，包括基于LLM的模型，结果显示，现有模型在有效遵循复杂、特定领域指令方面面临重大挑战。我们进一步深入分析，突出了这些限制，为未来检索器的发展提供了宝贵见解。

> We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.

[Arxiv](https://arxiv.org/abs/2503.04644)