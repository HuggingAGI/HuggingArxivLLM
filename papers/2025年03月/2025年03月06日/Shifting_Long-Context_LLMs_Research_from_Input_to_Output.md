# # 从输入到输出：长上下文 LLMs 研究的视角转变
长上下文 LLMs 的研究正经历从输入到输出的焦点转移。这一变化凸显了对模型生成内容的关注，特别是在复杂任务中的输出表现。

发布时间：2025年03月06日

`LLM应用` `内容生成`

> Shifting Long-Context LLMs Research from Input to Output

# 摘要

> 长上下文大型语言模型（LLMs）的最新进展主要集中在处理长输入上下文，显著提升了长上下文理解能力。然而，同样重要的长文本生成能力却相对被忽视。本文呼吁NLP研究范式的转变，以应对长输出生成的挑战。小说创作、长期规划和复杂推理等任务不仅需要模型理解广泛上下文，还需生成连贯、内容丰富且逻辑一致的长文本。这些需求凸显了现有LLM能力的关键缺口。我们强调这一未充分探索领域的重要性，并呼吁集中开发专门用于生成高质量长文本的基础LLM，这在实际应用中具有巨大潜力。

> Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.

[Arxiv](https://arxiv.org/abs/2503.04723)