# 学习预测性视觉运动协调

发布时间：2025年03月29日

`其他` `机器人学` `人机交互`

> Learning Predictive Visuomotor Coordination

# 摘要

> 理解并预测人类的视觉运动协调对于机器人学、人机交互以及辅助技术等领域具有重要意义。本研究提出了一种基于预测的视觉运动建模任务，目标是通过第一人称视觉和运动学观察预测头部姿势、注视点和上半身动作。我们引入了	extit{视觉运动协调表示}（VCR），用于学习这些多模态信号之间的结构化时间依赖关系。我们扩展了一种基于扩散模型的运动建模框架，整合了第一人称视觉和运动学序列，从而实现了在时间上连贯且准确的视觉运动预测。我们的方法在大规模的EgoExo4D数据集上进行了评估，展示了在各种真实世界活动中强大的泛化能力。实验结果突出了多模态整合在理解视觉运动协调中的重要性，为视觉运动学习和人类行为建模研究做出了贡献。

> Understanding and predicting human visuomotor coordination is crucial for applications in robotics, human-computer interaction, and assistive technologies. This work introduces a forecasting-based task for visuomotor modeling, where the goal is to predict head pose, gaze, and upper-body motion from egocentric visual and kinematic observations. We propose a \textit{Visuomotor Coordination Representation} (VCR) that learns structured temporal dependencies across these multimodal signals. We extend a diffusion-based motion modeling framework that integrates egocentric vision and kinematic sequences, enabling temporally coherent and accurate visuomotor predictions. Our approach is evaluated on the large-scale EgoExo4D dataset, demonstrating strong generalization across diverse real-world activities. Our results highlight the importance of multimodal integration in understanding visuomotor coordination, contributing to research in visuomotor learning and human behavior modeling.

[Arxiv](https://arxiv.org/abs/2503.23300)