# 结构化推理促进公平性：通过多智能体方法检测文本数据中的偏见

发布时间：2025年03月01日

`LLM应用` `AI应用` `推荐系统`

> Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data

# 摘要

> 从AI聊天机器人散播的虚假信息，到AI推荐系统无意间强化的刻板印象，文本偏见正严重威胁着大型语言模型的可信度。本文提出了一种多智能体框架，通过将每条声明拆解为事实或观点、分配偏见强度评分并提供事实依据，系统性地识别偏见。在WikiNPOV数据集的1,500个样本上，该框架达到了84.9%的准确率——比零-shot基线提升了13.0%——证明了在量化偏见强度前明确区分事实与观点的重要性。通过将更高的检测精度与可解释的解释相结合，这一方法为提升现代语言模型的公平性和可追溯性奠定了基础。

> From disinformation spread by AI chatbots to AI recommendations that inadvertently reinforce stereotypes, textual bias poses a significant challenge to the trustworthiness of large language models (LLMs). In this paper, we propose a multi-agent framework that systematically identifies biases by disentangling each statement as fact or opinion, assigning a bias intensity score, and providing concise, factual justifications. Evaluated on 1,500 samples from the WikiNPOV dataset, the framework achieves 84.9% accuracy$\unicode{x2014}$an improvement of 13.0% over the zero-shot baseline$\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact versus opinion prior to quantifying bias intensity. By combining enhanced detection accuracy with interpretable explanations, this approach sets a foundation for promoting fairness and accountability in modern language models.

[Arxiv](https://arxiv.org/abs/2503.00355)