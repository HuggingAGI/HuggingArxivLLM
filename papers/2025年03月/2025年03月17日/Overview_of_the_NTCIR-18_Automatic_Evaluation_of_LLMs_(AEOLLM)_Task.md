# 第18届NTCIR大语言模型（LLMs）自动评估任务概述

发布时间：2025年03月17日

`LLM应用` `自动评估`

> Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task

# 摘要

> # 摘要
本文将全面概述 NTCIR-18 自动评估大语言模型（LLMs）任务（AEOLLM）。随着 LLMs 在学术界和工业界越来越受欢迎，如何有效评估 LLMs 的能力成为日益重要但仍然具有挑战性的课题。现有方法主要分为两类：昂贵的手动评估和自动评估，后者面临诸多限制，包括任务格式（大多数属于多项选择题）和评估标准（以基于参考的指标为主）。为了推动自动评估的创新，我们提出了专注于生成任务并鼓励无参考方法的 AEOLLM 任务。此外，我们还设置了对话生成、文本扩展、摘要生成和非事实性问答等多种子任务，以全面测试不同方法。今年，我们总共收到了来自 4 个团队的 48 个提交结果。本文将分别介绍任务背景、数据集、评估指标和评估结果。

> In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both academia and industry, how to effectively evaluate the capacity of LLMs becomes an increasingly critical but still challenging issue. Existing methods can be divided into two types: manual evaluation, which is expensive, and automatic evaluation, which faces many limitations including task format (the majority belong to multiple-choice questions) and evaluation criteria (occupied by reference-based metrics). To advance the innovation of automatic evaluation, we propose the AEOLLM task which focuses on generative tasks and encourages reference-free methods. Besides, we set up diverse subtasks such as dialogue generation, text expansion, summary generation and non-factoid question answering to comprehensively test different methods. This year, we received 48 runs from 4 teams in total. This paper will describe the background of the task, the data set, the evaluation measures and the evaluation results, respectively.

[Arxiv](https://arxiv.org/abs/2503.13038)