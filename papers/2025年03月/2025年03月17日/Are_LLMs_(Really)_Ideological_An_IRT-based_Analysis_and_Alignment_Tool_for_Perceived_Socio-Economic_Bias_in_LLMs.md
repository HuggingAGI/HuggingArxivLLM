# LLMs 是否真的具有意识形态倾向？基于 IRT 的感知社会经济偏见分析与对齐工具

发布时间：2025年03月17日

`LLM理论` `人工智能` `AI治理`

> Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs

# 摘要

> 我们提出了一种基于项目反应理论（IRT）的框架，用于检测和量化大型语言模型（LLMs）中的社会经济偏见，无需依赖主观的人类判断。与传统方法不同，IRT通过考虑项目难度，显著提升了对意识形态偏见的估计精度。我们对Meta-LLaMa 3.2-1B-Instruct 和 Chat-GPT 3.5 两个LLM家族进行了微调，以代表不同的意识形态立场，并创新性地提出了两阶段方法：首先建模回应回避行为，其次估计回答中的感知偏见。研究发现，现成的LLMs更倾向于回避意识形态互动，而非表现出明显偏见，这一发现挑战了此前关于LLMs存在党派倾向的主张。这一经实证验证的框架不仅推动了AI对齐研究的发展，也为实现更公平的AI治理提供了有力工具。

> We introduce an Item Response Theory (IRT)-based framework to detect and quantify socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional methods, IRT accounts for item difficulty, improving ideological bias estimation. We fine-tune two LLM families (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct ideological positions and introduce a two-stage approach: (1) modeling response avoidance and (2) estimating perceived bias in answered responses. Our results show that off-the-shelf LLMs often avoid ideological engagement rather than exhibit bias, challenging prior claims of partisanship. This empirically validated framework enhances AI alignment research and promotes fairer AI governance.

[Arxiv](https://arxiv.org/abs/2503.13149)