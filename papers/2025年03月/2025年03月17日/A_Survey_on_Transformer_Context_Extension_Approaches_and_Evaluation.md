# # Transformer上下文扩展综述：方法与评估

发布时间：2025年03月17日

`LLM理论` `NLP`

> A Survey on Transformer Context Extension: Approaches and Evaluation

# 摘要

> 基于 Transformer 的大型语言模型（LLMs）在 NLP 领域表现优异，尤其擅长处理短文本任务。然而，在长上下文场景中，LLMs 的性能会因一些挑战而下降。为了解决这一问题，近期出现了多种方法。本综述首先总结了将预训练 LLMs 应用于长上下文时的主要挑战，然后系统性地回顾了相关方法，并将它们归类为四类：位置编码、上下文压缩、检索增强和注意力模式。除了方法本身，我们还深入探讨了长上下文的评估问题，基于现有基准整理了相关数据、任务和指标。最后，我们总结了长上下文领域尚未解决的问题，并对未来发展方向提出了见解。

> Large language models (LLMs) based on Transformer have been widely applied in the filed of natural language processing (NLP), demonstrating strong performance, particularly in handling short text tasks. However, when it comes to long context scenarios, the performance of LLMs degrades due to some challenges. To alleviate this phenomenon, there is a number of work proposed recently. In this survey, we first list the challenges of applying pre-trained LLMs to process long contexts. Then systematically review the approaches related to long context and propose our taxonomy categorizing them into four main types: positional encoding, context compression, retrieval augmented, and attention pattern. In addition to the approaches, we focus on the evaluation of long context, organizing relevant data, tasks, and metrics based on existing long context benchmarks. Finally, we summarize unresolved issues in the long context domain and put forward our views on future developments.

[Arxiv](https://arxiv.org/abs/2503.13299)