# # 基于人类隐含偏好的策略微调在无人船群多智能体强化学习中的应用

发布时间：2025年03月05日

`Agent` `机器人技术` `人工智能`

> Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm

# 摘要

> 多智能体强化学习（MARL）在解决涉及智能体间合作与竞争的复杂问题方面展现出了巨大的潜力，例如用于搜救、监视和船舶保护的无人水面车辆（USV）集群。然而，由于将专家直觉编码到奖励函数中的难度，使系统行为与用户偏好保持一致颇具挑战性。为了解决这一问题，我们提出了一种结合人类反馈的强化学习（RLHF）方法，用于MARL。该方法通过一种将反馈分类为智能体内、智能体间和团队内的Agent-Level Feedback系统来解决信用分配挑战。为了克服直接人类反馈的挑战，我们采用了大型语言模型（LLM）评估器，通过区域约束、碰撞避免和任务分配等反馈场景来验证我们的方法。我们的方法成功优化了USV集群策略，解决了多智能体系统中的关键挑战，同时保持公平性和性能一致性。

> Multi-Agent Reinforcement Learning (MARL) has shown promise in solving complex problems involving cooperation and competition among agents, such as an Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance, and vessel protection. However, aligning system behavior with user preferences is challenging due to the difficulty of encoding expert intuition into reward functions. To address the issue, we propose a Reinforcement Learning with Human Feedback (RLHF) approach for MARL that resolves credit-assignment challenges through an Agent-Level Feedback system categorizing feedback into intra-agent, inter-agent, and intra-team types. To overcome the challenges of direct human feedback, we employ a Large Language Model (LLM) evaluator to validate our approach using feedback scenarios such as region constraints, collision avoidance, and task allocation. Our method effectively refines USV swarm policies, addressing key challenges in multi-agent systems while maintaining fairness and performance consistency.

[Arxiv](https://arxiv.org/abs/2503.03796)