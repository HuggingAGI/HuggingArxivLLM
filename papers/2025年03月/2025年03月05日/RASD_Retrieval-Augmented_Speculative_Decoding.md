# RASD: 检索增强推测解码

发布时间：2025年03月05日

`LLM应用` `大型语言模型`

> RASD: Retrieval-Augmented Speculative Decoding

# 摘要

> 基于推测解码（Speculative decoding）的加速推理方法在大型语言模型（LLMs）中备受关注。该方法通过为模型生成候选词块，显著提升了推理速度。然而，现有方法主要依赖轻量级候选模型或额外结构生成候选词块并检索上下文，存在两大局限：候选模型规模小、训练数据有限导致领域外场景效果不佳；生成阶段耗时长，限制了验证阶段的长度上限，进而影响整体效率。针对这些问题，我们提出了一种基于检索增强的推测解码方法（RASD，Retrieval-Augmented Speculative Decoding）。通过引入剪枝策略和树融合方法，我们实现了更高效的推测解码。具体而言，我们设计了一种基于候选模型概率分布的剪枝方法，用于构建最优检索树。同时，采用最长前缀匹配算法将候选模型生成的树与检索树进行融合，最终形成统一的验证树。实验结果表明，RASD在DocQA、Summary、Code和领域内问答（In-Domain QA）等任务中均达到了当前最优的加速效果。此外，RASD具有良好的扩展性，能够与多种推测解码方法无缝集成，包括基于生成和基于检索的方法。

> Speculative decoding accelerates inference in large language models (LLMs) by generating draft tokens for target model verification. Current approaches for obtaining draft tokens rely on lightweight draft models or additional model structures to generate draft tokens and retrieve context from databases. Due to the draft model's small size and limited training data, model-based speculative decoding frequently becomes less effective in out-of-domain scenarios. Additionally, the time cost of the drafting phase results in a low upper limit on acceptance length during the verification step, limiting overall efficiency. This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which adopts retrieval methods to enhance model-based speculative decoding. We introduce tree pruning and tree fusion to achieve this. Specifically, we develop a pruning method based on the draft model's probability distribution to construct the optimal retrieval tree. Second, we employ the longest prefix matching algorithm to merge the tree generated by the draft model with the retrieval tree, resulting in a unified tree for verification. Experimental results demonstrate that RASD achieves state-of-the-art inference acceleration across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD exhibits strong scalability, seamlessly integrating with various speculative decoding approaches, including both generation-based and retrieval-based methods.

[Arxiv](https://arxiv.org/abs/2503.03434)