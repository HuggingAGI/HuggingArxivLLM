# 随机性而非表征：评估大语言模型文化对齐的可靠性问题

发布时间：2025年03月11日

`LLM理论` `社会学` `文化研究`

> Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs

# 摘要

> 随着人们对跨不同利益相关者群体中表征理解的兴趣日益增长，关于大型语言模型（LLMs）的“文化对齐”研究应运而生。目前用于评估文化对齐的方法虽然借鉴了社会科学的方法论，但常常忽视了系统的稳健性检验。我们识别并测试了当前评估方法背后的三个关键假设：（1）稳定性：文化对齐是LLMs的固有属性，而非评估设计的结果；（2）外推性：在一个狭窄的问题集上与某一文化对齐，可以预测在其他问题上与该文化的对齐；（3）可引导性：LLMs可以被可靠地提示以呈现特定的文化视角。通过实验考察领先LLMs的显式和隐式偏好，我们发现呈现格式的高度不稳定性，评估与保留的文化维度之间的不一致，以及在提示引导下的异常行为。这些不一致可能导致评估结果对方法学的微小变化非常敏感。最后，我们在一个关于评估设计的案例研究中证明，狭窄的实验和对证据的选择性评估可以用来描绘LLMs文化对齐属性的不完整图景。总体而言，这些结果突显了当前评估LLMs文化对齐的方法的重要局限性。

> Research on the 'cultural alignment' of Large Language Models (LLMs) has emerged in response to growing interest in understanding representation across diverse stakeholders. Current approaches to evaluating cultural alignment borrow social science methodologies but often overlook systematic robustness checks. Here, we identify and test three assumptions behind current evaluation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolability: that alignment with one culture on a narrow set of issues predicts alignment with that culture on others, and (3) Steerability: that LLMs can be reliably prompted to represent specific cultural perspectives. Through experiments examining both explicit and implicit preferences of leading LLMs, we find a high level of instability across presentation formats, incoherence between evaluated versus held-out cultural dimensions, and erratic behavior under prompt steering. We show that these inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a selective assessment of evidence can be used to paint an incomplete picture of LLMs' cultural alignment properties. Overall, these results highlight significant limitations of current approaches for evaluating the cultural alignment of LLMs.

[Arxiv](https://arxiv.org/abs/2503.08688)