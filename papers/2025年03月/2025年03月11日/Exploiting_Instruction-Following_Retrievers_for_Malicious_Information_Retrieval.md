# 巧用遵循指令的检索器，精准捕获恶意信息

发布时间：2025年03月11日

`RAG` `人工智能安全` `网络安全`

> Exploiting Instruction-Following Retrievers for Malicious Information Retrieval

# 摘要

> 指令跟随检索器在与大型语言模型 (LLMs) 的实际应用中被广泛应用，但其日益增长的搜索能力带来的安全风险却鲜有研究关注。我们通过实证研究发现，无论是直接使用还是在检索增强生成的设置下，检索器都有能力满足恶意查询。具体而言，我们研究了六种领先的检索器，包括 NV-Embed 和 LLM2Vec，并发现面对恶意请求时，大多数检索器（超过50%的查询）能够检索出相关的有害内容。例如，LLM2Vec 能够正确检索恶意查询的 61.35%。我们进一步发现，指令跟随检索器存在一项新兴风险：通过利用其指令跟随能力，可以检索出与恶意查询高度相关但有害的信息。最后，我们发现即使是对齐了安全目标的 LLM，例如 Llama3，在提供上下文中有害检索内容时，也能够满足恶意请求。总之，我们的研究结果强调了随着检索器能力的增强，其被恶意滥用的风险也在增加。

> Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.

[Arxiv](https://arxiv.org/abs/2503.08644)