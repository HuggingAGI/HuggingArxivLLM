# LLM 扩展即将撞墙？海量边缘设备的分布式资源助力突破瓶颈。

发布时间：2025年03月11日

`LLM理论` `AI开发` `分布式计算`

> Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed Resources on Massive Edge Devices

# 摘要

> 基础模型的成功主要得益于扩展定律：模型性能会随着训练数据和规模的增加而稳步提升。然而，这一扩展路径面临两大挑战：高质量公共数据的枯竭，以及训练大型模型所需的巨大算力，这些资源已被科技巨头垄断。这两个瓶颈严重阻碍了 AI 的发展。在这篇论文中，我们主张利用海量分布式边缘设备突破这些限制。我们揭示了边缘设备中未被充分利用的数据和算力的巨大潜力，并回顾了分布式/联邦学习领域的最新进展，这些技术使这一新范式成为可能。我们的分析表明，通过边缘设备协作，每个人都能使用小型设备参与大型语言模型的训练。这种向分布式边缘训练的转变，有望推动 AI 开发的民主化，培养一个更加包容的 AI 社区。

> The remarkable success of foundation models has been driven by scaling laws, demonstrating that model performance improves predictably with increased training data and model size. However, this scaling trajectory faces two critical challenges: the depletion of high-quality public data, and the prohibitive computational power required for larger models, which have been monopolized by tech giants. These two bottlenecks pose significant obstacles to the further development of AI. In this position paper, we argue that leveraging massive distributed edge devices can break through these barriers. We reveal the vast untapped potential of data and computational resources on massive edge devices, and review recent technical advancements in distributed/federated learning that make this new paradigm viable. Our analysis suggests that by collaborating on edge devices, everyone can participate in training large language models with small edge devices. This paradigm shift towards distributed training on edge has the potential to democratize AI development and foster a more inclusive AI community.

[Arxiv](https://arxiv.org/abs/2503.08223)