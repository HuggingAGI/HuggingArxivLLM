# 别太激动——在大型语言模型中激发情感

发布时间：2025年03月04日

`LLM应用` `情感控制` `对话系统`

> Don't Get Too Excited -- Eliciting Emotions in LLMs

# 摘要

> 本文探讨了大型语言模型 (LLMs) 在情感控制方面的挑战，重点关注其在长时间对话中表达适当情感状态的能力。我们评估了先进的开源大型语言模型，以衡量它们在唤醒和效价方面的情感表达范围。研究采用了一种结合基于 LLM 的情感分析与多轮对话模拟的创新方法。我们量化了模型在广泛情感范围内的情感表达能力及其在互动中的波动情况。研究发现，不同 LLM 在保持一致情感方面的能力存在显著差异，某些模型表现出更稳定的情感轨迹。此外，我们还识别了情感控制中的关键挑战，包括生成和维持极端情感状态的困难以及在变化的对话环境中适应情感的局限性。这些发现对开发更具情感智能的 AI 系统具有重要意义，并强调了在 LLM 中改进情感建模的必要性。

> This paper investigates the challenges of affect control in large language models (LLMs), focusing on their ability to express appropriate emotional states during extended dialogues. We evaluated state-of-the-art open-weight LLMs to assess their affective expressive range in terms of arousal and valence. Our study employs a novel methodology combining LLM-based sentiment analysis with multiturn dialogue simulations between LLMs. We quantify the models' capacity to express a wide spectrum of emotions and how they fluctuate during interactions. Our findings reveal significant variations among LLMs in their ability to maintain consistent affect, with some models demonstrating more stable emotional trajectories than others. Furthermore, we identify key challenges in affect control, including difficulties in producing and maintaining extreme emotional states and limitations in adapting affect to changing conversational contexts. These findings have important implications for the development of more emotionally intelligent AI systems and highlight the need for improved affect modelling in LLMs.

[Arxiv](https://arxiv.org/abs/2503.02457)