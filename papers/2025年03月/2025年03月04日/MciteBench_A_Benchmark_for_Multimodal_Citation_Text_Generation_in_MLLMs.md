# MciteBench：一个多模态引文文本生成基准测试

发布时间：2025年03月04日

`LLM应用` `人工智能` `学术研究`

> MciteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs

# 摘要

> 多模态大型语言模型 (MLLMs) 在整合多种模态方面取得了显著进展，但常常面临幻觉问题。为了解决这一问题，生成带引用的文本提供了一个有前景的解决方案，因为它能够提供透明的验证链。然而，现有的研究主要集中在为纯文本内容生成引用，忽视了多模态上下文中的挑战和机遇。为了解决这一差距，我们引入了 MCiteBench，这是首个专门设计用于评估和分析 MLLMs 多模态引用文本生成能力的基准。我们的基准包含来自学术论文和审阅-反驳互动的数据，涵盖了多样化的信息来源和多模态内容。我们从多个维度全面评估模型，包括引用质量、来源可靠性和回答准确性。通过广泛的实验，我们发现 MLLMs 在多模态引用文本生成方面存在困难。我们还对模型性能进行了深入分析，发现瓶颈在于正确归属来源，而不是理解多模态内容。

> Multimodal Large Language Models (MLLMs) have advanced in integrating diverse modalities but frequently suffer from hallucination. A promising solution to mitigate this issue is to generate text with citations, providing a transparent chain for verification. However, existing work primarily focuses on generating citations for text-only content, overlooking the challenges and opportunities of multimodal contexts. To address this gap, we introduce MCiteBench, the first benchmark designed to evaluate and analyze the multimodal citation text generation ability of MLLMs. Our benchmark comprises data derived from academic papers and review-rebuttal interactions, featuring diverse information sources and multimodal content. We comprehensively evaluate models from multiple dimensions, including citation quality, source reliability, and answer accuracy. Through extensive experiments, we observe that MLLMs struggle with multimodal citation text generation. We also conduct deep analyses of models' performance, revealing that the bottleneck lies in attributing the correct sources rather than understanding the multimodal content.

[Arxiv](https://arxiv.org/abs/2503.02589)