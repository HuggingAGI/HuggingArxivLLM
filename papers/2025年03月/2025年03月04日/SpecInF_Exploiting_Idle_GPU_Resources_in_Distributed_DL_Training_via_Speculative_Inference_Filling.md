# # **SpecInF：利用分布式深度学习任务中的空闲GPU资源进行推测推理填充**

发布时间：2025年03月04日

`其他

摘要中讨论了深度学习，特别是大型语言模型（LLMs）在资源管理方面的优化方法，提出了SpecInF方法以提高GPU利用率。这属于系统优化和资源管理，而非直接涉及LLM的应用或理论，因此归类为其他。` `计算机科学` `高性能计算`

> SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via Speculative Inference Filling

# 摘要

> 深度学习（DL），尤其是大型语言模型（LLMs），正在为各领域带来革命性提升。然而，由于资源分配和集体通信等因素，深度学习训练系统通常会导致GPU资源闲置问题突出。为了提高GPU利用率，我们提出了SpecInF，它采用了一种	extbf{Spec}ulative 	extbf{In}ference 	extbf{F}illing方法，以利用闲置的GPU资源。SpecInF将每个主要训练实例与额外的推理实例 colocated 在同一GPU上，检测训练气泡，并自适应地填充在线或离线推理任务。实验结果表明，SpecInF可以在主流的并行训练模式下显著提升GPU利用率，与TGS相比，离线推理吞吐量额外提升高达14$	imes$，与MPS相比，在线推理 p95 延迟降低67%，同时保证 colocated 训练吞吐量。

> Deep Learning (DL), especially with Large Language Models (LLMs), brings benefits to various areas. However, DL training systems usually yield prominent idling GPU resources due to many factors, such as resource allocation and collective communication. To improve GPU utilization, we present SpecInF, which adopts a \textbf{Spec}ulative \textbf{In}ference \textbf{F}illing method to exploit idle GPU resources. It collocates each primary training instance with additional inference instances on the same GPU, detects the training bubbles and adaptively fills with online or offline inference workloads. Our results show that SpecInF can effectively enhance GPU utilization under mainstream parallel training modes, delivering additional up to 14$\times$ offline inference throughputs than TGS and 67\% reduction in online inference p95 latency than MPS, while guaranteeing collocated training throughput.

[Arxiv](https://arxiv.org/abs/2503.02550)