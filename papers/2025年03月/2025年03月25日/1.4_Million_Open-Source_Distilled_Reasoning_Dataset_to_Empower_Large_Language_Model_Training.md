# # 140万开源推理数据集赋能大型语言模型训练
140万开源推理数据集助力大型语言模型训练

发布时间：2025年03月25日

`LLM应用

理由：这篇论文主要介绍了AM-DeepSeek-R1-Distilled数据集的构建及其在训练大型语言模型中的应用。论文详细描述了数据集的来源、清洗过程以及回答的生成方法，并展示了该数据集如何用于监督微调以提升模型在推理任务中的性能。这些内容属于大型语言模型的实际应用和数据集构建，因此归类为“LLM应用”。` `大型语言模型`

> 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training

# 摘要

> AM-DeepSeek-R1-Distilled 是一个用于通用推理任务的大规模数据集，包含高质量且具有挑战性的推理问题。这些问题来自众多开源数据集，并经过语义去重和细致清洗以消除测试集污染。数据集中所有回答均来自推理模型（主要是 DeepSeek-R1）的蒸馏，并经过严格验证程序。数学问题通过参考答案验证，代码问题使用测试用例验证，其他任务则借助奖励模型进行评估。仅通过这批数据进行简单监督微调（SFT）训练的 AM-Distill-Qwen-32B 模型，在 AIME2024、MATH-500、GPQA-Diamond 和 LiveCodeBench 四个基准测试中超越了 DeepSeek-R1-Distill-Qwen-32B 模型。此外，AM-Distill-Qwen-72B 模型也在所有基准测试中超越了 DeepSeek-R1-Distill-Llama-70B 模型。我们向研究界发布了这 140 万个问题及其对应回答，旨在促进以强大推理为导向的大型语言模型（LLMs）的发展。该数据集已发布在 \href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}。

> The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces for general reasoning tasks, composed of high-quality and challenging reasoning problems. These problems are collected from a multitude of open-source datasets, subjected to semantic deduplication and meticulous cleaning to eliminate test set contamination. All responses within the dataset are distilled from reasoning models (predominantly DeepSeek-R1) and have undergone rigorous verification procedures. Mathematical problems are validated by checking against reference answers, code problems are verified using test cases, and other tasks are evaluated with the aid of a reward model. The AM-Distill-Qwen-32B model, which was trained through only simple Supervised Fine-Tuning (SFT) using this batch of data, outperformed the DeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500, GPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model surpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We are releasing these 1.4 million problems and their corresponding responses to the research community with the objective of fostering the development of powerful reasoning-oriented Large Language Models (LLMs). The dataset was published in \href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}.

[Arxiv](https://arxiv.org/abs/2503.19633)