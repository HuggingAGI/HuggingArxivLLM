# # 增强大型语言模型生成代码的稳健性：实证研究与框架体系
本研究旨在提升大型语言模型生成代码的稳健性，通过实证研究与框架体系的构建，探索提升代码质量与可靠性的有效方法。

发布时间：2025年03月25日

`LLM应用` `软件工程` `软件开发`

> Enhancing the Robustness of LLM-Generated Code: Empirical Study and Framework

# 摘要

> 提升大型语言模型生成代码的健壮性对于实际应用中的可靠性至关重要。然而，现有评估方法主要关注代码正确性，往往忽视了输入验证缺失和错误处理不足等关键健壮性问题。本文首次开展关于 LLM 生成代码健壮性的实证研究，提出创新性健壮性指标，并对四个先进代码 LLM 进行分析，发现其生成的代码平均有 43.1% 的健壮性低于人工编写代码。值得注意的是，90% 以上的健壮性缺陷源于缺少条件检查，其中 70% 的问题出现在代码首行。此外，在 69% 的情况下，尽管需要条件语句但缺失时，“if” 令牌仍位列模型预测概率前三，表明模型对控制结构有隐含认知。基于此，我们提出 RobGen，一个无需重新训练模型即可提升代码健壮性的框架。RobGen 包含两种通用技术：RobGen-Adj 在解码时动态调整令牌概率，鼓励包含控制结构；RobGen-Ins 在生成后补充缺失条件，优化代码。实验表明，RobGen 将不健壮代码比例降低 20.0%，显著提升代码可靠性。作为轻量级解决方案，RobGen 有效应对 LLM 生成代码的健壮性挑战。所有代码和数据已开源，地址为 https://github.com/SYSUSELab/RobGen。


> Ensuring the robustness of code generated by large language models (LLMs) is crucial for real-world reliability. However, existing evaluations predominantly focus on correctness, often neglecting key robustness concerns such as missing input validation and insufficient error handling. In this paper, we present the first empirical study on the robustness of LLM-generated code. We introduce novel robustness metrics and analyze four state-of-the-art code LLMs, revealing that, on average, 43.1% of their generated code is less robust than human-written counterparts. Notably, over 90% of robustness deficiencies stem from missing conditional checks, with 70% of these omissions occurring in the first line of code. Additionally, in 69% of cases where a conditional statement is necessary but absent, the "if" token still ranks third or higher in the model's predicted token probabilities, indicating an implicit recognition of control structures. Building on these findings, we propose RobGen, a framework designed to enhance code robustness without requiring model retraining. RobGen leverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts token probabilities during decoding to encourage the inclusion of control structures, and RobGen-Ins, which improves generated code by inserting missing conditionals after generation. Experimental results demonstrate that RobGen reduces the proportion of less robust model-generated code by 20.0%, significantly enhancing code reliability across diverse tasks. As a lightweight and adaptable solution, RobGen effectively mitigates robustness challenges in LLM-generated code. All code and data are available at https://github.com/SYSUSELab/RobGen.

[Arxiv](https://arxiv.org/abs/2503.20197)