# MMLU-ProX: 用于评估先进大型语言模型的多语言基准测试

发布时间：2025年03月13日

`LLM应用` `多语言模型` `跨文化评估`

> MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation

# 摘要

> 传统的基准测试难以有效评估多语言和文化多元背景下日益复杂的语言模型。为填补这一空白，我们推出了 MMLU-ProX，这是一个涵盖 13 种类型多样化语言的全面多语言基准测试，每种语言包含约 11,829 个问题。基于 MMLU-Pro 挑战性的推理导向设计，我们的框架采用了半自动化的翻译流程：通过最先进的大型语言模型（LLMs）生成翻译，并由专家标注员严格评估，确保概念准确性、术语一致性和文化相关性。我们利用 5-shot chain-of-thought（CoT）和零-shot 提示策略，对 25 个最先进的 LLMs 进行全面评估，分析其跨语言和跨文化的表现。实验结果显示，从高资源语言到低资源语言，模型性能持续下降，最佳模型在英语上准确率超过 70%，但对于斯瓦希里语等语言则降至约 40%，凸显了尽管近期有所进展，多语言能力方面仍存在显著差距。MMLU-ProX 项目仍在进行中，我们正在通过增加更多语言和评估更多语言模型来扩展基准测试，以提供更全面的多语言能力评估。

> Traditional benchmarks struggle to evaluate increasingly sophisticated language models in multilingual and culturally diverse contexts. To address this gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark covering 13 typologically diverse languages with approximately 11,829 questions per language. Building on the challenging reasoning-focused design of MMLU-Pro, our framework employs a semi-automatic translation process: translations generated by state-of-the-art large language models (LLMs) are rigorously evaluated by expert annotators to ensure conceptual accuracy, terminological consistency, and cultural relevance. We comprehensively evaluate 25 state-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot prompting strategies, analyzing their performance across linguistic and cultural boundaries. Our experiments reveal consistent performance degradation from high-resource languages to lower-resource ones, with the best models achieving over 70% accuracy on English but dropping to around 40% for languages like Swahili, highlighting persistent gaps in multilingual capabilities despite recent advances. MMLU-ProX is an ongoing project; we are expanding our benchmark by incorporating additional languages and evaluating more language models to provide a more comprehensive assessment of multilingual capabilities.

[Arxiv](https://arxiv.org/abs/2503.10497)