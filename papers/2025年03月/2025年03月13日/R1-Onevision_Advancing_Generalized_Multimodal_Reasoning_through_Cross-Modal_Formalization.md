# R1-Onevision：跨模态形式化助力通用多模态推理的突破

发布时间：2025年03月13日

`LLM应用` `人工智能` `计算机视觉`

> R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization

# 摘要

> 大型语言模型在复杂文本任务中展现出了卓越的推理能力，但在需要整合视觉与文本信息的多模态推理领域，仍面临重大挑战。现有视觉-语言模型往往难以有效分析和推理视觉内容，导致在复杂推理任务中表现欠佳。此外，缺乏全面的基准测试也限制了对多模态推理能力的准确评估。本文中，我们介绍了R1-Onevision，一个多模态推理模型，旨在弥合视觉感知与深度推理之间的差距。为此，我们提出了一种跨模态推理管道，将图像转换为正式的文本表示，从而实现精确的语言推理。借助这一管道，我们构建了R1-Onevision数据集，提供了跨不同领域详细、分步骤的多模态推理标注。我们进一步通过有监督微调和强化学习开发了R1-Onevision模型，以培养先进的推理和强大的泛化能力。为了全面评估不同等级的多模态推理性能，我们引入了R1-Onevision-Bench，一个与人类教育阶段相匹配的基准测试，涵盖了从初中到大学及以上的各类考试。实验结果表明，R1-Onevision实现了最先进的性能，在多个具有挑战性的多模态推理基准测试中优于GPT-4o和Qwen2.5-VL等模型。

> Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.

[Arxiv](https://arxiv.org/abs/2503.10615)