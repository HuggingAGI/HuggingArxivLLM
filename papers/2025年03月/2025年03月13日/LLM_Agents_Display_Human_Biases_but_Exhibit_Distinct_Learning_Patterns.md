# LLM智能体展现人类偏见，却拥有独特的学习模式

发布时间：2025年03月13日

`LLM应用` `心理学` `人工智能`

> LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns

# 摘要

> 我们研究了大型语言模型（LLMs）在基于经验的决策任务中的选择模式，并将其行为与人类进行对比。总体来看，LLMs展现出与人类相似的行为偏差：二者都低估了罕见事件的重要性，并受到相关性效应的影响。然而，深入分析选择模式后发现，背后的原因大相径庭。LLMs表现出强烈的近期偏差，而人类则以更复杂的方式应对。尽管总体行为可能相似，但基于近期事件的选择模式在两组间存在显著差异。具体而言，"意外触发变化"和"罕见事件的波动近期效应"在人类中普遍存在，但在LLMs中则完全缺失。这些发现揭示了LLMs在模拟和预测人类学习行为方面的局限性，并强调了在研究其是否复制人类决策倾向时，需要对其行为进行更细致的分析。

> We investigate the choice patterns of Large Language Models (LLMs) in the context of Decisions from Experience tasks that involve repeated choice and learning from feedback, and compare their behavior to human participants. We find that on the aggregate, LLMs appear to display behavioral biases similar to humans: both exhibit underweighting rare events and correlation effects. However, more nuanced analyses of the choice patterns reveal that this happens for very different reasons. LLMs exhibit strong recency biases, unlike humans, who appear to respond in more sophisticated ways. While these different processes may lead to similar behavior on average, choice patterns contingent on recent events differ vastly between the two groups. Specifically, phenomena such as ``surprise triggers change" and the ``wavy recency effect of rare events" are robustly observed in humans, but entirely absent in LLMs. Our findings provide insights into the limitations of using LLMs to simulate and predict humans in learning environments and highlight the need for refined analyses of their behavior when investigating whether they replicate human decision making tendencies.

[Arxiv](https://arxiv.org/abs/2503.10248)