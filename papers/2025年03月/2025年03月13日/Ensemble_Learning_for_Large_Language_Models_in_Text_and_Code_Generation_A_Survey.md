# 集成学习在大型语言模型文本与代码生成中的应用研究综述

发布时间：2025年03月13日

`LLM应用` `软件工程`

> Ensemble Learning for Large Language Models in Text and Code Generation: A Survey

# 摘要

> 生成式预训练转换器（GPT）作为常见的大型语言模型（LLMs），广泛应用于从自然语言输入生成文本。然而，单个LLM中语言参数的固定性质可能导致生成输出的不一致性。这一限制还由于固有偏见，限制了模型表示多样化语言模式的能力。此外，许多强大的LLMs是闭源的，这使得组织无法将其数据整合到这些系统中，引发了关于数据隐私的担忧，并限制了行业应用。受到LLM集合模型在文本生成中成功应用的启发，最近的文献也研究了它们在代码生成中的潜力。本文综述了这些新兴的LLM集合方法。我们的目标是提高读者对现有技术的理解，并鼓励进一步的研究和实际实现，旨在扩大LLM集合模型在文本和代码生成中的实际应用。我们将这些方法分为七种主要方法：权重合并、知识融合、专家混合、奖励集合、输出集合、路由和级联。从这个列表中，我们专注于四种表现出强大性能和更广泛应用潜力的方法和模型。我们分析了它们的建模步骤、训练方法和输出特征，以提供对其能力的清晰理解。我们的研究结果突显了LLM集合技术的优势，包括更好地表示多样性、提高输出质量和在应用程序中更大的灵活性。这些信息为选择涉及文本和代码生成的各种实际任务中的模型提供了有价值的见解，并有可能将方法应用于多模式LLMs。

> Generative pretrained transformers (GPT) are the common large language models (LLMs) used for generating text from natural language inputs. However, the fixed properties of language parameters in individual LLMs can lead to inconsistencies in the generated outputs. This limitation also restricts the models' ability to represent diverse language patterns due to inherent biases. Moreover, many powerful LLMs are closed-source. This prevents organizations from integrating their data into these systems, raising concerns about data privacy and limiting industry applications. Inspired by the successful application of LLM ensemble models in text generation, recent literature has also investigated their potential in code generation. This article reviews these emerging LLM ensemble approaches. Our goal is to enhance readers' understanding of existing techniques and encourage further research and practical implementation, aiming to expand the real-world applications of LLM ensemble models in both text and code generation. We categorize these approaches into seven main methods: weight merging, knowledge fusion, mixture of experts, reward ensemble, output ensemble, routing, and cascading. From this list, we focus on four methods and models that show strong performance and potential for broader applications. We analyze their modeling steps, training methods, and output features to provide a clear understanding of their capabilities. Our findings highlight the benefits of LLM ensemble techniques. These include better representation of diversity, improved output quality, and greater flexibility in applications. This information offers valuable insights for selecting models for various real-world tasks involving text and code generation, and potentially applying methods to multimodal LLMs.

[Arxiv](https://arxiv.org/abs/2503.13505)