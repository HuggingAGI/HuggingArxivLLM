# MiLiC-Eval: 评估多语言LLM：聚焦中国少数民族语言

发布时间：2025年03月02日

`LLM应用

这篇论文主要探讨了大型语言模型在低资源语言中的应用表现，并引入了一个新的基准测试来评估这些模型在少数民族语言中的性能。研究的重点在于实际应用中的评估和分析，因此归类为LLM应用。` `少数民族语言` `低资源语言`

> MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages

# 摘要

> 大型语言模型 (LLMs) 在高资源语言中表现出色，但在低资源语言 (LRLs) 中表现欠佳，尤其是像藏文、维吾尔文、哈萨克文和蒙古文等中国少数民族语言。为了系统跟踪这些语言的进步，我们引入了 MiLiC-Eval，这是一个为中国少数民族语言设计的基准测试，涵盖了 9 个任务的 24,000 个实例。MiLiC-Eval 专注于代表性不足的书写系统，并对语言和问题解决能力进行细致评估。我们的评估发现，LLMs 在句法密集型任务和多脚本语言上表现不佳。我们进一步展示了 MiLiC-Eval 如何帮助推进 LRL 研究，以处理多样化的书写系统并理解语言适应过程。

> Large language models (LLMs) excel in high-resource languages but struggle with low-resource languages (LRLs), particularly those spoken by minority communities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To systematically track the progress in these languages, we introduce MiLiC-Eval, a benchmark designed for minority languages in China, featuring 24K instances across 9 tasks. MiLiC-Eval focuses on underrepresented writing systems and provides a fine-grained assessment of linguistic and problem-solving skills. Our evaluation reveals that LLMs perform poorly on syntax-intensive tasks and multi-script languages. We further demonstrate how MiLiC-Eval can help advance LRL research in handling diverse writing systems and understanding the process of language adaptation.

[Arxiv](https://arxiv.org/abs/2503.01150)