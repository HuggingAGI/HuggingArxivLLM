# ALinFiK：学习近似线性化未来影响核，用于可扩展的第三方LLM数据估值

发布时间：2025年03月02日

`LLM理论` `数据科学` `数据估值`

> ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Parity LLM Data Valuation

# 摘要

> 大型语言模型（LLMs）的性能 heavily relies on 高质量训练数据，因此在预算有限的情况下，进行数据估值以优化模型性能至关重要。本研究旨在提供一种第三方数据估值方法，使数据提供者和模型开发者都能从中获益。我们引入了一种线性化未来影响核（LinFiK），用于评估单个数据样本在训练过程中对提升LLM性能的价值。进一步地，我们提出了ALinFiK，这是一种学习策略，用于近似LinFiK，从而实现可扩展的数据估值。我们的综合评估表明，这种方法在有效性和效率上均优于现有基准，并且随着LLM参数的增加，展现出显著的可扩展性优势。

> Large Language Models (LLMs) heavily rely on high-quality training data, making data valuation crucial for optimizing model performance, especially when working within a limited budget. In this work, we aim to offer a third-party data valuation approach that benefits both data providers and model developers. We introduce a linearized future influence kernel (LinFiK), which assesses the value of individual data samples in improving LLM performance during training. We further propose ALinFiK, a learning strategy to approximate LinFiK, enabling scalable data valuation. Our comprehensive evaluations demonstrate that this approach surpasses existing baselines in effectiveness and efficiency, demonstrating significant scalability advantages as LLM parameters increase.

[Arxiv](https://arxiv.org/abs/2503.01052)