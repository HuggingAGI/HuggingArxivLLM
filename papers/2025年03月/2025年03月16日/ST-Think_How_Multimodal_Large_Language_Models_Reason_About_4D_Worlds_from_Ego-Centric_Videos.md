# ST-Think: 多模态大语言模型如何从第一人称视角视频推理4D世界

发布时间：2025年03月16日

`LLM应用` `多模态` `视频推理`

> ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos

# 摘要

> 人类在空间时间推理方面表现出色，能够轻松地从以自我为中心的视角理解动态视觉事件。然而，多模态大语言模型（MLLMs）是否能同样理解4D世界仍是一个未解之谜。本文从以自我为中心的视角探索多模态空间时间推理，旨在赋予MLLMs类似人类的推理能力。我们引入了Ego-ST Bench，这是一个包含超过5,000个问答对的新基准测试，涵盖四个类别，系统性地评估空间、时间和综合空间时间推理能力。此外，我们提出了ST-R1 Video模型，这是一种基于视频的推理模型，将逆向思维融入强化学习过程，显著提升了性能。我们结合长链式思考（long-CoT）监督微调与组相对策略优化（GRPO）强化学习，在有限高质量数据的情况下取得了显著改进。Ego-ST Bench和ST-R1为基于视频的空间时间推理研究提供了宝贵的见解和资源。

> Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic visual events from an egocentric viewpoint. However, whether multimodal large language models (MLLMs) can similarly comprehend the 4D world remains uncertain. This paper explores multimodal spatio-temporal reasoning from an egocentric perspective, aiming to equip MLLMs with human-like reasoning capabilities. To support this objective, we introduce Ego-ST Bench, a novel benchmark containing over 5,000 question-answer pairs across four categories, systematically evaluating spatial, temporal, and integrated spatio-temporal reasoning. Additionally, we propose the ST-R1 Video model, a video-based reasoning model that incorporates reverse thinking into its reinforcement learning process, significantly enhancing performance. We combine long-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative Policy Optimization (GRPO) reinforcement learning, achieving notable improvements with limited high-quality data. Ego-ST Bench and ST-R1 provide valuable insights and resources for advancing video-based spatio-temporal reasoning research.

[Arxiv](https://arxiv.org/abs/2503.12542)