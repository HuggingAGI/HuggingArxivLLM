# 基于大型语言模型的合成数据生成：在文本与代码领域的最新进展

发布时间：2025年03月18日

`LLM应用` `软件工程`

> Synthetic Data Generation Using Large Language Models: Advances in Text and Code

# 摘要

> 大型语言模型（LLMs）为生成自然语言和代码的合成训练数据带来了革命性突破。通过生成人工但与任务相关的示例，这些模型能够显著增强甚至替代真实世界的数据集，尤其是在标记数据稀缺或敏感的情况下。本文综述了利用LLMs创建合成文本和代码的最新进展，重点介绍了基于提示的生成、检索增强的流水线以及迭代自我优化等方法。这些方法通过实现功能正确性的自动化验证，为分类、问答等低资源任务，以及指令微调、代码翻译和错误修复等代码为中心的应用注入了新的活力。除了成本效益、广泛覆盖和可控多样性等潜在优势外，我们还探讨了生成文本的事实不准确、缺乏风格现实性以及偏见放大的风险等挑战。建议的缓解措施包括过滤和加权输出，以及利用执行反馈的强化学习来改进代码生成。最后，我们提出了开放的研究方向，如自动提示工程、跨模态数据合成和稳健的评估框架，强调了LLM生成的合成数据在推动AI发展中的重要性，同时强调了伦理和质量保障的必要性。

> Large language models (LLMs) have unlocked new possibilities for generating synthetic training data in both natural language and code. By producing artificial but task-relevant examples, these models can significantly augment or even replace real-world datasets, especially when labeled data is scarce or sensitive. This paper surveys recent advances in using LLMs to create synthetic text and code, emphasizing prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We show how these methods enrich low-resource tasks such as classification and question answering, as well as code-centric applications such as instruction tuning, code translation, and bug repair, by enabling automated verification of functional correctness. Alongside potential benefits like cost-effectiveness, broad coverage, and controllable diversity, we address challenges such as factual inaccuracies in generated text, lack of stylistic realism, and the risk of bias amplification. Proposed mitigations include filtering and weighting outputs and reinforcement learning with execution feedback for code. We conclude with open research directions like automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, highlighting the importance of LLM-generated synthetic data in advancing AI while emphasizing ethical and quality safeguards.

[Arxiv](https://arxiv.org/abs/2503.14023)