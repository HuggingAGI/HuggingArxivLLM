# # 多模态大语言模型的对抗鲁棒性综述

发布时间：2025年03月18日

`LLM理论

LLM理论` `人工智能` `多模态`

> Survey of Adversarial Robustness in Multimodal Large Language Models

# 摘要

> 多模态大型语言模型 (MLLMs) 在人工智能领域展现了卓越的跨模态综合理解能力，涵盖文本、图像、视频、音频和语音等多种形式。然而，这些模型在实际应用中面临显著的对抗漏洞风险，可能威胁其安全性和可靠性。与单一模态模型不同，MLLMs 因模态间的相互依赖性，面临独特挑战，易受特定模态威胁和跨模态对抗攻击。本文综述了 MLLMs 的对抗鲁棒性，涵盖不同模态。首先，我们概述了 MLLMs，并针对每种模态制定了对抗攻击分类法。接着，我们回顾了用于评估 MLLMs 鲁棒性的关键数据集和评估指标。随后，我们深入综述了针对不同模态 MLLMs 的攻击方法。我们的调查还指出了关键挑战，并提出了有前景的未来研究方向。

> Multimodal Large Language Models (MLLMs) have demonstrated exceptional performance in artificial intelligence by facilitating integrated understanding across diverse modalities, including text, images, video, audio, and speech. However, their deployment in real-world applications raises significant concerns about adversarial vulnerabilities that could compromise their safety and reliability. Unlike unimodal models, MLLMs face unique challenges due to the interdependencies among modalities, making them susceptible to modality-specific threats and cross-modal adversarial manipulations. This paper reviews the adversarial robustness of MLLMs, covering different modalities. We begin with an overview of MLLMs and a taxonomy of adversarial attacks tailored to each modality. Next, we review key datasets and evaluation metrics used to assess the robustness of MLLMs. After that, we provide an in-depth review of attacks targeting MLLMs across different modalities. Our survey also identifies critical challenges and suggests promising future research directions.

[Arxiv](https://arxiv.org/abs/2503.13962)