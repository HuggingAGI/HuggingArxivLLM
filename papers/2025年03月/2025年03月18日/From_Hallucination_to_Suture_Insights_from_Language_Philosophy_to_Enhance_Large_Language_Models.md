# 从“幻觉”到“缝合”：语言哲学的见解，助力大型语言模型的提升

发布时间：2025年03月18日

`LLM理论` `语言模型` `理论框架`

> From "Hallucination" to "Suture": Insights from Language Philosophy to Enhance Large Language Models

# 摘要

> 本文通过语言哲学和心理分析的视角，深入探讨大型语言模型中的幻觉现象。我们引入拉康的“符号链”和“缝合点”概念，提出了一种名为Anchor-RAG的新型框架，为缓解幻觉问题提供了创新思路。与当前研究中普遍依赖的试错实验、持续调整数学公式，或是以数量取胜的资源密集型方法不同，我们的方法回归语言学的基本原理，深入剖析大型语言模型中幻觉现象的本质成因。基于坚实的理论基础，我们推导出一系列算法和模型，不仅能够有效减少幻觉现象，还能显著提升大型语言模型的性能并改善输出质量。本文旨在构建一个全面理解大型语言模型幻觉现象的理论框架，并挑战当前领域中普遍存在的“猜测与测试”方法及资源竞赛心态。我们希望为可解释的大型语言模型开辟一个新时代，深入揭示基于语言的人工智能系统内在运行机制，推动人工智能技术向更透明、更可控的方向发展。

> This paper explores hallucination phenomena in large language models (LLMs) through the lens of language philosophy and psychoanalysis. By incorporating Lacan's concepts of the "chain of signifiers" and "suture points," we propose the Anchor-RAG framework as a novel approach to mitigate hallucinations. In contrast to the predominant reliance on trial-and-error experiments, constant adjustments of mathematical formulas, or resource-intensive methods that emphasize quantity over quality, our approach returns to the fundamental principles of linguistics to analyze the root causes of hallucinations in LLMs. Drawing from robust theoretical foundations, we derive algorithms and models that are not only effective in reducing hallucinations but also enhance LLM performance and improve output quality. This paper seeks to establish a comprehensive theoretical framework for understanding hallucinations in LLMs and aims to challenge the prevalent "guess-and-test" approach and rat race mentality in the field. We aspire to pave the way for a new era of interpretable LLMs, offering deeper insights into the inner workings of language-based AI systems.

[Arxiv](https://arxiv.org/abs/2503.14392)