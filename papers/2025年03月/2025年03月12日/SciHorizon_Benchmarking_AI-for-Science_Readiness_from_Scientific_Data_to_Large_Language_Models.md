# SciHorizon：科学视界：从科学数据到大型语言模型，评估AI在科学领域的准备情况。

发布时间：2025年03月12日

`LLM应用` `科学研究` `评估框架`

> SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models

# 摘要

> 近年来，人工智能技术，尤其是大型语言模型（LLMs），的迅猛发展彻底改变了科学研究的范式，使AI-for-Science（AI4Science）成为一个充满活力且快速发展的领域。然而，目前仍缺乏一个有效的框架来全面评估AI4Science，尤其是在数据质量和模型能力的全局视角上。因此，本研究提出了一套名为SciHorizon的全面评估框架，旨在从科学数据和LLM两个角度对AI4Science的准备程度进行基准测试。

首先，我们提出了一种通用的框架来评估AI准备好的科学数据，涵盖四个关键维度：质量、FAIR性、可解释性和合规性，细分为15个子维度。基于2018年至2023年间在同行评审期刊上发表的数据资源论文，我们为地球和生命科学领域提供了AI准备好的数据集推荐列表，为该领域做出了新颖和原创的贡献。

同时，为了评估LLMs在多个科学学科中的能力，我们基于五个核心指标——知识、理解、推理、多模态和价值观——建立了16个评估维度，涵盖数学、物理、化学、生命科学和地球与空间科学。基于这些基准数据集，我们对20多个具有代表性的开源和闭源LLMs进行了全面评估。所有结果均可公开获取，并可通过www.scihorizon.cn/en在线访问。

> In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for both Earth and Life Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 20 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.

[Arxiv](https://arxiv.org/abs/2503.13503)