# 显式学习在机器翻译中的大型语言模型应用

发布时间：2025年03月12日

`LLM理论` `语言模型` `机器学习`

> Explicit Learning and the LLM in Machine Translation

# 摘要

> 本研究探讨了大型语言模型 (LLMs) 的显性学习能力，这一过程涉及通过整合元语言解释来执行语言任务。我们采用通过密码学手段生成的构造语言作为受控测试环境，设计了实验以评估 LLM 显性学习和应用语法规则的能力。我们的结果显示，尽管 LLM 具备可测量的显性学习能力，但这种能力会随着手头语言现象复杂度的增加而减弱。对思维链的监督微调显著提升了 LLM 的性能，但难以泛化到类型上新颖或更复杂的语言特征。这些发现表明，需要更多样化的训练集和替代微调策略，以进一步提高 LLM 的显性学习能力。

> This study explores the capacity of large language models (LLMs) for explicit learning, a process involving the assimilation of metalinguistic explanations to carry out language tasks. Using constructed languages generated by cryptographic means as controlled test environments, we designed experiments to assess an LLM's ability to explicitly learn and apply grammar rules. Our results demonstrate that while LLMs possess a measurable capacity for explicit learning, this ability diminishes as the complexity of the linguistic phenomena at hand increases. Supervised fine-tuning on chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs.

[Arxiv](https://arxiv.org/abs/2503.09454)