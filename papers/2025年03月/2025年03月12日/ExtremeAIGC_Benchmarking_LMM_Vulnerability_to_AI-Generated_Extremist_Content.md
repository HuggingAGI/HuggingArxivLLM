# 极端AI生成内容：评估大型语言模型对AI生成极端主义内容的漏洞

发布时间：2025年03月12日

`LLM应用` `极端主义内容检测`

> ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist Content

# 摘要

> 大型多模态模型（LMMs）正日益面临AI生成极端主义内容的威胁，包括照片级真实的图像和文本，这些内容可被用于绕过安全机制，生成有害输出。然而，现有用于评估LMM鲁棒性的数据集对极端主义内容的探索十分有限，通常缺乏AI生成的图像、多样化的图像生成模型以及对历史事件的全面覆盖，这阻碍了对模型漏洞的全面评估。为填补这一空白，我们推出ExtremeAIGC——一个专为评估LMM在面对此类内容时的漏洞而设计的基准数据集和评估框架。通过策划使用先进图像生成技术创建的多样化文本和图像示例，ExtremeAIGC模拟现实世界中的事件和恶意用例。研究表明，LMMs存在令人担忧的弱点，即使是最先进的安全措施也无法防止极端主义材料的生成。我们系统地量化了各种攻击策略的成功率，揭示了当前防御机制中的关键漏洞，并强调了开发更 robust 缓解策略的迫切需求。

> Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated extremist content, including photorealistic images and text, which can be used to bypass safety mechanisms and generate harmful outputs. However, existing datasets for evaluating LMM robustness offer limited exploration of extremist content, often lacking AI-generated images, diverse image generation models, and comprehensive coverage of historical events, which hinders a complete assessment of model vulnerabilities. To fill this gap, we introduce ExtremeAIGC, a benchmark dataset and evaluation framework designed to assess LMM vulnerabilities against such content. ExtremeAIGC simulates real-world events and malicious use cases by curating diverse text- and image-based examples crafted using state-of-the-art image generation techniques. Our study reveals alarming weaknesses in LMMs, demonstrating that even cutting-edge safety measures fail to prevent the generation of extremist material. We systematically quantify the success rates of various attack strategies, exposing critical gaps in current defenses and emphasizing the need for more robust mitigation strategies.

[Arxiv](https://arxiv.org/abs/2503.09964)