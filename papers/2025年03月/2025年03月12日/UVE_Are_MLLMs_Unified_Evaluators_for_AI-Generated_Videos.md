# UVE：MLLMs 是否能够统一评估 AI 生成的视频？

发布时间：2025年03月12日

`LLM应用` `视频生成` `视频评估`

> UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?

# 摘要

> 视频生成模型的快速发展使得开发可靠的AI生成视频自动评估指标变得至关重要。现有方法要么使用针对其他任务优化的现成模型，要么依赖人工评估数据训练专门评估器，这些方法在特定评估方面受限且难以满足更细致和全面的评估需求。本研究探讨了使用多模态大型语言模型（MLLMs）作为统一AIGV评估器的可行性，利用其强大的视觉感知和语言理解能力。为了评估自动指标在统一AIGV评估中的性能，我们引入了UVE-Bench基准测试，它收集了最新VGMs生成的视频，并提供了涵盖15个评估方面的成对人类偏好标注。通过UVE-Bench，我们对16种MLLMs进行了全面评估，发现尽管先进的MLLMs（如Qwen2VL-72B和InternVL2.5-78B）仍落后于人工评估者，但它们在统一AIGV评估中展示了令人鼓舞的能力，显著超越了现有专门评估方法。此外，我们深入分析了影响MLLM驱动评估器性能的关键设计选择，为未来AIGV评估研究提供了宝贵见解。代码可在https://github.com/bytedance/UVE获取。

> With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.

[Arxiv](https://arxiv.org/abs/2503.09949)