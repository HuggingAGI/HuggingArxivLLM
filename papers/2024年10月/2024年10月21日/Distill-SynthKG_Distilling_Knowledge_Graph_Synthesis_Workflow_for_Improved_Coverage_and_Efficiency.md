# Distill-SynthKG: 优化知识图谱合成流程，提升覆盖范围与效率

发布时间：2024年10月21日

`RAG

理由：该论文主要讨论了如何利用大型语言模型（LLMs）生成知识图谱（KGs）来增强检索增强生成（RAG）应用。论文提出了SynthKG和Distill-SynthKG两种方法，旨在提高KG生成的效率和质量，并设计了一个图检索框架用于RAG。这些内容直接涉及RAG技术的改进和应用，因此应归类为RAG。` `知识图谱` `问答系统`

> Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for Improved Coverage and Efficiency

# 摘要

> # 摘要
大型语言模型（LLMs）生成的知识图谱（KGs）在需要知识密集型推理的检索增强生成（RAG）应用中日益重要。然而，现有的KG提取方法主要依赖基于提示的方法，处理大规模语料库时效率低下，且在处理长文档时容易丢失信息。此外，无本体KG构建的评估数据集和方法尚不完善。为此，我们提出了SynthKG，一种基于LLMs的多步骤、文档级无本体KG合成工作流。通过微调较小的LLM，我们将多步骤过程简化为单步骤KG生成方法Distill-SynthKG，显著减少了LLM推理调用次数。我们还重新利用现有问答数据集构建KG评估数据集，并引入新的评估指标。基于Distill-SynthKG生成的KGs，我们设计了一个新颖的图检索框架用于RAG。实验表明，Distill-SynthKG不仅在KG质量上超越所有基线模型（包括高达八倍大的模型），还在检索和问答任务中表现优异。我们的图检索框架在多个基准数据集上也优于所有KG检索方法。我们公开发布了SynthKG数据集和Distill-SynthKG模型，以促进进一步研究。

> Knowledge graphs (KGs) generated by large language models (LLMs) are becoming increasingly valuable for Retrieval-Augmented Generation (RAG) applications that require knowledge-intensive reasoning. However, existing KG extraction methods predominantly rely on prompt-based approaches, which are inefficient for processing large-scale corpora. These approaches often suffer from information loss, particularly with long documents, due to the lack of specialized design for KG construction. Additionally, there is a gap in evaluation datasets and methodologies for ontology-free KG construction. To overcome these limitations, we propose SynthKG, a multi-step, document-level ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM on the synthesized document-KG pairs, we streamline the multi-step process into a single-step KG generation approach called Distill-SynthKG, substantially reducing the number of LLM inference calls. Furthermore, we re-purpose existing question-answering datasets to establish KG evaluation datasets and introduce new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a novel graph-based retrieval framework for RAG. Experimental results demonstrate that Distill-SynthKG not only surpasses all baseline models in KG quality -- including models up to eight times larger -- but also consistently excels in retrieval and question-answering tasks. Our proposed graph retrieval framework also outperforms all KG-retrieval methods across multiple benchmark datasets. We release the SynthKG dataset and Distill-SynthKG model publicly to support further research and development.

[Arxiv](https://arxiv.org/abs/2410.16597)