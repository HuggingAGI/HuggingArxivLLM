# Allo-AVA：一个大规模多模态对话AI数据集，专为他中心化虚拟角色手势动画设计

发布时间：2024年10月21日

`其他

理由：这篇论文主要讨论的是多模态训练数据的创建和应用，特别是针对虚拟形象动画的生成。虽然涉及到了AI和虚拟形象，但并没有直接涉及到Agent、RAG、LLM应用或LLM理论的核心内容。因此，将其分类为“其他”更为合适。` `虚拟现实` `数字助手`

> Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for Allocentric Avatar Gesture Animation

# 摘要

> 高质量多模态训练数据的匮乏，严重制约了虚拟环境中对话AI逼真虚拟形象动画的创作。现有数据集往往缺少语音、表情和肢体动作之间的精细同步，而这正是自然人类交流的核心特征。为填补这一空白，我们推出了Allo-AVA——一个专为他者视角（第三人称）下文本与音频驱动的虚拟形象手势动画设计的大规模数据集。Allo-AVA囊括约1,250小时的多样化视频内容，包含音频、文字记录及提取的关键点，并创新性地将这些关键点与精确时间戳对应，确保人类动作（包括身体和面部表情）与语音完美同步。这一丰富资源为开发与评估更自然、情境感知的虚拟形象动画模型铺平道路，有望革新从虚拟现实到数字助手的众多应用领域。

> The scarcity of high-quality, multimodal training data severely hinders the creation of lifelike avatar animations for conversational AI in virtual environments. Existing datasets often lack the intricate synchronization between speech, facial expressions, and body movements that characterize natural human communication. To address this critical gap, we introduce Allo-AVA, a large-scale dataset specifically designed for text and audio-driven avatar gesture animation in an allocentric (third person point-of-view) context. Allo-AVA consists of $\sim$1,250 hours of diverse video content, complete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely maps these keypoints to precise timestamps, enabling accurate replication of human movements (body and facial gestures) in synchronization with speech. This comprehensive resource enables the development and evaluation of more natural, context-aware avatar animation models, potentially transforming applications ranging from virtual reality to digital assistants.

[Arxiv](https://arxiv.org/abs/2410.16503)