# 利用LLMs评估解释：突破传统用户研究的局限

发布时间：2024年10月23日

`LLM应用` `人工智能`

> Evaluating Explanations Through LLMs: Beyond Traditional User Studies

# 摘要

> 随着AI在医疗等领域的广泛应用，可解释AI（XAI）工具成为建立信任和透明度的关键。然而，传统用户研究成本高、耗时长且难以扩展。本文探索了使用大型语言模型（LLMs）模拟人类参与者，以简化XAI评估。我们复现了一项比较反事实和因果解释的用户研究，并在不同设置下使用七个LLMs模拟人类参与者。结果表明：（i）LLMs能复现大部分原始研究结论，（ii）不同LLMs的结果对齐性存在差异，（iii）LLM的记忆和输出变异性等实验因素会影响与人类响应的对齐性。这些发现表明，LLMs可能为XAI评估提供一种可扩展且经济高效的解决方案。

> As AI becomes fundamental in sectors like healthcare, explainable AI (XAI) tools are essential for trust and transparency. However, traditional user studies used to evaluate these tools are often costly, time consuming, and difficult to scale. In this paper, we explore the use of Large Language Models (LLMs) to replicate human participants to help streamline XAI evaluation. We reproduce a user study comparing counterfactual and causal explanations, replicating human participants with seven LLMs under various settings. Our results show that (i) LLMs can replicate most conclusions from the original study, (ii) different LLMs yield varying levels of alignment in the results, and (iii) experimental factors such as LLM memory and output variability affect alignment with human responses. These initial findings suggest that LLMs could provide a scalable and cost-effective way to simplify qualitative XAI evaluation.

[Arxiv](https://arxiv.org/abs/2410.17781)