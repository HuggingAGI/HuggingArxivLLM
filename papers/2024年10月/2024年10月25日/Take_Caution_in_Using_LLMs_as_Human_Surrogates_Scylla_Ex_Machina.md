# 使用大型语言模型（LLMs）作为人类替代品时需谨慎：Scylla Ex Machina

发布时间：2024年10月25日

`LLM应用` `社会科学` `人类行为研究`

> Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina

# 摘要

> 近期研究显示，大型语言模型（LLMs）能展现类似人类的推理能力，在经济实验、调查及政治论述中与人类行为相符。于是，很多人提议在社会科学研究中把LLMs当作人类的替代品。但LLMs和人类有根本差异，它们依靠概率模式，没有塑造人类认知的亲身经验或生存目标。我们通过11-20金钱请求游戏评估LLMs的推理深度。几乎所有先进方法都无法在众多模型中重现人类行为分布，只有一种使用大量人类行为数据进行微调的情况例外。失败原因多样，涉及输入语言、角色和保障措施等。这些结果警示我们，不要用LLMs研究人类行为或充当人类替代品。

> Recent studies suggest large language models (LLMs) can exhibit human-like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Almost all advanced approaches fail to replicate human behavior distributions across many models, except in one case involving fine-tuning using a substantial amount of human behavior data. Causes of failure are diverse, relating to input language, roles, and safeguarding. These results caution against using LLMs to study human behaviors or as human surrogates.

[Arxiv](https://arxiv.org/abs/2410.19599)