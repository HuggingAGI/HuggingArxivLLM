# FairMindSim: 伦理困境下人类与LLM代理的行为、情感与信念一致性研究

发布时间：2024年10月17日

`Agent

理由：这篇论文主要探讨了利用LLM代理模拟人类行为，特别是在不公平情境下的道德困境和社会经济动机。研究通过构建信念-奖励对齐行为演化模型（BREM）来模拟和分析这些行为，强调了LLM代理在模拟和干预人类行为中的应用。因此，这篇论文更适合归类为Agent，因为它主要关注的是LLM代理的行为模拟和干预。` `人工智能` `社会学`

> FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas

# 摘要

> AI对齐是关乎AI控制与安全的核心议题，需兼顾价值中立的人类偏好与道德伦理考量。本研究推出FairMindSim，通过模拟不公平情境探讨道德困境。我们利用LLM代理模拟人类行为，确保多阶段对齐。为探究驱动人类及LLM代理作为旁观者介入他人不公情境的社会经济动机（即信念），以及这些信念如何交织影响个体行为，我们融合社会学知识，基于递归奖励模型（RRM）构建了信念-奖励对齐行为演化模型（BREM）。研究发现，GPT-4o在行为上展现更强的社会正义感，而人类情感更为丰富。同时，我们探讨了情感对行为的潜在影响，为LLM与利他价值观对齐的应用奠定了理论基础。

> AI alignment is a pivotal issue concerning AI control and safety. It should consider not only value-neutral human preferences but also moral and ethical considerations. In this study, we introduced FairMindSim, which simulates the moral dilemma through a series of unfair scenarios. We used LLM agents to simulate human behavior, ensuring alignment across various stages. To explore the various socioeconomic motivations, which we refer to as beliefs, that drive both humans and LLM agents as bystanders to intervene in unjust situations involving others, and how these beliefs interact to influence individual behavior, we incorporated knowledge from relevant sociological fields and proposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on the recursive reward model (RRM). Our findings indicate that, behaviorally, GPT-4o exhibits a stronger sense of social justice, while humans display a richer range of emotions. Additionally, we discussed the potential impact of emotions on behavior. This study provides a theoretical foundation for applications in aligning LLMs with altruistic values.

[Arxiv](https://arxiv.org/abs/2410.10398)