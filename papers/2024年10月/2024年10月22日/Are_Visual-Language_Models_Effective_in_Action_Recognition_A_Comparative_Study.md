# 视觉-语言模型在动作识别中的有效性：一项比较研究

发布时间：2024年10月22日

`其他

理由：这篇论文主要讨论的是视觉-语言基础模型（如CLIP）在复杂细粒度动作识别任务中的表现，属于计算机视觉和自然语言处理的交叉领域，并不直接涉及Agent、RAG、LLM应用或LLM理论。因此，将其分类为“其他”更为合适。` `计算机视觉` `行为分析`

> Are Visual-Language Models Effective in Action Recognition? A Comparative Study

# 摘要

> 当前视觉-语言基础模型（如CLIP）在各类下游任务中表现亮眼，但其在复杂细粒度动作识别任务中的表现仍待验证。为探索未来野外人类行为分析的研究方向，本文通过对比当前顶尖视觉基础模型在零样本和逐帧动作识别任务中的迁移能力，提供了深入见解。实验覆盖了多个细粒度、以人为中心的动作识别数据集（如Toyota Smarthome、Penn Action、UAV-Human、TSU、Charades），涵盖动作分类与分割任务。

> Current vision-language foundation models, such as CLIP, have recently shown significant improvement in performance across various downstream tasks. However, whether such foundation models significantly improve more complex fine-grained action recognition tasks is still an open question. To answer this question and better find out the future research direction on human behavior analysis in-the-wild, this paper provides a large-scale study and insight on current state-of-the-art vision foundation models by comparing their transfer ability onto zero-shot and frame-wise action recognition tasks. Extensive experiments are conducted on recent fine-grained, human-centric action recognition datasets (e.g., Toyota Smarthome, Penn Action, UAV-Human, TSU, Charades) including action classification and segmentation.

[Arxiv](https://arxiv.org/abs/2410.17149)