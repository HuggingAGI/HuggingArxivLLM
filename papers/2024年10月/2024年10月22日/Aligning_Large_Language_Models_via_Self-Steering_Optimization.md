# 通过自我导向优化实现大型语言模型的对齐

发布时间：2024年10月22日

`Agent

理由：这篇论文主要讨论的是自动对齐系统，特别是通过自引导优化（$SSO$）算法来生成高质量的偏好信号，从而提升策略模型和奖励模型的训练效果。虽然涉及到LLM（如Qwen2和Llama3.1），但核心内容是关于如何通过自动化的方式（即Agent）来优化对齐过程，减少人工干预。因此，这篇论文更适合归类为Agent。` `人工智能` `机器学习`

> Aligning Large Language Models via Self-Steering Optimization

# 摘要

> # 自动对齐
自动对齐系统通过最小化人工干预实现对齐。其核心在于无需人工标注，即可为偏好学习提供可学习且准确的偏好信号。本文提出的自引导优化（$SSO$）算法，能够在迭代训练中基于预定义原则自主生成高质量偏好信号，完全摆脱手动标注的依赖。$SSO$ 通过保持所选与拒绝响应之间的稳定差距，确保信号准确性，同时使两者始终符合当前策略模型的学习能力。$SSO$ 不仅提升了策略模型的在线与离线训练效果，还显著增强了奖励模型的训练表现。我们通过 Qwen2 和 Llama3.1 两个基础模型验证了 $SSO$ 的有效性，证明其在迭代训练中持续提供准确且策略一致的偏好信号。在无需人工标注或外部模型的情况下，$SSO$ 在六个主客观基准上实现了显著的性能提升。此外，$SSO$ 生成的偏好数据大幅提升了奖励模型在 Rewardbench 上的表现。本研究为偏好优化提供了一种可扩展的方法，推动了自动对齐的高效与有效发展。

> Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. $SSO$ can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of $SSO$ with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, $SSO$ leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by $SSO$ significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.

[Arxiv](https://arxiv.org/abs/2410.17131)