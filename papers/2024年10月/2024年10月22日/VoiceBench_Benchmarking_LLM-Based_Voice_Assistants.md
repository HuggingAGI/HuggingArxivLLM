# VoiceBench: 基于LLM的语音助手性能评测

发布时间：2024年10月22日

`LLM应用

理由：这篇论文主要讨论了基于大型语言模型（LLMs）的语音助手的评估基准测试，特别是针对实时语音交互能力的评估。论文提出了一个新的基准测试VoiceBench，用于评估基于LLM的语音助手在复杂现实场景中的表现。这属于LLM在实际应用中的具体应用场景，因此归类为“LLM应用”。` `语音交互` `人工智能`

> VoiceBench: Benchmarking LLM-Based Voice Assistants

# 摘要

> # 摘要
随着大型语言模型（LLMs）的成功，GPT-4o等最新进展通过基于LLM的语音助手实现了实时语音交互，显著提升了用户体验。然而，缺乏专门评估语音交互能力的基准测试，阻碍了基于LLM的语音助手的发展。现有评估主要关注自动语音识别（ASR）或使用清晰语音的通用知识测试，忽略了涉及多样说话者特征、环境和内容因素的复杂现实场景。为此，我们推出了VoiceBench，首个针对基于LLM的语音助手的多维度评估基准。VoiceBench包含真实和合成的语音指令，涵盖上述三个关键现实变量。大量实验揭示了当前基于LLM的语音助手模型的不足，为该领域的未来研究提供了重要启示。

> Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.

[Arxiv](https://arxiv.org/abs/2410.17196)