# 企业级LLM评估基准

发布时间：2024年10月11日

`LLM应用

理由：这篇论文主要讨论了如何评估大型语言模型（LLMs）在企业应用中的表现，并提出了一个评估框架，利用领域特定数据集进行多任务基准测试。这属于LLM在实际应用中的评估和优化，因此归类为LLM应用。` `企业应用`

> Enterprise Benchmarks for Large Language Model Evaluation

# 摘要

> 随着大型语言模型（LLMs）的快速发展，如何系统评估其在企业应用中的复杂任务表现成为一大挑战。为此，LLMs需要能够对企业数据集进行多任务基准测试。本文系统探讨了针对LLM评估的基准测试策略，重点利用领域特定数据集，涵盖多种NLP任务。我们提出的评估框架整合了来自金融服务、法律、网络安全及气候与可持续发展等领域的25个公开数据集。13个模型在不同企业任务中的表现差异显著，强调了根据任务需求选择合适模型的重要性。相关代码和提示已开源在GitHub上。

> The advancement of large language models (LLMs) has led to a greater challenge of having a rigorous and systematic evaluation of complex tasks performed, especially in enterprise applications. Therefore, LLMs need to be able to benchmark enterprise datasets for various tasks. This work presents a systematic exploration of benchmarking strategies tailored to LLM evaluation, focusing on the utilization of domain-specific datasets and consisting of a variety of NLP tasks. The proposed evaluation framework encompasses 25 publicly available datasets from diverse enterprise domains like financial services, legal, cyber security, and climate and sustainability. The diverse performance of 13 models across different enterprise tasks highlights the importance of selecting the right model based on the specific requirements of each task. Code and prompts are available on GitHub.

[Arxiv](https://arxiv.org/abs/2410.12857)