# 《潘多拉魔盒再度开启：剖析大型语言模型在与人工智能生成假新闻的持续斗争中的角色》

发布时间：2024年10月24日

`LLM应用` `人工智能`

> The Reopening of Pandora's Box: Analyzing the Role of LLMs in the Evolving Battle Against AI-Generated Fake News

# 摘要

> 随着大型语言模型（LLMs）大规模生成的人工智能内容的涌现，人们对假新闻传播的担忧愈发强烈。LLMs 被认为能够大规模炮制令人信服的假新闻，这给人类和自动化的假新闻检测系统都带来新挑战。为填补这一空缺，本研究呈现了一场大学级别竞赛的成果，该竞赛旨在探究人类如何利用 LLMs 制造假新闻，以及评估人类标注员和 AI 模型检测假新闻的能力。总计 110 名参与者借助 LLMs 创作了 252 个独特的假新闻故事，84 名标注员参与了检测任务。我们的发现表明，LLMs 检测真实新闻的效果比人类高约 68%。然而，在假新闻检测方面，LLMs 和人类的表现旗鼓相当（准确率约 60%）。另外，我们考察了新闻中的视觉元素（比如图片）对检测假新闻故事准确性的影响。最后，我们还研究了假新闻制造者为增强其 AI 生成内容可信度所采用的各种策略。此项工作突显了检测 AI 生成假新闻的复杂性与日俱增，尤其在人类与 AI 协作的情境中。

> With the rise of AI-generated content spewed at scale from large language models (LLMs), genuine concerns about the spread of fake news have intensified. The perceived ability of LLMs to produce convincing fake news at scale poses new challenges for both human and automated fake news detection systems. To address this gap, this work presents the findings from a university-level competition which aimed to explore how LLMs can be used by humans to create fake news, and to assess the ability of human annotators and AI models to detect it. A total of 110 participants used LLMs to create 252 unique fake news stories, and 84 annotators participated in the detection tasks. Our findings indicate that LLMs are ~68% more effective at detecting real news than humans. However, for fake news detection, the performance of LLMs and humans remains comparable (~60% accuracy). Additionally, we examine the impact of visual elements (e.g., pictures) in news on the accuracy of detecting fake news stories. Finally, we also examine various strategies used by fake news creators to enhance the credibility of their AI-generated content. This work highlights the increasing complexity of detecting AI-generated fake news, particularly in collaborative human-AI settings.

[Arxiv](https://arxiv.org/abs/2410.19250)