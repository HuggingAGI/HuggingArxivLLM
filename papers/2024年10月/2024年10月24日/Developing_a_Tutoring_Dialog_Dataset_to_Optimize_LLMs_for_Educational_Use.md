# 构建辅导对话数据集，优化教育领域的大型语言模型

发布时间：2024年10月24日

`LLM应用

理由：这篇论文探讨了在教育领域中使用小型LLMs进行一对一辅导的可行性，并构建了一个合成辅导对话数据集来微调小型LLM。研究结果表明，微调模型在实际辅导场景中的表现不逊于大型模型，且成本更低。这属于LLM在实际应用中的具体应用场景，因此分类为“LLM应用”。` `辅导系统`

> Developing a Tutoring Dialog Dataset to Optimize LLMs for Educational Use

# 摘要

> # 摘要
最近大型语言模型（LLMs）的进展展现了其在教育领域的广阔应用前景，但基于对话的辅导系统仍面临教学策略有效性和高成本数据集的挑战。本研究聚焦于阅读理解问题，探索了使用更小、更经济的LLMs进行一对一辅导的可行性。我们构建了一个由人类教师评估的合成辅导对话数据集，并基于此对小型LLM进行微调。通过交互实验，我们对比了微调模型与大型模型在实际辅导场景中的表现。结果显示，微调模型在性能上不逊于大型模型，且成本更低，为教育领域提供了经济高效的LLM辅导系统解决方案。

> Recent advances in large language models (LLMs) have shown promise for scalable educational applications, but their use in dialog-based tutoring systems remains challenging due to the need for effective pedagogical strategies and the high costs associated with expert-curated datasets. Our study explores the use of smaller, more affordable LLMs for one-on-one tutoring in the context of solving reading comprehension problems. We developed a synthetic tutoring dialog dataset, evaluated by human teachers, and fine-tuned a smaller LLM using this dataset. Furthermore, we conducted an interactive experiment comparing the performance of the fine-tuned model with a larger model in real-world tutoring scenarios. Our results show that the fine-tuned model performs on par with the larger model but at a lower cost, demonstrating a viable, cost-effective approach for implementing LLM-based tutoring systems in educational settings.

[Arxiv](https://arxiv.org/abs/2410.19231)