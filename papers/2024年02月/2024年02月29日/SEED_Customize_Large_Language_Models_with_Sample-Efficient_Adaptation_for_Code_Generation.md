# SEED 方法致力于通过高效适应的方式定制大规模语言模型，以优化其在代码生成任务中的表现。

发布时间：2024年02月29日

`LLM应用`

> SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation

# 摘要

> 尽管LLMs在代码生成领域取得突破，但在某些特定场景下的代码生成任务上仍有困难。由于实际可用训练数据有限，LLMs往往难以满足特定需求，导致生成代码质量不高。针对如何用少量训练样本有效引导LLMs适应新场景这一挑战，本研究提出一种名为SEED的创新适应方法，它通过“基于错误驱动的高效样本学习”来提升代码生成能力。SEED巧妙利用LLMs产生的错误作为学习契机，通过自我修正机制不断完善自身，从而达到高效学习的目的。具体而言，SEED会找出LLMs生成的错误代码，运用Self-revise技术进行代码修订，然后依据修订后的代码优化模型，并通过迭代适应过程不断提升效果。实验证明，相较于传统微调策略，SEED只需较少训练样本就能显著提高性能，其中Pass@1指标提高了27.2%-325.0%。同时，我们证实了Self-revise在生成优化模型更为有效的修订代码方面的有效性。更重要的是，无论何种类型的LLMs，SEED都能展现出卓越且稳定的性能表现，充分体现了该方法的良好通用性。

> Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively adapting the process for continuous improvement. Experimental results show that, compared to traditional fine-tuning approaches, SEED achieves superior performance with fewer training samples, showing a relative improvement of 27.2%-325.0% in Pass@1. We also validate the effectiveness of Self-revise, which generates revised code that optimizes the model more efficiently compared to the code samples from datasets. Moreover, SEED consistently demonstrates strong performance across various LLMs, underscoring its generalizability.

[Arxiv](https://arxiv.org/abs/2403.00046)