# Multimodal ArXiv 数据集，旨在助力大型视觉-语言模型深化对科学内容的理解与解析。

发布时间：2024年02月29日

`LLM应用`

> Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models

# 摘要

> GPT-4V 等大型视觉语言模型在处理自然场景的具体图像任务时表现出卓越的能力，但在解析抽象图形如几何形状和科学图表方面却受限于科学领域训练数据的匮乏。为此，我们构建了“多模态ArXiv”项目，包括 ArXivCap 和 ArXivQA 数据集，助力增强 LVLM 对科学内容的解读力。其中，ArXivCap 数据集包含了从跨越多个学科的57.2万篇 ArXiv 论文中提取的640万幅图像及390万个相关标题。在此基础上，我们借助 ArXivCap 设计并生成了一个问题回答数据集 ArXivQA，它通过引导 GPT-4V 解析科学图形实现。ArXivQA 显著提升了 LVLM 在数学推理上的表现，在一项多模态数学推理基准测试中取得了10.4%的绝对准确率增长。同时，我们运用 ArXivCap 设计了四项图像转文本任务，用以衡量 LVLM 的性能。通过对最先进 LVLM 的评估发现，它们在理解学术图形的复杂语义方面存在挑战，而针对特定领域的训练能有效提升其性能。进一步的错误分析揭示了当前 LVLM 存在对视觉上下文误读、识别误差以及生成过于简化标题等问题，为未来优化提供了方向。

> Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, with domain-specific training yielding substantial performance gains. Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.

[Arxiv](https://arxiv.org/abs/2403.00231)