# 关于注意力的数学理论

发布时间：2020年07月06日

`LLM理论` `机器学习` `神经网络`

> A Mathematical Theory of Attention

# 摘要

> 摘要：注意力在现代神经网络的众多领域中都是强有力的组成部分。然而，尽管其在机器学习中广泛应用，但从理论视角来看，我们对注意力的理解仍存在空缺。为此，我们提出一个框架来填补这一空缺，即运用测度理论构建注意力的数学等效模型。借助此模型，我们能把自注意力诠释为自相互作用粒子的系统，从最大熵的角度阐释自注意力，并且证明在适当假设下，注意力实际上是利普希茨连续的（具备恰当的度量）。随后，我们把这些见解应用于输入数据误设的问题、无限深度且权重共享的自注意力网络，以及在同期研究中针对特定类型注意力的更普遍的利普希茨估计。

> 
Abstract:Attention is a powerful component of modern neural networks across a wide variety of domains. However, despite its ubiquity in machine learning, there is a gap in our understanding of attention from a theoretical point of view. We propose a framework to fill this gap by building a mathematically equivalent model of attention using measure theory. With this model, we are able to interpret self-attention as a system of self-interacting particles, we shed light on self-attention from a maximum entropy perspective, and we show that attention is actually Lipschitz-continuous (with an appropriate metric) under suitable assumptions. We then apply these insights to the problem of mis-specified input data; infinitely-deep, weight-sharing self-attention networks; and more general Lipschitz estimates for a specific type of attention studied in concurrent work.
    

[Arxiv](https://arxiv.org/pdf/2007.02876)