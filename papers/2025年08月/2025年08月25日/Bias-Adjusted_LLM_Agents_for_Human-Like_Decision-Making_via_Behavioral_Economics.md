# 借助行为经济学实现类人决策的偏差调整LLM智能体

发布时间：2025年08月25日

`LLM应用` `基础理论`

> Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics

# 摘要

> 大型语言模型（LLMs）在模拟人类决策方面的应用日益广泛，但其内在偏见常与真实人类行为存在偏差——这限制了它们对群体多样性的反映能力。我们提出一种基于角色的方法来应对这一挑战，该方法利用行为经济学中的个体行为数据来调整模型偏见。将该方法应用于最后通牒博弈（LLMs的一项标准却颇具难度的基准测试）后，我们发现模拟行为与实证行为的一致性显著提升，尤其在回应者层面。尽管特质表征仍需进一步优化，但研究结果表明，基于角色条件的LLMs在大规模模拟类人决策模式方面前景广阔。

> Large language models (LLMs) are increasingly used to simulate human decision-making, but their intrinsic biases often diverge from real human behavior--limiting their ability to reflect population-level diversity. We address this challenge with a persona-based approach that leverages individual-level behavioral data from behavioral economics to adjust model biases. Applying this method to the ultimatum game--a standard but difficult benchmark for LLMs--we observe improved alignment between simulated and empirical behavior, particularly on the responder side. While further refinement of trait representations is needed, our results demonstrate the promise of persona-conditioned LLMs for simulating human-like decision patterns at scale.

[Arxiv](https://arxiv.org/abs/2508.18600)