# 偏差校准的LLM智能体：借助行为经济学实现类人决策

发布时间：2025年08月25日

`LLM应用` `金融科技`

> Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics

# 摘要

> 大型语言模型（LLMs）在模拟人类决策方面的应用日益广泛，但其内在偏差常与真实人类行为存在差异，限制了它们对群体多样性的反映能力。为应对这一挑战，我们提出一种基于角色的方法，利用行为经济学中的个体层面行为数据来调整模型偏差。将此方法应用于最后通牒博弈（LLMs的一个标准但颇具难度的基准任务）后，我们发现模拟行为与实证行为的一致性显著提升，尤其在回应者一方表现突出。尽管特质表征仍需进一步优化，但研究结果显示，基于角色条件的LLMs在大规模模拟类人决策模式方面颇具潜力。

> Large language models (LLMs) are increasingly used to simulate human decision-making, but their intrinsic biases often diverge from real human behavior--limiting their ability to reflect population-level diversity. We address this challenge with a persona-based approach that leverages individual-level behavioral data from behavioral economics to adjust model biases. Applying this method to the ultimatum game--a standard but difficult benchmark for LLMs--we observe improved alignment between simulated and empirical behavior, particularly on the responder side. While further refinement of trait representations is needed, our results demonstrate the promise of persona-conditioned LLMs for simulating human-like decision patterns at scale.

[Arxiv](https://arxiv.org/abs/2508.18600)