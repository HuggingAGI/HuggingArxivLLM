# LFD：层融合解码用于在检索增强生成中利用外部知识

发布时间：2025年08月27日

`RAG` `基础理论`

> LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation

# 摘要

> 检索增强生成（RAG）技术将外部知识融入大型语言模型（LLMs），不仅增强了模型对下游任务的适应性，还支持信息的动态更新。然而令人意外的是，近期实证研究发现，在检索到的相关文档中注入噪声，反而能反常地促进外部知识的利用，提升生成质量。尽管这一现象看似违背直觉且实际应用颇具挑战，但它为LLMs整合外部知识的机制提供了精细控制和严谨分析的可能。为此，本文通过干预噪声注入，在LLM内部确立了分层功能分工：浅层负责局部上下文建模，中间层专注整合长程外部事实知识，深层则主要依赖模型的参数化内部知识。基于这一发现，我们提出层融合解码（LFD）策略——一种直接将中间层表示与最终层解码输出融合的简易方法，旨在充分挖掘外部事实知识的潜力。为定位最佳中间层，我们引入内部知识分数（IKS）准则，通过选择后半部分网络层中IKS值最低的层作为目标。多基准实验结果显示，LFD能以极低的成本帮助RAG系统更高效地激活并利用检索到的上下文知识。

> Retrieval-augmented generation (RAG) incorporates external knowledge into large language models (LLMs), improving their adaptability to downstream tasks and enabling information updates. Surprisingly, recent empirical evidence demonstrates that injecting noise into retrieved relevant documents paradoxically facilitates exploitation of external knowledge and improves generation quality. Although counterintuitive and challenging to apply in practice, this phenomenon enables granular control and rigorous analysis of how LLMs integrate external knowledge. Therefore, in this paper, we intervene on noise injection and establish a layer-specific functional demarcation within the LLM: shallow layers specialize in local context modeling, intermediate layers focus on integrating long-range external factual knowledge, and deeper layers primarily rely on parametric internal knowledge. Building on this insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that directly combines representations from an intermediate layer with final-layer decoding outputs to fully exploit the external factual knowledge. To identify the optimal intermediate layer, we introduce an internal knowledge score (IKS) criterion that selects the layer with the lowest IKS value in the latter half of layers. Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost.

[Arxiv](https://arxiv.org/abs/2508.19614)