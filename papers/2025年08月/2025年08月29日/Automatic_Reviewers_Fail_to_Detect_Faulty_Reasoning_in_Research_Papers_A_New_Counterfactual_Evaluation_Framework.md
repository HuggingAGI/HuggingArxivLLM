# # 自动评审系统难辨研究论文中的错误推理：一种新的反事实评估框架

发布时间：2025年08月29日

`LLM应用` `基础理论`

> Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework

# 摘要

> 大型语言模型（LLMs）在加速和支持学术同行评审方面潜力巨大，如今更被广泛用作全自动评审生成器（ARGs）。但潜在的偏见与系统性错误可能严重威胁科学诚信，因此深入了解当前最先进ARGs的具体能力与局限至关重要。我们聚焦于支撑高质量同行评审的核心技能——检测研究逻辑缺陷，这项技能要求评估论文结果、解释与主张之间的内在一致性。为此，我们构建了全自动反事实评估框架，在受控条件下专门隔离并测试这项技能。通过测试多种ARG方法，我们意外发现：研究逻辑缺陷对其生成的评审结果并无显著影响。基于这些发现，我们为未来研究提出了三项切实可行的建议，并公开了反事实数据集与评估框架。

> Large Language Models (LLMs) have great potential to accelerate and support scholarly peer review and are increasingly used as fully automatic review generators (ARGs). However, potential biases and systematic errors may pose significant risks to scientific integrity; understanding the specific capabilities and limitations of state-of-the-art ARGs is essential. We focus on a core reviewing skill that underpins high-quality peer review: detecting faulty research logic. This involves evaluating the internal consistency between a paper's results, interpretations, and claims. We present a fully automated counterfactual evaluation framework that isolates and tests this skill under controlled conditions. Testing a range of ARG approaches, we find that, contrary to expectation, flaws in research logic have no significant effect on their output reviews. Based on our findings, we derive three actionable recommendations for future work and release our counterfactual dataset and evaluation framework publicly.

[Arxiv](https://arxiv.org/abs/2508.21422)