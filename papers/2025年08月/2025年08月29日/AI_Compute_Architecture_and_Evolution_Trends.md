# # 人工智能计算架构及演进趋势

发布时间：2025年08月29日

`Agent` `基础理论`

> AI Compute Architecture and Evolution Trends

# 摘要

> 人工智能的发展重心已从学术研究转向实际应用，然而其发展在多个层面仍面临诸多挑战。本文将采用结构化方法，从多个角度剖析人工智能的机遇与挑战，并提出人工智能计算架构的七层模型——自下而上依次为物理层、链路层、神经网络层、上下文层、智能体层、编排层及应用层。文章还阐述了人工智能计算如何通过大型语言模型（LLMs）的三阶段演进，逐步形成这一七层架构。针对每一层，我们均梳理了其发展脉络与关键技术：在第1、2层，探讨了人工智能计算的核心问题及纵向扩展（Scale-Up）与横向扩展（Scale-Out）策略对架构的影响；第3层聚焦大型语言模型（LLMs）的两条差异化发展路径；第4层分析上下文记忆对LLMs的作用机制，并对比传统处理器内存特性；第5至7层则深入研究人工智能智能体的发展趋势，剖析从单个智能体到人工智能生态系统的演进难题及其对产业的影响。此外，人工智能发展不仅面临技术瓶颈，构建自我可持续生态系统的经济问题同样亟待解决。

> The focus of AI development has shifted from academic research to practical applications. However, AI development faces numerous challenges at various levels. This article will attempt to analyze the opportunities and challenges of AI from several different perspectives using a structured approach. This article proposes a seven-layer model for AI compute architecture, including Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer, Orchestrator Layer, and Application Layer, from bottom to top. It also explains how AI computing has evolved into this 7-layer architecture through the three-stage evolution on large-scale language models (LLMs). For each layer, we describe the development trajectory and key technologies. In Layers 1 and 2 we discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies on computing architecture. In Layer 3 we explore two different development paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs and compares it to traditional processor memory. In Layers 5 to 7 we discuss the trends of AI agents and explore the issues in evolution from a single AI agent to an AI-based ecosystem, and their impact on the AI industry. Furthermore, AI development involves not only technical challenges but also the economic issues to build self-sustainable ecosystem. This article analyzes the internet industry to provide predictions on the future trajectory of AI development.

[Arxiv](https://arxiv.org/abs/2508.21394)