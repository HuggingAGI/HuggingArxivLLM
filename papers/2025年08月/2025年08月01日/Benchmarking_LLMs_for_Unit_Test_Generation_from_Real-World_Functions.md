# 从真实世界函数生成单元测试的LLMs基准测试

发布时间：2025年08月01日

`LLM应用` `软件工程` `测试生成`

> Benchmarking LLMs for Unit Test Generation from Real-World Functions

# 摘要

> 大型语言模型（LLMs）在自动生成单元测试方面展现出巨大潜力，显著减轻了开发人员的手动工作负担。然而，要全面评估LLMs在这一领域的性能，需要一个能够真实反映实际场景并有效规避常见问题的基准。目前，现有的LLM测试生成基准存在两大局限：数据污染和函数代码结构简单。这些缺陷使得基于这些基准的实证研究结论的可靠性大打折扣。由于数据污染，研究结果可能带有偏见；而由于函数代码结构简单，这些基准可能无法有效推广到更复杂的实际程序中。
    为了解决这些问题，我们推出了ULT（UnLeakedTestbench），一个专为基于真实Python函数的函数级单元测试生成设计的新基准。通过多阶段的严格筛选流程，ULT确保了高圈复杂度并有效减少了测试用例污染。ULT包含3,909个经过精心挑选的函数级任务，为评估LLMs的测试生成能力提供了更加现实和具有挑战性的场景。此外，我们还开发了PLT（PreLeakedTestbench），一个与ULT配套的基准，其测试用例已泄露，旨在支持对测试生成中记忆与推理能力的受控分析。评估结果表明，ULT显著更具挑战性。例如，LLMs生成的测试用例在所有LLMs上的平均准确率、语句覆盖率、分支覆盖率和变异分数分别为41.32%、45.10%、30.22%和40.21%。这些结果远低于TestEval（91.79%、92.18%、82.04%和49.69%）和PLT（47.07%、55.13%、40.07%和50.80%）的相应指标。
    

> Recently, large language models (LLMs) have shown great promise in automating unit test generation, significantly reducing the manual effort required by developers. To effectively evaluate the capabilities of LLMs in this domain, it is crucial to have a well-designed benchmark that accurately reflects real-world scenarios and mitigates common pitfalls. Existing LLM test generation benchmarks are limited by two critical drawbacks: data contamination and structurally simple function code. As a result, we often cannot rely on the validity of scientific conclusions drawn from empirical studies using these limited benchmarks. The empirical evidence presented may be biased due to contamination and may fail to generalize beyond toy programs due to structural simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new benchmark specifically designed for function-level unit test generation from real-world Python functions. ULT is constructed through a multi-stage curation process that ensures high cyclomatic complexity and mitigates test case contamination. With 3,909 carefully selected function-level tasks, ULT provides a more realistic and challenging evaluation of LLMs' test generation capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT with leaked tests designed to enable a controlled analysis of memorization versus reasoning in test generation. Our evaluation results demonstrate that ULT is significantly more challenging. For example, test cases generated by LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy, statement coverage, branch coverage, and mutation score on average for all LLMs, respectively. These results are substantially lower than the corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

[Arxiv](https://arxiv.org/abs/2508.00408)