# LLM时代的医疗推理：系统综述增强技术与应用场景

发布时间：2025年08月01日

`LLM应用` `人工智能`

> Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications

# 摘要

> 大型语言模型（LLMs）在医学领域的广泛应用带来了令人惊叹的能力，但它们在系统性、透明性和可验证性推理方面的不足仍是临床实践中的重要短板。这一局限性推动了研究领域从单一答案生成转向专门设计用于医学推理的大型语言模型。本文对这一新兴领域进行了开创性的系统性综述。我们提出了一个推理增强技术分类法，将其分为训练时间策略（如监督微调、强化学习）和测试时间机制（如提示工程、多智能体系统）。我们分析了这些技术在文本、图像、代码等不同数据模态中的应用，以及在诊断、教育和治疗计划等关键临床场景中的表现。此外，我们探讨了评估基准的演变，从简单的准确性指标到对推理质量和视觉可解释性的复杂评估。通过对2022-2025年间60篇开创性研究的分析，我们总结了关键挑战，包括忠实性与合理性的差距以及对原生多模态推理的需求，并展望了构建高效、稳健且在社会技术层面负责任的医学人工智能的未来方向。

> The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.

[Arxiv](https://arxiv.org/abs/2508.00669)