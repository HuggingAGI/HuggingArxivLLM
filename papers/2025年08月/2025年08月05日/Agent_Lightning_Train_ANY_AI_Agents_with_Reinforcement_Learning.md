# Agent闪电：用强化学习轻松训练任何AI智能体

发布时间：2025年08月05日

`Agent` `AI代理`

> Agent Lightning: Train ANY AI Agents with Reinforcement Learning

# 摘要

> 我们很高兴推出 Agent Lightning，一个灵活且可扩展的框架，支持基于强化学习 (RL) 训练大型语言模型 (LLMs) 以应用于任何 AI 代理。与现有方法不同，这些方法要么将 RL 训练与代理紧密耦合，要么依赖于序列拼接和掩码处理，Agent Lightning 实现了代理执行与训练之间的完全解耦。这意味着它可以无缝集成通过多种方式开发的现有代理（例如使用 LangChain、OpenAI 代理 SDK、AutoGen 等框架，或从零开始构建），且几乎无需代码修改。通过将代理执行建模为马尔可夫决策过程，我们定义了一个统一的数据接口，并提出了一种分层 RL 算法——LightningRL。该算法包含一个信用分配模块，使我们能够将任何代理生成的轨迹分解为训练过渡。这使得 RL 能够处理复杂的交互逻辑，例如多代理场景和动态工作流。在系统设计方面，我们引入了 Training-Agent 解耦架构，并将代理可观测性框架集成到代理运行时中，提供了一个标准化的代理微调接口。通过在文本到 SQL、增强检索生成和数学工具使用等任务上的实验，我们展示了该框架能够实现稳定且持续的改进，证明了其在实际代理训练和部署中的潜力。

> We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.

[Arxiv](https://arxiv.org/abs/2508.03680)