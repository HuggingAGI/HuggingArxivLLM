# 基于对抗性强化学习的单元测试生成学习

发布时间：2025年08月28日

`强化学习` `教育科技`

> Learning to Generate Unit Test via Adversarial Reinforcement Learning

# 摘要

> 单元测试是编程的核心实践，能对人类开发者或大型语言模型（LLMs）编写的程序进行系统性评估。由于编写全面单元测试颇具挑战，LLMs已被用于自动化生成测试，然而训练LLMs产出高质量测试的方法尚未得到充分研究。为此，我们提出UTRL——一种新颖的强化学习框架，旨在训练LLM根据编程指令生成高质量单元测试。其核心思路是通过强化学习，以对抗方式迭代训练两个LLM——单元测试生成器与代码生成器。其中，单元测试生成器通过最大化“判别奖励”进行训练——该奖励衡量其生成能暴露代码生成器解决方案缺陷的测试的能力；代码生成器则通过最大化“代码奖励”训练，此奖励反映其生成能通过测试生成器所产单元测试的解决方案的能力。实验表明，经UTRL训练的Qwen3-4B生成的单元测试质量，优于在人工编写的真实单元测试上通过监督微调训练的同一模型；其生成的代码评估结果也更贴近真实测试的评估效果。此外，经UTRL训练的Qwen3-4B在生成高质量单元测试上甚至超越了GPT-4.1等前沿模型，充分彰显了UTRL训练LLMs完成该任务的有效性。

> Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.

[Arxiv](https://arxiv.org/abs/2508.21107)