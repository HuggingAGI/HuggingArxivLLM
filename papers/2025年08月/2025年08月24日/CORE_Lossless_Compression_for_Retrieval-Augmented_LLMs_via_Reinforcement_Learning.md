# CORE：基于强化学习的检索增强LLM无损压缩

发布时间：2025年08月24日

`RAG` `基础理论`

> CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning

# 摘要

> 检索增强生成（RAG）作为一种极具潜力的技术，有效提升了大型语言模型（LLMs）知识的时效性和回答的事实准确性。但纳入过多检索文档会大幅增加输入长度，进而推高计算成本。以往研究尝试在上下文整合前将检索文档压缩为简短文本，但这类方法常导致端任务性能下降。由于缺乏明确的压缩目标，许多方法不得不依赖固定启发式规则，难以确保压缩内容能有效支撑端任务。为克服这些局限，我们提出CORE——一种为RAG设计的新型无损上下文压缩方法。CORE利用强化学习优化压缩过程，且无需依赖预定义的压缩标签。具体来说，它将端任务性能作为奖励信号，并采用广义强化学习策略优化（GRPO）训练压缩器。这种端到端训练框架使压缩器能生成最大化LLM答案准确性的摘要。在四个数据集上的大量实验验证了我们方法的优越性：在3%的高压缩率下，不仅在所有数据集上避免了与添加完整文档相比的性能下降，还将平均精确匹配（EM）分数提升了3.3分。相关代码即将发布。

> Retrieval-Augmented Generation (RAG) has emerged as a promising approach to enhance the timeliness of knowledge and the factual accuracy of responses in Large Language Models (LLMs). However, the inclusion of excessive retrieved documents substantially increases the input length, leading to higher computational costs. Previous studies have attempted to compress retrieved documents into shorter texts before in-context integration, but such methods often compromise end-task performance. The lack of well-defined compression targets forces many approaches to rely on fixed heuristics, which cannot guarantee that the compressed content will effectively support the end task. To address these limitations, we propose CORE, a novel method designed to achieve lossless context compression for RAG. CORE employs reinforcement learning to optimize the compression process without relying on predefined compression labels. Specifically, it utilizes end-task performance as a reward signal and applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train the compressor. This end-to-end training framework enables the compressor to generate summaries that maximize the accuracy of answers generated by the LLM. Extensive experiments on four datasets demonstrate the superiority of our approach. With a high compression ratio of 3\%, our method not only avoids performance degradation compared to prepending full documents across all datasets but also improves the average Exact Match (EM) score by 3.3 points. The code will be released soon.

[Arxiv](https://arxiv.org/abs/2508.19282)