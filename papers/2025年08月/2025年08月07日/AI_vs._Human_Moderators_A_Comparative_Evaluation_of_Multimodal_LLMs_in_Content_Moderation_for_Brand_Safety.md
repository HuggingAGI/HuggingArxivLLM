# AI与人类审核员的对比评估：多模态大语言模型在品牌安全内容审核中的应用研究

发布时间：2025年08月07日

`LLM应用` `品牌安全` `内容审核`

> AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety

# 摘要

> 随着在线视频内容呈指数级增长，对不安全视频的审核需求已超出人类能力范围，带来了运营和心理健康方面的双重挑战。尽管近期研究展示了多模态大型语言模型（MLLMs）在各类视频理解任务中的优势，但它们在多模态内容审核这一需要对视觉和文本线索进行细致理解的领域中的应用仍相对未被探索。本研究聚焦于品牌安全分类这一内容审核的关键子领域，旨在保障广告的完整性。为此，我们引入了一个全新的多模态多语言数据集，该数据集由专业审核员在多种风险类别下进行了细致标注。通过详尽的对比分析，我们展示了如 Gemini、GPT 和 Llama 等 MLLMs 在多模态品牌安全任务中的有效性，并评估了它们与专业人工审核员相比的准确性和成本效益。此外，我们还深入探讨了 MLLMs 的局限性及失败案例。我们随本文一同发布了该数据集，以助力未来在品牌安全与内容审核领域的有效且负责任的研究。

> As the volume of video content online grows exponentially, the demand for moderation of unsafe videos has surpassed human capabilities, posing both operational and mental health challenges. While recent studies demonstrated the merits of Multimodal Large Language Models (MLLMs) in various video understanding tasks, their application to multimodal content moderation, a domain that requires nuanced understanding of both visual and textual cues, remains relatively underexplored. In this work, we benchmark the capabilities of MLLMs in brand safety classification, a critical subset of content moderation for safe-guarding advertising integrity. To this end, we introduce a novel, multimodal and multilingual dataset, meticulously labeled by professional reviewers in a multitude of risk categories. Through a detailed comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini, GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost efficiency compared to professional human reviewers. Furthermore, we present an in-depth discussion shedding light on limitations of MLLMs and failure cases. We are releasing our dataset alongside this paper to facilitate future research on effective and responsible brand safety and content moderation.

[Arxiv](https://arxiv.org/abs/2508.05527)