# 守护者与肇事者：有害内容生成与安全缓解研究综述

发布时间：2025年08月07日

`LLM应用` `内容安全`

> Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation

# 摘要

> 大型语言模型（LLMs）彻底革新了数字平台的内容创作方式，赋予了前所未有的自然语言生成与理解能力。这些模型支持内容生成、问答（Q&A）、编程及代码推理等多种有益应用，然而也可能无意中或故意生成有毒、冒犯性或有偏见的内容，带来严重风险。这种双重角色——既是解决现实问题的强大力器，又是潜在有害语言的来源——构成了紧迫的社会技术挑战。本综述系统性回顾了近年来涵盖非故意毒性、对抗性越狱攻击及内容审核技术的研究。我们提出了一个统一的LLM相关危害与防御分类法，分析了新兴的多模态及LLM辅助越狱策略，并评估了包括带有反馈的强化学习（RLHF）、提示工程及安全性对齐在内的缓解努力。我们的综合分析突显了LLM安全领域的不断演变，指出了现有评估方法的局限性，并为开发稳健且符合伦理的语言技术指明了未来研究方向。

> Large Language Models (LLMs) have revolutionized content creation across digital platforms, offering unprecedented capabilities in natural language generation and understanding. These models enable beneficial applications such as content generation, question and answering (Q&A), programming, and code reasoning. Meanwhile, they also pose serious risks by inadvertently or intentionally producing toxic, offensive, or biased content. This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. In this survey, we systematically review recent studies spanning unintentional toxicity, adversarial jailbreaking attacks, and content moderation techniques. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methodologies, and outlines future research directions to guide the development of robust and ethically aligned language technologies.

[Arxiv](https://arxiv.org/abs/2508.05775)