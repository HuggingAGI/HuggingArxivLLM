# 以第一视角感知与行动：用于人类-物体-人类交互的数据集和基准测试

发布时间：2025年08月06日

`Agent` `智能助手` `人机交互`

> Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions

# 摘要

> 从现实世界的人机交互数据中学习动作模型，对于打造高效通用的智能助手至关重要。然而，现有数据集多聚焦于专业交互类别，忽视了AI助手基于第一人称视角感知和行动的特点。我们认为，通用交互知识和自我中心模态缺一不可。本文将人机协作任务嵌入视觉-语言-动作框架，其中助手基于自我中心视觉和指令为用户提供服务。借助我们的混合RGB-MoCap系统，助手与用户根据GPT生成的脚本共同与多个物体和场景互动。由此，我们打造了InterVLA——首个大规模人-物-人交互数据集，包含11.4小时、120万帧的多模态数据，涵盖2个自我中心视频和5个外我中心视频，精确的人/物动作和语音指令。此外，我们还建立了新型基准测试，涵盖自我中心人体运动估计、交互合成和交互预测，并进行了全面分析。我们相信，InterVLA测试床和基准测试将为未来在物理世界中构建AI代理的研究工作提供有力支持。

> Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.

[Arxiv](https://arxiv.org/abs/2508.04681)