# 利用即所需，探索由此生

发布时间：2025年08月02日

`Agent` `探索策略`

> Exploitation Is All You Need... for Exploration

# 摘要

> 在训练元强化学习（meta-RL）代理解决新环境时，确保充分探索是核心挑战。传统方法通过随机化、不确定性奖励或内在奖励等显式激励机制来平衡探索与利用。在本研究中，我们提出：若满足以下三个条件，一个仅最大化贪婪目标的代理仍可自发探索。这三个条件为：(1) 环境的可重复结构，使历史经验能指导未来决策；(2) 代理的记忆能力，使其保留并利用历史数据；(3) 长时域信用分配，让探索的延迟收益影响当前决策。实验表明，在随机多臂老虎机和时间扩展网格世界中，当结构和记忆并存时，基于贪婪目标的策略会自发探索。受控消融实验显示，若缺少环境结构或记忆，探索行为消失。有趣的是，去除长时域信用分配并不总能抑制探索——这归因于伪-汤普森采样效应。这些发现表明，探索与利用并非对立，而是可源自统一的收益最大化过程，前提是满足特定条件。

> Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process.

[Arxiv](https://arxiv.org/abs/2508.01287)