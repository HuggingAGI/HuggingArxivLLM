# # 大型语言模型数据安全研究综述

发布时间：2025年08月04日

`LLM理论` `数据安全`

> A Survey on Data Security in Large Language Models

# 摘要

> 大型语言模型（LLMs）作为自然语言处理领域的基石，正在推动文本生成、机器翻译和对话系统等应用的革新。然而，这些模型依赖于海量数据，这些数据通常来自未经筛选的多样化来源，使其面临严峻的数据安全挑战。有害或恶意数据可能导致模型产生有毒输出、出现幻觉，并使其易受提示注入或数据投毒等威胁。随着LLMs在关键现实系统中的广泛应用，理解并应对这些数据安全风险对于维护用户信任和系统可靠性至关重要。本研究全面概述了LLMs面临的主要数据安全威胁，系统梳理了现有防御策略，包括对抗训练、RLHF和数据增强等方法。此外，我们对评估不同领域中模型鲁棒性和安全性的相关数据集进行了分类和深入分析，为未来研究提供方向指引。最后，我们聚焦于安全模型更新、可解释性驱动防御和有效治理框架等关键研究方向，致力于推动LLMs技术的安全和负责任发展。本研究旨在为研究人员、从业者和政策制定者提供参考，共同推进LLMs数据安全领域的进步。


> Large Language Models (LLMs), now a foundation in advancing natural language processing, power applications such as text generation, machine translation, and conversational systems. Despite their transformative potential, these models inherently rely on massive amounts of training data, often collected from diverse and uncurated sources, which exposes them to serious data security risks. Harmful or malicious data can compromise model behavior, leading to issues such as toxic output, hallucinations, and vulnerabilities to threats such as prompt injection or data poisoning. As LLMs continue to be integrated into critical real-world systems, understanding and addressing these data-centric security risks is imperative to safeguard user trust and system reliability. This survey offers a comprehensive overview of the main data security risks facing LLMs and reviews current defense strategies, including adversarial training, RLHF, and data augmentation. Additionally, we categorize and analyze relevant datasets used for assessing robustness and security across different domains, providing guidance for future research. Finally, we highlight key research directions that focus on secure model updates, explainability-driven defenses, and effective governance frameworks, aiming to promote the safe and responsible development of LLM technology. This work aims to inform researchers, practitioners, and policymakers, driving progress toward data security in LLMs.

[Arxiv](https://arxiv.org/abs/2508.02312)