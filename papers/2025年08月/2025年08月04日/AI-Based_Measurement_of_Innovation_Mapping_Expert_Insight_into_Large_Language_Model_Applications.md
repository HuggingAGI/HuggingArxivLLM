# # 标题
基于AI的创新度量：将专家见解融入大型语言模型应用中

发布时间：2025年08月04日

`LLM应用` `软件开发` `产品管理`

> AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications

# 摘要

> 创新的衡量往往依赖于基于具体情境的代理指标和专家评估，这使得实证创新研究通常局限于能够获取此类数据的场景。我们研究了如何利用大型语言模型（LLMs）突破人工专家评估的限制，助力研究人员衡量创新。为此，我们设计了一个可靠的LLM框架，能够从非结构化文本数据中近似模拟领域专家对创新的评估。通过两个不同情境下的研究，我们展示了该框架的性能和广泛应用性：（1）软件应用程序更新的创新性评估；（2）产品评论中用户生成反馈和改进建议的原创性分析。我们将我们的LLM框架与之前创新研究中使用的替代指标以及基于机器学习和深度学习的最新模型进行了比较，评估了其性能（F1分数）和可靠性（一致性率）。结果显示，LLM框架的F1分数显著高于其他方法，且其结果高度一致（即不同运行中结果保持不变）。本文为企业研发人员、研究人员、审稿人和编辑提供了知识和工具，帮助他们有效利用LLMs衡量创新并评估基于LLM的创新衡量方法的性能。在此过程中，我们探讨了重要设计决策对性能和可靠性的影响，包括模型选择、提示工程、训练数据规模、训练数据分布和参数设置。鉴于人工专家评估和现有基于文本的衡量指标所面临的挑战，我们的框架为将LLMs作为可靠、日益易用且广泛应用的研究工具来衡量创新提供了重要启示。


> Measuring innovation often relies on context-specific proxies and on expert evaluation. Hence, empirical innovation research is often limited to settings where such data is available. We investigate how large language models (LLMs) can be leveraged to overcome the constraints of manual expert evaluations and assist researchers in measuring innovation. We design an LLM framework that reliably approximates domain experts' assessment of innovation from unstructured text data. We demonstrate the performance and broad applicability of this framework through two studies in different contexts: (1) the innovativeness of software application updates and (2) the originality of user-generated feedback and improvement ideas in product reviews. We compared the performance (F1-score) and reliability (consistency rate) of our LLM framework against alternative measures used in prior innovation studies, and to state-of-the-art machine learning- and deep learning-based models. The LLM framework achieved higher F1-scores than the other approaches, and its results are highly consistent (i.e., results do not change across runs). This article equips R&D personnel in firms, as well as researchers, reviewers, and editors, with the knowledge and tools to effectively use LLMs for measuring innovation and evaluating the performance of LLM-based innovation measures. In doing so, we discuss, the impact of important design decisions-including model selection, prompt engineering, training data size, training data distribution, and parameter settings-on performance and reliability. Given the challenges inherent in using human expert evaluation and existing text-based measures, our framework has important implications for harnessing LLMs as reliable, increasingly accessible, and broadly applicable research tools for measuring innovation.

[Arxiv](https://arxiv.org/abs/2508.02430)