# 测评大型语言模型在高等教育领域自动化评估学生论文的可靠性和有效性

发布时间：2025年08月04日

`LLM应用` `教育技术`

> Assessing the Reliability and Validity of Large Language Models for Automated Assessment of Student Essays in Higher Education

# 摘要

> 本研究探讨了Claude 3.5、DeepSeek v2、Gemini 2.5、GPT-4和Mistral 24B这五种先进大型语言模型（LLMs）在真实高等教育场景下进行自动评分的可靠性和有效性。研究采用67篇意大利语学生作文，这些作文是心理学课程的一部分，根据四项评分标准（相关性、连贯性、原创性和可行性）进行评估。每个模型在三个提示复现中对所有作文进行了评分，以评估模型内部的稳定性。结果显示，人机评分的一致性始终较低且不显著（二次加权Kappa），模型在复现中的内部可靠性同样较弱（肯德尔W系数中位数 < 0.30）。系统性评分差异显现，包括对连贯性评分的倾向性偏高以及对情境依赖维度处理的不一致。跨模型一致性分析显示，连贯性和原创性方面存在中等程度的收敛性，但相关性和可行性方面则几乎没有一致性。尽管研究范围有限，但这些发现表明，当前LLMs可能难以在需要学科洞察力和情境敏感性的任务中复制人类判断。在评估开放性学术作品时，特别是在解释性领域，人类监督仍然至关重要。

> This study investigates the reliability and validity of five advanced Large Language Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral 24B, for automated essay scoring in a real world higher education context. A total of 67 Italian-language student essays, written as part of a university psychology course, were evaluated using a four-criterion rubric (Pertinence, Coherence, Originality, Feasibility). Each model scored all essays across three prompt replications to assess intra-model stability. Human-LLM agreement was consistently low and non-significant (Quadratic Weighted Kappa), and within-model reliability across replications was similarly weak (median Kendall's W < 0.30). Systematic scoring divergences emerged, including a tendency to inflate Coherence and inconsistent handling of context-dependent dimensions. Inter-model agreement analysis revealed moderate convergence for Coherence and Originality, but negligible concordance for Pertinence and Feasibility. Although limited in scope, these findings suggest that current LLMs may struggle to replicate human judgment in tasks requiring disciplinary insight and contextual sensitivity. Human oversight remains critical when evaluating open-ended academic work, particularly in interpretive domains.

[Arxiv](https://arxiv.org/abs/2508.02442)