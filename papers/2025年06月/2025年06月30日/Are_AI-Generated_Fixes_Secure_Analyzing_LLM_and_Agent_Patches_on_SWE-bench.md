# AI生成修复的安全性：深入分析LLM与代理补丁在SWE-bench上的表现

发布时间：2025年06月30日

`LLM应用` `软件开发` `代码安全`

> Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench

# 摘要

> 大型语言模型（LLMs）及其代理框架正日益被采用，用于自动化软件开发任务，如问题解决和程序修复。尽管先前的研究已经识别出LLM生成代码中的安全风险，但大多数评估集中在合成或孤立的环境中，关于这些系统在真实开发环境中的安全性仍存在诸多疑问。本研究首次针对SWE-bench数据集中的20,000多个问题，进行了大规模的LLM生成补丁安全分析。我们评估了独立LLM（Llama 3.3）生成的补丁，并将其与开发者编写的补丁进行了对比。同时，我们还评估了三个高性能代理框架（OpenHands、AutoCodeRover、HoneyComb）在我们数据子集上生成补丁的安全性。最后，我们分析了广泛的代码、问题和项目级别因素，以了解LLMs和代理最可能生成不安全代码的条件。研究结果表明，独立LLM生成的补丁引入的新漏洞数量几乎是开发者的9倍，其中许多漏洞具有在开发者代码中未见的独特模式。代理工作流也会生成大量漏洞，尤其是在赋予LLMs更多自主权时，这可能增加误解项目背景或任务需求的风险。我们发现，与更多文件关联、生成更多代码行以及GitHub问题中缺乏具体代码片段或预期代码行为和复现步骤信息的LLM补丁更容易出现漏洞。这些结果表明，上下文因素在生成代码的安全性中起着关键作用，并强调了需要主动风险评估方法，综合考虑代码和问题级别信息，以补充现有的漏洞检测工具。


> Large Language Models (LLMs) and their agentic frameworks are increasingly adopted to automate software development tasks such as issue resolution and program repair. While prior work has identified security risks in LLM-generated code, most evaluations have focused on synthetic or isolated settings, leaving open questions about the security of these systems in real-world development contexts. In this study, we present the first large-scale security analysis of LLM-generated patches using 20,000+ issues from the SWE-bench dataset. We evaluate patches produced by a standalone LLM (Llama 3.3) and compare them to developer-written patches. We also assess the security of patches generated by three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb) on a subset of our data. Finally, we analyze a wide range of code, issue, and project-level factors to understand the conditions under which LLMs and agents are most likely to generate insecure code. Our findings reveal that the standalone LLM introduces nearly 9x more new vulnerabilities than developers, with many of these exhibiting unique patterns not found in developers' code. Agentic workflows also generate a significant number of vulnerabilities, particularly when granting LLMs more autonomy, potentially increasing the likelihood of misinterpreting project context or task requirements. We find that vulnerabilities are more likely to occur in LLM patches associated with a higher number of files, more lines of generated code, and GitHub issues that lack specific code snippets or information about the expected code behavior and steps to reproduce. These results suggest that contextual factors play a critical role in the security of the generated code and point toward the need for proactive risk assessment methods that account for both code and issue-level information to complement existing vulnerability detection tools.

[Arxiv](https://arxiv.org/abs/2507.02976)