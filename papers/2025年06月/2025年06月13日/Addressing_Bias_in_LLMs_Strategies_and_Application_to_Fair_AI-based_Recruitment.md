# 应对LLMs中的偏见：策略及其在公平AI招聘中的应用

发布时间：2025年06月13日

`LLM应用` `人工智能伦理`

> Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment

# 摘要

> 近年来，语言技术在高风险环境中的应用日益增加，这主要得益于大型语言模型（LLMs）的成功。然而，尽管LLMs表现优异，但它们仍然面临伦理问题，如人口统计偏见、问责制或隐私问题。本研究旨在通过一个基于AI的自动化招聘案例，分析基于Transformers的系统在数据中学习人口统计偏见的能力。我们提出了一种增强隐私的框架，通过从学习管道中减少性别信息来缓解最终工具中的偏见行为。我们的实验分析了数据偏见对基于两种不同LLMs系统的影响，以及所提出的框架如何有效防止训练后的系统复制数据中的偏见。

> The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data.

[Arxiv](https://arxiv.org/abs/2506.11880)