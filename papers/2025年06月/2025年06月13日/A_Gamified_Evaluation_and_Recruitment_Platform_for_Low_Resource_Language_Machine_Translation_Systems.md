# 一个针对低资源语言机器翻译系统的游戏化评估与招聘平台

发布时间：2025年06月13日

`LLM应用` `机器翻译`

> A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems

# 摘要

> 人工评估者在评估大型语言模型中发挥着不可或缺的作用，特别是在低资源语言（LRLs）的机器翻译（MT）系统中。由于现有自动化指标多基于字符串匹配，难以全面反映系统行为的细微差别，具备专业知识的人工评估者在测试翻译的恰当性、流畅性等关键指标方面显得尤为重要。然而，低资源语言的特性导致数据集和评估者都极度匮乏，这对机器翻译系统的开发者提出了一个难题：如何找到足够的人工评估者和数据集？

本文通过全面回顾现有评估流程，旨在为解决机器翻译系统开发中的资源缺口问题，提出一个平台设计方案。研究结果是一个针对机器翻译系统开发者设计的招募与游戏化评估平台。同时，本文还探讨了评估该平台的挑战，以及它在更广泛自然语言处理（NLP）研究中的潜在应用。

> Human evaluators provide necessary contributions in evaluating large language models. In the context of Machine Translation (MT) systems for low-resource languages (LRLs), this is made even more apparent since popular automated metrics tend to be string-based, and therefore do not provide a full picture of the nuances of the behavior of the system. Human evaluators, when equipped with the necessary expertise of the language, will be able to test for adequacy, fluency, and other important metrics. However, the low resource nature of the language means that both datasets and evaluators are in short supply. This presents the following conundrum: How can developers of MT systems for these LRLs find adequate human evaluators and datasets? This paper first presents a comprehensive review of existing evaluation procedures, with the objective of producing a design proposal for a platform that addresses the resource gap in terms of datasets and evaluators in developing MT systems. The result is a design for a recruitment and gamified evaluation platform for developers of MT systems. Challenges are also discussed in terms of evaluating this platform, as well as its possible applications in the wider scope of Natural Language Processing (NLP) research.

[Arxiv](https://arxiv.org/abs/2506.11467)