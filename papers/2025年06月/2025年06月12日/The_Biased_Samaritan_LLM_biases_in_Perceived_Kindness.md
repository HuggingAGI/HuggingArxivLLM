# 偏见的善意：大型语言模型对善良感知中的偏见问题

发布时间：2025年06月12日

`LLM理论` `人工智能` `社会研究`

> The Biased Samaritan: LLM biases in Perceived Kindness

# 摘要

> 尽管大型语言模型（LLMs）已在多个领域广泛应用，理解和缓解LLM偏见仍是一个持续的挑战。本文提出了一种评估各类生成式AI模型人口统计偏见的新方法。通过提示模型评估道德患者主动干预的意愿，我们旨在定量评估不同LLMs在性别、种族和年龄等方面的偏见。我们的研究与现有工作不同之处在于，旨在确定各类商用模型的基线人口统计身份及其与其他人口统计特征的关系。我们努力了解这些偏见是积极的、中性的还是消极的，以及这些偏见的强度。本文有助于对大型语言模型偏见进行客观评估，并赋予用户或开发者在LLM输出或训练未来LLMs时考虑这些偏见的能力。我们的分析揭示了两个关键发现：模型将基线人口统计身份视为白种中年或年轻成年男性；然而，跨模型的普遍趋势表明，非基线人口统计群体比基线群体更愿意提供帮助。这些方法使我们能够区分这两种常常纠缠在一起的偏见。

> While Large Language Models (LLMs) have become ubiquitous in many fields, understanding and mitigating LLM biases is an ongoing issue. This paper provides a novel method for evaluating the demographic biases of various generative AI models. By prompting models to assess a moral patient's willingness to intervene constructively, we aim to quantitatively evaluate different LLMs' biases towards various genders, races, and ages. Our work differs from existing work by aiming to determine the baseline demographic identities for various commercial models and the relationship between the baseline and other demographics. We strive to understand if these biases are positive, neutral, or negative, and the strength of these biases. This paper can contribute to the objective assessment of bias in Large Language Models and give the user or developer the power to account for these biases in LLM output or in training future LLMs. Our analysis suggested two key findings: that models view the baseline demographic as a white middle-aged or young adult male; however, a general trend across models suggested that non-baseline demographics are more willing to help than the baseline. These methodologies allowed us to distinguish these two biases that are often tangled together.

[Arxiv](https://arxiv.org/abs/2506.11361)