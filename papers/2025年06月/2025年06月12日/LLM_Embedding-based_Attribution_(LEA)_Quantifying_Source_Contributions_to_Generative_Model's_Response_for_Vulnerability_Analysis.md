# LLM嵌入归因（LEA）：用于漏洞分析的生成模型响应来源贡献量化方法

发布时间：2025年06月12日

`LLM理论` `网络安全` `漏洞分析`

> LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis

# 摘要

> 安全漏洞的频率和复杂性持续攀升，威胁环境不断演变，给网络安全防护带来严峻挑战。大型语言模型（LLMs）已被广泛应用到网络安全威胁分析中。然而，处理新出现的未知漏洞对 LLMs 来说极具挑战性，因为这超出了其预训练数据的分布范围。检索增强生成（RAG）流水线通过将最新的权威信息注入模型上下文，有效减少了幻觉现象，提升了响应的准确性。与此同时，在安全敏感环境中部署 LLMs 带来了信任与安全方面的挑战。这引出了一个重要问题：如何量化生成响应中检索内容与模型预训练知识的贡献比例？本研究提出了一种基于 LLM 嵌入的归因方法（LEA），这是一种新颖的可解释指标，能够清晰展示每个生成响应中“预训练知识与检索内容的影响力占比”。我们通过 LEA 评估了过去十年中 100 个关键 CVE 的响应，验证了其在量化漏洞分析洞察力方面的有效性。通过开发 LEA，我们揭示了 LLM 隐藏状态中独立性的演变：早期层对上下文的重度依赖，使得 LEA 的提取成为可能；而后期层则表现出更高的独立性，这解释了为什么规模对 LLM 的有效性至关重要。这项工作为安全分析师提供了一种审核 LLM 辅助工作流的手段，为在网络安全运营中实现透明且高保障的 RAG 增强型 LLM 部署奠定了基础。

> Security vulnerabilities are rapidly increasing in frequency and complexity, creating a shifting threat landscape that challenges cybersecurity defenses. Large Language Models (LLMs) have been widely adopted for cybersecurity threat analysis. When querying LLMs, dealing with new, unseen vulnerabilities is particularly challenging as it lies outside LLMs' pre-trained distribution. Retrieval-Augmented Generation (RAG) pipelines mitigate the problem by injecting up-to-date authoritative sources into the model context, thus reducing hallucinations and increasing the accuracy in responses. Meanwhile, the deployment of LLMs in security-sensitive environments introduces challenges around trust and safety. This raises a critical open question: How to quantify or attribute the generated response to the retrieved context versus the model's pre-trained knowledge? This work proposes LLM Embedding-based Attribution (LEA) -- a novel, explainable metric to paint a clear picture on the 'percentage of influence' the pre-trained knowledge vs. retrieved content has for each generated response. We apply LEA to assess responses to 100 critical CVEs from the past decade, verifying its effectiveness to quantify the insightfulness for vulnerability analysis. Our development of LEA reveals a progression of independency in hidden states of LLMs: heavy reliance on context in early layers, which enables the derivation of LEA; increased independency in later layers, which sheds light on why scale is essential for LLM's effectiveness. This work provides security analysts a means to audit LLM-assisted workflows, laying the groundwork for transparent, high-assurance deployments of RAG-enhanced LLMs in cybersecurity operations.

[Arxiv](https://arxiv.org/abs/2506.12100)