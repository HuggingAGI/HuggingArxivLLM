# 从复现到重设计：探索基于LLM的同行评审中的配对比较

发布时间：2025年06月12日

`LLM应用` `学术出版` `同行评审`

> From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review

# 摘要

> 大型语言模型（LLMs）的出现为突破传统同行评审模式带来了前所未有的机遇。然而，现有研究多集中于用LLMs直接替代人工评审，而对如何根本性地创新LLMs在学术评审中的角色关注甚少。本文提出了一种创新机制，通过LLM代理进行稿件间的成对比较，而非传统的单独评分。这种方法通过整合大量成对评估结果，提供了更精准可靠的稿件质量评估。实验结果表明，相较于传统评分方法，该比较方法在识别高影响力论文方面表现显著更优。然而，我们的分析也揭示了这一过程中的潜在偏见，包括研究主题新颖性降低和机构间不平衡加剧等问题。这些发现不仅彰显了重新构想LLMs参与同行评审的变革潜力，也指出了未来系统需重点解决以确保公平与多样性的关键挑战。

> The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.

[Arxiv](https://arxiv.org/abs/2506.11343)