# Optimus-3：迈向全能型多模态《我的世界》代理，搭载可扩展任务专家

发布时间：2025年06月12日

`Agent`

> Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts

# 摘要

> # 摘要
基于多模态大型语言模型（MLLMs）的代理近期在各领域取得了显著进展。然而，在开放世界环境中（如Minecraft）构建具备综合能力的通用代理仍面临三大挑战：领域特定数据不足、异构任务间的相互干扰以及视觉多样性问题。针对这些挑战，本文提出三项关键创新：1）知识增强的数据生成流水线，为代理开发提供高质量且可扩展的训练数据；2）带有任务级别路由的专家混合（MoE）架构，有效缓解异构任务间的相互干扰；3）多模态推理增强的强化学习方法，提升代理在Minecraft中的视觉推理能力。基于这些创新，我们推出了一款通用代理Optimus-3。实验结果表明，Optimus-3在Minecraft环境中的广泛任务范围内，超越了现有通用多模态大型语言模型和最先进的代理。项目页面：https://cybertronagent.github.io/Optimus-3.github.io/

> Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/

[Arxiv](https://arxiv.org/abs/2506.10357)