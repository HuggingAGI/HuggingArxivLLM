# ResearchCodeBench：评估LLMs实现新型机器学习研究代码的基准测试。

发布时间：2025年06月02日

`LLM应用` `机器学习` `代码生成`

> ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code

# 摘要

> 大型语言模型（LLMs）在机器学习研究领域展现出巨大潜力，但其能否准确实现预训练阶段未曾接触的最新研究论文中的创新想法，仍是一个未知数。我们推出ResearchCodeBench，这是一个包含212个编码挑战的基准测试平台，旨在评估LLMs将2024-2025年顶尖研究论文中的机器学习成果转化为可执行代码的能力。我们评估了30多款专有和开源的LLMs，发现即使是最优的模型也只能正确实现不到40%的代码。我们发现Gemini-2.5-Pro-Preview以37.3%的成功率表现最佳，紧随其后的是O3 (High)和O4-mini (High)，成功率分别为32.3%和30.8%。我们还呈现了关于性能对比、污染问题和错误模式的经验性发现。通过提供一个严格且由社区驱动的评估平台，ResearchCodeBench助力持续理解并推动LLM在研究代码生成领域的创新驱动发展。

> Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement novel ideas from recent research papers-ideas unseen during pretraining-remains unclear. We introduce ResearchCodeBench, a benchmark of 212 coding challenges that evaluates LLMs' ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3% success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and 30.8% respectively. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous and community-driven evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation.

[Arxiv](https://arxiv.org/abs/2506.02314)