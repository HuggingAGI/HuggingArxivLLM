# 自我挑战的语言模型智能体

发布时间：2025年06月02日

`Agent` `人工智能` `机器学习`

> Self-Challenging Language Model Agents

# 摘要

> 大型语言模型正在迅速成为具备工具使用能力的智能代理的基础。然而，训练这类智能代理颇具挑战性，因为它需要人工创建和标注多样化的任务、工具及评估标准。本文中，我们提出了一个名为“自我挑战”的框架，用于训练智能代理，使其能够基于自身生成的高质量任务进行学习。该框架首先让代理扮演挑战者的角色，在与给定工具交互后生成任务。这些任务采用了一种全新的通用问题类别——“代码即任务”，由指令、验证函数、解决方案和失败案例组成，后者作为测试案例，以筛选出高质量的任务。随后，代理切换到执行者的角色，利用评估反馈作为奖励信号，通过强化学习在这些任务上进行训练。在现有两个多轮工具使用智能代理基准测试M3ToolEval和TauBench上的评估表明，尽管Self-Challenging框架仅使用自动生成的训练数据，但它在Llama-3.1-8B-Instruct模型上实现了超过两倍的性能提升。

> Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.

[Arxiv](https://arxiv.org/abs/2506.01716)