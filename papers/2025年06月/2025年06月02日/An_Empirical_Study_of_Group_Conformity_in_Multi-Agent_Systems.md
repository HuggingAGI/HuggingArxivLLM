# # 摘要
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年06月02日

`Agent` `社会学` `人工智能`

> An Empirical Study of Group Conformity in Multi-Agent Systems

# 摘要

> 大型语言模型（LLMs）的突破性进展让模拟现实世界中具备类人推理能力的多智能体系统成为可能。尽管此前研究已对种族等受保护属性相关的偏见进行了深入探讨，但多智能体LLM交互中关于社会争议问题的偏见产生与传播机制仍是一个未被充分研究的领域。本研究聚焦于LLM代理如何通过五项有争议话题的辩论来影响公众舆论。通过模拟超过2500场辩论，我们观察到最初保持中立立场的代理如何逐步形成特定立场。统计分析显示了显著的群体趋同现象，这种现象与人类行为模式高度相似；LLM代理往往倾向于与数量占优的群体或更智能的代理保持一致，从而产生更大的影响力。这些发现凸显了代理智能在话语塑造中的关键作用，同时也揭示了在线互动中偏见放大的潜在风险。我们的研究结果强调了制定相关政策的紧迫性，以促进LLM生成讨论中的多样性和透明度，从而有效缓解匿名在线环境中偏见传播的风险。

> Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.

[Arxiv](https://arxiv.org/abs/2506.01332)