# 类似TRuST的毒性识别方法：Span和Target

发布时间：2025年06月02日

`LLM应用` `内容安全` `社会群体`

> Something Just Like TRuST : Toxicity Recognition of Span and Target

# 摘要

> 在线内容毒性问题日益严峻，尤其语言模型生成内容的毒性可能引发严重心理和社会影响。本文提出TRuST数据集，整合现有资源，针对毒性、目标群体及毒性片段提供多维度标注。数据集涵盖种族、性别、宗教、残障及政治等多类社会群体，包含人工标注、机器标注及人机协作生成数据。我们对当前主流大型语言模型（LLMs）在毒性检测、目标群体识别和毒性片段提取任务中进行了全面评测。结果显示，微调模型在这些任务中表现优于零样本和少样本方法，但对部分社会群体的检测效果仍需提升。值得注意的是，推理能力的提升并未显著改善模型性能，反映出LLMs在社会推理方面的能力尚有欠缺。

> Toxicity in online content, including content generated by language models, has become a critical concern due to its potential for negative psychological and social impact. This paper introduces TRuST, a comprehensive dataset designed to improve toxicity detection that merges existing datasets, and has labels for toxicity, target social group, and toxic spans. It includes a diverse range of target groups such as ethnicity, gender, religion, disability, and politics, with both human/machine-annotated and human machine-generated data. We benchmark state-of-the-art large language models (LLMs) on toxicity detection, target group identification, and toxic span extraction. We find that fine-tuned models consistently outperform zero-shot and few-shot prompting, though performance remains low for certain social groups. Further, reasoning capabilities do not significantly improve performance, indicating that LLMs have weak social reasoning skills.

[Arxiv](https://arxiv.org/abs/2506.02326)