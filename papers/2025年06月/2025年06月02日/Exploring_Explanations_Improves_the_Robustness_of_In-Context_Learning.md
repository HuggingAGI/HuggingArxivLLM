# 通过探索解释提升上下文学习的鲁棒性

发布时间：2025年06月02日

`LLM理论` `机器学习`

> Exploring Explanations Improves the Robustness of In-Context Learning

# 摘要

> 上下文学习 (ICL) 在大型语言模型中已被证明是一个成功的范式。然而，它常常难以推广到提供的演示分布之外。最近，增强鲁棒性的进展是带有解释的ICL (X-ICL)，它通过引导LLMs理解并明确正确标签背后的推理，提高了预测的可靠性。在此基础上，我们提出了一种先进的框架，通过系统性地探索所有可能标签的解释，扩展了X-ICL，从而实现了更全面和稳健的决策。在多个自然语言理解数据集上的实验结果验证了X²-ICL的有效性，与现有ICL方法相比，它在处理分布外数据时的鲁棒性有了显著提升。

> In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.

[Arxiv](https://arxiv.org/abs/2506.02378)