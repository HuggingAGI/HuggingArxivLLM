# 聚焦学生，而非机器：基于事实依据的问题生成与自动化评分

发布时间：2025年06月02日

`LLM应用` `教育评估`

> Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading

# 摘要

> 数字技术在教育中的广泛应用为师生减负带来了新可能。然而，创建开放性学习或考试问题并评分仍然是耗时费力的工作。本论文构建了一个基于教学材料生成问题并自动评分学生答案的系统基础。它介绍了一种针对PDF文档的带视觉布局的文档分块高级方法，显著提升了包括检索增强生成（RAG）在内的下游任务的准确性。本论文证明了可以从学习材料中生成高质量的问题和参考答案。此外，它引入了新的自动评分简答题基准，以促进自动化评分系统的比较。对各种评分系统进行了评估，结果表明大型语言模型（LLMs）可以将其预训练任务泛化到简答题的自动化评分任务。与其他任务一样，增加LLMs的参数规模会提升性能。目前，现有系统仍需人工监督，尤其是在考试场景中。

> Digital technologies are increasingly used in education to reduce the workload of teachers and students. However, creating open-ended study or examination questions and grading their answers is still a tedious task. This thesis presents the foundation for a system that generates questions grounded in class materials and automatically grades student answers. It introduces a sophisticated method for chunking documents with a visual layout, specifically targeting PDF documents. This method enhances the accuracy of downstream tasks, including Retrieval Augmented Generation (RAG). Our thesis demonstrates that high-quality questions and reference answers can be generated from study material. Further, it introduces a new benchmark for automated grading of short answers to facilitate comparison of automated grading systems. An evaluation of various grading systems is conducted and indicates that Large Language Models (LLMs) can generalise to the task of automated grading of short answers from their pre-training tasks. As with other tasks, increasing the parameter size of the LLMs leads to greater performance. Currently, available systems still need human oversight, especially in examination scenarios.

[Arxiv](https://arxiv.org/abs/2506.12066)