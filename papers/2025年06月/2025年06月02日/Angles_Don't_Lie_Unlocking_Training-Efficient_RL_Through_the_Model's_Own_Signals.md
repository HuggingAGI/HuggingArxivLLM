# 角度无欺：通过模型自身信号实现高效训练强化学习

发布时间：2025年06月02日

`LLM理论` `计算机科学` `机器学习`

> Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals

# 摘要

> # 摘要  
当前针对大型语言模型 (LLMs) 的强化微调 (RFT) 方法由于均匀数据采样下相同查询的冗余暴露，面临着样本低效的问题。虽然之前的工作通过启发式难度指标探索了课程学习，但这些策略忽视了模型自身生成的内在学习信号，导致了次优的训练方案。  

本文发现了一种模型固有的信号——角度集中度，它能够有效反映 LLM 从特定数据中学习的能力。我们从理论和实证上证明了标记隐藏状态向量的角分布与梯度之间存在相关性，揭示了模型对角度集中度更高的数据具有学习偏好。  

基于这一发现，我们提出了 GAIN-RL，一种基于梯度的角度信息导航强化学习框架。通过利用模型自身的角度集中度信号，GAIN-RL 在每个训练周期中动态选择训练数据，确保持续有力的梯度更新，从而显著提升整体训练效率。  

实证评估表明，GAIN-RL (GRPO) 在多样化的数学和编码任务以及不同规模的模型上实现了超过 2.5 倍的训练效率提升。此外，GAIN-RL (GRPO) 的高效采样实现了数据高效的训练，在仅使用原始数据一半的情况下，相比使用完整训练数据的普通 GRPO，取得了更优的性能。代码已发布于 https://github.com/wangqinsi1/GAINRL/tree/main。


> Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

[Arxiv](https://arxiv.org/abs/2506.02281)