# # 正义女神的天平：关于LLMs安全评估的全面综述

发布时间：2025年06月06日

`LLM理论` `人工智能安全`

> The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs

# 摘要

> 人工智能技术的飞速发展使大型语言模型（LLMs）在自然语言处理（NLP）领域大放异彩，涵盖内容生成、人机交互、机器翻译及代码生成等多个方面。然而，其广泛应用也带来了不容忽视的安全隐患。近年来，LLMs生成内容在对抗性场景下偶尔出现毒性、偏见等不安全元素，引发学术界与产业界的广泛关注。尽管在评估LLMs安全风险方面已做出诸多努力，但系统性综述仍显匮乏。本研究旨在全面概述LLMs安全评估的最新进展，聚焦以下关键领域：(1) "为何评估"，解析LLMs安全评估的背景及其与通用评估的差异，彰显评估意义；(2) "评估内容"，基于毒性、稳健性、伦理、偏见与公平性、真实性等维度，分类现有安全评估任务；(3) "评估依据"，总结当前用于安全评估的指标、数据集与基准；(4) "评估方法"，回顾现有工具包并按评估者角色分类主流方法。最后，我们探讨LLMs安全评估的挑战，展望未来研究方向，强调安全评估的重要性，以确保LLMs在现实应用中的安全部署。

> With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have demonstrated remarkable potential in the field of Natural Language Processing (NLP), including areas such as content generation, human-computer interaction, machine translation, and code generation, among others. However, their widespread deployment has also raised significant safety concerns. In recent years, LLM-generated content has occasionally exhibited unsafe elements like toxicity and bias, particularly in adversarial scenarios, which has garnered extensive attention from both academia and industry. While numerous efforts have been made to evaluate the safety risks associated with LLMs, there remains a lack of systematic reviews summarizing these research endeavors. This survey aims to provide a comprehensive and systematic overview of recent advancements in LLMs safety evaluation, focusing on several key aspects: (1) "Why evaluate" that explores the background of LLMs safety evaluation, how they differ from general LLMs evaluation, and the significance of such evaluation; (2) "What to evaluate" that examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and so on; (3) "Where to evaluate" that summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (4) "How to evaluate" that reviews existing evaluation toolkit, and categorizing mainstream evaluation methods based on the roles of the evaluators. Finally, we identify the challenges in LLMs safety evaluation and propose potential research directions to promote further advancement in this field. We emphasize the importance of prioritizing LLMs safety evaluation to ensure the safe deployment of these models in real-world applications.

[Arxiv](https://arxiv.org/abs/2506.11094)