# 利用 LLM 辅助的查询理解实现实时检索增强生成

发布时间：2025年06月26日

`RAG` `信息检索`

> Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation

# 摘要

> 现实世界中实时增强的检索生成（RAG）系统在处理用户查询时面临巨大挑战，这些查询通常充满噪音、模糊且包含多种意图。尽管 RAG 通过外部知识增强了大型语言模型（LLMs），但当前系统通常难以处理如此复杂的输入，因为它们往往是在更干净的数据上进行训练或评估。本文介绍了 Omni-RAG，一个旨在提高 RAG 系统在实时开放领域环境下稳健性和有效性的全新框架。

Omni-RAG 通过 LLM 辅助的查询理解模块，结合三个关键组件对用户输入进行预处理：
1. **深度查询理解和分解**：利用带有定制提示的 LLM 对查询进行去噪（例如纠正拼写错误）并将多意图查询分解为结构化的子查询；
2. **基于意图的知识检索**：针对每个子查询从语料库（例如使用 OpenSearch 的 FineWeb）中进行检索并聚合结果；
3. **重新排序与生成**：其中重新排序器（例如 BGE）在最终响应生成之前优化文档选择，最终响应由 LLM（例如 Falcon-10B）使用链式思考提示生成。

Omni-RAG 的目标是弥合当前 RAG 能力与现实世界应用需求之间的差距，例如 SIGIR 2025 LiveRAG 挑战赛所强调的，通过稳健地处理复杂和嘈杂的查询来实现这一目标。
    

> Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.

[Arxiv](https://arxiv.org/abs/2506.21384)