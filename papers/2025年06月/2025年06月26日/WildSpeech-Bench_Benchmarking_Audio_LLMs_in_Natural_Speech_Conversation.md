# WildSpeech-Bench：在自然对话场景中评测音频 LLMs 的基准表现

发布时间：2025年06月26日

`由于摘要翻译失败，我无法准确理解论文内容并进行分类。请检查摘要是否正确翻译，或提供原始摘要以便进一步分析。` `问答系统`

> WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation

# 摘要

> <翻译失败>

> Recent multi-modal Large Language Models (LLMs) such as GPT-4o have demonstrated strong capabilities of direct speech interaction. However, the lack of specialized and comprehensive benchmarks for end-to-end speech LLM evaluation hinders optimizing the user experience of Audio LLMs in real-world applications. Existing evaluation methods often adapt text-based benchmarks, overlooking speech's unique characteristics and challenges, including prosody, homophones, stuttering, and differing user expectations. Here, we present a novel approach to thoroughly evaluate LLMs in practical speech conversations. We systematically curate real-world chat data relevant to spoken scenarios, introduce diversity in speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. We further design a query-aware evaluation method to use customized evaluation checklists and prompts to enhance the accuracy of automatic evaluation. We conduct comprehensive testing and detailed analysis of various mainstream speech models, revealing significant differences in model performance across different speech scenarios. The use of query-aware evaluation further enables a finer-grained assessment under various speech-specific scenarios. Our benchmark can provide valuable insights for speech model development and evaluation.

[Arxiv](https://arxiv.org/abs/2506.21875)