# # 偏见、准确性和信任：大型语言模型的多元性别视角

发布时间：2025年06月27日

`LLM应用` `社会学` `人工智能`

> Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models

# 摘要

> 大型语言模型（LLMs）正逐渐成为我们生活中不可或缺的一部分，但围绕LLMs偏见的担忧也随之浮现。本研究聚焦于性别多样性群体如何看待LLMs（特别是ChatGPT）中的偏见、准确性和可信度。通过与非二元/跨性别、男性和女性参与者进行的25次深入访谈，我们探讨了性别化和中性提示对模型回应的影响，以及用户如何评估这些回应。研究发现，性别化提示更可能引发身份特定的回应，而非二元参与者尤其容易感受到居高临下的态度和刻板印象。不同性别群体在感知准确性上保持一致，但技术话题和创造性任务中的错误更为显著。可信度因性别而异，男性表现出更高的信任度，而非二元参与者则在性能方面表现出更高的信任度。此外，参与者建议通过多样化训练数据、平衡性别化回应的深度以及加入澄清性问题来改进LLMs。这项研究为CSCW/HCI领域提供了重要见解，强调了在LLM开发及更广泛的AI领域中纳入性别多样性视角的重要性，以推动更加包容和可信的系统发展。

> Large language models (LLMs) are becoming increasingly ubiquitous in our daily lives, but numerous concerns about bias in LLMs exist. This study examines how gender-diverse populations perceive bias, accuracy, and trustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews with non-binary/transgender, male, and female participants, we investigate how gendered and neutral prompts influence model responses and how users evaluate these responses. Our findings reveal that gendered prompts elicit more identity-specific responses, with non-binary participants particularly susceptible to condescending and stereotypical portrayals. Perceived accuracy was consistent across gender groups, with errors most noted in technical topics and creative tasks. Trustworthiness varied by gender, with men showing higher trust, especially in performance, and non-binary participants demonstrating higher performance-based trust. Additionally, participants suggested improving the LLMs by diversifying training data, ensuring equal depth in gendered responses, and incorporating clarifying questions. This research contributes to the CSCW/HCI field by highlighting the need for gender-diverse perspectives in LLM development in particular and AI in general, to foster more inclusive and trustworthy systems.

[Arxiv](https://arxiv.org/abs/2506.21898)