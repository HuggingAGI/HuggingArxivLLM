# Q-Frame：感知查询的帧选择与视频大语言模型的多分辨率适配

发布时间：2025年06月27日

`LLM应用` `视频理解` `视频分析`

> Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs

# 摘要

> 多模态大型语言模型（MLLMs）在视觉理解领域取得了显著成就，但在视频理解方面仍面临挑战，主要源于数据量庞大和时间复杂度。现有视频-LLMs通常采用均匀帧采样，难以有效捕捉视频中与查询相关的关键时空线索。本文提出Q-Frame，一种针对视频内容和特定查询的自适应帧选择和多分辨率缩放方法。Q-Frame采用无需训练、即插即用的策略，由文本-图像匹配网络（如CLIP）生成，并利用Gumbel-Max技巧实现高效帧选择。该方法使视频-LLMs能够处理更多帧，同时不超出计算限制，从而保留关键时序和空间信息。我们在MLVU、LongVideoBench和Video-MME等基准数据集上进行了广泛实验，结果表明Q-Frame优于现有方法，并在各类视频理解任务中展现出广泛应用潜力。

> Multimodal Large Language Models (MLLMs) have demonstrated significant success in visual understanding tasks. However, challenges persist in adapting these models for video comprehension due to the large volume of data and temporal complexity. Existing Video-LLMs using uniform frame sampling often struggle to capture the query-related crucial spatiotemporal clues of videos effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive frame selection and multi-resolution scaling tailored to the video's content and the specific query. Q-Frame employs a training-free, plug-and-play strategy generated by a text-image matching network like CLIP, utilizing the Gumbel-Max trick for efficient frame selection. Q-Frame allows Video-LLMs to process more frames without exceeding computational limits, thereby preserving critical temporal and spatial information. We demonstrate Q-Frame's effectiveness through extensive experiments on benchmark datasets, including MLVU, LongVideoBench, and Video-MME, illustrating its superiority over existing methods and its applicability across various video understanding tasks.

[Arxiv](https://arxiv.org/abs/2506.22139)