# 大型语言模型陷入交火：恶意软件请求与越狱挑战

发布时间：2025年06月09日

`LLM应用` `网络安全`

> LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges

# 摘要

> 随着大型语言模型（LLMs）的广泛应用，它们在对抗越狱攻击方面的安全性问题日益凸显，尤其是攻击者通过精心设计的提示生成恶意输出的情况。尽管已有研究关注了LLMs的整体安全能力，但其在代码生成过程中对越狱攻击的易感性仍鲜有研究。为解决这一问题，我们提出了MalwareBench，这是一个包含3,520个恶意代码生成越狱提示的数据集，专门用于评估LLMs在面对此类威胁时的鲁棒性。该数据集基于320个手动设计的恶意代码生成需求，覆盖了11种越狱方法和29个代码功能类别。实验结果表明，主流LLMs在识别并拒绝恶意代码生成需求方面的能力有限，而结合多种越狱方法更进一步削弱了模型的安全性能：具体而言，恶意内容的平均拒绝率从60.93%下降至39.92%。我们的研究凸显了LLMs在代码安全能力方面仍需克服的重大挑战。

> The widespread adoption of Large Language Models (LLMs) has heightened concerns about their security, particularly their vulnerability to jailbreak attacks that leverage crafted prompts to generate malicious outputs. While prior research has been conducted on general security capabilities of LLMs, their specific susceptibility to jailbreak attacks in code generation remains largely unexplored. To fill this gap, we propose MalwareBench, a benchmark dataset containing 3,520 jailbreaking prompts for malicious code-generation, designed to evaluate LLM robustness against such threats. MalwareBench is based on 320 manually crafted malicious code generation requirements, covering 11 jailbreak methods and 29 code functionality categories. Experiments show that mainstream LLMs exhibit limited ability to reject malicious code-generation requirements, and the combination of multiple jailbreak methods further reduces the model's security capabilities: specifically, the average rejection rate for malicious content is 60.93%, dropping to 39.92% when combined with jailbreak attack algorithms. Our work highlights that the code security capabilities of LLMs still pose significant challenges.

[Arxiv](https://arxiv.org/abs/2506.10022)