# 缓解大型语言模型中的阿拉伯人与穆斯林文化偏见：提示工程技巧系统综述

发布时间：2025年06月22日

`LLM应用`

> Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review

# 摘要

> 大型语言模型在多个领域展现卓越能力，但其对阿拉伯人和穆斯林的文化偏见引发伦理争议，通过延续刻板印象和边缘化行为对社会造成负面影响。尽管人们对大型语言模型中的偏见有了更多认识，但专门针对阿拉伯人和穆斯林群体表现的提示工程策略仍未得到充分研究。本研究采用混合方法系统综述，探讨这些技术，并为研究人员和实践者提供基于证据的指导。

遵循PRISMA指南和Kitchenham的系统综述方法，我们分析了2021年至2024年间发表的8篇实证研究，探讨了偏见缓解策略。研究发现五种主要的提示工程方法：文化提示、情感启动、自我去偏技术、结构化多步骤流程以及参数优化的连续提示。尽管所有方法都显示出减少偏见的潜力，但不同研究和偏见类型之间的效果差异显著。证据表明，某些偏见类型可能比其他类型更难通过基于提示的方法来缓解。

结构化多步骤流程在整体效果上表现最佳，偏见减少了高达87.7%，尽管它们需要更高的技术专业知识。文化提示提供了更广泛的可访问性，同时具有显著的效果。这些结果强调了提示工程在缓解文化偏见方面的可访问性，而无需访问模型参数。

我们发现的研究数量有限，突显了这一关键领域的重要研究空白。未来的研究应致力于开发文化自适应的提示技术，创建针对阿拉伯人和穆斯林的特定评估资源，并将提示工程与互补的去偏方法相结合，以解决更深层次的刻板印象，同时保持模型的实用性。

> Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.

[Arxiv](https://arxiv.org/abs/2506.18199)