# 智能合约漏洞检测的增强型大型语言模型：Smart-LLaMA-DPO

发布时间：2025年06月22日

`LLM应用` `区块链安全` `机器学习`

> Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection

# 摘要

> 智能合约漏洞检测仍是区块链安全领域的重大挑战。现有漏洞检测方法面临两大核心问题：现有数据集缺乏对偏好学习的全面覆盖和高质量解释，且大型语言模型（LLMs）通常难以准确理解智能合约安全中的特定概念。实证分析表明，即使经过持续预训练（CPT）和监督微调（SFT），LLMs仍可能误解状态变更的执行顺序，导致尽管检测决策正确，但解释却有误。为解决这些挑战，我们基于LLaMA-3.1-8B提出了Smart-LLaMA-DPO方法。我们构建了一个涵盖四大主要漏洞类型及机器不可审计漏洞的综合性数据集，包括精准标签、解释和位置信息用于SFT，以及用于直接偏好优化（DPO）的高质量与低质量输出对。其次，我们利用大规模智能合约进行CPT，以提升LLM对智能合约特定安全实践的理解能力。此外，我们采用我们的综合性数据集进行SFT。最后，我们实施DPO，借助人类反馈和特别设计的损失函数，提高优选解释的概率，同时降低非优选输出的可能性。我们在四大主要漏洞类型（重入攻击、时间戳依赖、整数溢出/下溢、delegatecall）以及机器不可审计漏洞上评估了Smart-LLaMA-DPO。我们的方法显著超越了现有最优基线，F1分数平均提升10.43%，准确率平均提升7.87%。此外，无论是LLM评估还是人工评估，均证实我们的方法能生成更准确、详尽且清晰的解释。


> Smart contract vulnerability detection remains a major challenge in blockchain security. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensive coverage and high-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Empirical analysis shows that even after continual pre-training (CPT) and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of state changes, resulting in incorrect explanations despite making correct detection decisions. To address these challenges, we propose Smart-LLaMA-DPO based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major vulnerability types and machine-unauditable vulnerabilities, including precise labels, explanations, and locations for SFT, as well as high-quality and low-quality output pairs for Direct Preference Optimization (DPO). Second, we perform CPT using large-scale smart contract to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct SFT with our comprehensive dataset. Finally, we apply DPO, leveraging human feedback and a specially designed loss function that increases the probability of preferred explanations while reducing the likelihood of non-preferred outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation confirm that our method generates more correct, thorough, and clear explanations.

[Arxiv](https://arxiv.org/abs/2506.18245)