# 化威胁为工具：利用拒绝感知注入攻击实现模型安全对齐

发布时间：2025年06月07日

`LLM应用

理由：这篇论文探讨了如何通过合成数据和攻击技术来提高大型语言模型的安全对齐，属于应用层面的研究。` `人工智能安全`

> From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment

# 摘要

> 安全对齐大型语言模型（LLMs）通常需要大量人工标注的偏好数据，这一过程成本高昂且耗时。虽然合成数据提供了一个有前景的替代方案，但现有方法往往依赖于复杂的迭代提示或辅助模型。为了解决这一问题，我们提出了Refusal-Aware Adaptive Injection（RAAI），这是一个简单、无需训练、模型无关的框架，通过重新利用LLM攻击技术来实现目标。RAAI通过检测内部拒绝信号并自适应地注入预定义短语，以引发有害但流畅的完成结果。我们的实验表明，RAAI能够有效破解LLMs，将有害响应率从基准的2.15%提升至四个基准测试中的平均61.04%。至关重要的是，使用RAAI生成的合成数据对LLMs进行微调，可以提高模型在面对有害提示时的鲁棒性，同时保留其在MMLU和ARC等标准任务上的通用能力。这项工作展示了如何将LLM攻击方法重新构想为可扩展且可控的安全对齐实用工具。

> Safely aligning large language models (LLMs) often demands extensive human-labeled preference data, a process that's both costly and time-consuming. While synthetic data offers a promising alternative, current methods frequently rely on complex iterative prompting or auxiliary models. To address this, we introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward, training-free, and model-agnostic framework that repurposes LLM attack techniques. RAAI works by detecting internal refusal signals and adaptively injecting predefined phrases to elicit harmful, yet fluent, completions. Our experiments show RAAI effectively jailbreaks LLMs, increasing the harmful response rate from a baseline of 2.15% to up to 61.04% on average across four benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by RAAI improves model robustness against harmful prompts while preserving general capabilities on standard tasks like MMLU and ARC. This work highlights how LLM attack methodologies can be reframed as practical tools for scalable and controllable safety alignment.

[Arxiv](https://arxiv.org/abs/2506.10020)