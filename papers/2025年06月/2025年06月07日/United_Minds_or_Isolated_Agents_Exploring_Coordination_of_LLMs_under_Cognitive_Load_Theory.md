# # LLMs的协作机制：在认知负荷理论视角下探索“联合心智”与“孤立代理”的边界
本研究深入探讨了在认知负荷理论框架下，大型语言模型（LLMs）在协作任务中的行为模式，旨在揭示“联合心智”与“孤立代理”这两种极端状态之间的动态平衡与转换机制。

发布时间：2025年06月07日

`LLM理论` `人工智能` `协作系统`

> United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory

# 摘要

> 大型语言模型（LLMs）在复杂多维任务中表现出明显的性能上限，因其常难以整合多元信息或遵循多重约束。我们认为，这一局限源于任务需求超出LLM的有效认知负荷容量。这一观点与认知科学中的认知负荷理论（CLT）高度契合，该理论解释了人类思维中的类似性能边界，且有新兴证据表明LLMs具有有限的工作记忆特性。基于此，我们推出CoThinker——一个旨在缓解认知过载并增强协作解决问题能力的新型LLM多智能体框架。CoThinker通过智能体专业化分配固有认知负荷，并借助结构化沟通和集体工作记忆管理交易负荷，将CLT原则付诸实践。我们在复杂问题解决任务和高认知负荷场景中对CoThinker进行了实证验证，结果显示其在解决方案质量和效率上优于现有基线。我们的分析揭示了特征交互模式，为集体认知的出现和有效负荷管理提供了见解，从而为突破LLM性能上限提供了一种原理性方法。

> Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.

[Arxiv](https://arxiv.org/abs/2506.06843)