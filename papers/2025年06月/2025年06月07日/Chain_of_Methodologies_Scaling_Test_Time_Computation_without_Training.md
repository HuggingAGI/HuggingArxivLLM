# 方法链：无需训练，轻松扩展测试时间计算

发布时间：2025年06月07日

`LLM理论

摘要主要探讨了大型语言模型（LLMs）在复杂推理任务中的表现，并提出了一种新的提示框架Chain of Methodologies (CoM)来提升其推理能力。这属于对LLMs的理论和方法论层面的研究，因此归类为LLM理论。` `人工智能`

> Chain of Methodologies: Scaling Test Time Computation without Training

# 摘要

> 大型语言模型（LLMs）在复杂推理任务方面表现欠佳，主要因为它们的训练数据中缺乏深入的见解，而这些见解通常在公开文档中缺失。本文介绍了Chain of Methodologies (CoM)，一种创新且直观的提示框架，通过整合人类的方法论见解来提升结构化思维，让LLMs能够通过更深入的推理来应对复杂的任务。CoM利用了先进LLMs的元认知能力，通过用户定义的方法论激活系统性推理，而无需显式的微调。实验结果表明，CoM超越了有竞争力的基线方法，证明了无训练提示方法作为复杂推理任务的稳健解决方案的潜力，并通过类人化的方法论见解，朝着人类水平的推理迈出了重要一步。

> Large Language Models (LLMs) often struggle with complex reasoning tasks due to insufficient in-depth insights in their training data, which are typically absent in publicly available documents. This paper introduces the Chain of Methodologies (CoM), an innovative and intuitive prompting framework that enhances structured thinking by integrating human methodological insights, enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages the metacognitive abilities of advanced LLMs, activating systematic reasoning throught user-defined methodologies without explicit fine-tuning. Experiments show that CoM surpasses competitive baselines, demonstrating the potential of training-free prompting methods as robust solutions for complex reasoning tasks and bridging the gap toward human-level reasoning through human-like methodological insights.

[Arxiv](https://arxiv.org/abs/2506.06982)