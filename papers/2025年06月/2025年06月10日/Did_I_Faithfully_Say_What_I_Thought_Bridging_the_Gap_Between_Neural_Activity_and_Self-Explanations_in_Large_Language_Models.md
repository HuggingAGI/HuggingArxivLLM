# 大型语言模型中的神经活动与自我解释：连接内心与表达的桥梁

发布时间：2025年06月10日

`LLM理论` `人工智能`

> Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models

# 摘要

> 大型语言模型（LLM）能够生成自我自然语言解释（self-NLE）来证明其答案，但这些看似逻辑严谨的解释未必反映模型的真实决策过程，导致解释可能不够忠实。现有测量self-NLE忠实度的方法主要依赖行为测试或计算模块识别，却忽视了模型推理背后的神经活动。本研究提出了一种全新的灵活框架，通过直接对比LLM生成的self-NLE与模型内部隐藏状态的解释，实现对self-NLE忠实度的定量评估。该框架不仅适用性强，还通过建立self-NLE与模型推理之间的直接关联，深入揭示了self-NLE的忠实度本质。这一方法不仅推动了我们对self-NLE忠实度的理解，更为生成更忠实的self-NLE奠定了基础。

> Large Language Models (LLM) have demonstrated the capability of generating free text self Natural Language Explanation (self-NLE) to justify their answers. Despite their logical appearance, self-NLE do not necessarily reflect the LLM actual decision-making process, making such explanations unfaithful. While existing methods for measuring self-NLE faithfulness mostly rely on behavioral tests or computational block identification, none of them examines the neural activity underlying the model's reasoning. This work introduces a novel flexible framework for quantitatively measuring the faithfulness of LLM-generated self-NLE by directly comparing the latter with interpretations of the model's internal hidden states. The proposed framework is versatile and provides deep insights into self-NLE faithfulness by establishing a direct connection between self-NLE and model reasoning. This approach advances the understanding of self-NLE faithfulness and provides building blocks for generating more faithful self-NLE.

[Arxiv](https://arxiv.org/abs/2506.09277)