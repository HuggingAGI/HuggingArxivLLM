# 多智能体语言模型：提升协同合作、任务协调与灵活应变能力

发布时间：2025年06月10日

`Agent` `人工智能` `多智能体系统`

> Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation

# 摘要

> 现代大型语言模型（LLMs）在复杂自然语言任务中展现出了令人惊叹的零样本和少样本泛化能力，使其成为虚拟助手的理想选择，广泛应用于翻译、摘要等多样化场景。尽管它们仅通过大规模文本语料库进行训练，而没有明确监督作者意图，LLMs却似乎能够推断出文本交互的潜在含义。这引出了一个根本性问题：大型语言模型能否建模并推理他人的意图，即它们是否具备某种形式的心智理论？理解他人的意图对于有效协作至关重要，这构成了人类社会成功的基础，也是多个智能体（包括人类和自主系统）之间进行合作互动的关键。在本研究中，我们通过合作型多智能体强化学习（MARL）的视角，探讨大型语言模型的心智理论。智能体通过反复交互学习协作，模仿人类的社会推理过程。我们的方法旨在提升人工智能体的适应能力和与人工及人类伙伴的合作能力。通过利用具备自然语言交互能力的大型语言模型代理，我们朝着创建混合型人机系统的目标迈进，这种系统能够实现无缝协作，对未来人机交互的发展具有深远影响。

> Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.

[Arxiv](https://arxiv.org/abs/2506.09331)