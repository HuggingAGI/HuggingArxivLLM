# # 这真的是人类同伴支持者吗？
LLM支持交互中同伴与专家的不一致

发布时间：2025年06月10日

`LLM应用` `心理健康` `人机交互`

> "Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions

# 摘要

> 心理健康已成为日益严重的全球性问题，促使人们关注利用AI驱动的解决方案来扩大心理社会支持的获取渠道。基于真实经历的同伴支持为专业护理提供了宝贵补充。然而，培训、效果和定义上的差异引发了对质量、一致性和安全性的担忧。大型语言模型（LLMs）为增强同伴支持互动，特别是在实时、基于文本的互动中，带来了新机遇。

我们提出并评估了一个AI支持系统，该系统包括模拟情绪困扰客户的LLM、语境敏感的LLM生成建议，以及实时情绪可视化。两项混合方法研究，分别涉及12名同伴支持者和5名心理健康专业人士（即专家），考察了该系统的有效性及其实践意义。两组均认可其提升培训和改善互动质量的潜力。然而，我们发现了一个关键矛盾：尽管同伴支持者能够进行有意义的互动，但专家始终指出同伴支持者回应中的关键问题，如遗漏情绪困扰提示和过早提供建议。

这种不一致突显了当前同伴支持培训的潜在局限，特别是在情感激烈的背景下，安全性和遵循最佳实践至关重要。我们的研究结果强调了标准化、心理基础培训的必要性，尤其是在同伴支持向全球扩展之际。它们还展示了LLM支持系统如何促进这一发展——前提是设计得当并由专家监督。这项工作为负责任地将AI整合到心理健康领域以及LLMs在增强同伴支持护理中的不断发展的角色提供了新的见解。

> Mental health is a growing global concern, prompting interest in AI-driven solutions to expand access to psychosocial support. Peer support, grounded in lived experience, offers a valuable complement to professional care. However, variability in training, effectiveness, and definitions raises concerns about quality, consistency, and safety. Large Language Models (LLMs) present new opportunities to enhance peer support interactions, particularly in real-time, text-based interactions. We present and evaluate an AI-supported system with an LLM-simulated distressed client, context-sensitive LLM-generated suggestions, and real-time emotion visualisations. 2 mixed-methods studies with 12 peer supporters and 5 mental health professionals (i.e., experts) examined the system's effectiveness and implications for practice. Both groups recognised its potential to enhance training and improve interaction quality. However, we found a key tension emerged: while peer supporters engaged meaningfully, experts consistently flagged critical issues in peer supporter responses, such as missed distress cues and premature advice-giving. This misalignment highlights potential limitations in current peer support training, especially in emotionally charged contexts where safety and fidelity to best practices are essential. Our findings underscore the need for standardised, psychologically grounded training, especially as peer support scales globally. They also demonstrate how LLM-supported systems can scaffold this development--if designed with care and guided by expert oversight. This work contributes to emerging conversations on responsible AI integration in mental health and the evolving role of LLMs in augmenting peer-delivered care.

[Arxiv](https://arxiv.org/abs/2506.09354)