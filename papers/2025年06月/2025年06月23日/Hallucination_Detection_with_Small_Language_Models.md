# 小语言模型幻觉检测新思路

发布时间：2025年06月23日

`RAG` `问答系统` `AI协作`

> Hallucination Detection with Small Language Models

# 摘要

> 自 ChatGPT 推出以来，大型语言模型（LLMs）在检索增强生成等任务中展现了巨大潜力。通过向量化数据库检索上下文，为 LLMs 回答生成提供基础。然而，回答中的幻觉现象严重威胁了 LLMs 的可靠性，尤其在问答场景中缺乏真实答案时难以检测。本文提出了一种创新框架，利用多个小语言模型，结合向量化数据库检索的上下文，对 LLMs 生成的回答进行验证。通过将回答分解为单句，并基于多个模型在给定问题、答案和上下文下的输出中生成“是”令牌的概率，实现幻觉检测。实验结果表明，该框架在包含超过 100 组问题、答案和上下文的真实数据集上，正确回答的 F1 分数相比幻觉检测提高了 10%，证明了多个小语言模型在答案验证中的有效性，为学术研究和实际应用提供了高效可扩展的解决方案。

> Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating "Yes" tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.

[Arxiv](https://arxiv.org/abs/2506.22486)