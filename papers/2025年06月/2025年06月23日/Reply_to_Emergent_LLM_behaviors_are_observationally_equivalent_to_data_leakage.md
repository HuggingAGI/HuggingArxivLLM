# 回复：新兴LLM行为与数据泄露在观察上等价

发布时间：2025年06月23日

`Agent` `人工智能` `智能体`

> Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"

# 摘要

> 在模拟大型语言模型 (LLMs) 群体时，数据污染是一个值得关注的问题，即训练数据可能以意想不到的方式影响结果。尽管这一担忧可能阻碍某些多智能体模型的实验，但它并不妨碍我们研究 LLM 群体中真正涌现的动力学。Barrie 和 Törnberg [1] 对 Flint Ashery 等人 [2] 的研究批评，为我们提供了一个机会，可以澄清：自组织和模型依赖的涌现动力学可以在 LLM 群体中进行研究，并且这些动力学在特定情况下（如社会惯例）已被实证观察到。

> A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and Törnberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.

[Arxiv](https://arxiv.org/abs/2506.18600)