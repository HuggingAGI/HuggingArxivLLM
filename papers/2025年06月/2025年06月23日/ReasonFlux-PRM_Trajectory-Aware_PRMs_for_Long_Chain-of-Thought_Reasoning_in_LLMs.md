# ReasonFlux-PRM: 轨迹感知的PRMs，助力LLMs实现长链式推理

发布时间：2025年06月23日

`LLM应用` `人工智能` `机器学习`

> ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs

# 摘要

> 过程奖励模型（PRMs）作为一种强大的框架，最近在监督大型语言模型（LLMs）的中间推理步骤方面崭露头角。然而，现有的PRMs主要依赖于模型的最终输出响应进行训练，难以对中间思考轨迹进行稳健评估，尤其是在前沿推理模型（如Deepseek-R1）生成的轨迹-响应输出的新兴场景下表现不足。为此，我们提出了ReasonFlux-PRM——一种专为评估轨迹-响应类型推理轨迹设计的新型轨迹感知型PRM。该模型创新性地结合了步骤级和轨迹级监督机制，实现了与结构化思维链数据高度对齐的精细奖励分配。我们进一步将ReasonFlux-PRM扩展至支持线下和线上两种场景下的奖励监督，具体包括：(i) 从海量数据中筛选高质量的蒸馏样本用于下游小模型的监督微调，(ii) 在强化学习过程中提供密集的过程级奖励以优化策略性能，以及(iii) 实现基于奖励的Best-of-N测试时扩展。在AIME、MATH500和GPQA-Diamond等具有挑战性的下游基准测试中，ReasonFlux-PRM-7B在数据质量选择上超越了强大的PRMs（如Qwen2.5-Math-PRM-72B）和人工编纂的基线。实验结果表明，ReasonFlux-PRM-7B在监督微调、强化学习和测试时扩展三个关键环节均实现了显著性能提升，平均提升幅度分别达到12.1%、4.5%和6.3%。此外，我们还开源了轻量级的ReasonFlux-PRM-1.5B，特别适用于资源受限的场景和边缘端部署。项目地址：https://github.com/Gen-Verse/ReasonFlux

> Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux

[Arxiv](https://arxiv.org/abs/2506.18896)