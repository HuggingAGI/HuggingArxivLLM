# SlimMoE：通过专家剪枝与蒸馏实现大型MoE模型的结构化压缩

发布时间：2025年06月23日

`LLM理论` `人工智能` `模型优化`

> SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation

# 摘要

> 专家混合（MoE）架构作为一种强大的范式，能够在保持推理效率的同时实现大型语言模型（LLMs）的扩展。然而，MoE模型巨大的内存需求使其在资源受限的环境中难以进行微调或部署。为了解决这一挑战，我们提出了SlimMoE，这是一个多阶段压缩框架，旨在将大型MoE模型转化为更小、更高效的变体，而无需承担从头训练的高昂成本。通过系统性地减少参数数量，并利用中间阶段的知识迁移，我们的方法有效缓解了一次性剪枝方法常见的性能下降问题。借助这一框架，我们仅使用4000亿个token（不足原模型训练数据的10%），成功将Phi 3.5-MoE（总计419亿/激活66亿参数）压缩为Phi-mini-MoE（总计76亿/激活24亿参数）和Phi-tiny-MoE（总计38亿/激活11亿参数）。这些压缩后的模型可以在单个GPU（Phi-mini-MoE使用A100，Phi-tiny-MoE使用A6000）上进行微调，使其非常适合学术研究和资源受限的环境。实验结果表明，这些压缩模型在与同尺寸模型的对比中表现更优，并且在与更大模型的竞争中也毫不逊色。例如，Phi-mini-MoE仅使用2/3的激活参数即可达到与Phi-3-mini相当或更好的性能，并且在延迟大幅降低的情况下，其MMLU得分仍与Llama 3.1 8B模型相当。我们的研究证明，结构化剪枝与分阶段蒸馏相结合，为创建高质量、紧凑的MoE模型提供了一条有效的路径，这为MoE架构的广泛应用铺平了道路。我们已将这些模型公开发布于https://huggingface.co/microsoft/Phi-mini-MoE-instruct 和 https://huggingface.co/microsoft/Phi-tiny-MoE-instruct。

> The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

[Arxiv](https://arxiv.org/abs/2506.18349)