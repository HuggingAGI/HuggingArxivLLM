# 从词语到声音：研究语音和文本模型中的概念形成过程

发布时间：2025年06月01日

`LLM理论

理由：这篇论文探讨了大型语言模型（LLMs）在跨模态（如语音和文本）学习中的语义理解能力，分析了模型如何形成潜在概念结构。研究使用潜在概念分析方法，属于对模型内在机制的理论分析，因此归类为LLM理论。`

> From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models

# 摘要

> 大型语言模型（LLMs）的出现证明，仅通过文本训练的系统能够获取广泛的世界知识、发展推理能力，并内化抽象语义概念，展现出与通用智能相关的特性。这引出了一个有趣的问题：类似的概念是否也存在于语音等其他模态的模型中？此外，当模型同时接受多种模态的训练时，它们是否能形成更丰富、更结构化的语义理解？为了探索这一问题，我们分析了语音和文本模型在单独及联合训练时所学习到的概念结构。我们采用了一种无监督的方法——潜在概念分析，用于揭示和解释神经网络中的潜在表征，以此研究跨模态的语义抽象是如何形成的。为了确保研究的可复现性，我们向社区开放了脚本和其他资源。

> The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.

[Arxiv](https://arxiv.org/abs/2506.01133)