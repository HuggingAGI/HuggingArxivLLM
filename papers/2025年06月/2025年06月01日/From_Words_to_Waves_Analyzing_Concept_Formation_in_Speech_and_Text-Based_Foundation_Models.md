# # 从词到波：解析语音与文本基础模型中的概念构建

发布时间：2025年06月01日

`LLM理论` `人工智能` `语音处理`

> From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models

# 摘要

> 大型语言模型（LLMs）的出现展示了仅通过文本训练的系统能够获取大量世界知识、发展推理能力并内化抽象语义概念，展现出与通用智能相关的特性。这引发了一个引人深思的问题：语音模型是否也会出现类似的语义抽象能力？进一步地，当模型同时接受多模态训练时，它们是否能够形成更丰富、更有结构的语义理解？为了探索这些问题，我们分析了语音和文本模型在单独训练和联合训练时所学习到的概念结构。我们采用了一种无监督方法——潜在概念分析（Latent Concept Analysis），用于揭示和解释神经网络中的潜在表示，以此研究跨模态的语义抽象是如何形成的。为了方便复现，我们向社区提供了相关脚本和其他资源。

> The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.

[Arxiv](https://arxiv.org/abs/2506.01133)