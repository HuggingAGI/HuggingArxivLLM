# XGUARD：评估大型语言模型在极端内容安全漏洞的分级基准

发布时间：2025年06月01日

`LLM理论` `人工智能`

> XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content

# 摘要

> 大语言模型（LLMs）既能生成意识形态修辞，也能输出具体暴力指令。然而，现有的安全评估往往过于简单地采用二元标签（安全与不安全），忽视了这些输出带来的风险复杂性。为此，我们推出了XGUARD——一个评估LLMs生成极端内容严重性的基准框架。该框架包含3,840个红队提示，这些提示源自社交媒体和新闻等真实数据，覆盖了多种思想倾向的场景。

我们的框架将模型响应划分为五个危险等级（0至4），从而实现对故障频率和严重性的细致分析。我们还引入了可解释的攻击严重性曲线（ASC），用于可视化模型漏洞并比较不同威胁强度下的防御机制。通过XGUARD，我们对六种流行LLMs和两种轻量级防御策略进行了评估，揭示了当前安全防护的缺口以及鲁棒性与表达自由之间的权衡关系。这项研究凸显了分级安全指标在打造值得信赖的LLMs中的重要价值。


> Large Language Models (LLMs) can generate content spanning ideological rhetoric to explicit instructions for violence. However, existing safety evaluations often rely on simplistic binary labels (safe and unsafe), overlooking the nuanced spectrum of risk these outputs pose. To address this, we present XGUARD, a benchmark and evaluation framework designed to assess the severity of extremist content generated by LLMs. XGUARD includes 3,840 red teaming prompts sourced from real world data such as social media and news, covering a broad range of ideologically charged scenarios. Our framework categorizes model responses into five danger levels (0 to 4), enabling a more nuanced analysis of both the frequency and severity of failures. We introduce the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and compare defense mechanisms across threat intensities. Using XGUARD, we evaluate six popular LLMs and two lightweight defense strategies, revealing key insights into current safety gaps and trade-offs between robustness and expressive freedom. Our work underscores the value of graded safety metrics for building trustworthy LLMs.

[Arxiv](https://arxiv.org/abs/2506.00973)