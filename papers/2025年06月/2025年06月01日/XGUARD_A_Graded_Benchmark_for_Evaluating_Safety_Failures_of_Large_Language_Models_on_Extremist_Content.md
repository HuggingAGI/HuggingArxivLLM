# # XGUARD：分级评测基准，专为大型语言模型在极端内容中的安全漏洞而设计

发布时间：2025年06月01日

`LLM应用` `内容安全` `意识形态`

> XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content

# 摘要

> 大型语言模型（LLMs）生成的内容涵盖从意识形态宣传到具体暴力指令的广泛范围。然而，现有的安全评估方法往往依赖简单的二元标签（安全与不安全），忽视了这些输出所具有的风险细微差别。为此，我们提出了XGUARD，一个用于评估LLMs生成极端主义内容严重程度的基准和评估框架。XGUARD包含3,840个对抗性测试提示，这些提示源自社交媒体和新闻等真实世界数据，覆盖了多种意识形态高度敏感的场景。我们的框架将模型回应分为五个危险等级（0到4），从而能够更细致地分析失败的频率和严重程度。我们引入了可解释的攻击严重性曲线（ASC）来可视化模型的漏洞，并在不同威胁强度下比较防御机制。通过XGUARD，我们评估了六种流行的LLMs和两种轻量级防御策略，揭示了当前安全缺口的关键见解，以及鲁棒性与表达自由之间的权衡。我们的工作强调了分级安全指标在构建值得信赖的LLMs中的重要性。

> Large Language Models (LLMs) can generate content spanning ideological rhetoric to explicit instructions for violence. However, existing safety evaluations often rely on simplistic binary labels (safe and unsafe), overlooking the nuanced spectrum of risk these outputs pose. To address this, we present XGUARD, a benchmark and evaluation framework designed to assess the severity of extremist content generated by LLMs. XGUARD includes 3,840 red teaming prompts sourced from real world data such as social media and news, covering a broad range of ideologically charged scenarios. Our framework categorizes model responses into five danger levels (0 to 4), enabling a more nuanced analysis of both the frequency and severity of failures. We introduce the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and compare defense mechanisms across threat intensities. Using XGUARD, we evaluate six popular LLMs and two lightweight defense strategies, revealing key insights into current safety gaps and trade-offs between robustness and expressive freedom. Our work underscores the value of graded safety metrics for building trustworthy LLMs.

[Arxiv](https://arxiv.org/abs/2506.00973)