# 确保LLM-MAS值得信赖，全面的漏洞分析至关重要。

发布时间：2025年06月01日

`Agent` `多智能体系统`

> Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS

# 摘要

> 本文强调，构建值得信赖的基于大型语言模型的多智能体系统（LLM-MAS）需要进行全面的漏洞分析。这些系统由多个基于LLM的智能体协同工作，在高风险应用中被广泛应用，但其复杂的结构带来了全新的安全威胁。虽然单智能体漏洞已有较多研究，但LLM-MAS在智能体间通信、信任关系和工具集成方面引入了独特的攻击面，这些方面尚未得到充分探索。

我们提出了一种统一的LLM-MAS漏洞分析框架，整合了各种相关研究。对于每一种类型的漏洞，我们基于实际攻击者的能力定义了正式的威胁模型，并通过真实世界的LLM-MAS应用实例进行了说明。这种建模方法使我们能够严格量化不同架构下的漏洞，并为设计有意义的评估基准奠定了基础。

我们的分析表明，LLM-MAS面临由组合效应带来的更高风险——单个组件的漏洞可能通过智能体间的通信蔓延，形成单智能体系统中不存在的威胁模型。最后，我们指出了三个关键开放挑战：（1）开发专门针对LLM-MAS漏洞评估的基准测试，（2）考虑特定于多智能体架构的新潜在攻击，（3）实现能够强制LLM-MAS安全性的信任管理系统。这项研究为未来增强LLM-MAS可信度的努力奠定了重要基础，因为这些系统正在不断扩展到关键应用中。

> This paper argues that a comprehensive vulnerability analysis is essential for building trustworthy Large Language Model-based Multi-Agent Systems (LLM-MAS). These systems, which consist of multiple LLM-powered agents working collaboratively, are increasingly deployed in high-stakes applications but face novel security threats due to their complex structures. While single-agent vulnerabilities are well-studied, LLM-MAS introduces unique attack surfaces through inter-agent communication, trust relationships, and tool integration that remain significantly underexplored. We present a systematic framework for vulnerability analysis of LLM-MAS that unifies diverse research. For each type of vulnerability, we define formal threat models grounded in practical attacker capabilities and illustrate them using real-world LLM-MAS applications. This formulation enables rigorous quantification of vulnerability across different architectures and provides a foundation for designing meaningful evaluation benchmarks. Our analysis reveals that LLM-MAS faces elevated risk due to compositional effects -- vulnerabilities in individual components can cascade through agent communication, creating threat models not present in single-agent systems. We conclude by identifying critical open challenges: (1) developing benchmarks specifically tailored to LLM-MAS vulnerability assessment, (2) considering new potential attacks specific to multi-agent architectures, and (3) implementing trust management systems that can enforce security in LLM-MAS. This research provides essential groundwork for future efforts to enhance LLM-MAS trustworthiness as these systems continue their expansion into critical applications.

[Arxiv](https://arxiv.org/abs/2506.01245)