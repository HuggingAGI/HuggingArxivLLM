# ReTern: 基于计算内存的三进制LLMs容错增强：利用自然冗余与符号变换

发布时间：2025年06月01日

`LLM应用` `人工智能` `计算机体系结构`

> ReTern: Exploiting Natural Redundancy and Sign Transformations for Enhanced Fault Tolerance in Compute-in-Memory based Ternary LLMs

# 摘要

> 三元大型语言模型（LLMs）采用三元精度权重和8位激活，在大幅降低全精度LLMs的高计算和内存需求的同时，仍保持了强劲的性能。通过部署在三元计算内存（TCiM）加速器上，三元LLMs的能效和性能进一步提升，有效缓解了冯·诺依曼瓶颈。然而，TCiM加速器易受内存固定故障（SAFs）影响，导致模型精度下降，而这一问题在LLMs中尤为突出，因其权重稀疏度较低。为增强TCiM加速器对SAFs的容忍度，我们提出了ReTern，该方法基于（i）故障感知符号变换（FAST）和（ii）利用TCiM位单元的自然冗余进行重新编程。核心思想是通过FAST减少+1/-1权重因SAFs导致的计算错误，同时利用自然位单元冗余针对性修复0权重的SAFs（零修复）。我们在BitNet b1.58 700M和3B三元LLMs上的实验表明，我们的技术显著提升了故障容忍能力，在存在故障的情况下，WikiText数据集上的困惑度降低了35%。这些改进仅带来小于3%的能耗、7%的延迟和1%的面积开销。

> Ternary large language models (LLMs), which utilize ternary precision weights and 8-bit activations, have demonstrated competitive performance while significantly reducing the high computational and memory requirements of full-precision LLMs. The energy efficiency and performance of Ternary LLMs can be further improved by deploying them on ternary computing-in-memory (TCiM) accelerators, thereby alleviating the von-Neumann bottleneck. However, TCiM accelerators are prone to memory stuck-at faults (SAFs) leading to degradation in the model accuracy. This is particularly severe for LLMs due to their low weight sparsity. To boost the SAF tolerance of TCiM accelerators, we propose ReTern that is based on (i) fault-aware sign transformations (FAST) and (ii) TCiM bit-cell reprogramming exploiting their natural redundancy. The key idea is to utilize FAST to minimize computations errors due to SAFs in +1/-1 weights, while the natural bit-cell redundancy is exploited to target SAFs in 0 weights (zero-fix). Our experiments on BitNet b1.58 700M and 3B ternary LLMs show that our technique furnishes significant fault tolerance, notably 35% reduction in perplexity on the Wikitext dataset in the presence of faults. These benefits come at the cost of < 3%, < 7%, and < 1% energy, latency and area overheads respectively.

[Arxiv](https://arxiv.org/abs/2506.01140)