# ReTern：通过自然冗余与符号变换提升基于计算内存的三元LLM容错能力

发布时间：2025年06月01日

`LLM应用` `计算硬件` `人工智能`

> ReTern: Exploiting Natural Redundancy and Sign Transformations for Enhanced Fault Tolerance in Compute-in-Memory based Ternary LLMs

# 摘要

> 三元大语言模型（LLMs）采用三元精度权重和8位激活，展现出极具竞争力的表现，同时大幅降低了全精度LLMs的高计算和内存需求。通过在三元计算内存（TCiM）加速器上部署三元LLMs，其能效和性能可以进一步提升，从而缓解冯·诺依曼瓶颈。然而，TCiM加速器容易受到内存固定在故障（SAFs）的影响，导致模型精度下降。这种情况对于LLMs尤为严重，因为它们的权重稀疏性较低。为增强TCiM加速器对SAFs的容忍度，我们提出了ReTern，该方法基于（i）容错感知符号变换（FAST）和（ii）利用TCiM存储单元自然冗余的重新编程。核心思路是利用FAST最小化+1/-1权重因SAFs导致的计算误差，同时利用自然存储单元冗余针对0权重的SAFs（零修复）。我们在BitNet b1.58 700M和3B三元LLMs上的实验表明，我们的技术提供了显著的容错能力，在存在故障的情况下，Wikitext数据集上的困惑度降低了35%。这些优势的代价是分别低于3%、7%和1%的能源、延迟和面积开销。

> Ternary large language models (LLMs), which utilize ternary precision weights and 8-bit activations, have demonstrated competitive performance while significantly reducing the high computational and memory requirements of full-precision LLMs. The energy efficiency and performance of Ternary LLMs can be further improved by deploying them on ternary computing-in-memory (TCiM) accelerators, thereby alleviating the von-Neumann bottleneck. However, TCiM accelerators are prone to memory stuck-at faults (SAFs) leading to degradation in the model accuracy. This is particularly severe for LLMs due to their low weight sparsity. To boost the SAF tolerance of TCiM accelerators, we propose ReTern that is based on (i) fault-aware sign transformations (FAST) and (ii) TCiM bit-cell reprogramming exploiting their natural redundancy. The key idea is to utilize FAST to minimize computations errors due to SAFs in +1/-1 weights, while the natural bit-cell redundancy is exploited to target SAFs in 0 weights (zero-fix). Our experiments on BitNet b1.58 700M and 3B ternary LLMs show that our technique furnishes significant fault tolerance, notably 35% reduction in perplexity on the Wikitext dataset in the presence of faults. These benefits come at the cost of < 3%, < 7%, and < 1% energy, latency and area overheads respectively.

[Arxiv](https://arxiv.org/abs/2506.01140)