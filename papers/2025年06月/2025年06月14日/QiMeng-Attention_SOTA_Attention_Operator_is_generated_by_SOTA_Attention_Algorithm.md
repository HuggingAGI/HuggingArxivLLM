# 齐蒙-注意力机制：通过先进注意力算法生成的当前最优注意力操作符

发布时间：2025年06月14日

`LLM应用

理由：这篇论文主要探讨了如何优化大型语言模型中的注意力机制，特别是在生成高性能注意力代码方面的应用。虽然涉及LLM的内部机制，但核心在于应用层面的优化和改进，属于LLM的应用范畴。` `GPU加速` `代码生成`

> QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm

# 摘要

> 注意力机制是大型语言模型（LLMs）中的关键性能瓶颈，尤其在长上下文场景中表现突出。虽然FlashAttention是目前最广泛应用且高效的GPU感知加速算法，但其耗时且特定于硬件的手动实现限制了跨GPU架构的适应性。现有LLMs在代码生成任务中潜力无限，但生成高性能注意力代码仍具挑战。核心问题在于，LLMs难以理解注意力机制的复杂数据流与计算过程，也无法有效利用低级原语释放GPU性能。

为解决这一难题，我们提出了一种适用于LLM的思考语言（LLM-TL），帮助LLMs实现高级优化逻辑生成与GPU低级实现的解耦，并深化其对注意力机制的理解。结合两阶段推理工作流——TL代码生成与翻译，LLMs能够自动在多种GPU上生成FlashAttention实现，为以注意力为核心的算法构建了一种自我优化的高性能注意力操作生成范式。在A100、RTX8000和T4 GPU上验证，我们的方法性能远超原始LLMs，实现了最高35.16倍的加速。此外，我们的方法不仅在大多数场景下超越了人工优化的库（如cuDNN和官方库），还扩展了对未支持硬件和数据类型的兼容性，将开发时间从数月缩短至数分钟，相较于人类专家，效率大幅提升。

> The attention operator remains a critical performance bottleneck in large language models (LLMs), particularly for long-context scenarios. While FlashAttention is the most widely used and effective GPU-aware acceleration algorithm, it must require time-consuming and hardware-specific manual implementation, limiting adaptability across GPU architectures. Existing LLMs have shown a lot of promise in code generation tasks, but struggle to generate high-performance attention code. The key challenge is it cannot comprehend the complex data flow and computation process of the attention operator and utilize low-level primitive to exploit GPU performance.
  To address the above challenge, we propose an LLM-friendly Thinking Language (LLM-TL) to help LLMs decouple the generation of high-level optimization logic and low-level implementation on GPU, and enhance LLMs' understanding of attention operator. Along with a 2-stage reasoning workflow, TL-Code generation and translation, the LLMs can automatically generate FlashAttention implementation on diverse GPUs, establishing a self-optimizing paradigm for generating high-performance attention operators in attention-centric algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our methods significantly outshines that of vanilla LLMs, achieving a speed-up of up to 35.16x. Besides, our method not only surpasses human-optimized libraries (cuDNN and official library) in most scenarios but also extends support to unsupported hardware and data types, reducing development time from months to minutes compared with human experts.

[Arxiv](https://arxiv.org/abs/2506.12355)