# 探索安全边界：ATLAS 2025技术报告

发布时间：2025年06月14日

`LLM应用` `AI安全` `对抗攻击`

> Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025

# 摘要

> 多模态大型语言模型（MLLMs）在各类应用中实现了突破性进展，但仍然面临安全威胁，尤其是能够诱导生成有害输出的越狱攻击。为系统性评估和提升其安全性，我们发起了对抗测试与大型模型对齐安全挑战赛（ATLAS 2025）。本次技术报告展示了比赛成果，共有86支队伍参与，通过对抗性图文攻击测试MLLMs的漏洞，比赛分为白盒和黑盒两个阶段。比赛结果凸显了保障MLLMs安全的持续挑战，为开发更强大的防御机制提供了宝贵指导。这项挑战赛为MLLM安全评估设立了新基准，并为构建更安全的多模态AI系统奠定了基础。比赛的代码和数据已公开发布在https://github.com/NY1024/ATLAS_Challenge_2025。

> Multimodal Large Language Models (MLLMs) have enabled transformative advancements across diverse applications but remain susceptible to safety threats, especially jailbreak attacks that induce harmful outputs. To systematically evaluate and improve their safety, we organized the Adversarial Testing & Large-model Alignment Safety Grand Challenge (ATLAS) 2025}. This technical report presents findings from the competition, which involved 86 teams testing MLLM vulnerabilities via adversarial image-text attacks in two phases: white-box and black-box evaluations. The competition results highlight ongoing challenges in securing MLLMs and provide valuable guidance for developing stronger defense mechanisms. The challenge establishes new benchmarks for MLLM safety evaluation and lays groundwork for advancing safer multimodal AI systems. The code and data for this challenge are openly available at https://github.com/NY1024/ATLAS_Challenge_2025.

[Arxiv](https://arxiv.org/abs/2506.12430)