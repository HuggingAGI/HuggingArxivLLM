# 提示中的认知偏见如何影响大型语言模型的输出？

发布时间：2025年06月14日

`LLM应用` `人工智能`

> Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs

# 摘要

> 本文探讨了认知偏差对大型语言模型 (LLMs) 输出的影响。认知偏差，如确认性偏差和可得性偏差，可能通过提示扭曲用户输入，导致 LLMs 产生不忠实且具有误导性的输出。通过系统性框架，我们的研究将各种认知偏差引入提示，并在多个基准数据集（包括通用和金融问答场景）上评估其对 LLM 准确性的影响。结果显示，即使是细微的偏差也可能会显著改变 LLM 的回答选择，凸显出需要具备偏见意识的提示设计和缓解策略的重要性。此外，我们的注意力权重分析揭示了这些偏差如何改变 LLM 内部决策过程，影响注意力分布，从而与输出不准确相关联。这项研究对 AI 开发者和用户具有重要意义，有助于提升 AI 应用在各个领域的健壮性和可靠性。

> This paper investigates the influence of cognitive biases on Large Language Models (LLMs) outputs. Cognitive biases, such as confirmation and availability biases, can distort user inputs through prompts, potentially leading to unfaithful and misleading outputs from LLMs. Using a systematic framework, our study introduces various cognitive biases into prompts and assesses their impact on LLM accuracy across multiple benchmark datasets, including general and financial Q&A scenarios. The results demonstrate that even subtle biases can significantly alter LLM answer choices, highlighting a critical need for bias-aware prompt design and mitigation strategy. Additionally, our attention weight analysis highlights how these biases can alter the internal decision-making processes of LLMs, affecting the attention distribution in ways that are associated with output inaccuracies. This research has implications for Al developers and users in enhancing the robustness and reliability of Al applications in diverse domains.

[Arxiv](https://arxiv.org/abs/2506.12338)