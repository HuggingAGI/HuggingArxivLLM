# 多模态多图像推理基准用于评估多语言学习模型

发布时间：2025年06月04日

`LLM应用` `多模态` `视觉推理`

> Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark

# 摘要

> 随着多模态大型语言模型（MLLMs）能力的提升和广泛应用，同时处理和推理多个图像的需求日益增加。然而，现有MLLM基准测试在多图像输入的推理能力方面存在明显不足，要么专注于单图像视觉推理，要么仅通过最终答案评估处理多图像理解任务。为填补这一空白，我们推出了**Multimodal Multi-image Reasoning Benchmark (MMRB)**，这是首个专为跨多图像的结构化视觉推理设计的基准测试。MMRB涵盖**92个子任务**，涉及空间、时间和语义推理，其多解决方案的CoT风格注释由GPT-4o生成并经人类专家优化。我们还设计了一个派生子集，用于评估多图像场景中的多模态奖励模型。为了实现快速且可扩展的评估，我们提出了一种基于开源LLMs的句子级别匹配框架。在涵盖**40个MLLMs**的广泛基线实验中，包括9个推理特定模型和8个奖励模型，结果显示开源MLLMs在多图像推理任务中仍显著落后于商业MLLMs。此外，当前多模态奖励模型几乎无法应对多图像奖励排名任务。

> With enhanced capabilities and widespread applications, Multimodal Large Language Models (MLLMs) are increasingly required to process and reason over multiple images simultaneously. However, existing MLLM benchmarks focus either on single-image visual reasoning or on multi-image understanding tasks with only final-answer evaluation, leaving the reasoning capabilities of MLLMs over multi-image inputs largely underexplored. To address this gap, we introduce the $\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first benchmark designed to evaluate structured visual reasoning across multiple images. MMRB comprises $\textbf{92 sub-tasks}$ covering spatial, temporal, and semantic reasoning, with multi-solution, CoT-style annotations generated by GPT-4o and refined by human experts. A derivative subset is designed to evaluate multimodal reward models in multi-image scenarios. To support fast and scalable evaluation, we propose a sentence-level matching framework using open-source LLMs. Extensive baseline experiments on $\textbf{40 MLLMs}$, including 9 reasoning-specific models and 8 reward models, demonstrate that open-source MLLMs still lag significantly behind commercial MLLMs in multi-image reasoning tasks. Furthermore, current multimodal reward models are nearly incapable of handling multi-image reward ranking tasks.

[Arxiv](https://arxiv.org/abs/2506.04280)