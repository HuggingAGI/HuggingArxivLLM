# # MANBench：你的多模态模型比人类更聪明吗？

发布时间：2025年06月04日

`LLM应用` `多模态` `基准测试`

> MANBench: Is Your Multimodal Model Smarter than Human?

# 摘要

> 多模态大型语言模型（MLLMs）的迅猛发展引发了关于其能否在多模态任务中超越人类表现的热烈讨论。为此，我们推出了MANBench——一个涵盖1314个问题的中英双语基准测试，覆盖知识型与非知识型两大领域，包含九项具体任务。MANBench着重考察直观推理、跨模态无缝融合以及现实场景的复杂性，构建了一个全面的评估体系。

通过大规模多样化的人类实验，我们对比了人类与当前最先进的MLLMs的表现。结果显示，MLLMs在知识掌握和图文理解等任务中表现优异，但在转化理解、图像一致性分析和多图理解等跨模态深度推理任务中仍显不足。此外，无论是人类还是MLLMs，在面对解谜和空间想象等高度复杂任务时都面临挑战。

MANBench不仅揭示了MLLMs的能力边界，也表明即使是先进的模型，要在多个领域达到人类水平仍有差距。我们期待MANBench能激发更多研究，推动MLLMs向人类多模态能力迈进。相关代码与数据集已开放，获取地址为：https://github.com/micdz/MANBench。


> The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework.
  Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination.
  MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at https://github.com/micdz/MANBench.

[Arxiv](https://arxiv.org/abs/2506.11080)