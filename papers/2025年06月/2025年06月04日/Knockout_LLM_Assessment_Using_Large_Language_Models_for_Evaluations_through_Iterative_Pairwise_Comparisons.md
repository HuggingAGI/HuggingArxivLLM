# 大型语言模型的对决式评估：通过迭代配对比较进行评估

发布时间：2025年06月04日

`LLM应用` `机器翻译` `科学领域`

> Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons

# 摘要

> 大型语言模型（LLMs）在机器翻译和科学领域等不同场景中展现出强大的评估能力。然而，现有的LLM作为裁判的方法主要依赖于个体评估或一轮成对评估，这限制了评估模型的全局视角。针对这一问题，我们提出了Knockout Assessment方法，这是一种基于淘汰赛系统的LLM评估方法，通过迭代的成对比较来进行评估。实验结果表明，在两个数据集上对三种LLM进行测试，淘汰赛评估显著提高了评分准确性，使皮尔逊相关系数在大学水平考试评分和机器翻译评估中平均提升了0.07，使LLM评估结果更加贴近人类评分标准。

> Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective. To address this, we present Knockout Assessment, an LLM-asa Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.

[Arxiv](https://arxiv.org/abs/2506.03785)