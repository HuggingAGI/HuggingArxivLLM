# 深入理解从业者需求，准确衡量LLM系统引发的表征性伤害

发布时间：2025年06月04日

`其他

理由：这篇论文主要探讨了评估大型语言模型（LLM）表征性伤害的工具的有效性和可用性，以及研究人员在使用这些工具时遇到的挑战。它不属于Agent、RAG或LLM应用，因为没有涉及具体的应用场景或生成技术。虽然它与LLM相关，但它主要关注的是评估工具和方法，而不是模型本身的理论发展，因此更适合归类为其他。` `人工智能`

> Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by LLM-Based Systems

# 摘要

> NLP研究社区已公开发布了众多用于衡量大型语言模型（LLM）系统造成的表征性伤害的工具，这些工具以数据集、指标、工具等形式呈现。本文探讨了这些工具在多大程度上能够满足负责评估LLM系统的研究人员的需求。通过与12位研究人员进行半结构化访谈，我们发现研究人员通常无法使用公开可用的工具来衡量表征性伤害。我们识别出两类挑战：在某些情况下，工具没有实际效用，因为它们无法有意义地衡量研究人员想要测量的内容，或者与研究者的需求不相符合；在其他情况下，即使是有效的工具，研究者也因实际和制度性的障碍而无法采用。基于测量理论和实用测量方法，我们提出了一些建议，以应对这些挑战，更好地满足研究者的需求。

> The NLP research community has made publicly available numerous instruments for measuring representational harms caused by large language model (LLM)-based systems. These instruments have taken the form of datasets, metrics, tools, and more. In this paper, we examine the extent to which such instruments meet the needs of practitioners tasked with evaluating LLM-based systems. Via semi-structured interviews with 12 such practitioners, we find that practitioners are often unable to use publicly available instruments for measuring representational harms. We identify two types of challenges. In some cases, instruments are not useful because they do not meaningfully measure what practitioners seek to measure or are otherwise misaligned with practitioner needs. In other cases, instruments - even useful instruments - are not used by practitioners due to practical and institutional barriers impeding their uptake. Drawing on measurement theory and pragmatic measurement, we provide recommendations for addressing these challenges to better meet practitioner needs.

[Arxiv](https://arxiv.org/abs/2506.04482)