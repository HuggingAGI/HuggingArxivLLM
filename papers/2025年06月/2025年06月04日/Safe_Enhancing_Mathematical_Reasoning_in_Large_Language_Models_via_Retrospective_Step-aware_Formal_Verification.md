# Safe：通过回顾式步骤感知方法提升大型语言模型的数学推理能力

发布时间：2025年06月04日

`LLM理论` `人工智能`

> Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification

# 摘要

> Chain-of-Thought (CoT) 提示已成为从大型语言模型 (LLMs) 中激发推理能力的事实标准方法。然而，针对 CoT 中难以检测的幻觉问题，现有方法如过程奖励模型 (PRMs) 或自洽性方法如同黑箱，无法提供可验证的证据，这可能限制了它们的效用。为解决这一挑战，我们从“支持数学声明的黄金标准是提供证明”的理念中汲取灵感，提出了一种回顾式、步骤感知的正式验证框架 $Safe$。我们的方法不仅限于随意打分，而是致力于在每一步推理中用正式的数学语言 Lean 4 表达数学声明，并提供正式证明以识别幻觉。通过在多个语言模型和各种数学数据集上的评估，$Safe$ 框架不仅展现了显著的性能提升，还提供了可解释和可验证的证据。此外，我们提出了 $FormalStep$ 作为逐步正确性定理证明的基准，包含 30,809 个正式声明。据我们所知，这是首次尝试利用正式数学语言 Lean 4 来验证 LLM 生成的自然语言内容，这与正式数学语言最初创建的初衷相契合：为容易产生幻觉的人类编写证明提供坚实的基础。

> Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that "the gold standard for supporting a mathematical claim is to provide a proof". We propose a retrospective, step-aware formal verification framework $Safe$. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework $Safe$ across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose $FormalStep$ as a benchmark for step correctness theorem proving with $30,809$ formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying natural language content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.

[Arxiv](https://arxiv.org/abs/2506.04592)