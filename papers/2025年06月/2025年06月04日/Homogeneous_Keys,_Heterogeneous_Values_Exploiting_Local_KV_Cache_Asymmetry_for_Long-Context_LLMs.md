# 同构键与异构值：利用局部键值缓存不对称性提升长上下文LLMs性能

发布时间：2025年06月04日

`LLM理论` `模型优化`

> Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs

# 摘要

> 大语言模型（LLMs）的最新进展凸显了延长上下文长度的关键重要性。然而，注意力机制的二次复杂度为高效长上下文建模带来了重大挑战。KV缓存压缩作为应对这一挑战的关键方法，通过广泛的实证分析，我们揭示了其一个基本却常被忽视的不对称现象：相邻键接收相似的关注权重（局部同质性），而相邻值则展现出截然不同的异质性分布。这种键值不对称性揭示了现有压缩方法在将键和值同等对待时的局限性。为了解决这一限制，我们提出了一种无训练压缩框架（AsymKV），该框架结合了基于同质性的键合并和经过数学证明的无损值压缩。大量实验表明，AsymKV在各类任务和基础模型上均优于现有的长上下文方法。例如，在LLaMA3.1-8B模型上，AsymKV在LongBench上达到了平均43.95分，大幅领先于H$_2$O（38.89）等SOTA方法。

> Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights (local homogeneity), adjacent values demonstrate distinct heterogeneous distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.

[Arxiv](https://arxiv.org/abs/2506.05410)