# 评估大型语言模型及其相关系统的实用指南

发布时间：2025年06月15日

`LLM应用` `评估框架` `应用部署`

> A Practical Guide for Evaluating LLMs and LLM-Reliant Systems

# 摘要

> 生成式AI的迅猛发展引发了人们对基于大型语言模型（LLMs）系统在实际应用中潜力的浓厚兴趣。然而，这些系统在实际场景中的有效评估面临一系列独特挑战，现有文献中常用的合成基准和事实标准往往难以应对。我们提出了一套实用的评估框架，旨在指导如何主动构建具有代表性的数据集、选择有效的评估指标，并采用与实际开发和部署紧密结合的评估方法，确保这些依赖LLMs的系统能够满足现实需求并实现用户价值。

> Recent advances in generative AI have led to remarkable interest in using systems that rely on large language models (LLMs) for practical applications. However, meaningful evaluation of these systems in real-world scenarios comes with a distinct set of challenges, which are not well-addressed by synthetic benchmarks and de-facto metrics that are often seen in the literature. We present a practical evaluation framework which outlines how to proactively curate representative datasets, select meaningful evaluation metrics, and employ meaningful evaluation methodologies that integrate well with practical development and deployment of LLM-reliant systems that must adhere to real-world requirements and meet user-facing needs.

[Arxiv](https://arxiv.org/abs/2506.13023)