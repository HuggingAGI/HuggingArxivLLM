# 生成模型持续学习的全面综述

发布时间：2025年06月15日

`LLM理论` `生成模型` `持续学习`

> A Comprehensive Survey on Continual Learning in Generative Models

# 摘要

> 生成模型的快速发展让现代AI系统能够理解和生成高度复杂的文本内容，甚至在某些领域达到了人类水平的表现。然而，这些模型仍然从根本上受到灾难性遗忘的限制——这是一个持续存在的挑战，即适应新任务通常会导致之前学习任务性能的显著下降。为了应对这一实际限制，提出了许多方法来提高生成模型在现实世界应用中的适应性和扩展性。在这项研究中，我们对主流生成模型的持续学习方法进行了全面调查，包括大型语言模型、多模态大型语言模型、视觉语言动作模型和扩散模型。借鉴人类大脑的记忆机制，我们系统地将这些方法分类为基于架构、基于正则化和基于重放的三种范式，同时阐明了它们的基本方法和动机。我们还分析了不同生成模型的持续学习设置，包括训练目标、基准测试和核心框架，为该领域提供了更深入的见解。本文的项目页面可访问 https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models 获取。


> The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models remain fundamentally constrained by catastrophic forgetting - a persistent challenge where adapting to new tasks typically leads to significant degradation in performance on previously learned tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative models in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative models, including large language models, multimodal large language models, vision language action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, offering deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.

[Arxiv](https://arxiv.org/abs/2506.13045)