# 在大型语言模型中，辨别何为重要：多维度评估其道德能力

发布时间：2025年06月15日

`LLM应用` `道德评估` `AI伦理`

> Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs

# 摘要

> 道德能力是指根据道德原则行事的能力。随着大型语言模型（LLMs）越来越多地被部署到需要道德能力的场景中，人们对这种能力的实证评估越来越感兴趣。我们回顾了现有文献，识别出三个主要的不足之处：(i) 过度依赖预设的道德场景，其中道德特征被明确突出；(ii) 侧重于预测判决结果，而非道德推理过程；(iii) 对模型（无法）识别何时需要额外信息的能力测试不足。基于哲学研究中关于道德技能的探讨，我们提出了一种评估LLMs道德能力的新方法。我们的方法超越了简单的判决比较，从五个维度评估道德能力：识别道德相关的特征、评估其重要性、将道德原因归因于这些特征、综合连贯的道德判断，以及识别信息缺口。我们进行了两项实验，将六种领先的LLMs与非专家人类和专业哲学家进行比较。在第一项实验中，使用现有工作中标准的伦理小故事，LLMs在多个道德推理维度上普遍优于非专家人类。然而，第二项实验中，我们设计了新的场景，通过在无关细节中嵌入相关特征来测试道德敏感性，结果出现了显著的逆转：几种LLMs的表现明显劣于人类。我们的研究结果表明，当前的评估可能通过消除从噪声信息中辨别道德相关性的任务，从而大大高估了LLMs的道德推理能力，我们认为这是真正道德技能的前提条件。这项工作为评估AI的道德能力提供了一个更细致的框架，并强调了提高先进AI系统道德能力的重要方向。

> Moral competence is the ability to act in accordance with moral principles. As large language models (LLMs) are increasingly deployed in situations demanding moral competence, there is increasing interest in evaluating this ability empirically. We review existing literature and identify three significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with explicitly highlighted moral features; (ii) Focus on verdict prediction rather than moral reasoning; and (iii) Inadequate testing of models' (in)ability to recognize when additional information is needed. Grounded in philosophical research on moral skill, we then introduce a novel method for assessing moral competence in LLMs. Our approach moves beyond simple verdict comparisons to evaluate five dimensions of moral competence: identifying morally relevant features, weighting their importance, assigning moral reasons to these features, synthesizing coherent moral judgments, and recognizing information gaps. We conduct two experiments comparing six leading LLMs against non-expert humans and professional philosophers. In our first experiment using ethical vignettes standard to existing work, LLMs generally outperformed non-expert humans across multiple dimensions of moral reasoning. However, our second experiment, featuring novel scenarios designed to test moral sensitivity by embedding relevant features among irrelevant details, revealed a striking reversal: several LLMs performed significantly worse than humans. Our findings suggest that current evaluations may substantially overestimate LLMs' moral reasoning capabilities by eliminating the task of discerning moral relevance from noisy information, which we take to be a prerequisite for genuine moral skill. This work provides a more nuanced framework for assessing AI moral competence and highlights important directions for improving moral competence in advanced AI systems.

[Arxiv](https://arxiv.org/abs/2506.13082)