# <翻译失败>

发布时间：2025年06月03日

`LLM理论` `人工智能` `持续学习`

> The Future of Continual Learning in the Era of Foundation Models: Three Key Directions

# 摘要

> 持续学习——即随着时间推移获取、保留和精炼知识的能力——是人类与人工智能智能的核心。历史上，不同AI范式都认识到这一需求：早期专家系统和生产系统专注知识的逐步巩固，强化学习则强调动态适应。深度学习兴起后，深度持续学习致力于通过学习稳健且可重用的表征解决日益复杂的任务序列。然而，大型语言模型（LLMs）和基础模型的出现引发一个问题：在中心化、单体模型能够借助互联网规模知识处理多样化任务的情况下，我们是否仍需持续学习？

我们认为持续学习依然至关重要，原因有三：首先，持续预训练确保基础模型保持最新，缓解知识老化和分布偏移，同时整合新信息；其次，持续微调使模型能够专业化和个性化，适应特定领域的任务、用户偏好和现实约束，无需完全重新训练，避免了计算成本高昂的长上下文窗口；最后，持续组合性提供了一种可扩展且模块化的方法构建智能，使基础模型和智能体的编排能够动态组合、重组和适应。

尽管持续预训练和微调被视为小众研究方向，我们认为正是持续组合性将标志着持续学习的重生。未来人工智能将不再由单一静态模型定义，而是由一个不断演变和交互的模型生态系统定义，使持续学习比以往任何时候都更加重要。


> Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.

[Arxiv](https://arxiv.org/abs/2506.03320)