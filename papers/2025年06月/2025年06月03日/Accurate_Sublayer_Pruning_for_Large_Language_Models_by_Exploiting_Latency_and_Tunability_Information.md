# # 准确的大型语言模型子层剪枝：利用延迟和可调性信息优化模型结构

发布时间：2025年06月03日

`LLM理论` `模型优化`

> Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information

# 摘要

> 如何在不牺牲准确性的情况下加速大型语言模型（LLMs）？LLMs 的推理速度缓慢限制了我们在多种应用中充分挖掘其卓越性能的潜力。这一问题的主要原因在于 LLMs 中堆叠了大量子层。子层剪枝通过移除不必要的子层来实现模型压缩和加速。然而，现有的子层剪枝算法在准确性上存在局限，因为它们简单地选择要剪枝的子层，忽视了每个子层的独特特性。在本文中，我们提出了 SPRINT（基于延迟和可调性信息的子层剪枝方法），一种针对 LLMs 的高精度子层剪枝方法。SPRINT 通过综合考虑 1) 剪枝后延迟减少的量和 2) 子层的可调性，精准选择要剪枝的目标子层。SPRINT 通过迭代剪枝冗余子层，并迅速调整剩余子层的参数，实现高效的模型优化。实验结果表明，SPRINT 在准确率与加速比之间实现了最佳平衡，与现有的剪枝算法相比，在零样本常识推理基准测试中准确率提高了高达 23.88%。

> How can we accelerate large language models(LLMs) without sacrificing accuracy? The slow inference speed of LLMs hinders us to benefit from their remarkable performance in diverse applications. This is mainly because numerous sublayers are stacked together in LLMs. Sublayer pruning compresses and expedites LLMs via removing unnecessary sublayers. However, existing sublayer pruning algorithms are limited in accuracy since they naively select sublayers to prune, overlooking the different characteristics of each sublayer. In this paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability Information), an accurate sublayer pruning method for LLMs. SPRINT accurately selects a target sublayer to prune by considering 1) the amount of latency reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively prunes redundant sublayers and swiftly tunes the parameters of remaining sublayers. Experiments show that SPRINT achieves the best accuracy-speedup trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense reasoning benchmarks compared to existing pruning algorithms.

[Arxiv](https://arxiv.org/abs/2506.03510)