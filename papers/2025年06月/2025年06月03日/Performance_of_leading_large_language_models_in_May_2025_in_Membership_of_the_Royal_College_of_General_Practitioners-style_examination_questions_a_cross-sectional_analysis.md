# 2025年5月领先大型语言模型在英国皇家全科医师学院风格模拟考试中的表现横断面分析

发布时间：2025年06月03日

`LLM应用`

> Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis

# 摘要

> **背景**  
大型语言模型（LLMs）在支持临床实践方面展现出了巨大的潜力。除了Chat GPT4及其前身外，很少有LLMs，特别是那些更强大、更擅长推理的模型类别，接受过医学专科考试问题的测试，包括在全科医疗领域。本文旨在测试截至2025年5月领先LLMs（o3、Claude Opus 4、Grok3 和 Gemini 2.5 Pro）在全科医疗教育中的能力，特别是在回答英国皇家全科医学院会员（MRCGP）风格的考试问题方面。

**方法**  
o3、Claude Opus 4、Grok3 和 Gemini 2.5 Pro 被要求在2025年5月25日回答从英国皇家全科医学院GP SelfTest中随机选取的100道多选题。问题包含文本信息、实验室结果和临床图像。每个模型被提示以英国全科医生的身份作答，并提供完整的问题信息。每个模型对每道问题各作答一次。回答根据GP SelfTest提供的正确答案进行评分。

**结果**  
o3、Claude Opus 4、Grok3 和 Gemini 2.5 Pro 的总分分别为99.0%、95.0%、95.0%和95.0%。同一问题的平均同行得分为73.0%。

**讨论**  
所有模型表现优异，均远超回答相同问题的全科医生和全科医生注册医师的平均水平。o3表现最佳，而其他领先模型的表现彼此相当，且与o3相比并无明显差距。这些发现进一步支持了LLMs，特别是推理模型，在支持全科医疗服务中的应用，尤其是那些专门针对全科医疗临床数据进行训练的模型。


> Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data.

[Arxiv](https://arxiv.org/abs/2506.02987)