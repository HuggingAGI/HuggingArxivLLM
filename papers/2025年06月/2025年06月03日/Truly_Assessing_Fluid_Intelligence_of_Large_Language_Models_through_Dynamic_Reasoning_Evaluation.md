# 通过动态推理评估真实评估大型语言模型的流体智能

发布时间：2025年06月03日

`LLM理论` `人工智能` `认知科学`

> Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation

# 摘要

> 近期，大型语言模型（LLMs）在推理能力方面取得了令人瞩目的进展，展现出与人类思维相仿的推理能力。然而，关于LLMs是否具备真正的流体智力（即在新颖情境下进行抽象推理和规则归纳的能力）仍是一个未解之谜。现有的推理基准测试要么专注于特定领域知识（即晶体智力），要么缺乏可解释性。为了解决这些问题，我们提出了DRE-Bench，这是一个基于分层认知框架的动态推理评估基准。DRE-Bench包含36个抽象推理任务，分布在四个认知水平上，每个任务都设计了多个动态变体，用于测试相同的潜在规则。这种设计使得对流体智力的评估更加细致、可解释且可靠。我们对一系列先进的LLMs进行了评估，包括通用型LLMs（如GPT-4o、Claude 3.7）和专门用于推理的LLMs（如o1、DeepSeek-R1、QwQ、Skywork-OR1）。实验结果表明，尽管大多数LLMs在低级认知任务中表现得相当出色且稳健，但面对高级认知任务时却显得力不从心，且随着任务复杂度的增加，其概括能力也显得相当有限。我们的研究发现凸显了当前LLMs与真正人类流体智力之间的差距，并为系统性追踪LLMs推理能力的提升提供了一条新的路径。

> Recent advances in large language models (LLMs) have demonstrated impressive reasoning capacities that mirror human-like thinking. However, whether LLMs possess genuine fluid intelligence (i.e., the ability to reason abstractly and generalize rules in novel situations) remains an open question. Existing reasoning benchmarks either focus on domain-specific knowledge (crystallized intelligence) or lack interpretability. To address these limitations, we propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning tasks organized across four cognitive levels, with each task featuring multiple dynamic variants that test the same underlying latent rule. This design enables fine-grained, interpretable, and reliable assessments of fluid intelligence. We evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o, Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1). Experimental results reveal that although most LLMs achieve competent and robust performance in low-level cognition, they struggle with high-level cognition and exhibit limited generalization as task complexity grows. Our findings highlight the gap between current LLMs and true human-like fluid intelligence and offer a new path for systematically tracking reasoning progress in LLMs.

[Arxiv](https://arxiv.org/abs/2506.02648)