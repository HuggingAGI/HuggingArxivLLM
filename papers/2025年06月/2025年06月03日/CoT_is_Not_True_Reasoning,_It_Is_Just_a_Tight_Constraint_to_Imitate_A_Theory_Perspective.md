# CoT 不是推理，只是模仿的严格约束：从理论视角看

发布时间：2025年06月03日

`LLM理论` `人工智能` `推理系统`

> CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective

# 摘要

> 思维链（CoT）提示显著提升了大型语言模型在多步推理任务中的表现，这引发了人们关于模型推理能力的广泛讨论。然而，本文提出了一个理论反观点：思维链并未激发真正的抽象推理。实际上，它是一种强大的结构约束，引导模型模仿推理的形式。通过强制生成中间步骤，思维链利用了模型在序列预测和模式匹配方面的强大能力，使其输出更贴近连贯的思维过程。

> Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes. Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes.

[Arxiv](https://arxiv.org/abs/2506.02878)