# EssayBench：多体裁中文写作中的大型语言模型评估

发布时间：2025年06月03日

`LLM应用` `教育科技`

> EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing

# 摘要

> 中文作文写作与评价在教育领域具有重要意义，但大型语言模型（LLMs）在这一领域的潜力尚未得到充分挖掘。现有评估基准多依赖粒度较粗的文本质量指标，忽视了中文作文在结构和修辞上的复杂性，尤其在不同文体间存在显著差异。为此，我们提出了enchName，这是一个专为中文作文写作设计的多文体基准，涵盖四大主要文体：议论文、记叙文、说明文和描写文。我们精选并优化了728个真实世界的话题，确保其真实性和多样性，并将它们细致地分类为	extit{开放性}和	extit{限制性}两类，以涵盖多种写作场景。为实现可靠评估，我们开发了一个细致入微、针对不同文体的评分框架，能够分层次地聚合评分。通过全面的人类一致性研究，我们验证了评估方案的可靠性。最后，我们对15个大型LLMs进行了基准测试，深入分析它们在不同文体和指令类型下的表现。借助enchName，我们不仅推动了基于LLMs的中文作文评价研究，更为未来在教育环境中改进作文生成技术提供了重要参考。

> Chinese essay writing and its evaluation are critical in educational contexts, yet the capabilities of Large Language Models (LLMs) in this domain remain largely underexplored. Existing benchmarks often rely on coarse-grained text quality metrics, largely overlooking the structural and rhetorical complexities of Chinese essays, particularly across diverse genres. To address this gap, we propose \benchName, a multi-genre benchmark specifically designed for Chinese essay writing across four major genres: Argumentative, Narrative, Descriptive, and Expository. We curate and refine a total of 728 real-world prompts to ensure authenticity and meticulously categorize them into the \textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing scenarios. To reliably evaluate generated essays, we develop a fine-grained, genre-specific scoring framework that hierarchically aggregates scores. We further validate our evaluation protocol through a comprehensive human agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their strengths and limitations across genres and instruction types. With \benchName, we aim to advance LLM-based Chinese essay evaluation and inspire future research on improving essay generation in educational settings.

[Arxiv](https://arxiv.org/abs/2506.02596)