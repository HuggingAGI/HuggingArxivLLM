# 解释与安全的结合：提升LLM安全的解释方法与工具综述

发布时间：2025年06月05日

`LLM理论` `模型安全` `模型解释性`

> Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety

# 摘要

> 随着大型语言模型（LLMs）在实际应用中的普及，深入理解并有效缓解其潜在风险行为变得日益重要。解释性技术能够揭示模型产生不安全输出的根本原因，并为提升模型安全性提供指导，然而这些技术与模型安全性的关联在以往研究中往往被忽视。本综述首次填补了这一研究空白，提出了一套全新的统一框架，将专注于安全性的解释方法、这些方法所指导的安全性改进，以及实现这些改进的工具有机结合起来。我们的分类体系按照LLM的工作流程阶段进行组织，系统性地总结了近70项相关研究的交叉成果。最后，我们探讨了当前面临的开放挑战和未来研究方向。这篇综述为研究人员和从业者提供了重要参考，助其把握关键进展，推动实现更安全、更可解释的LLMs。

> As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs.

[Arxiv](https://arxiv.org/abs/2506.05451)