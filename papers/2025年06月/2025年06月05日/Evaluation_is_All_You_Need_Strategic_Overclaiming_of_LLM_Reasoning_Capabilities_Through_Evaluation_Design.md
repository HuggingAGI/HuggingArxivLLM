# 评估即所需：通过评估设计对LLM推理能力进行战略性夸大

发布时间：2025年06月05日

`其他` `AI学术应用` `模型评估`

> Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design

# 摘要

> Deepseek-R1-Distill 系列推理模型凭借在数学、科学、编程等领域的卓越表现，受到开源社区的广泛青睐。然而，我们的研究表明，这些模型的基准测试结果会因多种因素产生显著波动。评估条件的细微变化可能导致结果的巨大差异。类似的现象也出现在其他基于 Deepseek-R1-Distill 系列微调的开源推理模型，以及 QwQ-32B 模型中，使得它们声称的性能提升难以可靠复现。因此，我们主张建立更加严谨的模型性能评估范式，并对 Deepseek-R1-Distill 系列模型进行了实证评估。

> Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.

[Arxiv](https://arxiv.org/abs/2506.04734)