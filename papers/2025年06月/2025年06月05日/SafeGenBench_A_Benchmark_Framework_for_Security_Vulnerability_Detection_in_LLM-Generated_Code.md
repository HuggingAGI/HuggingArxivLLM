# # SafeGenBench：用于检测大型语言模型生成代码中的安全漏洞的基准框架

发布时间：2025年06月05日

`LLM应用` `软件开发` `安全性`

> SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code

# 摘要

> 大型语言模型（LLMs）的代码生成能力已成为评估其整体性能的关键维度。然而，先前的研究大多忽视了生成代码中固有的安全风险。在本研究中，我们引入了enchmark，这是一个专门设计用于评估LLM生成代码安全性的基准测试。该数据集涵盖了广泛的常见软件开发场景和漏洞类型。基于此基准，我们开发了一个自动评估框架，结合静态应用安全测试（SAST）和LLM判断，以评估模型生成代码中是否存在安全漏洞。通过对当前最先进的LLMs在enchmark上的实证评估，我们揭示了它们在生成无漏洞代码方面存在的显著不足。我们的研究结果突显了亟待解决的挑战，并为未来提升LLMs在安全代码生成方面的性能提供了可操作的见解。数据和代码即将发布。

> The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce \benchmark, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on \benchmark, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.

[Arxiv](https://arxiv.org/abs/2506.05692)