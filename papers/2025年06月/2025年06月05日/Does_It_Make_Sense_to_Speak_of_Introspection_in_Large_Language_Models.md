# 大型语言模型中是否存在内省的可能性？

发布时间：2025年06月05日

`LLM理论` `人工智能`

> Does It Make Sense to Speak of Introspection in Large Language Models?

# 摘要

> 大型语言模型（LLMs）展现出引人注目的语言行为，有时会做出自我陈述，即关于自身本质、内部机制或行为的描述。在人类中，这类陈述通常被认为是内省能力的表现，并与意识紧密相关。那么，随着LLMs语言流畅度和认知能力的不断提升，我们应如何解读它们生成的自我报告呢？内省的概念在多大程度上（如果有的话）可以被有意义地应用到LLMs身上？本文将呈现并分析两个看似内省的LLM自我报告案例。在第一个案例中，一个LLM试图解释其“创造性”写作背后的生成过程，但我们认为这并非一个有效的内省实例。在第二个案例中，一个LLM成功推断出了自身温度参数的值，我们认为这可以被合理地视为内省的一个极简示例，尽管它（假设）并不伴随意识体验。

> Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, we present and critique two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own ``creative'' writing, and we argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and we argue that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.

[Arxiv](https://arxiv.org/abs/2506.05068)