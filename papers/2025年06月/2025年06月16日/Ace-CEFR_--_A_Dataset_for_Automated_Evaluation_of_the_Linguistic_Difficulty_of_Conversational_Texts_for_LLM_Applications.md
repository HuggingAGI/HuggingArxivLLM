# # Ace-CEFR -- 用于大语言模型应用的对话文本语言难度自动评估数据集

发布时间：2025年06月16日

`LLM应用

理由：这篇论文专注于评估对话文本的语言难度，并通过创建数据集和实验模型来提升大型语言模型的应用能力，属于应用层面的研究。` `人工智能`

> Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications

# 摘要

> 目前迫切需要一种方法来评估短篇对话文本的语言难度，尤其是用于大型语言模型 (LLMs) 的训练和筛选。我们推出了 Ace-CEFR 数据集，其中包含由英语对话文本段落，并由专家标注了对应的语言难度级别。我们在 Ace-CEFR 数据集上实验了多种模型，包括基于 Transformer 的模型和 LLMs。结果显示，经过 Ace-CEFR 数据集训练的模型能够更准确地衡量文本难度，且延迟也适合生产环境。最后，我们向公众发布了 Ace-CEFR 数据集，以促进相关研究和开发工作。

> There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

[Arxiv](https://arxiv.org/abs/2506.14046)