# 多模态 "木偶师"：基于大语言模型的语音与手势交互在增强现实环境中的机器人远程操作研究

发布时间：2025年06月16日

`LLM应用

理由：这篇论文探讨了大型语言模型在多模态增强现实人机交互中的应用，展示了其在实际操作中的潜力和效果。研究重点在于通过LLM驱动的语音和手势交互提升人机协作，属于LLM的实际应用范畴。` `机器人` `增强现实`

> Multimodal "Puppeteer": An Exploration of Robot Teleoperation Via Virtual Counterpart with LLM-Driven Voice and Gesture Interaction in Augmented Reality

# 摘要

> 机器人与增强现实（AR）的结合正在推动人机交互（HRI）领域的变革，为提升交互的直观性、易用性和协作性提供了全新可能。本文提出并评估了一种创新性的多模态AR机器人木偶框架，该框架通过大型语言模型（LLM）驱动的语音指令和手势交互实现直观的远程操作。借助Meta Quest 3设备，用户能够在AR环境中实时与虚拟机器人互动，从而“操纵”其物理对应体。我们开展了一项包含42名参与者的单组实验，分别在仅手势交互和语音-手势结合交互条件下完成机器人立方体抓取与放置任务。研究通过客观性能指标和主观用户体验（UX）评估了系统表现，并对机器人专家与非专家进行了对比分析。研究结果揭示了多模态输入对基于AR的人机交互任务效率、可用性和用户满意度的影响。这些发现为设计更有效的增强现实人机交互系统提供了重要指导。


> The integration of robotics and augmented reality (AR) holds transformative potential for advancing human-robot interaction (HRI), offering enhancements in usability, intuitiveness, accessibility, and collaborative task performance. This paper introduces and evaluates a novel multimodal AR-based robot puppeteer framework that enables intuitive teleoperation via virtual counterpart through large language model (LLM)-driven voice commands and hand gesture interactions. Utilizing the Meta Quest 3, users interact with a virtual counterpart robot in real-time, effectively "puppeteering" its physical counterpart within an AR environment. We conducted a within-subject user study with 42 participants performing robotic cube pick-and-place with pattern matching tasks under two conditions: gesture-only interaction and combined voice-and-gesture interaction. Both objective performance metrics and subjective user experience (UX) measures were assessed, including an extended comparative analysis between roboticists and non-roboticists. The results provide key insights into how multimodal input influences contextual task efficiency, usability, and user satisfaction in AR-based HRI. Our findings offer practical design implications for designing effective AR-enhanced HRI systems.

[Arxiv](https://arxiv.org/abs/2506.13189)