# 大型语言模型：虚假评论的“隐形劝说者”——人类与机器都难辨真伪

发布时间：2025年06月16日

`LLM应用` `电子商务` `消费者行为`

> Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines

# 摘要

> 阅读和评估产品评论是大多数人决定在线购买和消费的核心。然而，大型语言模型和生成式人工智能的最新出现，意味着撰写欺诈性或虚假评论可能比以往任何时候都更容易。通过三项研究，我们证明了：

（1）人类已不再能够区分机器生成的真实和虚假产品评论，整体准确率仅为50.8%——与随机猜测无异；

（2）大型语言模型同样无法区分虚假和真实评论，表现与人类相当甚至更差；

（3）人类和大型语言模型在评估真实性时采用不同的策略，导致准确率相当糟糕，但精确率、召回率和F1分数不同——表明它们在不同方面的判断能力更差。

这些结果表明，如果不依赖可信的购买验证来保证评论者的身份，各地的评论系统都可能受到机械化欺诈的影响。此外，这些结果揭示了消费者判断真实性的心理机制，证明了对积极评论的“怀疑偏见”以及对虚假负面评论的特殊误判漏洞。此外，这些结果还首次揭示了判断虚假评论的“机器心理”，表明大型语言模型在评估真实性时采取的策略与人类截然不同，尽管在准确性上同样错误，但在误判方式上却有所不同。


> Reading and evaluating product reviews is central to how most people decide what to buy and consume online. However, the recent emergence of Large Language Models and Generative Artificial Intelligence now means writing fraudulent or fake reviews is potentially easier than ever. Through three studies we demonstrate that (1) humans are no longer able to distinguish between real and fake product reviews generated by machines, averaging only 50.8% accuracy overall - essentially the same that would be expected by chance alone; (2) that LLMs are likewise unable to distinguish between fake and real reviews and perform equivalently bad or even worse than humans; and (3) that humans and LLMs pursue different strategies for evaluating authenticity which lead to equivalently bad accuracy, but different precision, recall and F1 scores - indicating they perform worse at different aspects of judgment. The results reveal that review systems everywhere are now susceptible to mechanised fraud if they do not depend on trustworthy purchase verification to guarantee the authenticity of reviewers. Furthermore, the results provide insight into the consumer psychology of how humans judge authenticity, demonstrating there is an inherent 'scepticism bias' towards positive reviews and a special vulnerability to misjudge the authenticity of fake negative reviews. Additionally, results provide a first insight into the 'machine psychology' of judging fake reviews, revealing that the strategies LLMs take to evaluate authenticity radically differ from humans, in ways that are equally wrong in terms of accuracy, but different in their misjudgments.

[Arxiv](https://arxiv.org/abs/2506.13313)