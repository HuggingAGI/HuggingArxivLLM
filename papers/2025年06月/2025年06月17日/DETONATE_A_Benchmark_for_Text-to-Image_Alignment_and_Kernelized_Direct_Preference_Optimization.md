# DETONATE：针对文本到图像对齐与核化直接偏好优化的基准测试

发布时间：2025年06月17日

`LLM应用` `生成式AI` `图像生成`

> DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization

# 摘要

> 对齐在文本到图像模型中的重要性不容忽视，它确保生成的图像不仅准确传达用户意图，还能在安全性和公平性上达到高标准。Direct Preference Optimization (DPO) 在大型语言模型领域表现卓越，如今其影响力正延伸至T2I系统。本文提出了一种名为DPO-Kernels的创新方法，通过三个维度全面提升模型的对齐能力：(i) 混合损失，将基于嵌入的目标与传统概率损失相结合，优化效果更上一层楼；(ii) 核化表示，借助径向基函数（RBF）、多项式和小波核，实现更丰富的特征转换，同时更精准地区分安全与不安全输入；(iii) 散度选择，突破DPO默认的Kullback-Leibler散度限制，引入Wasserstein和R'enyi散度，显著提升了模型的稳定性和鲁棒性。

为验证这一方法，我们推出了DETONATE——首个大规模同类基准测试，包含约10万对精选图像，分为“入选”和“淘汰”两类。DETONATE深入捕捉了社会偏见和歧视的三大维度：种族、性别和残疾。我们从仇恨言论数据集中提取提示语，并通过Stable Diffusion 3.5 Large、Stable Diffusion XL和Midjourney等主流T2I模型生成图像。此外，我们还引入了对齐质量指数（AQI），这一创新的几何度量工具能够量化安全与不安全图像激活在潜在空间中的可分性，揭示模型潜在的漏洞。

实证研究证实，DPO-Kernels通过Heavy-Tailed Self-Regularization（HT-SR）机制，成功实现了强大的泛化能力。DETONATE基准测试和完整代码现已公开发布，为研究者和开发者提供了宝贵的资源和工具。

> Alignment is crucial for text-to-image (T2I) models to ensure that generated images faithfully capture user intent while maintaining safety and fairness. Direct Preference Optimization (DPO), prominent in large language models (LLMs), is extending its influence to T2I systems. This paper introduces DPO-Kernels for T2I models, a novel extension enhancing alignment across three dimensions: (i) Hybrid Loss, integrating embedding-based objectives with traditional probability-based loss for improved optimization; (ii) Kernelized Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet kernels for richer feature transformations and better separation between safe and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and R'enyi divergences for enhanced stability and robustness. We introduce DETONATE, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs categorized as chosen and rejected. DETONATE encapsulates three axes of social bias and discrimination: Race, Gender, and Disability. Prompts are sourced from hate speech datasets, with images generated by leading T2I models including Stable Diffusion 3.5 Large, Stable Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality Index (AQI), a novel geometric measure quantifying latent-space separability of safe/unsafe image activations, revealing hidden vulnerabilities. Empirically, we demonstrate that DPO-Kernels maintain strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are publicly released.

[Arxiv](https://arxiv.org/abs/2506.14903)