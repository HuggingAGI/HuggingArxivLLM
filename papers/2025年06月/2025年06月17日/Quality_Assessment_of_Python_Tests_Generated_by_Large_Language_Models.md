# 大型语言模型生成的Python测试质量评估

发布时间：2025年06月17日

`LLM应用` `软件工程` `软件质量保障`

> Quality Assessment of Python Tests Generated by Large Language Models

# 摘要

> # 摘要  
手动生成测试脚本不仅耗时、昂贵，还容易出错，凸显了自动化解决方案的价值。大型语言模型（LLMs）凭借其丰富知识，在这一领域展现出了巨大潜力，能够更高效地生成测试代码。本研究探讨了GPT-4o、Amazon Q和LLama 3.3这三种LLM生成的Python测试代码质量。  
我们评估了在两种不同提示上下文中生成的测试套件的结构可靠性：Text2Code（T2C）和Code2Code（C2C）。分析涵盖了错误和测试异味的识别，并着重研究了这些问题与不充分设计模式之间的关联。  
研究发现，大多数LLM生成的测试套件至少包含一个错误或测试异味。断言错误最为常见，占所有识别错误的64%，而测试异味“缺乏测试用例内聚性”则是最常见的（41%）。提示上下文对测试质量有显著影响；文本提示中详细指令通常生成错误较少但测试异味较多的测试。  
在评估的LLM中，GPT-4o在两种上下文中产生的错误最少（C2C为10%，T2C为6%），而Amazon Q的错误率最高（C2C为19%，T2C为28%）。对于测试异味，Amazon Q在C2C上下文中的检测率最低（9%），而LLama 3.3在T2C上下文中表现最佳（10%）。此外，我们观察到特定错误（如断言或缩进问题）与测试用例内聚性异味之间存在强烈关联。  
这些发现为提升LLM生成测试的质量提供了改进机会，并强调了未来研究探索优化生成场景和更好提示工程策略的必要性。

> The manual generation of test scripts is a time-intensive, costly, and error-prone process, indicating the value of automated solutions. Large Language Models (LLMs) have shown great promise in this domain, leveraging their extensive knowledge to produce test code more efficiently. This study investigates the quality of Python test code generated by three LLMs: GPT-4o, Amazon Q, and LLama 3.3. We evaluate the structural reliability of test suites generated under two distinct prompt contexts: Text2Code (T2C) and Code2Code (C2C). Our analysis includes the identification of errors and test smells, with a focus on correlating these issues to inadequate design patterns. Our findings reveal that most test suites generated by the LLMs contained at least one error or test smell. Assertion errors were the most common, comprising 64% of all identified errors, while the test smell Lack of Cohesion of Test Cases was the most frequently detected (41%). Prompt context significantly influenced test quality; textual prompts with detailed instructions often yielded tests with fewer errors but a higher incidence of test smells. Among the evaluated LLMs, GPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C), whereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For test smells, Amazon Q had fewer detections in the C2C context (9%), while LLama 3.3 performed best in the T2C context (10%). Additionally, we observed a strong relationship between specific errors, such as assertion or indentation issues, and test case cohesion smells. These findings demonstrate opportunities for improving the quality of test generation by LLMs and highlight the need for future research to explore optimized generation scenarios and better prompt engineering strategies.

[Arxiv](https://arxiv.org/abs/2506.14297)