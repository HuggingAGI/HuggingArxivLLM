# AgentDistill：无训练智能体蒸馏，采用可泛化的MCP框

发布时间：2025年06月17日

`Agent` `智能体` `机器人`

> AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes

# 摘要

> 尽管知识蒸馏已发展为将大型语言模型（LLMs）压缩为更小模型的成熟技术，但涉及规划、记忆和工具使用的LLM智能体蒸馏仍相对研究较少。现有方法通常通过重放教师轨迹或模仿教师工具使用来训练学生智能体，但难以使其在新环境中进行动态规划和行动。我们提出AgentDistill，这是一种无需训练的新型智能体蒸馏框架，通过直接复用由教师智能体生成的结构化可重用模块（MCPs），实现高效且可扩展的知识迁移。这些蒸馏后的MCPs使学生智能体能够跨领域泛化能力，并在极少干预下解决新问题。实验表明，基于小型语言模型构建的蒸馏学生智能体可达到使用大型LLMs（如OctoTools（GPT-4o））的先进系统相当的性能，凸显了我们框架在构建智能体方面的有效性。
    

> While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

[Arxiv](https://arxiv.org/abs/2506.14728)