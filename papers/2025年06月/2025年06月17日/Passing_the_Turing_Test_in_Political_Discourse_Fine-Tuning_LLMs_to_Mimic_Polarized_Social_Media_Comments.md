# 成功通过政治话语图灵测试：微调LLM精准模仿两极化社交媒体评论

发布时间：2025年06月17日

`LLM应用

理由：这篇论文探讨了大型语言模型在生成具有说服力且带有偏见内容的应用，特别是在政治极化方面的潜在影响。研究者们通过微调LLMs来生成情境感知且意识形态一致的回应，并评估其真实性和一致性。这些研究属于LLMs在特定应用场景中的实际应用，因此归类为LLM应用。` `政治学` `公共政策`

> Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments

# 摘要

> 随着大型语言模型（LLMs）能力的不断提升，其可能通过自动生成具有说服力且带有偏见的内容来加剧意识形态极化的潜在作用引发了越来越多的担忧。本研究旨在探讨经过微调的LLMs在在线环境中复制和放大两极化言论的能力。我们从Reddit上提取了政治色彩浓厚的讨论数据，用于微调开源LLMs，使其能够生成情境感知且意识形态一致的回应。通过语言分析、情感评分和人工标注，我们评估了模型的输出，特别关注其真实性和与原始言论修辞的一致性。研究结果显示，当LLMs基于党派数据进行训练时，能够生成极具说服力且煽动性的评论，这些评论往往与人类撰写的内容难以区分。这些发现引发了关于AI在政治话语、虚假信息和操纵活动中的伦理使用的重大问题。本文最后讨论了这些发现对AI治理、平台监管以及开发用于缓解对抗性微调风险的检测工具的更广泛影响。

> The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.

[Arxiv](https://arxiv.org/abs/2506.14645)