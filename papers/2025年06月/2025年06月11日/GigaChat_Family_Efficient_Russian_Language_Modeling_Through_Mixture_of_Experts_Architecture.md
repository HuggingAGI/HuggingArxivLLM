# GigaChat 家族：通过专家混合架构实现高效俄语建模

发布时间：2025年06月11日

`LLM应用` `NLP` `多语言模型`

> GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture

# 摘要

> 生成式大型语言模型（LLMs）已成为现代NLP研究和多语言应用中的关键工具。然而，专门针对俄语的基础模型开发却十分有限，这主要归因于其对计算资源的巨大需求。本文介绍了GigaChat系列俄语LLMs，提供多种规模版本，包括基础模型和指令微调版本。我们深入探讨了模型架构、预训练流程以及实验设计选择。此外，我们在俄语和英语基准上评估了模型性能，并与多语言模型进行了对比。论文展示了通过API、Telegram机器人和Web界面访问的最优模型的系统演示。我们还开源了三个GigaChat模型（https://huggingface.co/ai-sage），旨在拓展NLP研究机遇并支持俄语工业解决方案的开发。

> Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (https://huggingface.co/ai-sage), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language.

[Arxiv](https://arxiv.org/abs/2506.09440)