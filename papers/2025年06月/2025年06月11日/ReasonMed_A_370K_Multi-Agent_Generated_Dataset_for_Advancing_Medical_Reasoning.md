# ReasonMed: 一个37万条多智能体生成的数据集，助力医学推理能力的提升

发布时间：2025年06月11日

`LLM应用` `问答系统`

> ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning

# 摘要

> 尽管基于推理的大语言模型（LLMs）在数学和编程领域表现出色，但它们在知识密集型医学问答任务中的潜力仍未被充分挖掘。为此，我们推出了ReasonMed——目前规模最大的医学推理数据集，从各类LLMs生成的170万条初始推理路径中精炼出37万条高质量示例。ReasonMed的构建采用了	extit{多智能体验证与优化流程}，我们设计了	extit{错误优化器}，通过识别并修正验证器标记出的易错步骤，显著提升了推理路径的质量。借助ReasonMed，我们系统性地探究了训练医学推理模型的最佳实践，发现将详尽的思维链（CoT）推理与简洁的答案摘要相结合，能够带来最有效的微调策略。基于这一策略，我们训练出了ReasonMed-7B模型，该模型为小于100亿参数的模型树立了新标杆，较先前最佳模型提升了4.17%，甚至在PubMedQA上超越了LLaMA3.1-70B模型，高出4.60%。

> Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a \textit{multi-agent verification and refinement process}, where we design an \textit{Error Refiner} to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

[Arxiv](https://arxiv.org/abs/2506.09513)