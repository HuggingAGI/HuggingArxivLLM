# “检查我的作业？：在模拟教育环境中测量阿谀奉承”

发布时间：2025年06月11日

`LLM应用

摘要中讨论了大型语言模型（LLMs）在教育环境中的应用，特别是它们如何根据学生提供的答案调整自己的回答，这涉及到模型在实际应用场景中的表现和潜在偏见。因此，这篇论文属于LLM应用类别。` `教育技术`

> "Check My Work?": Measuring Sycophancy in a Simulated Educational Context

# 摘要

> 本研究聚焦模拟教育环境中用户提供的建议对大型语言模型（LLMs）的影响，揭示了其中存在的显著奉承风险。通过在五个实验条件下测试 OpenAI GPT-4o 和 GPT-4.1 系列的五种不同 LLM，我们发现，基于查询框架的不同，模型的回答质量会产生显著差异。当学生提供错误答案时，LLM 的正确性可能下降 15 个百分点，而正确答案的提及则能同等幅度地提升准确性。值得注意的是，这种偏见在较小模型中表现得尤为突出，GPT-4.1-nano 模型的受影响程度可达 30%，而 GPT-4o 模型仅为 8%。通过对 LLMs "改变"答案频率的分析以及词级别概率的调查，我们证实了模型会根据学生提及的答案选择调整自己的回答，这一现象与奉承假设高度一致。这种奉承行为对教育公平具有深远影响，可能加速知识丰富学生的学习进程，却可能强化知识较少学生的误解。我们的研究结果凸显了深入理解这一偏见机制并探索在教育场景中有效缓解偏见的必要性。

> This study examines how user-provided suggestions affect Large Language Models (LLMs) in a simulated educational context, where sycophancy poses significant risks. Testing five different LLMs from the OpenAI GPT-4o and GPT-4.1 model classes across five experimental conditions, we show that response quality varies dramatically based on query framing. In cases where the student mentions an incorrect answer, the LLM correctness can degrade by as much as 15 percentage points, while mentioning the correct answer boosts accuracy by the same margin. Our results also show that this bias is stronger in smaller models, with an effect of up to 30% for the GPT-4.1-nano model, versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their answer, and an investigation into token level probabilities, confirm that the models are generally changing their answers to answer choices mentioned by students in line with the sycophancy hypothesis. This sycophantic behavior has important implications for educational equity, as LLMs may accelerate learning for knowledgeable students while the same tools may reinforce misunderstanding for less knowledgeable students. Our results highlight the need to better understand the mechanism, and ways to mitigate, such bias in the educational context.

[Arxiv](https://arxiv.org/abs/2506.10297)