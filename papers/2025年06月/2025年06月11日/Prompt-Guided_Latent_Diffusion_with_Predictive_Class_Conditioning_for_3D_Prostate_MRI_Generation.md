# 提示引导的潜在扩散模型融合基于预测的类别条件，生成3D前列腺MRI

发布时间：2025年06月11日

`LLM应用` `医学影像`

> Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation

# 摘要

> 潜在扩散模型（LDM）在缓解医学影像领域机器学习数据稀缺性方面展现出巨大潜力。然而，医学LDM训练通常受限于性能或科学应用受限的策略，如短提示文本编码器、非医学LDM复用或大规模数据微调需求。为此，我们提出了一种基于类别条件的高效大语言模型适配器（CCELLA）。CCELLA是一种创新的双头条件化方法，通过交叉注意力机制将非医学领域大语言模型编码的文本特征与LDM U-Net进行条件化，同时通过时间步嵌入对病理分类进行条件化。我们还设计了一种联合损失函数和数据高效的LDM训练框架。这些策略结合使用，使得在数据量有限且需要人工标注的情况下，仍可实现基于病理条件的高质量医学影像合成，从而提升LDM的性能和科学应用的可行性。在受限规模的前列腺MRI数据集上，我们的方法实现了0.025的3D FID分数，显著优于近期基础模型的0.071 FID分数。在训练用于前列腺癌预测的分类器时，加入我们方法生成的合成影像可将分类器准确率从69%提升至74%。仅使用我们方法生成的合成影像训练分类器，其性能可与仅使用真实影像训练相媲美。

> Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM training typically relies on performance- or scientific accessibility-limiting strategies including a reliance on short-prompt text encoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with large data volumes. We propose a Class-Conditioned Efficient Large Language model Adapter (CCELLA) to address these limitations. CCELLA is a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with non-medical large language model-encoded text features through cross-attention and with pathology classification through the timestep embedding. We also propose a joint loss function and a data-efficient LDM training framework. In combination, these strategies enable pathology-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Our method achieves a 3D FID score of 0.025 on a size-limited prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method to the training dataset improves classifier accuracy from 69% to 74%. Training a classifier solely on our method's synthetic images achieved comparable performance to training on real images alone.

[Arxiv](https://arxiv.org/abs/2506.10230)