# 提示词变异性对LLM代码生成的影响

发布时间：2025年06月11日

`LLM应用` `软件工程` `人工智能`

> Prompt Variability Effects On LLM Code Generation

# 摘要

> 代码生成是大型语言模型（LLMs）应用最活跃的领域之一。尽管LLMs降低了编写代码的门槛并加速了开发进程，但生成程序的质量取决于输入提示的质量。具体来说，生成代码的功能和质量可能对用户的背景和软件开发经验较为敏感。因此，量化LLM对输入变化的敏感性至关重要。为此，我们提出了一种用于LLMs代码生成的合成评估流水线，以及一种基于角色的系统化评估方法，以揭示LLM响应在不同用户背景下的定性差异。两种方法均独立于特定编程任务和LLMs，因此具有广泛适用性。我们通过实验结果展示了方法的实用性，并为社区分享了代码。


> Code generation is one of the most active areas of application of Large Language Models (LLMs). While LLMs lower barriers to writing code and accelerate development process, the overall quality of generated programs depends on the quality of given prompts. Specifically, functionality and quality of generated code can be sensitive to user's background and familiarity with software development. It is therefore important to quantify LLM's sensitivity to variations in the input. To this end we propose a synthetic evaluation pipeline for code generation with LLMs, as well as a systematic persona-based evaluation approach to expose qualitative differences of LLM responses dependent on prospective user background. Both proposed methods are completely independent from specific programming tasks and LLMs, and thus are widely applicable. We provide experimental evidence illustrating utility of our methods and share our code for the benefit of the community.

[Arxiv](https://arxiv.org/abs/2506.10204)