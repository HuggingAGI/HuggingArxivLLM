# 人类与LLM的礼貌策略对比：自由生成中的表现解析

发布时间：2025年06月11日

`LLM应用` `人机交互`

> Comparing human and LLM politeness strategies in free production

# 摘要

> 礼貌用语对大型语言模型（LLMs）提出了根本性的对齐挑战。人类在交流中运用丰富多样的语言策略，平衡信息传递和社会目标——从建立关系的积极策略（如赞美、表达兴趣）到减少冒犯的消极策略（如委婉、间接表达）。我们通过比较人类与LLM在受控和开放生成任务中的回应，研究LLMs是否采用了同样语境敏感的策略组合。我们发现，较大的模型（$\ge$700亿参数）成功复制了计算语用学文献中的关键偏好，令人惊讶的是，人类评估者在开放语境下更倾向于选择LLM生成的回应。然而，进一步的语言分析揭示，模型在积极语境中过度依赖消极礼貌策略，这可能导致误解。尽管现代LLMs在礼貌策略上表现出色，但这些细微差异引发了关于AI系统语用对齐的重要问题。

> Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.

[Arxiv](https://arxiv.org/abs/2506.09391)