# # 大型语言模型在判断共情沟通时的可靠性评估

发布时间：2025年06月11日

`LLM应用` `对话系统` `情感计算`

> When Large Language Models are Reliable for Judging Empathic Communication

# 摘要

> 大型语言模型（LLMs）在文本对话中善于生成共情回应，但它们对共情交流的细微差别判断有多可靠？我们通过比较专家、众包工作者和LLMs在四个跨领域框架下的标注，分析了200段真实对话中支持性回应的共情表现。基于3,150个专家标注、2,844个众包标注和3,150个LLM标注，我们发现专家意见高度一致，但不同框架的子组件因特性不同而有所差异。专家意见比标准分类指标更能准确衡量LLM表现。在所有框架中，LLMs的表现接近专家基准，且可靠性优于众包工作者。这表明，经过特定任务验证的LLMs可为情感敏感应用提供透明和可靠的对话支持。

> Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? We investigate this question by comparing how experts, crowdworkers, and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing, and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess inter-rater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks' sub-components depending on their clarity, complexity, and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.

[Arxiv](https://arxiv.org/abs/2506.10150)