# # PersonaLens：对话式AI助手个性化评估的基准测试
PersonaLens 是一个专门用于评估对话式AI助手个性化能力的基准测试工具。

发布时间：2025年06月11日

`LLM应用` `人工智能` `对话系统`

> PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants

# 摘要

> 大型语言模型（LLMs）显著提升了对话式AI助手的能力。然而，系统性评估这些助手在完成任务的同时如何适应个体用户偏好的个性化能力，仍是一项挑战。现有的个性化基准测试主要集中在闲聊、非对话任务或狭窄领域，未能充分反映个性化任务导向型协助的复杂性。为此，我们推出了PersonaLens，一个全面的基准测试，专为评估任务导向型AI助手的个性化能力而设计。我们的基准测试包含多样化的用户档案，每个档案都配备了丰富的偏好和交互历史，并配备了两个专门的LLM驱动代理：用户代理能够参与现实的任务导向型对话与AI助手互动，而评委代理则采用LLM作为评委的范式，用于评估个性化、响应质量和任务成功。通过对当前LLM助手在各种任务中的广泛实验，我们发现它们在个性化能力方面存在显著差异，这些发现为推动对话式AI系统的发展提供了重要见解。

> Large language models (LLMs) have advanced conversational AI assistants. However, systematically evaluating how well these assistants apply personalization--adapting to individual user preferences while completing tasks--remains challenging. Existing personalization benchmarks focus on chit-chat, non-conversational tasks, or narrow domains, failing to capture the complexities of personalized task-oriented assistance. To address this, we introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants. Our benchmark features diverse user profiles equipped with rich preferences and interaction histories, along with two specialized LLM-based agents: a user agent that engages in realistic task-oriented dialogues with AI assistants, and a judge agent that employs the LLM-as-a-Judge paradigm to assess personalization, response quality, and task success. Through extensive experiments with current LLM assistants across diverse tasks, we reveal significant variability in their personalization capabilities, providing crucial insights for advancing conversational AI systems.

[Arxiv](https://arxiv.org/abs/2506.09902)