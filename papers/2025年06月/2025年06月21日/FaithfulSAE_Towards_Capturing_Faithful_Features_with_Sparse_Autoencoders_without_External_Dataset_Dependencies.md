# FaithfulSAE: 基于稀疏自动编码器捕获忠实特征，无需依赖外部数据集

发布时间：2025年06月21日

`LLM理论

理由：这篇论文探讨了稀疏自动编码器（SAEs）在大型语言模型中的应用，特别是如何通过改进训练方法来提高模型的可解释性和稳定性。研究集中在模型的内部特征捕捉和训练数据的影响上，属于对模型理论的深入分析和改进，因此归类为LLM理论。` `人工智能` `机器学习`

> FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies

# 摘要

> 稀疏自动编码器（SAEs）作为一种有前景的解决方案，能够将大型语言模型的表示分解为可解释的特征。然而，Paulo和Belrose（2025）指出，SAEs在不同初始化种子下的稳定性存在问题，而Heap等人（2025）则指出SAEs可能无法捕捉模型内部特征。这些问题很可能源于在外部数据集上训练SAEs——无论是从网络收集的数据，还是由其他模型生成的数据，这些数据可能包含超出模型泛化能力范围的分布外（OOD）数据。这可能导致SAEs生成所谓的“Fake Features”（虚假特征），这些特征无法准确反映模型的内部激活状态。为了解决这些问题，我们提出了 FaithfulSAE，一种在模型自身生成的合成数据集上训练SAEs的方法。通过 FaithfulSAEs，我们证明了在OOD程度较低的指令数据集上训练SAEs，能够使SAEs在不同种子下更加稳定。值得注意的是，在SAE探查任务中， FaithfulSAEs 在基于网络的数据集上训练的SAEs表现更优，并且在7个模型中的5个模型中，虚假特征比率更低。总体而言，我们的方法摆脱了对外部数据集的依赖，通过更好地捕捉模型内部特征，推动了模型可解释性的发展，同时突显了常常被忽视的SAE训练数据集的重要性。

> Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term "Fake Features", that misrepresent the model's internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on web-based datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets.

[Arxiv](https://arxiv.org/abs/2506.17673)