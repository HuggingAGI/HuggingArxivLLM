# 生成图像能否成为文本为中心的多模态学习的有效模式？

发布时间：2025年06月21日

`LLM应用` `计算机视觉` `多模态模型`

> Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?

# 摘要

> 文本数据的丰富性和多模态模型的强大能力之间存在着显著的“模态差距”。本研究系统性地探讨了文本到图像（T2I）模型生成的即时图像是否能作为以文本为中心任务的宝贵补充模态。通过在文本分类任务上的全面评估框架，我们分析了关键变量的影响，包括T2I模型质量、提示工程策略以及多模态融合架构。我们的研究发现，这种“合成感知”能够带来显著的性能提升，即使是在增强强大的大型语言模型基线时也是如此。然而，我们发现这一方法的效果高度依赖于文本与生成图像之间的语义对齐、任务本身的“视觉可定位性”以及T2I模型的生成保真度。我们的工作为这一范式建立了首个严谨的基准，清晰地分析了其潜力与当前局限性，并证明了其在传统单模态场景中丰富语言理解的可行性。

> A significant ``modality gap" exists between the abundance of text-only data and the increasing power of multimodal models. This work systematically investigates whether images generated on-the-fly by Text-to-Image (T2I) models can serve as a valuable complementary modality for text-centric tasks. Through a comprehensive evaluation framework on text classification, we analyze the impact of critical variables, including T2I model quality, prompt engineering strategies, and multimodal fusion architectures. Our findings demonstrate that this``synthetic perception" can yield significant performance gains, even when augmenting strong large language model baselines. However, we find the effectiveness of this approach is highly conditional, depending critically on the semantic alignment between text and the generated image, the inherent ``visual groundability" of the task, and the generative fidelity of the T2I model. Our work establishes the first rigorous benchmark for this paradigm, providing a clear analysis of its potential and current limitations, and demonstrating its viability as a pathway to enrich language understanding in traditionally unimodal scenarios.

[Arxiv](https://arxiv.org/abs/2506.17623)