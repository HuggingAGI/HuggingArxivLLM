# # 研究标题
以答案为中心，还是以推理为导向？揭示 LLMs 中的潜在记忆锚点

发布时间：2025年06月21日

`LLM理论` `人工智能` `计算机科学`

> Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs

# 摘要

> 尽管大型语言模型（LLMs）展现出了令人惊叹的推理能力，但越来越多的证据表明，它们的成功更多源于记忆中的答案-推理模式，而非真正的推理。本研究探讨了一个核心问题：LLMs主要是锚定于最终答案，还是推理链的文本模式？我们提出了一种五级答案可见性提示框架，系统性地操控答案线索，并通过间接的行为分析来探测模型行为。对当前最先进的LLMs进行的实验表明，模型对显式答案存在强烈且一致的依赖性。当答案线索被遮蔽时，即使完整的推理链仍然存在，性能也会下降26.90%。这些发现表明，LLMs所展现的推理可能更多是事后合理化，而非真正的推理，这使得它们的推理深度受到质疑。我们的研究通过严格的实证验证揭示了答案锚定现象，并强调了对LLMs推理本质进行更细致理解的必要性。

> While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90\% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs.

[Arxiv](https://arxiv.org/abs/2506.17630)