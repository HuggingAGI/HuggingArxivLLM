# Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation
识别角色越界行为：开放生成任务中基于原子级评估的角色一致性研究

发布时间：2025年06月24日

`LLM应用` `人机交互` `对话系统`

> Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation

# 摘要

> 在大型语言模型（LLMs）中保持角色一致性（persona fidelity）对实现连贯且吸引人的交互至关重要。然而，LLMs常会出现越界（Out-of-Character, OOC）行为，导致生成回复偏离预设角色，影响模型可靠性。现有评估方法通常为整个回复赋予单一评分，难以捕捉细微的角色偏离，尤其在长文本生成中。为解决这一问题，我们提出了一种原子级评估框架，以更精细的粒度量化角色一致性。通过三个关键指标，我们衡量了生成内容在角色对齐和一致性方面的表现。这种方法能够识别真实用户可能遇到的细微偏差，实现更精确和现实的角色一致性评估。实验表明，我们的框架能够有效检测先前方法忽视的角色不一致。通过对不同任务和人格类型的角色一致性分析，我们揭示了任务结构和角色吸引力对模型适应性的影响，凸显了保持角色表达一致性的挑战。

> Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.

[Arxiv](https://arxiv.org/abs/2506.19352)