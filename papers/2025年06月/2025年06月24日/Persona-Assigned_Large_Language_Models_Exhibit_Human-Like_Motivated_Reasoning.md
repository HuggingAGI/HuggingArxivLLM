# 赋予人格设定的大型语言模型具备类似人类的动机推理能力。

发布时间：2025年06月24日

`LLM理论` `社会学` `政治学`

> Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning

# 摘要

> 人类推理常受偏见困扰，这些偏见源于身份保护等动机，削弱理性决策。集体层面的动机推理在气候变化或疫苗安全等关键议题上危害社会，加剧政治分裂。先前研究显示，大型语言模型（LLMs）也易受人类认知偏见影响，但LLMs在身份一致结论上的选择性推理程度尚待探索。本研究探讨了分配8种人设（基于4种政治和社会人口属性）是否会在LLMs中引发动机推理。我们测试了8种LLMs（开源与专有）在两项推理任务上的表现，任务来自人类受试者研究：辨别错误信息标题的真实性和评估数值型科学证据。结果显示，分配了人设的LLMs在辨别错误信息标题真实性方面比未分配人设的模型准确率降低9%。尤其，政治人设的LLMs在枪支管制的科学证据评估中，当真相与其诱导的政治身份一致时，正确评估的可能性提高了90%。基于提示的去偏见方法在缓解这些影响方面效果有限。综合来看，我们的研究首次表明，分配了人设的LLMs表现出类似人类的动机推理，而这种推理难以通过传统的去偏见提示缓解，这引发了对LLMs和人类身份一致推理加剧的担忧。


> Reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. Here, we investigate whether assigning 8 personas across 4 political and socio-demographic attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and proprietary) across two reasoning tasks from human-subject studies -- veracity discernment of misinformation headlines and evaluation of numeric scientific evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects. Taken together, our empirical findings are the first to suggest that persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts -- raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

[Arxiv](https://arxiv.org/abs/2506.20020)