# 从信息几何与量子度量新视角重新审视大语言模型训练

发布时间：2025年06月18日

`LLM理论` `机器学习` `量子计算`

> Rethinking LLM Training through Information Geometry and Quantum Metrics

# 摘要

> 大语言模型的优化发生在高维非欧几里得参数空间中。信息几何借助费舍尔信息度量为这一复杂景观提供框架，使我们能通过自然梯度下降实现更系统的学习。尽管这种方法在实际应用中常具挑战性，但其几何视角却能清晰解释尖锐极小值、泛化能力和缩放定律等现象。我们认为，关注曲率的方法能帮助我们更深入地理解大语言模型的训练机制。最后，我们从Fubini-Study度量和量子费舍尔信息出发，探讨了量子类比的可能性，这或许预示着量子增强系统中存在更高效的优化途径。

> Optimization in large language models (LLMs) unfolds over high-dimensional parameter spaces with non-Euclidean structure. Information geometry frames this landscape using the Fisher information metric, enabling more principled learning via natural gradient descent. Though often impractical, this geometric lens clarifies phenomena such as sharp minima, generalization, and observed scaling laws. We argue that curvature-aware approaches deepen our understanding of LLM training. Finally, we speculate on quantum analogies based on the Fubini-Study metric and Quantum Fisher Information, hinting at efficient optimization in quantum-enhanced systems.

[Arxiv](https://arxiv.org/abs/2506.15830)