# 大型语言模型能否胜任论文评审？一项可行性研究

发布时间：2025年06月18日

`LLM应用` `学术评审`

> Can Large Language Models Be Trusted Paper Reviewers? A Feasibility Study

# 摘要

> 学术论文评审是一项耗时费力的工作，需要专业知识和大量人力资源。大型语言模型（LLMs）凭借其丰富的训练数据、广泛的知识基础和较低的使用成本，为评审过程的自动化提供了有前景的方法。本研究通过提出一个自动评审系统，探讨了在学术论文评审中使用LLMs的可行性。该系统整合了检索增强生成（RAG）、AutoGen多智能体系统以及思维链提示，支持格式检查、标准化评估、评论生成和评分等任务。在WASA 2024大会的290篇投稿中使用GPT-4进行的实验表明，基于LLM的评审显著减少了评审时间（平均2.48小时）和成本（平均104.28美元）。然而，LLM评审的论文与实际录用论文的相似度仍然较低（平均38.6%），表明存在幻觉、缺乏独立判断和检索偏好等问题。因此，建议将LLM作为辅助工具来支持人工评审，而不是取代人工评审。


> Academic paper review typically requires substantial time, expertise, and human resources. Large Language Models (LLMs) present a promising method for automating the review process due to their extensive training data, broad knowledge base, and relatively low usage cost. This work explores the feasibility of using LLMs for academic paper review by proposing an automated review system. The system integrates Retrieval Augmented Generation (RAG), the AutoGen multi-agent system, and Chain-of-Thought prompting to support tasks such as format checking, standardized evaluation, comment generation, and scoring. Experiments conducted on 290 submissions from the WASA 2024 conference using GPT-4o show that LLM-based review significantly reduces review time (average 2.48 hours) and cost (average \$104.28 USD). However, the similarity between LLM-selected papers and actual accepted papers remains low (average 38.6\%), indicating issues such as hallucination, lack of independent judgment, and retrieval preferences. Therefore, it is recommended to use LLMs as assistive tools to support human reviewers, rather than to replace them.

[Arxiv](https://arxiv.org/abs/2506.17311)