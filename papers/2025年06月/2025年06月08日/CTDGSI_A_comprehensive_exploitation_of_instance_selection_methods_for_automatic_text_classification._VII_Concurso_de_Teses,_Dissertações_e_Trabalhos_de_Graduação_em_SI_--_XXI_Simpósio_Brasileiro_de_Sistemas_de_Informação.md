# CTDGSI: 探索实例选择方法在自动文本分类中的全面应用。第七届信息学学士论文、硕士论文和博士论文竞赛——第21届巴西信息系统研讨会

发布时间：2025年06月08日

`其他` `自动文本分类`

> CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação

# 摘要

> 自然语言处理（NLP）的发展一直遵循“量多则优”的法则：更多数据、更强算力、更复杂的模型，大型语言模型（LLMs）便是这一趋势的典型代表。然而，针对特定应用场景训练（或微调）大型密集模型通常需要消耗大量计算资源。本博士学位论文聚焦于一种尚未得到充分研究的NLP数据工程方法——实例选择（IS），它在当前背景下展现了巨大潜力。IS的目标是通过去除噪声或冗余实例来缩减训练集规模，同时保持模型性能并降低训练成本。我们针对一项关键的NLP任务——自动文本分类（ATC），全面对比了多种IS方法在不同分类解决方案和数据集上的表现。研究发现，IS解决方案中仍存在巨大的潜力尚未被挖掘。我们提出了两种新型IS解决方案：噪声导向和冗余感知，专门针对大规模数据集和Transformer架构设计。最终，我们的解决方案在所有数据集上实现了平均41%的训练集缩减，同时保持了相同的模型效果。更重要的是，我们的解决方案实现了1.67倍（最高可达2.46倍）的速度提升，使其能够扩展应用于包含数十万文档的数据集。

> Progress in Natural Language Processing (NLP) has been dictated by the rule of more: more data, more computing power and more complexity, best exemplified by the Large Language Models. However, training (or fine-tuning) large dense models for specific applications usually requires significant amounts of computing resources. This \textbf{Ph.D. dissertation} focuses on an under-investi\-gated NLP data engineering technique, whose potential is enormous in the current scenario known as Instance Selection (IS). The IS goal is to reduce the training set size by removing noisy or redundant instances while maintaining the effectiveness of the trained models and reducing the training process cost. We provide a comprehensive and scientifically sound comparison of IS methods applied to an essential NLP task -- Automatic Text Classification (ATC), considering several classification solutions and many datasets. Our findings reveal a significant untapped potential for IS solutions. We also propose two novel IS solutions that are noise-oriented and redundancy-aware, specifically designed for large datasets and transformer architectures. Our final solution achieved an average reduction of 41\% in training sets, while maintaining the same levels of effectiveness in all datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x (up to 2.46x), making them scalable for datasets with hundreds of thousands of documents.

[Arxiv](https://arxiv.org/abs/2506.07169)