# 个性化宪法导向智能体超我：安全AI行为与多元人类价值观对齐

发布时间：2025年06月08日

`Agent` `人工智能`

> Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values

# 摘要

> 具备自主规划与行动能力的智能体AI系统，在各领域展现出巨大潜力。然而，实际部署面临诸多挑战，包括与人类价值观、复杂安全需求及特定合规要求的对齐问题。现有对齐方法在提供深度个性化情境信息时往往力不从心，容易引发错误推理或效率低下。本文提出了一种创新解决方案：'超我'代理，作为智能体AI的个性化监督机制。该系统通过参考用户选定的“信条宪法”（涵盖多样化的规则集）来动态引导AI规划，可调节的遵循程度以适应不可协商的核心价值。实时合规性执行器会在计划执行前，将其与信条宪法及普遍道德底线进行验证。我们提供了一个功能完备的系统，包括演示界面（www.Creed.Space）——一个原型化的宪法共享门户，并通过模型上下文协议（MCP）成功集成第三方模型。全面的基准评估（HarmBench、AgentHarm）显示，我们的超我代理大幅降低了有害输出，使领先大语言模型（如Gemini 2.5 Flash和GPT-4o）的有害得分降低高达98.3%，并在AgentHarm的有害测试集上实现近乎完美的拒绝率（如Claude Sonnet 4的100%拒绝率）。此方法显著简化了个性化AI对齐过程，使智能体系统更可靠地适应个体及文化背景，同时带来显著的安全性提升。更多研究概述及示例请访问https://superego.creed.space。


> Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

[Arxiv](https://arxiv.org/abs/2506.13774)