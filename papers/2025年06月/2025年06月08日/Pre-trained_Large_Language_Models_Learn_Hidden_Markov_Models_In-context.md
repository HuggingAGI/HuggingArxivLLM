# 预训练大型语言模型学习隐马尔可夫模型

发布时间：2025年06月08日

`LLM应用` `生物学` `数据分析`

> Pre-trained Large Language Models Learn Hidden Markov Models In-context

# 摘要

> # 隐马尔可夫模型与上下文学习

隐马尔可夫模型（HMMs）是建模具有潜在马尔可夫结构的序列数据的经典工具，但在实际应用中，将其拟合到真实数据仍然面临计算挑战。本研究发现，预训练的大型语言模型（LLMs）可以通过上下文学习（ICL）——即从提示中的示例中推理出模式的能力，有效地建模由HMM生成的数据。

在多样化的合成HMM实验中，LLMs的预测准确率接近理论最优值。我们发现了一系列受HMM属性影响的新颖缩放趋势，并提出了相应的理论假设来解释这些实证观察。此外，我们为科学家提供了实用指南，帮助他们将ICL作为复杂数据分析的诊断工具。

在真实世界中的动物决策任务中，ICL的表现与人类专家设计的模型相媲美。这一成果标志着首次证明ICL能够学习和预测HMM生成的序列，不仅深化了我们对LLMs中上下文学习机制的理解，还为其作为揭示复杂科学数据中隐藏结构的强大工具奠定了基础。


> Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)$unicode{x2013}$their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences$unicode{x2013}$an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.

[Arxiv](https://arxiv.org/abs/2506.07298)