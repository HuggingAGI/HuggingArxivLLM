# 破解评审：大型语言模型在自动化同行评审中的脆弱性评估——面对文本对抗攻击的分析

发布时间：2025年06月08日

`LLM应用` `学术出版` `人工智能`

> Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks

# 摘要

> 同行评审是学术质量的基石，但投稿量激增给审稿人带来了沉重负担。大型语言模型（LLMs）虽然有潜力成为审稿助手，但其易受文本对抗攻击的弱点令人担忧。本研究探讨了LLMs作为自动审稿工具在对抗攻击下的稳健性，并重点回答三个关键问题：（1）LLMs生成的审稿意见与人工审稿的质量对比；（2）对抗攻击对LLM审稿可靠性的冲击；（3）基于LLM的审稿面临的挑战及应对策略。研究发现，LLMs在文本操控下易受影响，评估结果可能失真。我们全面评估了LLMs在自动化审稿中的表现，并深入分析了其对抗稳健性。这些发现强调了应对对抗风险的重要性，以确保AI真正助力学术交流的严谨性，而非成为其隐患。


> Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.

[Arxiv](https://arxiv.org/abs/2506.11113)