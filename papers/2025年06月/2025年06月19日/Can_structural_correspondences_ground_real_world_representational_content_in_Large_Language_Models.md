# 结构对应关系能否支撑起大语言模型中对现实世界内容的表征能力？

发布时间：2025年06月19日

`LLM理论` `人工智能` `认知科学`

> Can structural correspondences ground real world representational content in Large Language Models?

# 摘要

> 大型语言模型（LLMs）如GPT-4能够对各种提示产生引人注目的回应。然而，它们的表征能力尚不明确。许多LLMs与超语言现实没有直接接触：它们的输入、输出和训练数据仅包含文本，这引出了两个问题：（1）LLMs能否表征任何事物？（2）如果可以，那么它们能表征什么？在这篇论文中，我探讨了如何根据基于结构对应关系的表征理论来回答这些问题，并对相关证据进行了初步调查。我主张，仅仅存在于LLMs与世界实体之间的结构对应关系不足以支撑对这些实体的表征。然而，如果这些结构对应关系扮演了适当的角色——它们以一种能够解释成功任务执行的方式被利用——那么它们就可能支撑对现实世界的表征。这需要克服一个挑战：从表面上看，LLMs的文本限制似乎阻碍了它们参与适当类型的任务。

> Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.

[Arxiv](https://arxiv.org/abs/2506.16370)