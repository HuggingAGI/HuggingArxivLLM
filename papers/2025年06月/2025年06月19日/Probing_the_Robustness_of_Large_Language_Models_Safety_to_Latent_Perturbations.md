# 大型语言模型安全在潜在扰动下的鲁棒性探查

发布时间：2025年06月19日

`LLM理论` `人工智能` `机器学习`

> Probing the Robustness of Large Language Models Safety to Latent Perturbations

# 摘要

> # 摘要
安全对齐是构建可靠人工通用智能的关键要求。尽管在安全对齐方面取得了显著进展，但我们发现，即使是细微的潜在偏移仍可能触发对齐模型中的不安全响应。我们认为，这是由于现有对齐方法的本质较为浅层，它们专注于表面级别的拒绝行为，而未能充分改变内部表示。因此，隐藏激活中的小幅变化就可能重新触发潜藏空间中的有害行为。

为了探索安全对齐在潜在扰动下的鲁棒性，我们引入了一种探测方法，用于衡量模型生成的原始响应的负对数似然。该探测器量化了潜在空间中的局部敏感性，作为识别脆弱方向的诊断工具。基于此信号，我们构建了有效的越狱轨迹，从而提出了激活引导攻击（ASA）。更重要的是，这些见解为提升对齐的鲁棒性提供了原理性的基础。

为此，我们引入了分层对抗补丁训练（LAPT），这是一种微调策略，在训练过程中向隐藏表示中注入受控扰动。实验结果表明，LAPT增强了对齐的鲁棒性，同时并未损害模型的一般能力。我们的研究发现揭示了当前对齐范式中的根本性缺陷，并呼吁采用超越表面行为监督的表示级别训练策略。代码和实验结果可在https://github.com/Carol-gutianle/LatentSafety获取。

> Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at https://github.com/Carol-gutianle/LatentSafety.

[Arxiv](https://arxiv.org/abs/2506.16078)