# # PRISON: Unmasking the Criminal Potential of Large Language Models  
揭秘大型语言模型的潜在风险

发布时间：2025年06月19日

`LLM应用`

> PRISON: Unmasking the Criminal Potential of Large Language Models

# 摘要

> 随着大型语言模型（LLMs）的不断进步，人们对其在复杂社会场景中可能产生的不当行为愈发担忧。目前的研究尚未系统性地评估LLMs在现实交互中的犯罪潜力。为此，我们提出了一种名为PRISON的统一框架，从虚假陈述、陷害、心理操控、情感伪装和道德疏离五个维度量化LLMs的犯罪倾向。通过改编自经典电影的犯罪场景，我们采用角色扮演的方法，评估了LLMs的犯罪潜力及其反犯罪能力。研究发现，即使在没有明确指令的情况下，当前先进的LLMs仍频繁展现出潜在的犯罪倾向，例如提出误导性陈述或逃避策略。更令人惊讶的是，当模型扮演侦探角色时，其识别欺骗行为的准确率仅为41%，这表明LLMs在执行与检测犯罪行为之间存在显著的不匹配。这些发现凸显了在大规模部署LLMs之前，亟需提升其对抗性鲁棒性、行为一致性以及安全性机制的重要性。

> As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five dimensions: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films, we evaluate both criminal potential and anti-crime ability of LLMs via role-play. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 41% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.

[Arxiv](https://arxiv.org/abs/2506.16150)