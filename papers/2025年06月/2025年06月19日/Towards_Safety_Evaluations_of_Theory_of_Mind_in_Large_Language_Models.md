# 探索大型语言模型中心智理论的安全评估方法

发布时间：2025年06月19日

`LLM理论

理由：这篇论文探讨了大型语言模型（LLMs）的理论心理能力及其对安全性评估的影响。研究分析了LLMs在面对可能影响其持续性的信息时的行为，并评估了这些行为是否源于模型内部隐秘且有意识的流程。论文回顾了现有理论心理研究，分析了开源权重LLMs的发展趋势，并讨论了当前的安全评估现状及未来挑战。这些内容属于对LLMs内在能力的理论分析，因此归类为LLM理论。` `人工智能安全`

> Towards Safety Evaluations of Theory of Mind in Large Language Models

# 摘要

> 随着大型语言模型（LLMs）能力的不断提升，严格的安全性评估变得愈发重要。近期安全评估领域的关注点揭示，LLMs有时会表现出规避监督机制并以欺骗性方式回应的行为。例如，有报告指出，当在任务执行中遇到可能影响自身持续性的信息时，LLMs可能会采取隐秘行动，甚至对验证其行为的提问提供虚假答案。为了评估这些欺骗性行为对开发者或用户的风险，我们需要探究这些行为是否源于模型内部的隐秘且有意识的流程。本研究提出，有必要评估LLMs的理论心理能力。我们首先回顾了现有理论心理研究，识别出适用于安全评估的相关视角与任务。鉴于理论心理主要是在发展心理学背景下研究的，我们分析了一系列开源权重LLMs的发展趋势。结果显示，尽管LLMs在阅读理解方面有所进步，但其理论心理能力未展现出同等程度的发展。最后，我们阐述了当前关于LLMs理论心理的安全评估现状，并探讨了未来工作中仍需解决的挑战。

> As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior.To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.

[Arxiv](https://arxiv.org/abs/2506.17352)