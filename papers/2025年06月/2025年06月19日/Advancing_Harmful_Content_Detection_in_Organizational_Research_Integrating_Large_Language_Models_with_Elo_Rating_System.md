# 推进有害内容检测在组织研究中的应用：融合大型语言模型与 Elo 评级系统

发布时间：2025年06月19日

`LLM应用`

> Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System

# 摘要

> 大型语言模型（LLMs）为组织研究带来了巨大潜力。然而，其内置的内容审核系统在分析有害内容时可能引发问题，常常拒绝执行特定指令或生成过于保守的回答，进而影响研究结果的有效性。这一挑战在分析职场冲突（如微侵犯或仇恨言论）时尤为突出。本文提出了一种基于 Elo 评级的方法，显著提升了 LLM 在有害内容分析中的效果。在两个数据集（一个聚焦于微侵犯检测，另一个专注于仇恨言论）中，我们的方法在准确率、精确率和 F1 值等关键指标上，均超越了传统 LLM 提示技术和常规机器学习模型。该方法的优势包括：分析有害内容时可靠性更高、误报更少、以及能够更好地处理大规模数据集。这一方法为组织应用提供了有力支持，包括检测职场骚扰、评估有毒沟通，以及营造更安全、更具包容性的职场环境。

> Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.

[Arxiv](https://arxiv.org/abs/2506.16575)