# 从提示到构建：心理学视角下LLM研究的双重有效性框架

发布时间：2025年06月19日

`LLM应用` `心理学` `人工智能`

> From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology

# 摘要

> 大型语言模型（LLMs）正在心理学领域迅速被采用，它们既可以作为研究工具，也可以作为实验对象，模拟人类行为，还能构建认知计算模型。然而，将人类测量工具应用于这些系统时，常常会产生矛盾的结果，这引发了人们的担忧：许多研究发现可能是“测量幽灵”——统计学上的产物，而非真实的心理现象。在本文中，我们主张：构建一个坚实的人工智能心理学科学体系，需要将我们领域中的两大基石结合起来：可靠的测量原则和严谨的因果推断标准。我们提出了一种双重有效性框架，以指导这一整合过程，该框架明确了支持某一主张所需的证据如何与其科学目标相匹配。例如，使用LLM进行文本分类可能只需要基本的准确性验证，而如果声称它能够模拟焦虑，则需要进行远更为严格和全面的验证流程。当前的研究实践系统性地未能满足这些要求，常常将统计模式匹配误认为是心理现象的证据。同一个模型输出——比如认可“我感到焦虑”——需要根据研究者声称的测量、表征、模拟或建模心理构念的不同，采取不同的验证策略。未来的发展需要构建心理构念的计算模拟，并建立清晰、可扩展的证据标准，而不是简单地将人类测量工具不加批判地应用于AI系统。

> Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field's foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing "I am anxious"--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.

[Arxiv](https://arxiv.org/abs/2506.16697)