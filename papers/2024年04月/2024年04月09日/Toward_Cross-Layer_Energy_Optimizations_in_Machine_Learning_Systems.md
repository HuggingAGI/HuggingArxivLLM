# 探索机器学习系统中的跨层能源优化

发布时间：2024年04月09日

`LLM理论` `机器学习` `能源优化`

> Toward Cross-Layer Energy Optimizations in Machine Learning Systems

# 摘要

> 机器学习和生成型AI任务的能耗之巨，不仅令运营成本承压，更对电力供应和环境可持续性构成挑战。尽管能效硬件研究已持续多年，我们却发现，软件在优化ML能耗方面发挥着至关重要的作用，尤其是通过宙斯和珀尔修斯这两个最新项目。这一点在大型语言模型上表现得尤为明显，因为它们的规模和能耗需求的增速，已超过了硬件效率的提升速度。为此，我们主张在ML系统的能量优化上采取跨层次策略，即硬件提供架构支持以推动软件的能效进步，而软件则利用硬件特性，开发出普适的节能技术。

> The enormous energy consumption of machine learning (ML) and generative AI workloads shows no sign of waning, taking a toll on operating costs, power delivery, and environmental sustainability. Despite a long line of research on energy-efficient hardware, we found that software plays a critical role in ML energy optimization through two recent works: Zeus and Perseus. This is especially true for large language models (LLMs) because their model sizes and, therefore, energy demands are growing faster than hardware efficiency improvements. Therefore, we advocate for a cross-layer approach for energy optimizations in ML systems, where hardware provides architectural support that pushes energy-efficient software further, while software leverages and abstracts the hardware to develop techniques that bring hardware-agnostic energy-efficiency gains.

[Arxiv](https://arxiv.org/abs/2404.06675)