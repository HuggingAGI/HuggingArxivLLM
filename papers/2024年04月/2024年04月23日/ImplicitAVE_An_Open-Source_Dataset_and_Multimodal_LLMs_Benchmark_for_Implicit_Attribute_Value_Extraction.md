# ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。

发布时间：2024年04月23日

`LLM应用` `属性值提取` `多模态学习`

> ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction

# 摘要

> 目前针对属性值提取（AVE）的数据集多集中于明显属性值，而对隐性属性值视而不见，往往缺乏产品图像，不对外开放，且在多个领域内缺少详尽的人工审查。为克服这些不足，我们推出了ImplicitAVE——首个公开的多模态隐式属性值提取数据集。ImplicitAVE基于MAVE数据集，经过精心筛选和扩充，纳入了隐性AVE和多模态特性，形成了一个优化后的数据集，包含68,000条训练样本和1,600条测试样本，覆盖五个不同的领域。我们还研究了多模态大型语言模型（MLLMs）在隐式AVE中的应用，并为MLLMs在ImplicitAVE数据集上设立了一个全面的评估标准。对六种最新的MLLMs及其十一种变体进行了广泛设置的评估，发现隐性值提取对于MLLMs而言仍是一大挑战。本研究的贡献包括开发并发布了ImplicitAVE数据集，以及对多种MLLMs在隐式AVE任务中的探索和性能评估，为未来研究提供了宝贵的洞见和可能的研究方向。相关数据集和代码已在 https://github.com/HenryPengZou/ImplicitAVE 上公开。

> Existing datasets for attribute value extraction (AVE) predominantly focus on explicit attribute values while neglecting the implicit ones, lack product images, are often not publicly available, and lack an in-depth human inspection across diverse domains. To address these limitations, we present ImplicitAVE, the first, publicly available multimodal dataset for implicit attribute value extraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated and expanded to include implicit AVE and multimodality, resulting in a refined dataset of 68k training and 1.6k testing data across five domains. We also explore the application of multimodal large language models (MLLMs) to implicit AVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE dataset. Six recent MLLMs with eleven variants are evaluated across diverse settings, revealing that implicit value extraction remains a challenging task for MLLMs. The contributions of this work include the development and release of ImplicitAVE, and the exploration and benchmarking of various MLLMs for implicit AVE, providing valuable insights and potential future research directions. Dataset and code are available at https://github.com/HenryPengZou/ImplicitAVE

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x1.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x2.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x3.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x4.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x5.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x6.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x7.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x8.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x9.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x10.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x11.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x12.png)

![ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x13.png)

[Arxiv](https://arxiv.org/abs/2404.15592)