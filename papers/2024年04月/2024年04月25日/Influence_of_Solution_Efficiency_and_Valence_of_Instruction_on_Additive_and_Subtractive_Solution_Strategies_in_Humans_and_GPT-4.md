# 本文探讨了解决方案的效率性以及指令的正负价态如何影响人类和 GPT-4 在执行加法和减法策略时的行为模式。

发布时间：2024年04月25日

`LLM应用

这篇论文主要研究了人类和大型语言模型（LLM）在解决问题时的行为差异，特别是关于“添加偏好”的认知倾向。通过比较人类和GPT-4模型在不同实验任务中的表现，论文探讨了解决效率、指令情感色彩等因素对实验结果的影响。由于论文主要关注LLM在实际应用中的表现和行为特点，因此可以归类为LLM应用。` `心理学` `人工智能`

> Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4

# 摘要

> 通过四项预注册实验，我们深入探究了“添加偏好”这一认知倾向，即人们更倾向于通过增加而非减少元素来改变初始状态或结构。这些实验比较了人类与 OpenAI 的 GPT-4 大型语言模型在解决问题时的行为，共有 588 名美国参与者和 GPT-4 模型进行了 680 轮迭代。实验任务包括在网格中创造对称性（实验 1 和 3）和编辑摘要（实验 2 和 4）。正如预期，我们普遍观察到了添加偏好的存在。解决效率和指令的情感色彩对实验结果有显著影响。在减法更为高效的情况下，人类参与者较少采用累加策略。而 GPT-4 则表现出相反的倾向，即使在减法更有效时也展现出强烈的添加偏好。对于指令的情感色彩，GPT-4 在被要求“改进”时更倾向于增加词汇，而“编辑”时则不然，人类则未显示这种差异。在不同条件下观察添加偏好时，GPT-4 相比人类展现出更明显的偏差。这些发现提醒我们，在解决问题时，应考虑等效甚至更优的减法方法，并重新审视自己尤其是语言模型的解题策略。

> We explored the addition bias, a cognitive tendency to prefer adding elements over removing them to alter an initial state or structure, by conducting four preregistered experiments examining the problem-solving behavior of both humans and OpenAl's GPT-4 large language model. The experiments involved 588 participants from the U.S. and 680 iterations of the GPT-4 model. The problem-solving task was either to create symmetry within a grid (Experiments 1 and 3) or to edit a summary (Experiments 2 and 4). As hypothesized, we found that overall, the addition bias was present. Solution efficiency (Experiments 1 and 2) and valence of the instruction (Experiments 3 and 4) played important roles. Human participants were less likely to use additive strategies when subtraction was relatively more efficient than when addition and subtraction were equally efficient. GPT-4 exhibited the opposite behavior, with a strong addition bias when subtraction was more efficient. In terms of instruction valence, GPT-4 was more likely to add words when asked to "improve" compared to "edit", whereas humans did not show this effect. When we looked at the addition bias under different conditions, we found more biased responses for GPT-4 compared to humans. Our findings highlight the importance of considering comparable and sometimes superior subtractive alternatives, as well as reevaluating one's own and particularly the language models' problem-solving behavior.

[Arxiv](https://arxiv.org/abs/2404.16692)