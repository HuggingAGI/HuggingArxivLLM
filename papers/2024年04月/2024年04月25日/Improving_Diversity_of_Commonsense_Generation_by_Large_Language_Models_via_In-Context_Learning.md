# 通过上下文学习，提升大型语言模型在常识生成任务中的多样性表现。

发布时间：2024年04月25日

`LLM应用` `人工智能`

> Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning

# 摘要

> 生成常识推理（GCR）任务要求模型不仅要运用常识知识进行推理，还需产出连贯的语句。生成语句的品质固然关键，但多样性亦同样重要，因为它展现了模型调用不同常识知识的能力。大型语言模型（LLMs）通过上下文学习（ICL）在多项任务中提升了生成品质，且无需经过精细调整。尽管如此，对于LLMs输出的多样性，此前尚未有系统性的研究。为此，我们提出了一种新方法，旨在丰富LLMs的生成多样性，同时保持其品质。在三个GCR基准数据集上的实验结果显示，该方法在品质与多样性之间取得了完美的平衡。此外，通过该方法生成的语句，还可以作为训练数据，以增强现有常识生成器的多样性。

> Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.

[Arxiv](https://arxiv.org/abs/2404.16807)