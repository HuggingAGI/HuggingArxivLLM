# 借助上下文学习，探究基础大型语言模型潜在的滥用风险

发布时间：2024年04月16日

`LLM应用` `人工智能` `伦理学`

> Unveiling the Misuse Potential of Base Large Language Models via In-Context Learning

# 摘要

> 大型语言模型（LLMs）的开源推动了应用开发、创新和科学进步的步伐。这不仅包括未经特定对齐的基础模型，也包括特意设计以符合伦理标准和人类价值的对齐模型。尽管普遍认为基础LLMs的指令遵循局限能有效防止滥用，我们的研究却发现这一假设存在重大疏忽。通过精心设计的示例引导，我们发现基础LLMs能够理解和执行恶意指令。为此，我们提出了一套新的风险评估指标，以系统地审视这些风险。研究结果表明，基础LLMs产生的结果可能与那些专门为恶意目的而调整的模型一样危险。这种安全隐患无需专业知识或训练，任何人都可能加以利用，这突显了对基础LLMs安全措施的迫切关注和需求。

> The open-sourcing of large language models (LLMs) accelerates application development, innovation, and scientific progress. This includes both base models, which are pre-trained on extensive datasets without alignment, and aligned models, deliberately designed to align with ethical standards and human values. Contrary to the prevalent assumption that the inherent instruction-following limitations of base LLMs serve as a safeguard against misuse, our investigation exposes a critical oversight in this belief. By deploying carefully designed demonstrations, our research demonstrates that base LLMs could effectively interpret and execute malicious instructions. To systematically assess these risks, we introduce a novel set of risk evaluation metrics. Empirical results reveal that the outputs from base LLMs can exhibit risk levels on par with those of models fine-tuned for malicious purposes. This vulnerability, requiring neither specialized knowledge nor training, can be manipulated by almost anyone, highlighting the substantial risk and the critical need for immediate attention to the base LLMs' security protocols.

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x1.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x2.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x3.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x4.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x5.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x6.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x7.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x8.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x9.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x10.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x11.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x12.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x13.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x14.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x15.png)

![借助上下文学习，探究基础大型语言模型潜在的滥用风险](../../../paper_images/2404.10552/x16.png)

[Arxiv](https://arxiv.org/abs/2404.10552)