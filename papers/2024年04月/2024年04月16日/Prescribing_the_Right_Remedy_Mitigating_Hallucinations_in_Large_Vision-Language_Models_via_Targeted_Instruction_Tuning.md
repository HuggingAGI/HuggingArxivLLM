# 对症下药：通过针对性指令调整，缓解大型视觉-语言模型中的幻觉问题

发布时间：2024年04月16日

`LLM应用` `视觉识别` `人工智能`

> Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning

# 摘要

> 虽然大型视觉-语言模型（LVLMs）在跨模态任务上表现出色，但它们仍然面临着生成内容与图像不一致的幻觉问题。研究指出，指令数据的质量不佳，尤其是正负样本的不平衡，是导致模型幻觉的关键因素。为此，有研究者提出了如LRV-Instruction这样的高质量指令数据集来减少幻觉现象。然而，我们发现不同LVLMs产生的幻觉内容具有其独特性，即各模型的幻觉概念分布差异显著。现有数据集设计时未充分考虑这一特异性，导致其在缓解幻觉方面的效力受限。本文提出了一个名为DFTG的定制化指令数据生成框架，专门针对不同模型的幻觉特性。该框架分为两个阶段：幻觉诊断阶段从模型响应和图像中提取相关信息，以及针对性数据生成阶段，根据诊断结果定制生成指令数据。实验证明，我们的方法生成的数据在减少幻觉方面比以往数据集更为有效。

> Despite achieving outstanding performance on various cross-modal tasks, current large vision-language models (LVLMs) still suffer from hallucination issues, manifesting as inconsistencies between their generated responses and the corresponding images. Prior research has implicated that the low quality of instruction data, particularly the skewed balance between positive and negative samples, is a significant contributor to model hallucinations. Recently, researchers have proposed high-quality instruction datasets, such as LRV-Instruction, to mitigate model hallucination. Nonetheless, our investigation reveals that hallucinatory concepts from different LVLMs exhibit specificity, i.e. the distribution of hallucinatory concepts varies significantly across models. Existing datasets did not consider the hallucination specificity of different models in the design processes, thereby diminishing their efficacy in mitigating model hallucination. In this paper, we propose a targeted instruction data generation framework named DFTG that tailored to the hallucination specificity of different models. Concretely, DFTG consists of two stages: hallucination diagnosis, which extracts the necessary information from the model's responses and images for hallucination diagnosis; and targeted data generation, which generates targeted instruction data based on diagnostic results. The experimental results on hallucination benchmarks demonstrate that the targeted instruction data generated by our method are more effective in mitigating hallucinations compared to previous datasets.

[Arxiv](https://arxiv.org/abs/2404.10332)