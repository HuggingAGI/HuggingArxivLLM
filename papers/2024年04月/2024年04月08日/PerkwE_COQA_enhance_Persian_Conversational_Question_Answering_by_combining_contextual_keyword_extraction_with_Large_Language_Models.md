# PerkwE_COQA：融合上下文关键词抓取与大型语言模型，提升波斯语对话问答系统的性能。

发布时间：2024年04月08日

`LLM应用` `智慧城市` `对话式问答系统`

> PerkwE_COQA: enhance Persian Conversational Question Answering by combining contextual keyword extraction with Large Language Models

# 摘要

> 智慧城市的建设离不开居民的积极参与，以提升生活品质。对话式问答系统作为一种新兴的用户互动方式，正受到越来越多的青睐。现有研究显示，大型语言模型（LLMs）在对话式问答（CQA）方面展现出巨大潜力，但在捕捉对话细节上仍显不足。本研究提出了一种创新方法，通过理解对话内容并进行多轮互动，更好地满足用户需求，从而提升波斯语CQA系统的性能。该方法结合了LLMs的强大能力和上下文关键词提取技术，提取与对话流程紧密相关的关键词，为模型提供更多上下文信息，帮助其更准确地把握用户意图，生成更为贴切和连贯的回应。经过多种指标的评估，我们的方法在CQA任务上的表现显著优于仅使用LLM的基线，提高了处理隐含问题、提供相关回答和应对复杂对话的能力。研究结果显示，相较于现有技术和LLM基线，我们的方法在评估中最高可提升8%的性能。

> Smart cities need the involvement of their residents to enhance quality of life. Conversational query-answering is an emerging approach for user engagement. There is an increasing demand of an advanced conversational question-answering that goes beyond classic systems. Existing approaches have shown that LLMs offer promising capabilities for CQA, but may struggle to capture the nuances of conversational contexts. The new approach involves understanding the content and engaging in a multi-step conversation with the user to fulfill their needs. This paper presents a novel method to elevate the performance of Persian Conversational question-answering (CQA) systems. It combines the strengths of Large Language Models (LLMs) with contextual keyword extraction. Our method extracts keywords specific to the conversational flow, providing the LLM with additional context to understand the user's intent and generate more relevant and coherent responses. We evaluated the effectiveness of this combined approach through various metrics, demonstrating significant improvements in CQA performance compared to an LLM-only baseline. The proposed method effectively handles implicit questions, delivers contextually relevant answers, and tackles complex questions that rely heavily on conversational context. The findings indicate that our method outperformed the evaluation benchmarks up to 8% higher than existing methods and the LLM-only baseline.

[Arxiv](https://arxiv.org/abs/2404.05406)