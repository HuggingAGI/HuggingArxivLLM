# 任务人员提示：对大型语言模型在软件验证与错误检测方法中的下游任务进行分类

发布时间：2024年04月14日

`LLM应用` `软件工程` `软件测试`

> Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches

# 摘要

> 提示法已成为发挥大型语言模型潜能的核心策略[Brown等人。NeurIPS 2020，Wei等人。TMLR 2022，Wei等人。NeurIPS 2022]。近期，研究者和实践者通过尝试不同的提示方法，探索如何最大化LLM的应用潜力。本文通过细致分析80篇文献，深入探讨了软件测试与验证领域如何构建基于LLM的解决方案。首先，我们要确认下游任务这一概念是否适合描述基于提示的解决方案框架。同时，我们意在明确这些解决方案中任务的种类与特性。为达成此目的，我们创立了一种创新的下游任务分类体系，它能帮助我们在软件工程的广泛问题领域（包括测试、模糊测试、调试、漏洞检测、静态分析及程序验证等）中精确识别出各种工程模式。

> Prompting has become one of the main approaches to leverage emergent capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and practitioners have been playing with prompts to see how to make the most of LLMs. By homogeneously dissecting 80 papers, we investigate in deep how software testing and verification research communities have been abstractly architecting their LLM-enabled solutions. More precisely, first, we want to validate whether downstream tasks are an adequate concept to convey the blueprint of prompt-based solutions. We also aim at identifying number and nature of such tasks in solutions. For such goal, we develop a novel downstream task taxonomy that enables pinpointing some engineering patterns in a rather varied spectrum of Software Engineering problems that encompasses testing, fuzzing, debugging, vulnerability detection, static analysis and program verification approaches.

[Arxiv](https://arxiv.org/abs/2404.09384)