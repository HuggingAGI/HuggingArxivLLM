# 我们通过模拟信任游戏的方式，揭露了大型语言模型中潜藏的基于姓名的偏见。

发布时间：2024年04月22日

`LLM应用` `社会心理学` `人工智能伦理`

> Uncovering Name-Based Biases in Large Language Models Through Simulated Trust Game

# 摘要

> 姓名中透露的性别和种族信息常常成为社会互动中的隐性偏见和刻板印象的来源。众多的人类实验已经证明，当一个人的姓名指向一个主要的性别或种族时，他们往往会受到更优惠的待遇。随着大型语言模型功能日益增强并逐渐融入日常应用，探究这些模型在处理复杂社会互动中的姓名时是否会产生相似的偏见变得尤为关键。与先前研究语言模型中基于姓名的偏见的工作相比，我们采取了不同的方法，挑战了三个主流模型，让它们预测一个经过修改的信任游戏的结果，该游戏是研究信任和互惠的经典框架。为了保障实验的内部有效性，我们精心挑选了一组具有种族代表性的姓氏来识别信任游戏中的参与者，并对实验提示的结构有效性进行了严格的验证。实验结果表明，我们的方法能够有效检测出基础模型和指令调整模型中的姓名偏见。

> Gender and race inferred from an individual's name are a notable source of stereotypes and biases that subtly influence social interactions. Abundant evidence from human experiments has revealed the preferential treatment that one receives when one's name suggests a predominant gender or race. As large language models acquire more capabilities and begin to support everyday applications, it becomes crucial to examine whether they manifest similar biases when encountering names in a complex social interaction. In contrast to previous work that studies name-based biases in language models at a more fundamental level, such as word representations, we challenge three prominent models to predict the outcome of a modified Trust Game, a well-publicized paradigm for studying trust and reciprocity. To ensure the internal validity of our experiments, we have carefully curated a list of racially representative surnames to identify players in a Trust Game and rigorously verified the construct validity of our prompts. The results of our experiments show that our approach can detect name-based biases in both base and instruction-tuned models.

[Arxiv](https://arxiv.org/abs/2404.14682)