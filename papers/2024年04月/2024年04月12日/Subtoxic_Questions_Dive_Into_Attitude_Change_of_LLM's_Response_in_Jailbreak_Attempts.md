# Subtoxic Questions: 探究在越狱尝试中大型语言模型回应的态度转变

发布时间：2024年04月12日

`LLM理论` `安全性` `语言模型`

> Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts

# 摘要

> 随着大型语言模型的提示越狱技术日益受到重视，建立一个通用的研究框架以评估攻击能力并设计基础模型进行精细实验显得尤为重要。本文提出了一种创新方法，专注于那些对越狱提示反应更为敏感的问题集，以期突破加强版LLM安全性所带来的局限。本文通过精心设计和分析这些问题，发现了一种更为高效的手段来识别LLM中的安全漏洞，为LLM安全性的提升做出了贡献。这项研究不仅对现有的越狱技术提出了挑战，还增强了LLM抵御潜在攻击的能力。

> As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and more attention, it is of great significance to raise a generalized research paradigm to evaluate attack strengths and a basic model to conduct subtler experiments. In this paper, we propose a novel approach by focusing on a set of target questions that are inherently more sensitive to jailbreak prompts, aiming to circumvent the limitations posed by enhanced LLM security. Through designing and analyzing these sensitive questions, this paper reveals a more effective method of identifying vulnerabilities in LLMs, thereby contributing to the advancement of LLM security. This research not only challenges existing jailbreaking methodologies but also fortifies LLMs against potential exploits.

![Subtoxic Questions: 探究在越狱尝试中大型语言模型回应的态度转变](../../../paper_images/2404.08309/Pic1.jpg)

![Subtoxic Questions: 探究在越狱尝试中大型语言模型回应的态度转变](../../../paper_images/2404.08309/Pic2.jpg)

[Arxiv](https://arxiv.org/abs/2404.08309)