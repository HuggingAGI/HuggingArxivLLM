# URL：通过任务驱动的表示压缩技术，实现知识的普遍引用链接。

发布时间：2024年04月24日

`LLM应用` `信息检索` `知识链接`

> URL: Universal Referential Knowledge Linking via Task-instructed Representation Compression

# 摘要

> 将信息主张与确凿的参考资源相连接是人类追求真实和可信信息的基本需求。目前的研究多聚焦于特定领域，如信息检索或语义匹配，这些领域的主张与参考关系明确且单一。相较之下，现实世界中的参照知识链接（RKL）要复杂和多变得多。本文提出了一种全新的通用参照知识链接（URL）概念，其目的是通过统一的模型框架，解决多样化的参照知识链接问题。我们引入了一种由大型语言模型（LLM）支持的任务驱动表示压缩技术，结合多视角学习方法，以提升模型在参照知识链接任务中的指令执行和语义理解能力。此外，我们还开发了一个新的评估基准，用以测试模型在不同情境下进行参照知识链接的表现。实验结果证明，现有的方法在处理通用RKL时面临挑战，而我们提出的框架能够在多种不同情境下有效解决问题，显著优于以往的解决方案。

> Linking a claim to grounded references is a critical ability to fulfill human demands for authentic and reliable information. Current studies are limited to specific tasks like information retrieval or semantic matching, where the claim-reference relationships are unique and fixed, while the referential knowledge linking (RKL) in real-world can be much more diverse and complex. In this paper, we propose universal referential knowledge linking (URL), which aims to resolve diversified referential knowledge linking tasks by one unified model. To this end, we propose a LLM-driven task-instructed representation compression, as well as a multi-view learning approach, in order to effectively adapt the instruction following and semantic understanding abilities of LLMs to referential knowledge linking. Furthermore, we also construct a new benchmark to evaluate ability of models on referential knowledge linking tasks across different scenarios. Experiments demonstrate that universal RKL is challenging for existing approaches, while the proposed framework can effectively resolve the task across various scenarios, and therefore outperforms previous approaches by a large margin.

![URL：通过任务驱动的表示压缩技术，实现知识的普遍引用链接。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.16248/x1.png)

![URL：通过任务驱动的表示压缩技术，实现知识的普遍引用链接。](../../..//opt/data/Projects/HuggingArxiv/paper_images/2404.16248/x2.png)

[Arxiv](https://arxiv.org/abs/2404.16248)