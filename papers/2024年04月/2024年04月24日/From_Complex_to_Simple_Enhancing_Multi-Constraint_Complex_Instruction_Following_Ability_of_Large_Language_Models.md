# 化繁为简：提升大型语言模型执行多约束复杂指令的能力

发布时间：2024年04月24日

`LLM应用` `人工智能`

> From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models

# 摘要

> 对于大型语言模型（LLMs）而言，准确执行复杂指令至关重要。尽管如何提升LLMs遵循多重约束指令的能力尚待深入研究，我们的研究已迈出关键一步。我们首先探讨了哪些训练数据能够有效提升LLMs处理复杂指令的能力，并发现通过多重约束的指令进行训练，尤其是在较低复杂度层面，能够显著提升LLMs对复杂指令的把握。这种提升甚至能够扩展到跨领域约束的组合。进一步地，我们提出了获取和利用这些有效训练数据的方法。最终，通过一系列广泛的实验，我们验证了这些方法在提升整体表现、训练效率以及在四种不同设置下的泛化能力方面的显著效果。

> It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. The improvement can even generalize to compositions of out-of-domain constraints. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance, training efficiency, and generalization abilities under four settings.

[Arxiv](https://arxiv.org/abs/2404.15846)