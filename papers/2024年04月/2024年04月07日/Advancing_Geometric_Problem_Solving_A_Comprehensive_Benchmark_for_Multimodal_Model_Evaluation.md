# 在几何问题求解领域取得新进展：构建全方位多模态模型评测的权威基准

发布时间：2024年04月07日

`LLM应用` `几何计算`

> Advancing Geometric Problem Solving: A Comprehensive Benchmark for Multimodal Model Evaluation

# 摘要

> 本研究向我们介绍了 MM-MATH 数据集，这是一个创新的评估标准，专为测试大型语言和多模态模型（如 GPT-4、GPT-4V 和 Claude 等）在几何计算领域的性能而设计。该数据集由 5,929 个精心构造的几何题目及其配套图像组成，反映了九年级数学的典型难度和要求。MM-MATH 的开发源于对多模态技术日益增长的兴趣和重大进展，这要求我们从仅分析结果转变为更全面地评估，包括推理和程序的正确性。尽管这些模型在多个基准测试中取得了显著进步，但我们的分析发现，它们在准确解读图像中的几何信息方面仍存在超过 60% 的显著错误。通过同时关注最终结果和解题过程的双重评估方法，我们发现当前多模态模型与人类水平之间存在明显能力差距。MM-MATH 的推出为该领域带来了三方面的贡献：它不仅是一个全面的挑战性基准，用于评估解决几何问题的能力，同时也揭示了当前模型在理解和处理文本与视觉信息方面的关键不足。我们期望通过这项工作，激发更多旨在缩小这些差距的研究与开发，推动多模态模型的能力达到新的高度。

> In this work, we present the MM-MATH dataset, a novel benchmark developed to rigorously evaluate the performance of advanced large language and multimodal models - including but not limited to GPT-4, GPT-4V, and Claude - within the domain of geometric computation. This dataset comprises 5,929 meticulously crafted geometric problems, each paired with a corresponding image, aimed at mirroring the complexity and requirements typical of ninth-grade mathematics. The motivation behind MM-MATH stems from the burgeoning interest and significant strides in multimodal technology, which necessitates a paradigm shift in assessment methodologies from mere outcome analysis to a more holistic evaluation encompassing reasoning and procedural correctness. Despite impressive gains in various benchmark performances, our analysis uncovers a persistent and notable deficiency in these models' ability to parse and interpret geometric information accurately from images, accounting for over 60% of observed errors. By deploying a dual-focused evaluation approach, examining both the end results and the underlying problem-solving processes, we unearthed a marked discrepancy between the capabilities of current multimodal models and human-level proficiency. The introduction of MM-MATH represents a tripartite contribution to the field: it not only serves as a comprehensive and challenging benchmark for assessing geometric problem-solving prowess but also illuminates critical gaps in textual and visual comprehension that current models exhibit. Through this endeavor, we aspire to catalyze further research and development aimed at bridging these gaps, thereby advancing the state of multimodal model capabilities to new heights.

[Arxiv](https://arxiv.org/abs/2404.05091)