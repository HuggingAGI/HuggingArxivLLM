# 闭环学习中生成模型的热寂困境

发布时间：2024年04月02日

`LLM理论` `机器学习` `生成式模型`

> Heat Death of Generative Models in Closed-Loop Learning

# 摘要

> 随着生成式机器学习模型的不断进步和普及，如大型语言模型在文本处理和图像生成模型的广泛应用，我们面临着一个新的问题：当这些模型生成的数据再次被用于训练时，会产生什么影响？现有研究表明，这种闭环训练过程可能会导致模型输出质量下降，甚至出现模式崩溃。尽管我们对此现象有了一定的实证了解，但由于深度学习网络的复杂性，对其背后原理的理论认识仍然有限。本文旨在深入探讨这一现象，即“生成式闭环学习”，通过分析模型在自我反馈内容影响下的动态变化，揭示在缺乏外部数据补充时，模型如何逐渐走向退化。

> Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as "knowledge", remains stable or collapses.
  Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been only limited theoretical understanding of this process, in part due to the complexity of the deep networks underlying these generative models.
  The aim of this paper is to provide insights into this process (that we refer to as "generative closed-loop learning") by studying the learning dynamics of generative models that are fed back their own produced content in addition to their original training dataset. The sampling of many of these models can be controlled via a "temperature" parameter. Using dynamical systems tools, we show that, unless a sufficient amount of external data is introduced at each iteration, any non-trivial temperature leads the model to asymptotically degenerate. In fact, either the generative distribution collapses to a small set of outputs, or becomes uniform over a large set of outputs.

[Arxiv](https://arxiv.org/abs/2404.02325)