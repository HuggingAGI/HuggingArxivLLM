# 日语预训练模型现已发布。

发布时间：2024年04月02日

`LLM应用` `人工智能` `多语言处理`

> Release of Pre-Trained Models for the Japanese Language

# 摘要

> 我们致力于构建一个人人都能享受人工智能红利的世界。为此，许多研究机构努力将研究成果公之于众。尤其是那些基于海量数据训练的大型预训练模型，展现了巨大的潜力，它们的面世对社会产生了深远的影响。然而，目前大多数模型都偏重英语，导致非英语区域在享受人工智能成果上相对滞后。为了缩小这一差距，我们推出了日语版本的生成预训练变换器（GPT）、对比语言和图像预训练（CLIP）、稳定扩散和隐藏单元双向编码器表示（HuBERT）。这些模型的推出，让用户能够无障碍地与符合日本文化特色的人工智能互动，确保了日本文化的传承，进一步推动了人工智能的普及。实验也证明，这些专为日语定制的预训练模型，在处理日语任务时能够带来丰富的成果。

> AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.

![日语预训练模型现已发布。](../../../paper_images/2404.01657/x1.png)

![日语预训练模型现已发布。](../../../paper_images/2404.01657/x2.png)

![日语预训练模型现已发布。](../../../paper_images/2404.01657/x3.png)

[Arxiv](https://arxiv.org/abs/2404.01657)