# # 摘要
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年04月28日

`LLM应用

理由：这篇论文探讨了如何将多模态大型语言模型（MLLMs）应用于城市设计评估任务，特别是通过整合专家知识来提升模型在评估街景图像步行性方面的性能。研究重点在于模型的应用和优化，属于LLM应用类别。` `城市设计` `城市规划`

> Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels

# 摘要

> 城市街道环境对公共场所的人类活动至关重要。街景图像（SVIs）与多模态大型语言模型（MLLMs）的结合，正在改变城市环境语义和视觉元素的研究、测量与评估方式。由于利用MLLMs创建自动化评估工作流的门槛较低，探索其概率模型的风险与机遇变得尤为重要。特别是，专家知识的整合对MLLMs城市设计质量评估性能的影响尚未得到充分研究。本研究旨在探讨如何将更正式和结构化的城市设计专业知识整合到MLLM（如ChatGPT-4）的输入提示中，以提升其利用街景图像（SVIs）评估建成环境步行性的能力和可靠性。我们从现有文献中收集并分类步行性指标，重点关注行人安全和吸引力两个子主题，开发相应的MLLM提示。通过分析提示中对评估标准清晰度和具体性的不同水平，我们评估MLLM在SVI步行性子主题上的表现。实验表明，MLLMs能够基于通用知识提供评估和解释，支持多模态图像-文本评估的自动化。然而，它们通常给出更乐观的分数，并可能在解释指标时出错，导致不准确的评估。通过整合专家知识，MLLM的评估性能表现出更高的稳定性和集中度。

> Urban street environments are vital to supporting human activity in public spaces. The emergence of big data, such as street view images (SVIs) combined with multimodal large language models (MLLMs), is transforming how researchers and practitioners investigate, measure, and evaluate semantic and visual elements of urban environments. Considering the low threshold for creating automated evaluative workflows using MLLMs, it is crucial to explore both the risks and opportunities associated with these probabilistic models. In particular, the extent to which the integration of expert knowledge can influence the performance of MLLMs in evaluating the quality of urban design has not been fully explored. This study sets out an initial exploration of how integrating more formal and structured representations of expert urban design knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's capability and reliability in evaluating the walkability of built environments using SVIs. We collect walkability metrics from the existing literature and categorize them using relevant ontologies. We then select a subset of these metrics, focusing on the subthemes of pedestrian safety and attractiveness, and develop prompts for the MLLM accordingly. We analyze the MLLM's ability to evaluate SVI walkability subthemes through prompts with varying levels of clarity and specificity regarding evaluation criteria. Our experiments demonstrate that MLLMs are capable of providing assessments and interpretations based on general knowledge and can support the automation of multimodal image-text evaluations. However, they generally provide more optimistic scores and can make mistakes when interpreting the provided metrics, resulting in incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative performance exhibits higher consistency and concentration.

[Arxiv](https://arxiv.org/abs/2504.21040)