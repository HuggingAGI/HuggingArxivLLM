# 大型语言模型生成的假新闻对新闻生态系统真相衰减的影响：神经新闻推荐案例研究

发布时间：2025年04月28日

`LLM应用` `在线平台`

> LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation

# 摘要

> 在线假新闻 moderation 面临新挑战：恶意利用大型语言模型（LLMs）生成假新闻。尽管现有研究表明，LLM生成的假新闻从个体角度看难以被识别，但大规模发布对新闻生态系统的影响尚未被深入研究。本研究开发了一个模拟管道和包含约56,000条各种类型生成新闻的数据集，深入探讨神经新闻推荐系统中LLM生成的假新闻的影响。研究发现，真实新闻在新闻排名中逐渐失去优势，出现“真相衰减”现象。我们从熟悉度角度解析这一现象，并揭示了困惑度与新闻排名的正相关关系。最后，探讨了LLM生成假新闻的威胁，并提出应对策略。我们呼吁各方共同应对这一新兴挑战，维护新闻生态系统的完整性。

> Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.

[Arxiv](https://arxiv.org/abs/2504.20013)