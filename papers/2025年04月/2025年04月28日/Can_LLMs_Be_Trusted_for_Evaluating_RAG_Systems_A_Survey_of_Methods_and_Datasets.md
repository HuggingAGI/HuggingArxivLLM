# # **摘要**  
    最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年04月28日

`RAG` `信息检索`

> Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets

# 摘要

> 检索增强生成（RAG）近年来发展迅猛。然而，RAG系统涉及索引、检索和生成等多个组件，其复杂性为系统性评估和质量提升带来了重大挑战。本研究通过回顾63篇学术文章，全面概述了当前先进的RAG评估方法，重点关注数据集、检索器、索引和数据库，以及生成器组件四个关键领域。我们发现，对于RAG系统每个组件，采用自动化评估方法是可行的，这需要利用一种既能生成评估数据集又能进行评估的大型语言模型（LLM）。此外，我们发现，进一步的实践研究对于为企业提供明确的指导至关重要，帮助企业了解在实施和评估RAG系统时的注意事项。通过综合RAG关键组件的评估方法，并强调创建和适应用于基准测试的特定领域数据集，我们为系统性评估方法的进步和RAG系统评估严谨性的提升做出了贡献。此外，通过探讨利用LLMs的自动化方法与人工判断之间的相互作用，我们为平衡自动化与人工输入的持续讨论做出了贡献，明确了它们在实现稳健可靠评估中的各自贡献、局限性和挑战。

> Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.

[Arxiv](https://arxiv.org/abs/2504.20119)