# CO-Bench: 用于组合优化中算法搜索的语言模型代理基准测试

发布时间：2025年04月05日

`LLM应用` `组合优化`

> CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization

# 摘要

> 尽管基于LLM的代理在软件工程和机器学习等领域备受关注，但它们在组合优化（CO）领域的潜力仍未得到充分挖掘。这一现状凸显了建立全面基准以系统研究LLM代理在解决结构化、高约束问题中潜力的迫切需求。为此，我们推出了CO-Bench——一个包含36个跨领域、多复杂度级别现实CO问题的基准套件。CO-Bench提供了结构化的问题描述和精心整理的数据集，为深入研究LLM代理提供了坚实基础。通过对比多个代理框架与传统人类设计算法的表现，我们揭示了当前方法的优劣势，并为未来研究指明了方向。CO-Bench现已开源，访问地址为https://github.com/sunnweiwei/CO-Bench。

> Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems-a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agent frameworks against established human-designed algorithms, revealing key strengths and limitations of current approaches and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.

[Arxiv](https://arxiv.org/abs/2504.04310)