# 利用多模态大型语言模型评估图形感知能力

发布时间：2025年04月05日

`LLM应用` `数据可视化` `图像分析`

> Evaluating Graphical Perception with Multimodal LLMs

# 摘要

> 多模态大型语言模型（MLLMs）在图像分析与理解领域取得了显著进展。然而，对于MLLMs而言，准确回归图表中的数值仍是一个未充分探索的领域。我们的研究聚焦于可视化任务，探讨MLLMs在图形感知任务中的表现。通过复现Cleveland和McGill具有里程碑意义的1984年实验，并将其与人类任务表现进行对比，我们深入分析了这一问题。本研究重点评估了微调和预训练模型以及零样本提示的表现，以考察它们是否能与人类的图形感知能力相匹配。研究发现表明，MLLMs在某些情况下优于人类表现，但在其他情况下则不然。我们详细介绍了所有实验的结果，以增进对MLLMs在数据可视化应用中成功与失败的理解。


> Multimodal Large Language Models (MLLMs) have remarkably progressed in analyzing and understanding images. Despite these advancements, accurately regressing values in charts remains an underexplored area for MLLMs. For visualization, how do MLLMs perform when applied to graphical perception tasks? Our paper investigates this question by reproducing Cleveland and McGill's seminal 1984 experiment and comparing it against human task performance. Our study primarily evaluates fine-tuned and pretrained models and zero-shot prompting to determine if they closely match human graphical perception. Our findings highlight that MLLMs outperform human task performance in some cases but not in others. We highlight the results of all experiments to foster an understanding of where MLLMs succeed and fail when applied to data visualization.

[Arxiv](https://arxiv.org/abs/2504.04221)