# # 高效视频理解的LMM：通过强化压缩视频立方体

发布时间：2025年04月21日

`其他` `视频处理` `视频分析`

> An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes

# 摘要

> 大型多模态模型（LMMs）在处理视频时因均匀感知视频帧而面临效率问题，尤其对于时间信息密度变化的视频。本文提出	extbf{Quicksviewer}，一种采用创新感知范式的LMM，通过Gumbel Softmax将密度不均匀的视频划分为不同大小的立方体，并对每个立方体进行统一重采样，从而实现高效视频理解。这一方法能够根据视频的时间密度动态在线压缩视频，将时空冗余减少45倍，同时支持高效训练，具有较大的感受野。我们通过三个逐步推进的阶段从语言骨干网络训练模型，每个阶段平均处理长度为420秒/1帧的长视频，得益于高效的感知能力。仅使用0.8M的视频-文本训练样本，我们的模型在准确性上比采用固定划分策略的直接基线模型高出最大8.72，证明了其有效性。在Video-MME上，Quicksviewer在适度的序列长度下达到最优性能，每帧仅需基线所需5%的token。通过这一范式，增加输入帧的数量可以清晰地揭示模型能力的幂律关系。实证验证表明，立方体网络生成的片段有助于分析视频中的连续事件。

> Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present \textbf{Quicksviewer}, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos.

[Arxiv](https://arxiv.org/abs/2504.15270)