# 可解释性人工智能在隐私与安全的可用性中的挑战与机遇

发布时间：2025年04月17日

`LLM应用` `隐私与安全` `人工智能`

> Explainable AI in Usable Privacy and Security: Challenges and Opportunities

# 摘要

> 大型语言模型（LLMs）在自动化评估和解释中的应用日益广泛，但解释质量、一致性和幻觉等问题仍是开放的研究挑战，尤其在隐私和安全等高风险领域，这些因素直接影响用户的信任和决策。本文聚焦于PRISMe——一个利用LLMs评估和解释网站隐私政策的交互式工具，探讨这些问题。基于22名参与者的研究，我们发现用户对LLM判断透明度、一致性和忠实性的担忧，以及对解释详细程度和互动方式的个性化偏好。为解决这些问题，我们提出了结构化评估标准、不确定性估计和检索增强生成（RAG）等策略。我们还发现，针对不同用户群体定制适应性解释策略是提升LLM作为评估工具效果的关键。我们的研究展示了可用隐私与安全领域在以人为中心的可解释人工智能（HCXAI）中具有广阔的应用前景，能够带来实质性的影响。

> Large Language Models (LLMs) are increasingly being used for automated evaluations and explaining them. However, concerns about explanation quality, consistency, and hallucinations remain open research challenges, particularly in high-stakes contexts like privacy and security, where user trust and decision-making are at stake. In this paper, we investigate these issues in the context of PRISMe, an interactive privacy policy assessment tool that leverages LLMs to evaluate and explain website privacy policies. Based on a prior user study with 22 participants, we identify key concerns regarding LLM judgment transparency, consistency, and faithfulness, as well as variations in user preferences for explanation detail and engagement. We discuss potential strategies to mitigate these concerns, including structured evaluation criteria, uncertainty estimation, and retrieval-augmented generation (RAG). We identify a need for adaptive explanation strategies tailored to different user profiles for LLM-as-a-judge. Our goal is to showcase the application area of usable privacy and security to be promising for Human-Centered Explainable AI (HCXAI) to make an impact.

[Arxiv](https://arxiv.org/abs/2504.12931)