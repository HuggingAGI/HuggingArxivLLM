# # GraphAttack：利用 LLM 安全机制中的表示法盲区

发布时间：2025年04月17日

`LLM应用` `人工智能安全` `网络安全`

> GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms

# 摘要

> 大型语言模型 (LLMs) 虽然配备了防止有害输出的安全机制，但这些防护措施常被“越狱”提示绕过。本文提出了一种基于图的创新方法，通过语义转换系统地生成越狱提示。我们将恶意提示表示为图结构中的节点，边表示不同转换，利用抽象意义表示 (AMR) 和资源描述框架 (RDF) 将用户目标解析为可操作的语义组件，从而绕过安全过滤器。我们通过指示 LLM 生成实现语义图中描述意图的代码，展示了特别有效的攻击向量，在对抗领先商用 LLM 时实现了高达 87% 的成功率。我们的分析表明，上下文框架和抽象尤其擅长规避安全措施，突显了当前主要关注表面模式的安全对齐技术的关键缺口。这些发现为开发抵御有结构语义攻击的更强大保障提供了见解。我们的研究为系统性地测试 LLM 安全机制提供了理论框架和实用方法论。

> Large Language Models (LLMs) have been equipped with safety mechanisms to prevent harmful outputs, but these guardrails can often be bypassed through "jailbreak" prompts. This paper introduces a novel graph-based approach to systematically generate jailbreak prompts through semantic transformations. We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters. We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs. Our analysis reveals that contextual framing and abstraction are particularly effective at circumventing safety measures, highlighting critical gaps in current safety alignment techniques that focus primarily on surface-level patterns. These findings provide insights for developing more robust safeguards against structured semantic attacks. Our research contributes both a theoretical framework and practical methodology for systematically stress-testing LLM safety mechanisms.

[Arxiv](https://arxiv.org/abs/2504.13052)