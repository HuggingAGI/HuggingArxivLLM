# 探索大型语言模型在艺术语境中的应用：批判性评论生成与心智理论评估

发布时间：2025年04月17日

`LLM应用

理由：这篇论文探讨了大型语言模型在艺术评论生成和心理状态推理方面的应用，展示了LLMs在这些具体任务中的能力和局限。研究涉及模型在艺术评论中的表现以及在ToM任务中的推理能力，属于LLM的应用层面。` `艺术评论`

> Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation

# 摘要

> 本研究聚焦于大型语言模型（LLMs）在艺术领域的两项能力：撰写艺术评论和在艺术相关情境中进行心理状态推理（即心智理论，ToM）。在艺术评论生成方面，我们创建了一个结合Noel Carroll评估框架与多种艺术批评理论的系统。模型首先被引导撰写一篇完整的评论，随后通过逐步提示生成更简短、更连贯的版本。这些AI生成的评论与人类专家的评论进行了图灵测试风格的对比评估。结果显示，人类参与者往往难以分辨两者的差异，表明只要经过精心指导，LLMs不仅能生成风格合理的评论，还能提供深入的解读。在心理状态推理部分，我们设计了基于情境的新型简单ToM任务，涵盖解释、情感和道德张力等艺术背景中常见的元素。这些任务超越了传统的错误信念测试，支持更复杂、更具社会情境性的推理形式。我们对41个近期的LLMs进行了测试，发现它们在不同任务和模型间的表现存在显著差异。特别是涉及情感或模糊情境的任务更能凸显出这些差异。综合来看，这些结果不仅揭示了LLMs在应对复杂解释性挑战时的潜力，也暴露了其认知局限。虽然我们的发现并未直接反驳所谓的“生成AI悖论”——即LLMs可在缺乏真正理解的情况下生成类似专家的输出——但它们表明，通过精心设计的提示等指导方式，LLMs可能会展现出更接近理解的行为，超出了我们的预期。

> This study explored how large language models (LLMs) perform in two areas related to art: writing critiques of artworks and reasoning about mental states (Theory of Mind, or ToM) in art-related situations. For the critique generation part, we built a system that combines Noel Carroll's evaluative framework with a broad selection of art criticism theories. The model was prompted to first write a full-length critique and then shorter, more coherent versions using a step-by-step prompting process. These AI-generated critiques were then compared with those written by human experts in a Turing test-style evaluation. In many cases, human subjects had difficulty telling which was which, and the results suggest that LLMs can produce critiques that are not only plausible in style but also rich in interpretation, as long as they are carefully guided. In the second part, we introduced new simple ToM tasks based on situations involving interpretation, emotion, and moral tension, which can appear in the context of art. These go beyond standard false-belief tests and allow for more complex, socially embedded forms of reasoning. We tested 41 recent LLMs and found that their performance varied across tasks and models. In particular, tasks that involved affective or ambiguous situations tended to reveal clearer differences. Taken together, these results help clarify how LLMs respond to complex interpretative challenges, revealing both their cognitive limitations and potential. While our findings do not directly contradict the so-called Generative AI Paradox--the idea that LLMs can produce expert-like output without genuine understanding--they suggest that, depending on how LLMs are instructed, such as through carefully designed prompts, these models may begin to show behaviors that resemble understanding more closely than we might assume.

[Arxiv](https://arxiv.org/abs/2504.12805)