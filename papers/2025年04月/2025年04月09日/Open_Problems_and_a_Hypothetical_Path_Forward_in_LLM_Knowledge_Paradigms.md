# 大语言模型知识范式中的开放问题与未来探索之路

发布时间：2025年04月09日

`LLM理论

理由：这篇论文探讨了大型语言模型的知识表示和应用中的关键问题，属于理论层面的分析和假设，因此归类为LLM理论。` `人工智能`

> Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms

# 摘要

> 知识是大型语言模型（LLMs）能力的核心。模型的知识范式决定了其如何编码和运用知识，这对性能至关重要。尽管在现有知识范式下，LLMs不断发展，但这些框架中的问题仍然限制了模型的潜力。

    本文聚焦于三个关键问题，这些问题是当前模型能力的瓶颈：（1）LLMs的知识更新难题，（2）逆向知识泛化的失败（即反转诅咒），（3）内部知识的冲突。我们梳理了针对这些问题的最新研究进展，并探讨了可能的通用解决方案。基于这些领域的观察，我们提出了一种基于上下文知识扩展的假设范式，并进一步规划了在现有技术下可行的实现路径。研究表明，这一方法有望弥补当前模型的不足，成为未来模型范式的重要方向。

    本文旨在为研究人员提供LLM知识系统进展的简要概述，同时激发下一代模型架构的创新思路。

> Knowledge is fundamental to the overall capabilities of Large Language Models (LLMs). The knowledge paradigm of a model, which dictates how it encodes and utilizes knowledge, significantly affects its performance. Despite the continuous development of LLMs under existing knowledge paradigms, issues within these frameworks continue to constrain model potential.
  This blog post highlight three critical open problems limiting model capabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of reverse knowledge generalization (the reversal curse), and (3) conflicts in internal knowledge. We review recent progress made in addressing these issues and discuss potential general solutions. Based on observations in these areas, we propose a hypothetical paradigm based on Contextual Knowledge Scaling, and further outline implementation pathways that remain feasible within contemporary techniques. Evidence suggests this approach holds potential to address current shortcomings, serving as our vision for future model paradigms.
  This blog post aims to provide researchers with a brief overview of progress in LLM knowledge systems, while provide inspiration for the development of next-generation model architectures.

[Arxiv](https://arxiv.org/abs/2504.06823)