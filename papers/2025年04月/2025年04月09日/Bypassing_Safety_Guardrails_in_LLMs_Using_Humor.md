# # 用幽默轻松突破LLMs的安全护栏

发布时间：2025年04月09日

`LLM应用` `人机交互`

> Bypassing Safety Guardrails in LLMs Using Humor

# 摘要

> 在本文中，我们展示了通过包含不安全请求的幽默提示，成功绕过大型语言模型（LLMs）的安全护栏的可能性。我们的方法无需修改不安全请求，而是遵循固定模板，简单易行，无需额外的LLMs设计提示。大量实验表明，该方法在不同LLMs中均表现出色。此外，我们发现，无论是减少还是增加幽默元素，都会影响方法效果——过多幽默可能分散LLMs处理不安全请求的注意力。因此，我们认为，LLM越狱现象发生的关键在于对不安全请求的关注与幽默元素的恰当平衡。

> In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor.

[Arxiv](https://arxiv.org/abs/2504.06577)