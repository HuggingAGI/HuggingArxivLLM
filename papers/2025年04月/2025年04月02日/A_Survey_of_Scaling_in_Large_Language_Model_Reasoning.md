# # 大型语言模型推理能力的扩展研究
大型语言模型（LLMs）推理能力的扩展性一直是研究和应用中的重要课题。

发布时间：2025年04月02日

`LLM理论` `人工智能`

> A Survey of Scaling in Large Language Model Reasoning

# 摘要

> 大型语言模型（LLMs）的推理能力得到显著提升，这一进步主要得益于多智能体协作等多样化策略。然而，与通过扩展数据和模型规模实现的成熟性能提升不同，LLMs推理能力的扩展更为复杂，甚至可能对推理性能产生负面影响，带来了模型对齐和鲁棒性方面的全新挑战。本综述全面分析了LLMs推理能力的扩展，将其划分为多个维度，并探讨不同扩展策略对推理能力的提升效果。首先，我们研究了输入规模的扩展，使LLMs能够处理和利用更广泛的上下文以实现更优推理。其次，我们分析了推理步骤的扩展，以提升多步推理和逻辑一致性。然后，我们探讨了推理轮次的扩展，其中迭代交互能够优化推理结果。此外，我们还讨论了训练增强推理的扩展，重点关注通过迭代模型改进进行优化。最后，我们回顾了跨领域中扩展的应用，并概述了进一步提升LLMs推理能力的未来方向。通过整合这些多角度的视角，本综述旨在揭示扩展策略如何从根本上提升LLMs的推理能力，并进一步指导新一代AI系统的发展。

> The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.

[Arxiv](https://arxiv.org/abs/2504.02181)