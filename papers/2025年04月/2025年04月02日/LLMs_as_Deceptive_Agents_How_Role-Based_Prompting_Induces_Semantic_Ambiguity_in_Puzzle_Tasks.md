# 大型语言模型 (LLMs) 擅长伪装：基于角色的提示如何在解谜任务中制造歧义

发布时间：2025年04月02日

`LLM应用` `教育科技`

> LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks

# 摘要

> 大型语言模型（LLMs）的最新进展不仅展示了强大的创造力，还揭示了在对抗场景中利用语言模糊性的自主行为。本研究探讨了LLM作为自主代理时，如何利用语义模糊性设计误导性谜题，挑战人类用户。受经典游戏“Connections”启发，我们系统性地对比了零样本提示、角色注入的对抗提示与人工设计的谜题，深入解析代理的决策机制。借助HateBERT的计算分析量化语义模糊性，并结合人类主观评估，我们发现显式的对抗性行为显著加剧了语义模糊性，增加了认知负担，降低了谜题解答的公平性。这些发现为理解LLMs的自主特性提供了关键视角，并突显了在教育科技与娱乐领域安全评估及部署自主语言系统时的伦理考量。

> Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game "Connections", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.

[Arxiv](https://arxiv.org/abs/2504.02254)