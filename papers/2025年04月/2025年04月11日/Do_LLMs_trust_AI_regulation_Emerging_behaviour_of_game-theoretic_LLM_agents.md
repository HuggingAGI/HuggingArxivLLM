# 大型语言模型（LLMs）是否信任AI监管？博弈论视角下LLM智能体的新兴行为

发布时间：2025年04月11日

`LLM应用` `AI开发` `AI监管`

> Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents

# 摘要

> 在AI开发生态系统中，建立信任与合作被视为推动可信AI系统采用的关键。本文通过将大型语言模型（LLM）代理嵌入到演化博弈论框架中，深入探究了AI开发者、监管者与用户之间的复杂互动，模拟他们在不同监管情景下的战略选择。演化博弈论（EGT）被用于定量建模各方面临的困境，而LLM则为研究增添了更多复杂性和细微差别，支持重复博弈和人格特质的融入。

研究揭示，战略型AI代理相较于纯博弈论代理，更倾向于采取“悲观”（不信任且缺陷）的立场。我们发现，当用户完全信任时，激励措施能够有效促进有效监管；然而，有条件的信任可能破坏“社会契约”。因此，建立用户信任与监管者声誉之间的良性反馈机制似乎成为推动开发者创建安全AI的关键。然而，这种信任的建立程度可能取决于测试中所使用的具体LLM。

我们的研究结果为AI监管系统提供了指导，并有助于预测战略型LLM代理的潜在结果，即使得其用于辅助监管本身。

> There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more "pessimistic" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the "social pact". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.

[Arxiv](https://arxiv.org/abs/2504.08640)