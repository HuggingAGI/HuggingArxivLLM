# 评估大型语言模型在医疗领域意见调查和决策中的偏见问题

发布时间：2025年04月11日

`Agent` `人工智能`

> Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare

# 摘要

> 生成式智能体（Generative agents）正越来越多地被用于基于大型语言模型（LLMs）的硅基环境中模拟人类行为。这些模拟器为研究人类行为提供了无需牺牲隐私或安全性的沙盒环境。然而，目前尚不清楚这些智能体是否能够真正代表真实个体。本研究通过比较来自"美国理解研究"（UAS）的医疗决策调查数据与生成式智能体的模拟响应，探讨这一问题。通过基于人口统计学的提示工程，我们创建了调查受访者的数字双胞胎，并分析不同LLMs在多大程度上能够再现真实世界的行为。研究发现，某些LLMs未能反映现实中的决策过程，例如预测普遍的疫苗接受度。然而，Llama 3在捕捉种族和收入差异方面更为准确，但也引入了UAS数据中不存在的偏见。这项研究凸显了生成式智能体在行为研究中的潜力，同时也强调了LLMs和提示策略带来的偏差风险。

> Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies.

[Arxiv](https://arxiv.org/abs/2504.08260)