# 大型语言模型知道什么？隐性知识作为潜在的因果解释结构

发布时间：2025年04月16日

`LLM理论` `人工智能` `认知科学`

> What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure

# 摘要

> 人们常常认为大型语言模型（LLMs）了解语言，比如它们知道巴黎是法国的首都。但LLMs究竟了解什么？或者说它们是否真的了解什么？在这篇论文中，我主张LLMs能够获得Martin Davies（1990）所定义的默会知识。而戴维斯本人认为神经网络无法获得默会知识，但我会证明LLMs的某些架构特征满足语义描述、句法结构和因果系统性的约束条件。因此，默会知识可以作为描述、解释和干预LLMs及其行为的概念框架。

> It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.

[Arxiv](https://arxiv.org/abs/2504.12187)