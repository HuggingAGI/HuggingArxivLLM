# 似曾相识：透过机器翻译评估的视角看多语言大模型评估

发布时间：2025年04月16日

`LLM理论

这篇论文探讨了多语言大语言模型（mLLMs）生成能力的评估问题，并提出了一套基于机器翻译评估经验的评估体系。它关注的是模型的评估方法和理论框架，属于对LLM理论的研究。`

> Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation

# 摘要

> 多语言大语言模型 (mLLMs) 的生成能力和语言覆盖范围正在快速提升。然而，目前针对 mLLMs 生成能力的评估实践在全面性、科学性和一致性方面仍有欠缺，这限制了其在指导 mLLM 开发中的实际作用。我们从机器翻译 (MT) 评估领域汲取经验，该领域曾面临类似挑战，并逐步形成了透明的报告标准和可靠的多语言生成模型评估体系。通过在生成评估流程的关键环节开展针对性实验，我们展示了 MT 评估中的最佳实践如何能更深入地揭示不同模型的质量差异。此外，我们还明确了构建稳健 mLLMs 元评估体系所需的关键要素，以确保评估方法本身的科学性和可靠性。我们将这些经验总结为一份实用建议清单，为 mLLM 研究与开发提供参考。

> Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.

[Arxiv](https://arxiv.org/abs/2504.11829)