# # 自我对齐：优化正则化偏好驱动的大规模视频语言模型

发布时间：2025年04月16日

`LLM应用` `视频处理` `视频问答`

> Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization

# 摘要

> 尽管大型视频语言模型（LVLMs）取得了显著进展，但它们在精细的时间理解、避免 hallucination 以及简单视频问答任务上的表现仍不尽如人意。这些局限性给其在实际应用中的安全可靠部署带来了重大挑战。为此，我们提出了一种自我对齐框架，帮助 LVLMs 从自身的错误中学习。我们的框架首先构建了一个包含优选和非优选响应对的训练集，其中非优选响应通过整合常见的错误模式生成，这些错误源于时空理解不足、伴随概念间的虚假关联、过度依赖语言线索而忽视视觉信息等原因。为了促进 LVLMs 与这些响应对的自我对齐，我们提出了一种新型偏好优化方法——Refined Regularized Preference Optimization (RRPO)，它通过子序列级别的精细奖励和基于 token 的 KL 正则化克服了 Direct Preference Optimization (DPO) 的局限性。实验表明，RRPO 在对齐精度和训练稳定性方面优于 DPO。我们的方法在视频 hallucination、短视频与长视频理解以及精细时间推理等多种任务中均表现出色，验证了其有效性。


> Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose significant challenges to their safe and reliable deployment in real-world applications. To address these limitations, we propose a self-alignment framework that enables LVLMs to learn from their own errors. Our proposed framework first obtains a training set of preferred and non-preferred response pairs, where non-preferred responses are generated by incorporating common error patterns that often occur due to inadequate spatio-temporal understanding, spurious correlations between co-occurring concepts, and over-reliance on linguistic cues while neglecting the vision modality, among others. To facilitate self-alignment of LVLMs with the constructed preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), a novel preference optimization method that utilizes sub-sequence-level refined rewards and token-wise KL regularization to address the limitations of Direct Preference Optimization (DPO). We demonstrate that RRPO achieves more precise alignment and more stable training compared to DPO. Our experiments and analysis validate the effectiveness of our approach across diverse video tasks, including video hallucination, short- and long-video understanding, and fine-grained temporal reasoning.

[Arxiv](https://arxiv.org/abs/2504.12083)