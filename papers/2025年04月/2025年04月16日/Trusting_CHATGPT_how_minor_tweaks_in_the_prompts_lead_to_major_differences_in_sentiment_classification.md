# 信任 ChatGPT：只需微调提示，情感分类效果立现差异

发布时间：2025年04月16日

`LLM理论` `社会科学` `政治学`

> Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification

# 摘要

> 如何信任像ChatGPT这样的复杂预测模型？这是当今社会科学领域的重要课题。本研究通过实证检验：提示结构的微小变化是否会影响大型语言模型GPT-4o mini的情感分析结果？

研究采用10万条西语评论（对象为四位拉美总统），让模型在10次实验中对评论进行分类（正面/负面/中性），每次实验都对提示进行细微调整。研究采用探索性与验证性分析方法，寻找分类结果中的显著差异。

研究发现，即使是最微小的提示调整（如词汇、句法或情态变化）或提示缺乏结构，都会对分类结果产生影响。模型有时会出现混淆分类、自动生成解释或使用非西语表达等不一致现象。统计分析显示，绝大多数情况下不同提示的分类结果存在显著差异，仅当提示结构高度相似时例外。

这些发现揭示了大型语言模型在分类任务中的脆弱性，其分类结果易受指令变化影响。研究还表明，提示缺乏结构化语法会增加模型产生幻觉现象的频率。讨论指出，信任大型语言模型不仅取决于其技术表现，更取决于其背后的社会制度支持。


> One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT? This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini. Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time. The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications.
  The results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications. In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish. Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar.
  These findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions. Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations. The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use.

[Arxiv](https://arxiv.org/abs/2504.12180)