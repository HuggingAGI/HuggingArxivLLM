# 基于RDF的多语言大语言模型评估的结构化质量评估表示方法

发布时间：2025年04月30日

`LLM应用

理由：这篇论文探讨了大型语言模型在多语言环境下的知识冲突问题，并提出了一种基于RDF的框架来评估模型在不同上下文条件下的表现。研究集中在模型的应用和实际性能分析上，属于LLM的应用层面。` `火灾安全` `多语言处理`

> RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations

# 摘要

> 大型语言模型（LLMs）正越来越多地充当知识接口，但系统性地评估它们在面对相互冲突信息时的可靠性仍然困难重重。我们提出了一种基于RDF的框架，专注于评估多语言LLM的知识冲突问题。我们的方法捕捉了模型在德语和英语中四种不同上下文条件下的响应（完整、不完整、冲突和无上下文信息）。这种结构化的表示使我们能够全面分析知识泄露、错误检测，以及多语言一致性。通过火灾安全领域的实验，我们揭示了上下文优先级和语言特定性能的关键模式，并证明我们的词汇足以表达在28个问题的研究中遇到的每一个评估方面。

> Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.

[Arxiv](https://arxiv.org/abs/2504.21605)