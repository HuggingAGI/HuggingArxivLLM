# CoCo-Bench：一个多任务大型语言模型的综合性代码基准测试

发布时间：2025年04月29日

`LLM应用` `软件工程` `代码生成`

> CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation

# 摘要

> 大型语言模型（LLMs）在软件工程中表现突出，尤其在代码生成与维护方面。然而，现有评估基准往往局限于单一任务，缺乏全面的框架来反映实际应用场景。为解决这一问题，我们推出了CoCo-Bench（综合代码基准测试），从代码理解、生成、修改和评审四个维度全面评估LLMs。这些维度覆盖了开发人员的核心需求，确保评估更系统、更具代表性。CoCo-Bench支持多种编程语言和不同难度的任务，并通过严格人工审核确保数据质量。实证结果显示，CoCo-Bench不仅与现有基准保持一致，还揭示了模型性能的显著差异，有效凸显了各模型的优势与不足。通过提供全面而客观的评估，CoCo-Bench为未来面向代码的LLMs研究与技术进步提供了重要参考，为该领域建立了一个可靠基准框架。

> Large language models (LLMs) play a crucial role in software engineering, excelling in tasks like code generation and maintenance. However, existing benchmarks are often narrow in scope, focusing on a specific task and lack a comprehensive evaluation framework that reflects real-world applications. To address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark), designed to evaluate LLMs across four critical dimensions: code understanding, code generation, code modification, and code review. These dimensions capture essential developer needs, ensuring a more systematic and representative evaluation. CoCo-Bench includes multiple programming languages and varying task difficulties, with rigorous manual review to ensure data quality and accuracy. Empirical results show that CoCo-Bench aligns with existing benchmarks while uncovering significant variations in model performance, effectively highlighting strengths and weaknesses. By offering a holistic and objective evaluation, CoCo-Bench provides valuable insights to guide future research and technological advancements in code-oriented LLMs, establishing a reliable benchmark for the field.

[Arxiv](https://arxiv.org/abs/2504.20673)