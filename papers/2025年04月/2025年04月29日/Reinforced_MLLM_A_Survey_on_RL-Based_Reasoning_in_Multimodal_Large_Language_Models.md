# 强化多模态大语言模型（Reinforced MLLM）：基于强化学习的多模态大语言模型推理综述

发布时间：2025年04月29日

`LLM应用

理由：这篇论文探讨了强化学习（RL）与多模态大语言模型（MLLMs）推理能力的结合，属于将强化学习应用于提升语言模型推理能力的研究，属于LLM应用类别。` `人工智能` `多模态模型`

> Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models

# 摘要

> 强化学习（RL）与多模态大语言模型（MLLMs）推理能力的结合，正成为一项极具潜力的研究方向。尽管MLLMs已显著扩展了传统大型语言模型（LLMs）的功能，使其能够处理视觉、音频和视频等多种模态，但如何实现多模态输入的稳健推理仍是待解决的关键问题。本研究系统性地回顾了基于RL的MLLMs推理领域的最新进展，涵盖关键算法设计、奖励机制创新及实际应用场景。我们重点介绍了两种主要的RL范式——无价值导向方法与基于价值的方法，并深入分析了RL如何通过优化推理轨迹和多模态信息对齐来提升推理能力。此外，我们全面概述了基准数据集、评估协议及现有局限性，并提出了未来的研究方向，以解决当前瓶颈问题，如稀疏奖励、低效跨模态推理及实际部署限制。我们的目标是为致力于在多模态时代推进基于RL推理研究的学者们提供全面且结构化的指导。

> The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains a major challenge. This survey systematically reviews recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms--value-free and value-based methods--and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Furthermore, we provide an extensive overview of benchmark datasets, evaluation protocols, and existing limitations, and propose future research directions to address current bottlenecks such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to offer a comprehensive and structured guide to researchers interested in advancing RL-based reasoning in the multimodal era.

[Arxiv](https://arxiv.org/abs/2504.21277)