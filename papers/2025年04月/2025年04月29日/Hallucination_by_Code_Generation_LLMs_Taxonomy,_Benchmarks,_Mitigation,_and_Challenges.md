# 代码生成大型语言模型的幻觉现象：分类、基准、缓解与挑战

发布时间：2025年04月29日

`LLM应用` `软件开发` `代码生成`

> Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges

# 摘要

> 大型语言模型（LLMs）的最新技术突破使其能够流畅生成源代码。开发者常利用通用型和专门针对代码的LLMs来修改现有代码，甚至从头生成完整函数。这些能力在无代码或低代码环境中同样有用，让非技术人员也能编写程序。然而，LLMs易生成幻觉，即错误的、不合逻辑且无法验证的信息，且难以识别。这一问题在生成代码时同样存在。一旦生成幻觉代码，用户往往难以发现和修复，尤其当这些幻觉仅在特定执行路径下显现时，可能长期存在于代码库中。本次调研聚焦于CodeLLMs生成的幻觉，对幻觉类型进行了分类，回顾了现有基准和缓解策略，并指出了开放性挑战。基于此，调研提出了CodeLLMs生成幻觉的检测与消除的未来研究方向。

> Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.

[Arxiv](https://arxiv.org/abs/2504.20799)