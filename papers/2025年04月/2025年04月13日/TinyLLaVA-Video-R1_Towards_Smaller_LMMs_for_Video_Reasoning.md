# TinyLLaVA-Video-R1：探索视频推理领域的更小型LMMs

发布时间：2025年04月13日

`LLM应用` `视频理解` `问答系统`

> TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning

# 摘要

> 近期，通过强化学习提升大型多模态模型（LMMs）的推理能力取得了显著进展。然而，现有大多数研究基于数学与代码等高度推理密集型数据集，并普遍选择大规模模型作为基础。我们认为，对于计算资源有限的研究者而言，探索小型模型的推理能力依然具有重要价值。此外，使模型能够在通用问答数据集上解释其推理过程同样意义重大。因此，我们推出了小型视频推理模型 TinyLLaVA-Video-R1。该模型基于参数量不超过40亿的可追溯训练视频理解模型 TinyLLaVA-Video，不仅在通用视频问答数据集上通过强化学习显著提升了推理与思考能力，还展现出“顿悟时刻”的涌现特性。此外，我们分享了一系列实验发现，旨在为未来探索小型模型的视频推理（思考）能力提供实践见解。模型已开源，访问地址为 https://github.com/ZhangXJ199/TinyLLaVA-Video-R1。
    

> Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.

[Arxiv](https://arxiv.org/abs/2504.09641)