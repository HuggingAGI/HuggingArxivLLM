# 大型语言模型的创造力达到顶峰了吗？模型间与模型内的变异性分析

发布时间：2025年04月10日

`LLM应用` `人工智能` `生成式AI`

> Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability

# 摘要

> 2023年初ChatGPT的广泛应用后，大量研究指出大型语言模型（LLMs）在创造性任务中可匹敌甚至超越人类表现。然而，LLMs是否随时间变得更富创造力，以及其创造性输出的一致性如何，仍属未知。本研究评估了14个广泛应用的LLMs，包括GPT-4、Claude等，使用了发散联想任务（DAT）和替代用途任务（AUT）两项验证性评估。令人意外的是，过去18-24个月中，LLMs的创造性表现并无提升，GPT-4的表现甚至逊于此前研究。在更广泛应用的替代用途任务中，所有模型平均表现优于人类平均水平，GPT-4和Claude表现最佳。然而，仅0.28%的LLM生成回应能达到人类创造力基准的前10%。除了模型间的差异，同一LLM在相同提示下，输出的创造性也有显著波动，从低于平均到富有原创性。此波动对创造力研究和实际应用意义重大。忽视此波动可能导致对LLMs创造潜力的误判，要么夸大要么低估其能力。提示选择对不同LLMs的影响各异。我们的发现强调了更细致评估框架的必要性，并突显了在创造性情境下使用生成式AI工具时，模型选择、提示设计及重复评估的重要性。

> Following the widespread adoption of ChatGPT in early 2023, numerous studies reported that large language models (LLMs) can match or even surpass human performance in creative tasks. However, it remains unclear whether LLMs have become more creative over time, and how consistent their creative output is. In this study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama, Grok, Mistral, and DeepSeek -- across two validated creativity assessments: the Divergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary to expectations, we found no evidence of increased creative performance over the past 18-24 months, with GPT-4 performing worse than in previous studies. For the more widely used AUT, all models performed on average better than the average human, with GPT-4o and o3-mini performing best. However, only 0.28% of LLM-generated responses reached the top 10% of human creativity benchmarks. Beyond inter-model differences, we document substantial intra-model variability: the same LLM, given the same prompt, can produce outputs ranging from below-average to original. This variability has important implications for both creativity research and practical applications. Ignoring such variability risks misjudging the creative potential of LLMs, either inflating or underestimating their capabilities. The choice of prompts affected LLMs differently. Our findings underscore the need for more nuanced evaluation frameworks and highlight the importance of model selection, prompt design, and repeated assessment when using Generative AI (GenAI) tools in creative contexts.

[Arxiv](https://arxiv.org/abs/2504.12320)