# # 大型语言模型在教育领域的多语言性能偏差

发布时间：2025年04月24日

`LLM应用

理由：这篇论文探讨了大型语言模型在教育领域的应用，特别是跨语言教育环境中的表现。研究评估了LLMs在教育任务中的实际应用，并讨论了其在不同语言中的表现差异。因此，它属于LLM应用的范畴。` `多语言`

> Multilingual Performance Biases of Large Language Models in Education

# 摘要

> 大型语言模型（LLMs）正被越来越多地应用于教育领域，其应用范围也从英语扩展到了其他语言，尽管当前的LLMs仍主要以英语为中心。本研究旨在探讨在非英语教育环境中使用LLMs是否合理。我们评估了 popular LLMs 在四项教育任务上的表现：识别学生误解、提供针对性反馈、互动辅导以及对六种语言（印地语、阿拉伯语、波斯语、泰卢固语、乌克兰语、捷克语）以及英语的翻译进行评分。结果显示，这些任务的表现与训练数据中语言的表示量有一定对应关系，资源较少的语言在任务上的表现较差。虽然模型在大多数语言中表现尚可，但与英语相比的性能下降仍然显著。因此，我们建议实践者在部署前首先验证LLM在目标语言中是否能良好地完成其教育任务。

> Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.

[Arxiv](https://arxiv.org/abs/2504.17720)