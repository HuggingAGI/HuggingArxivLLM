# DeepDistill：通过大规模难度分级数据训练增强LLM推理能力

发布时间：2025年04月24日

`LLM应用` `数学推理`

> DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training

# 摘要

> 尽管大型语言模型（LLMs）在复杂推理任务中表现突出，但学术界对其基础模型训练过程和数据质量的理解仍有待深入。为此，我们构建了一个包含334万条独特查询和4千万条蒸馏响应的大规模难度分级推理数据集。通过分析通过率和变异系数（CV），我们精准筛选出最具价值的训练数据，显著提升了模型的推理能力。值得注意的是，我们的研究发现，基于基础模型的推理训练需要采用更高的学习率才能取得最佳效果。借助这一精心筛选的数据集，我们在AIME2024数学推理基准测试中取得了79.2\%的通过率，超越了当前大多数蒸馏模型，并接近最先进水平。我们详细介绍了数据处理、难度评估及训练方法，并公开发布了所有数据集和方法，以促进开源长推理LLMs的快速发展。数据集地址：https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

> Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

[Arxiv](https://arxiv.org/abs/2504.17565)