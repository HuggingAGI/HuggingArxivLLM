# <翻译失败>

发布时间：2025年04月24日

`LLM应用` `虚拟现实` `人机交互`

> Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality

# 摘要

> 移动方式在虚拟现实环境中的用户体验中至关重要。无手柄移动方式尤其有用，因为它支持可访问性并让用户摆脱了对手持控制器的依赖。然而，传统的基于语音的方法通常依赖 rigid command sets（ rigid command sets），这限制了交互的自然性和灵活性。本研究提出了一种由大型语言模型（LLMs）驱动的新型移动技术，使用户能够利用具有上下文感知能力的自然语言在虚拟环境中导航。我们评估了三种移动方法：基于控制器的瞬移、基于语音的转向以及我们的语言模型驱动方法。我们的评估指标包括眼动数据的分析，通过 SHAP 分析的可解释机器学习，以及用于评估可用性、临场感、网络眩晕和认知负荷的标准问卷，以考察用户的注意力和参与度。研究结果表明，LLM 驱动的移动方式在可用性、临场感和网络眩晕评分方面与瞬移等 established methods（ established methods）相当，展示了其作为一种舒适、基于自然语言且无手柄的替代方案的潜力。此外，它还增强了用户在虚拟环境中的注意力，表明了更高的参与度。SHAP 分析揭示了 fixation、saccade 和 pupil-related features（ pupil-related features）在不同技术之间存在差异，表明了视觉注意力和认知处理的不同模式。总体而言，我们的方法可以促进虚拟空间中的无手柄移动，特别是在支持可访问性方面。


> Locomotion plays a crucial role in shaping the user experience within virtual reality environments. In particular, hands-free locomotion offers a valuable alternative by supporting accessibility and freeing users from reliance on handheld controllers. To this end, traditional speech-based methods often depend on rigid command sets, limiting the naturalness and flexibility of interaction. In this study, we propose a novel locomotion technique powered by large language models (LLMs), which allows users to navigate virtual environments using natural language with contextual awareness. We evaluate three locomotion methods: controller-based teleportation, voice-based steering, and our language model-driven approach. Our evaluation measures include eye-tracking data analysis, including explainable machine learning through SHAP analysis as well as standardized questionnaires for usability, presence, cybersickness, and cognitive load to examine user attention and engagement. Our findings indicate that the LLM-driven locomotion possesses comparable usability, presence, and cybersickness scores to established methods like teleportation, demonstrating its novel potential as a comfortable, natural language-based, hands-free alternative. In addition, it enhances user attention within the virtual environment, suggesting greater engagement. Complementary to these findings, SHAP analysis revealed that fixation, saccade, and pupil-related features vary across techniques, indicating distinct patterns of visual attention and cognitive processing. Overall, we state that our method can facilitate hands-free locomotion in virtual spaces, especially in supporting accessibility.

[Arxiv](https://arxiv.org/abs/2504.17331)