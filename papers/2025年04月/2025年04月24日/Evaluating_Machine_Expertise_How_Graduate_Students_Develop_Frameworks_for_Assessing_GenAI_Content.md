# 研究生如何构建评估生成式AI内容的框架

发布时间：2025年04月24日

`LLM应用

理由：这篇论文探讨了研究生在与大型语言模型（LLMs）互动时如何构建评估机器生成专业知识的框架，属于LLM的应用层面研究。` `网络科学`

> Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content

# 摘要

> 本文研究了研究生在与大型语言模型（LLMs）的基于网络互动中构建评估机器生成专业知识框架的过程。通过整合调查、LLM互动记录及14名研究生深度访谈的定性研究，我们发现这些新兴专业人士在评估和使用AI生成内容时展现出独特模式。研究发现，学生的评估框架主要受三个因素影响：职业身份、验证能力和系统导航经验。他们根据职业身份的核心领域进行选择性保护，如管理者保留概念性工作、设计师保护创意流程、程序员把控核心技术专长，而非简单地全盘接受或拒绝LLM的输出。这些评估框架还受到学生验证不同类型内容的能力以及他们在复杂系统中导航经验的影响。这项研究为网络科学领域提供了新见解，揭示了人与生成AI互动的新模式，并为平台如何更好地支持用户在AI中介的网络环境中构建有效评估机器生成专业知识框架提供了建议。

> This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we identify patterns in how these emerging professionals assess and engage with AI-generated content. Our findings reveal that students construct evaluation frameworks shaped by three main factors: professional identity, verification capabilities, and system navigation experience. Rather than uniformly accepting or rejecting LLM outputs, students protect domains central to their professional identities while delegating others--with managers preserving conceptual work, designers safeguarding creative processes, and programmers maintaining control over core technical expertise. These evaluation frameworks are further influenced by students' ability to verify different types of content and their experience navigating complex systems. This research contributes to web science by highlighting emerging human-genAI interaction patterns and suggesting how platforms might better support users in developing effective frameworks for evaluating machine-generated expertise signals in AI-mediated web environments.

[Arxiv](https://arxiv.org/abs/2504.17964)