# # 长文本问答的实证评估研究
长文本问答的实证评估研究

发布时间：2025年04月25日

`LLM应用` `问答系统`

> An Empirical Study of Evaluating Long-form Question Answering

# 摘要

> \Ac{LFQA} 专注于生成复杂问题的长篇答案。这种场景在评估方面提供了很大的灵活性，同时也带来了显著的挑战。大多数评估依赖于基于字符串或n-gram匹配的确定性指标，而基于大型语言模型的长篇回答评估的可靠性仍相对未被探索。我们通过深入研究长篇回答评估来填补这一空白，提出以下研究问题：(i) 现有的自动评估指标在多大程度上可以替代人工评估？(ii) 与人工评估相比，现有评估指标的局限性是什么？(iii) 如何提高现有评估方法的有效性和鲁棒性？

我们收集了5,236个由不同大型语言模型生成的事实性和非事实性长篇答案，并对其中2,079个进行了人工评估，重点关注准确性和信息量。随后，我们通过评估这些答案，分析了自动评估指标与人工评估之间的一致性，从而研究了自动评估指标的性能。我们发现，答案的风格、长度以及问题类别可能会对自动评估指标产生偏差。然而，细粒度的评估在某些指标上有助于缓解这一问题。我们的发现对利用大型语言模型进行长篇问答评估具有重要意义。所有代码和数据集均可在 https://github.com/bugtig6351/lfqa_evaluation 获取。

> \Ac{LFQA} aims to generate lengthy answers to complex questions. This scenario presents great flexibility as well as significant challenges for evaluation. Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored. We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations? (ii) What are the limitations of existing evaluation metrics compared to human evaluations? (iii) How can the effectiveness and robustness of existing evaluation methods be improved? We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness. Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations. We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics. However, fine-grained evaluation helps mitigate this issue on some metrics. Our findings have important implications for the use of large language models for evaluating long-form question answering. All code and datasets are available at https://github.com/bugtig6351/lfqa_evaluation.

[Arxiv](https://arxiv.org/abs/2504.18413)