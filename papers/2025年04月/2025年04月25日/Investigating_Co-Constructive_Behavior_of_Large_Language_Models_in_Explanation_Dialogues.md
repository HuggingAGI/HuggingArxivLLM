# 探究大型语言模型在解释性对话中的协作构建行为

发布时间：2025年04月25日

`LLM应用` `人工智能` `人机协作`

> Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues

# 摘要

> 可解释人工智能的核心在于能够生成让解释对象理解的解释。由于理解依赖于解释对象的背景和需求，近期研究集中于协作解释对话，其中解释者会实时监测解释对象的理解情况，并动态调整解释内容。我们探讨了大型语言模型（LLMs）在协作解释对话中作为解释者的能力。具体而言，我们开展了一项用户研究，其中解释对象与LLMs进行了互动，其中部分LLMs被指示以协作方式解释预定义的主题。我们评估了解释对象在对话前后对主题的理解程度，以及他们对LLMs协作行为的感知。结果显示，当前的LLMs展现了一些协作行为，例如提出验证性问题，这些行为能够促进解释对象的参与度并提升他们对主题的理解。然而，它们在有效监测当前理解水平并相应地构建解释方面的能力仍然有限。

> The ability to generate explanations that are understood by explainees is the quintessence of explainable artificial intelligence. Since understanding depends on the explainee's background and needs, recent research has focused on co-constructive explanation dialogues, where the explainer continuously monitors the explainee's understanding and adapts explanations dynamically. We investigate the ability of large language models (LLMs) to engage as explainers in co-constructive explanation dialogues. In particular, we present a user study in which explainees interact with LLMs, of which some have been instructed to explain a predefined topic co-constructively. We evaluate the explainees' understanding before and after the dialogue, as well as their perception of the LLMs' co-constructive behavior. Our results indicate that current LLMs show some co-constructive behaviors, such as asking verification questions, that foster the explainees' engagement and can improve understanding of a topic. However, their ability to effectively monitor the current understanding and scaffold the explanations accordingly remains limited.

[Arxiv](https://arxiv.org/abs/2504.18483)