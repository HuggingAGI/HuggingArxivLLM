# # 应用与优化
基于提示微调的大模型在事实核查价值评估中的应用与优化

发布时间：2025年04月25日

`LLM应用` `信息管理`

> Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation

# 摘要

> 面对全球化与信息化背景下日益严重的虚假信息问题，本文提出了一种基于提示调优的事实核查价值评估分类方法。我们从方法论层面构建了一个事实核查价值评估模型，并利用提示调优技术。通过设计的提示模板应用于大型语言模型，我们建立了上下文学习机制，并借助提示调优技术来提高对声明是否具备事实核查价值判断的准确性，尤其是在处理有限或未标注数据时。通过在公共数据集上的广泛实验，我们证明了所提出的方法在事实核查价值评估分类任务中，无论是经典的预训练模型如BERT，还是近期流行的大型模型如GPT-3.5和GPT-4，均能超越或与多种基线方法相匹配。实验结果表明，本文所提出的基于提示调优的方法在F1分数和准确率等评估指标上展现出一定优势，从而有效验证了其在事实核查价值评估任务中的有效性与先进性。

> In response to the growing problem of misinformation in the context of globalization and informatization, this paper proposes a classification method for fact-check-worthiness estimation based on prompt tuning. We construct a model for fact-check-worthiness estimation at the methodological level using prompt tuning. By applying designed prompt templates to large language models, we establish in-context learning and leverage prompt tuning technology to improve the accuracy of determining whether claims have fact-check-worthiness, particularly when dealing with limited or unlabeled data. Through extensive experiments on public datasets, we demonstrate that the proposed method surpasses or matches multiple baseline methods in the classification task of fact-check-worthiness estimation assessment, including classical pre-trained models such as BERT, as well as recent popular large models like GPT-3.5 and GPT-4. Experiments show that the prompt tuning-based method proposed in this study exhibits certain advantages in evaluation metrics such as F1 score and accuracy, thereby effectively validating its effectiveness and advancement in the task of fact-check-worthiness estimation.

[Arxiv](https://arxiv.org/abs/2504.18104)