# # Auto-SLURP：智能个人助理多智能体框架评估基准数据集

发布时间：2025年04月25日

`LLM应用` `智能个人助手`

> Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant

# 摘要

> 近年来，大型语言模型（LLMs）驱动的多智能体框架发展迅猛。然而，专门用于评估其性能的基准数据集仍然匮乏。为解决这一问题，我们推出了Auto-SLURP，这是一个专注于评估LLM驱动的多智能体框架在智能个人助手领域表现的基准数据集。Auto-SLURP基于最初为自然语言理解任务设计的SLURP数据集，通过重新标注数据并引入模拟服务器与外部服务，显著提升了评估能力。这一改进使我们能够构建一个全面的端到端评估体系，涵盖语言理解、任务执行和响应生成。实验结果表明，Auto-SLURP对现有先进框架提出了严峻挑战，凸显出打造真正可靠且智能的多智能体个人助手仍是一项艰巨的任务。数据集及代码已开源，地址为https://github.com/lorashen/Auto-SLURP/。

> In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.

[Arxiv](https://arxiv.org/abs/2504.18373)