# 污名化和不当回应阻碍了大型语言模型安全替代心理健康服务提供者。

发布时间：2025年04月25日

`LLM应用

理由：这篇论文探讨了大型语言模型（LLM）在心理健康治疗中的应用及其潜在问题，属于LLM的实际应用场景研究。` `心理健康` `人工智能`

> Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers

# 摘要

> 是否应该让大型语言模型 (LLM) 担任治疗师？本文探讨了用 LLM 替代心理健康服务提供者这一在科技初创和研究领域备受关注的话题。我们分析了各大医疗机构的治疗指南，识别出治疗关系中的关键要素，如治疗师与客户之间的治疗联盟。通过实验测试了当前 LLM（如 `gpt-4o`）的表现，发现它们在以下方面与医学界最佳实践相悖：1) 对心理疾病患者存在偏见；2) 在自然治疗场景中对某些常见且关键的情况反应不当——例如，LLM 倾向于迎合客户，甚至鼓励其妄想思维。这些现象在更强大、更新的 LLM 中依然存在，表明现有安全措施可能未能解决这些问题。此外，我们还指出 LLM 成为治疗师所面临的根本性障碍，例如治疗联盟需要人类特有的属性（如身份认同和情感投入）。基于此，我们得出结论：LLM 不应替代治疗师，但在临床治疗中仍可发挥其他辅助作用。

> Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy.

[Arxiv](https://arxiv.org/abs/2504.18412)