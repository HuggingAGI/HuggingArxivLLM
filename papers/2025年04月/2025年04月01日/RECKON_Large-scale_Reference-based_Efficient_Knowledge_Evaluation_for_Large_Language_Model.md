# # RECKON: 针对大型语言模型的大规模高效知识评估方法

发布时间：2025年04月01日

`LLM应用` `知识管理` `模型评估`

> RECKON: Large-scale Reference-based Efficient Knowledge Evaluation for Large Language Model

# 摘要

> 随着大型语言模型（LLMs）的快速发展，高效的知识评估变得至关重要，以验证其能力。传统方法依赖于基准测试，但在资源消耗和信息损失方面存在局限性。我们提出了一种名为大规模参考基础的高效知识评估方法（RECKON），该方法直接利用参考数据来评估模型。RECKON将非结构化数据组织成易于管理的单元，并为每个聚类生成针对性问题，从而提高评估的准确性和效率。实验结果表明，与传统方法相比，RECKON将资源消耗降低了56.5%，同时在世界知识、代码、法律和生物医学等各类数据集上实现了超过97%的准确率。代码可在https://github.com/MikeGu721/reckon获取。

> As large language models (LLMs) advance, efficient knowledge evaluation becomes crucial to verifying their capabilities. Traditional methods, relying on benchmarks, face limitations such as high resource costs and information loss. We propose the Large-scale Reference-based Efficient Knowledge Evaluation for Large Language Model (RECKON), which directly uses reference data to evaluate models. RECKON organizes unstructured data into manageable units and generates targeted questions for each cluster, improving evaluation accuracy and efficiency. Experimental results show that RECKON reduces resource consumption by 56.5% compared to traditional methods while achieving over 97% accuracy across various domains, including world knowledge, code, legal, and biomedical datasets. Code is available at https://github.com/MikeGu721/reckon

[Arxiv](https://arxiv.org/abs/2504.00756)