# 经验语义信息与大脑对齐：多模态模型是否更胜一筹？

发布时间：2025年04月01日

`LLM理论` `人工智能` `神经科学`

> Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?

# 摘要

> 在计算语言学中，人们普遍认为，多模态模型的文本表示比纯语言模型更丰富、更贴近人类，因为它们以图像或音频为依托，这与人类语言植根于现实经验相似。然而，验证这一观点的实证研究却寥寥无几。我们通过对比多模态模型与纯语言模型在捕获经验信息——基于现有规范的“经验模型”定义——以及与人类fMRI反应的对齐程度，填补了这一研究空白。结果显示，令人意外的是，纯语言模型在这两个方面均优于多模态模型。此外，它们还掌握了更多超越经验模型的、与大脑相关的独特语义信息。总的来说，本研究凸显了开发能更好地融合多模态数据源提供的互补语义信息的计算模型的迫切需求。

> A common assumption in Computational Linguistics is that text representations learnt by multimodal models are richer and more human-like than those by language-only models, as they are grounded in images or audio -- similar to how human language is grounded in real-world experiences. However, empirical studies checking whether this is true are largely lacking. We address this gap by comparing word representations from contrastive multimodal models vs. language-only ones in the extent to which they capture experiential information -- as defined by an existing norm-based 'experiential model' -- and align with human fMRI responses. Our results indicate that, surprisingly, language-only models are superior to multimodal ones in both respects. Additionally, they learn more unique brain-relevant semantic information beyond that shared with the experiential model. Overall, our study highlights the need to develop computational models that better integrate the complementary semantic information provided by multimodal data sources.

[Arxiv](https://arxiv.org/abs/2504.00942)