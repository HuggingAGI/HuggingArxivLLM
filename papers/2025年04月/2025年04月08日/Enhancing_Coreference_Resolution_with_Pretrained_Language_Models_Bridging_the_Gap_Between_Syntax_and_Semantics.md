# # 增强共指消解
预训练语言模型助力共指消解，弥合句法与语义之间的鸿沟

发布时间：2025年04月08日

`LLM应用` `人工智能`

> Enhancing Coreference Resolution with Pretrained Language Models: Bridging the Gap Between Syntax and Semantics

# 摘要

> 大型语言模型在多种自然语言处理任务中取得了突破性进展，包括核心ference解析。然而，传统方法往往难以有效区分指代关系，原因在于句法和语义信息未能有效整合。本研究提出了一种创新框架，旨在通过利用预训练语言模型来提升核心ference解析性能。我们的方法结合句法分析与语义角色标注，以更精准地捕捉指代关系的细微差别。通过采用先进的预训练模型获取上下文嵌入，并运用注意力机制进行微调，我们提升了核心ference任务的性能。在多种数据集上的实验结果表明，我们的方法超越了传统核心ference解析系统，在消除指代歧义方面达到了显著的准确率。这一进展不仅改善了核心ference解析的结果，还对依赖于精准指代理解的其他自然语言处理任务产生了积极影响。

> Large language models have made significant advancements in various natural language processing tasks, including coreference resolution. However, traditional methods often fall short in effectively distinguishing referential relationships due to a lack of integration between syntactic and semantic information. This study introduces an innovative framework aimed at enhancing coreference resolution by utilizing pretrained language models. Our approach combines syntax parsing with semantic role labeling to accurately capture finer distinctions in referential relationships. By employing state-of-the-art pretrained models to gather contextual embeddings and applying an attention mechanism for fine-tuning, we improve the performance of coreference tasks. Experimental results across diverse datasets show that our method surpasses conventional coreference resolution systems, achieving notable accuracy in disambiguating references. This development not only improves coreference resolution outcomes but also positively impacts other natural language processing tasks that depend on precise referential understanding.

[Arxiv](https://arxiv.org/abs/2504.05855)