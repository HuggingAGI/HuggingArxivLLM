# 语言模型与知识蒸馏中的多义词嵌入

发布时间：2025年04月08日

`LLM理论

理由：这篇论文探讨了大型语言模型（LLMs）中上下文嵌入的改进方法，提出了多义词嵌入和知识蒸馏方法，属于对模型内部机制和训练方法的理论研究。` `机器学习`

> Multi-Sense Embeddings for Language Models and Knowledge Distillation

# 摘要

> 基于Transformer的大型语言模型 (LLMs) 依赖于上下文嵌入，这些嵌入会根据同一个标记周围的上下文生成不同的（连续）表示。然而，词语和标记通常只有有限数量的含义（或意义）。我们提出多义词嵌入作为每个标记的替代方案，以捕捉其在语言中的多种用法。为了构建一个意义嵌入字典，我们对大型语言模型生成的嵌入应用聚类算法，并将簇中心视为代表性的意义嵌入。此外，我们提出了一种新颖的知识蒸馏方法，该方法利用意义字典从一个更大基础 LLM 模型中学习一个更小的学生模型，模仿其意义，同时节省了显著的存储空间和推理时间，同时保持了具有竞争力的性能。通过在各种基准上的彻底实验，我们展示了我们意义嵌入和知识蒸馏方法的有效性。我们分享我们的代码在 https://github.com/Qitong-Wang/SenseDict

> Transformer-based large language models (LLMs) rely on contextual embeddings which generate different (continuous) representations for the same token depending on its surrounding context. Nonetheless, words and tokens typically have a limited number of senses (or meanings). We propose multi-sense embeddings as a drop-in replacement for each token in order to capture the range of their uses in a language. To construct a sense embedding dictionary, we apply a clustering algorithm to embeddings generated by an LLM and consider the cluster centers as representative sense embeddings. In addition, we propose a novel knowledge distillation method that leverages the sense dictionary to learn a smaller student model that mimics the senses from the much larger base LLM model, offering significant space and inference time savings, while maintaining competitive performance. Via thorough experiments on various benchmarks, we showcase the effectiveness of our sense embeddings and knowledge distillation approach. We share our code at https://github.com/Qitong-Wang/SenseDict

[Arxiv](https://arxiv.org/abs/2504.06036)