# 人工智能搜索中的人类信任：大规模实证研究

发布时间：2025年04月08日

`LLM应用` `搜索引擎` `用户信任`

> Human Trust in AI Search: A Large-Scale Experiment

# 摘要

> # 摘要
大型语言模型（LLMs）正在重塑生成式搜索引擎，进而深刻影响人类的信息检索与决策行为。人类对生成式人工智能（GenAI）的信任程度，直接影响着我们的购物选择、投票行为和健康决策。然而，当前尚无研究明确揭示生成式搜索设计对人类信任的具体影响机制。

我们执行了横跨七个国家的约12,000次搜索查询，生成了约80,000条实时的生成式AI和传统搜索结果，以此深入洞察当前全球范围内生成式搜索的普及程度。随后，我们开展了一项针对具有美国人口代表性的大规模样本进行的预注册随机对照实验，发现尽管参与者平均而言对生成式搜索的信任度低于传统搜索，但参考链接和引文的加入显著增强了用户对生成式搜索的信任，即便这些链接和引文存在错误或虚构。

实验结果表明：
- 当生成式AI对其结论的置信度被明确标示时，无论是高置信还是低置信，都会降低用户对生成内容的信任和分享意愿。
- 正面的社会反馈会增加用户对生成式AI的信任，而负面反馈则会降低信任。
- 不同主题、用户人口统计特征、教育背景、行业从业经历和GenAI使用经验等因素都会影响用户对生成式AI的信任程度，这揭示了哪些亚群体最易受到生成式AI误导的影响。

信任程度直接影响用户行为：那些更信任生成式AI的用户会点击更多结果并减少对生成式搜索结果的评估时间。这些发现为设计更安全、更有效地弥合AI“信任鸿沟”的生成式AI提供了重要方向。


> Large Language Models (LLMs) increasingly power generative search engines which, in turn, drive human information seeking and decision making at scale. The extent to which humans trust generative artificial intelligence (GenAI) can therefore influence what we buy, how we vote and our health. Unfortunately, no work establishes the causal effect of generative search designs on human trust. Here we execute ~12,000 search queries across seven countries, generating ~80,000 real-time GenAI and traditional search results, to understand the extent of current global exposure to GenAI search. We then use a preregistered, randomized experiment on a large study sample representative of the U.S. population to show that while participants trust GenAI search less than traditional search on average, reference links and citations significantly increase trust in GenAI, even when those links and citations are incorrect or hallucinated. Uncertainty highlighting, which reveals GenAI's confidence in its own conclusions, makes us less willing to trust and share generative information whether that confidence is high or low. Positive social feedback increases trust in GenAI while negative feedback reduces trust. These results imply that GenAI designs can increase trust in inaccurate and hallucinated information and reduce trust when GenAI's certainty is made explicit. Trust in GenAI varies by topic and with users' demographics, education, industry employment and GenAI experience, revealing which sub-populations are most vulnerable to GenAI misrepresentations. Trust, in turn, predicts behavior, as those who trust GenAI more click more and spend less time evaluating GenAI search results. These findings suggest directions for GenAI design to safely and productively address the AI "trust gap."

[Arxiv](https://arxiv.org/abs/2504.06435)