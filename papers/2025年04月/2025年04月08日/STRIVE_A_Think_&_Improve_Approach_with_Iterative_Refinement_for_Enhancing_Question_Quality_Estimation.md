# STRIVE: 一种通过思考改进的迭代精炼方法，用于提升问题质量评估的效果

发布时间：2025年04月08日

`LLM应用

理由：这篇论文探讨了如何利用大型语言模型（LLMs）来自动评估问题质量，特别是在教育领域的应用。它提出了一种名为STRIVE的新方法，利用LLMs来生成多评估结果，并通过迭代审查和回应持续改进，以提高评估的准确性和深度。该研究的重点是将LLMs应用于实际问题，特别是教育实践中的问题质量评估，因此归类为LLM应用。` `教育技术`

> STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation

# 摘要

> 自动评估问题质量对教育工作者至关重要，因为它能节省时间、保证一致并提供即时反馈以优化教学材料。我们提出了一种名为 STRIVE 的新方法，利用一系列大型语言模型（LLMs）实现自动问题评估。该方法旨在提升问题质量评估的准确性和深度，最终支持多样化学习者并优化教育实践。STRIVE 通过基于问题的优缺点生成多评估结果，选择最佳解决方案，并通过迭代审查和回应持续改进，直到评估指标收敛。这种方法通过自动化问题质量评估，显著提升了估算效果。相关性得分显示，STRIVE 相比基线方法，能更好地与人工判断相关。错误分析表明，STRIVE 在相关性和恰当性等指标上显著优于人工判断。

> Automatically assessing question quality is crucial for educators as it saves time, ensures consistency, and provides immediate feedback for refining teaching materials. We propose a novel methodology called STRIVE (Structured Thinking and Refinement with multiLLMs for Improving Verified Question Estimation) using a series of Large Language Models (LLMs) for automatic question evaluation. This approach aims to improve the accuracy and depth of question quality assessment, ultimately supporting diverse learners and enhancing educational practices. The method estimates question quality in an automated manner by generating multiple evaluations based on the strengths and weaknesses of the provided question and then choosing the best solution generated by the LLM. Then the process is improved by iterative review and response with another LLM until the evaluation metric values converge. This sophisticated method of evaluating question quality improves the estimation of question quality by automating the task of question quality evaluation. Correlation scores show that using this proposed method helps to improve correlation with human judgments compared to the baseline method. Error analysis shows that metrics like relevance and appropriateness improve significantly relative to human judgments by using STRIVE.

[Arxiv](https://arxiv.org/abs/2504.05693)