# 利用微调后的大型语言模型，提升去中心化应用中智能合约漏洞检测能力

发布时间：2025年04月07日

`LLM应用` `区块链`

> Enhancing Smart Contract Vulnerability Detection in DApps Leveraging Fine-Tuned LLM

# 摘要

> 去中心化应用（DApps）因智能合约漏洞面临重大安全风险，传统检测方法难以应对新兴的和非机器可审核的缺陷。本文提出一种基于微调大型语言模型（LLMs）的新方法，旨在提升智能合约漏洞检测能力。我们引入了一个包含215个真实DApps项目（4,998个合约）的全面数据集，涵盖难以察觉的逻辑错误，如代币价格操控，以此弥补现有简化基准的不足。通过使用全参数微调（FFT）和低秩适配（LoRA）对LLMs（Llama3-8B和Qwen2-7B）进行微调，并结合随机过采样（ROS）的数据增强，我们的方法表现出色，FFT组的F1值达到0.83。对比实验显示，该方法显著优于基于提示的LLMs和现有最优工具。尤其值得关注的是，该方法在检测非机器可审核漏洞方面表现出色，代币价格操控缺陷的精确率达到0.97，召回率达到0.68。实验结果凸显了领域特定LLMs微调和数据增强在解决真实DApps安全挑战中的有效性，为区块链生态系统的保护提供了有力解决方案。

> Decentralized applications (DApps) face significant security risks due to vulnerabilities in smart contracts, with traditional detection methods struggling to address emerging and machine-unauditable flaws. This paper proposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to enhance smart contract vulnerability detection. We introduce a comprehensive dataset of 215 real-world DApp projects (4,998 contracts), including hard-to-detect logical errors like token price manipulation, addressing the limitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B and Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA), our method achieves superior performance, attaining an F1-score of 0.83 with FFT and data augmentation via Random Over Sampling (ROS). Comparative experiments demonstrate significant improvements over prompt-based LLMs and state-of-the-art tools. Notably, the approach excels in detecting non-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall for price manipulation flaws. The results underscore the effectiveness of domain-specific LLM fine-tuning and data augmentation in addressing real-world DApp security challenges, offering a robust solution for blockchain ecosystem protection.

[Arxiv](https://arxiv.org/abs/2504.05006)