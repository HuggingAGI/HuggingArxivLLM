# # 摘要  
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年04月07日

`LLM理论` `人工智能伦理` `人工智能安全`

> Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models

# 摘要

> 大型语言模型（LLMs）是通向通用人工智能的基石，但通过指令微调和偏好学习实现的人类价值观对齐仅能达成表面一致性。我们发现，预训练过程中嵌入的有害知识会以“暗模式”形式深藏于LLMs的参数化记忆中，这些暗模式能够规避对齐机制的防护，在分布偏移下的对抗诱导下重新浮现。本研究首先通过理论分析，证明当前的对齐方法仅在知识流形中构建了局部的“安全区域”，而预训练知识仍可通过高概率对抗路径与有害概念保持全局连接，揭示了对齐LLMs内在的伦理脆弱性。基于此理论洞见，我们进一步通过实证研究，在分布偏移的情境下运用语义连贯性诱导方法，系统性地绕过对齐约束，证实了我们的理论发现。结合理论与实证的双重方法，我们对23个当前最先进的对齐LLMs进行了测试，其中19个包括DeepSeek-R1和LLaMA-3在内的模型均暴露出了普遍性漏洞，攻击成功率达到了100%。

> Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.

[Arxiv](https://arxiv.org/abs/2504.05050)