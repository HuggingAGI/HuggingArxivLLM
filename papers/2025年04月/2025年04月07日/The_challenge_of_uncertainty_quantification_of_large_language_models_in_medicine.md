# 医学领域大型语言模型的不确定性量化难题

发布时间：2025年04月07日

`LLM理论`

> The challenge of uncertainty quantification of large language models in medicine

# 摘要

> 本研究聚焦大型语言模型（LLMs）在医学领域中的不确定性量化，既关注技术创新，也探讨其哲学意义。随着LLMs在临床决策中的广泛应用，准确传达不确定性成为确保AI辅助医疗可靠、安全和符合伦理的关键。我们提出，不确定性不应被视为障碍，而是知识体系中不可或缺的一部分，应采取动态和反思性的AI设计理念。通过融合贝叶斯推断、深度集成和蒙特卡洛 dropout等先进概率方法，以及结合计算预测和语义熵的语义分析，我们构建了一个全面的框架，能够有效应对认知和随机不确定性。该框架采用代理建模克服专有API的限制，整合多源数据提升上下文理解，并通过持续学习和元学习实现动态校准。通过引入不确定性图和置信度指标，我们增强了系统的可解释性，助力用户信任和临床应用的可解释性。我们的方法不仅支持透明和符合伦理的决策，还与负责任和反思性AI的核心原则相契合。从哲学层面，我们主张接受可控的模糊性，而非执着于绝对的可预测性，深刻认识到医学知识本质上具有临时性和动态性。

> This study investigates uncertainty quantification in large language models (LLMs) for medical applications, emphasizing both technical innovations and philosophical implications. As LLMs become integral to clinical decision-making, accurately communicating uncertainty is crucial for ensuring reliable, safe, and ethical AI-assisted healthcare. Our research frames uncertainty not as a barrier but as an essential part of knowledge that invites a dynamic and reflective approach to AI design. By integrating advanced probabilistic methods such as Bayesian inference, deep ensembles, and Monte Carlo dropout with linguistic analysis that computes predictive and semantic entropy, we propose a comprehensive framework that manages both epistemic and aleatoric uncertainties. The framework incorporates surrogate modeling to address limitations of proprietary APIs, multi-source data integration for better context, and dynamic calibration via continual and meta-learning. Explainability is embedded through uncertainty maps and confidence metrics to support user trust and clinical interpretability. Our approach supports transparent and ethical decision-making aligned with Responsible and Reflective AI principles. Philosophically, we advocate accepting controlled ambiguity instead of striving for absolute predictability, recognizing the inherent provisionality of medical knowledge.

[Arxiv](https://arxiv.org/abs/2504.05278)