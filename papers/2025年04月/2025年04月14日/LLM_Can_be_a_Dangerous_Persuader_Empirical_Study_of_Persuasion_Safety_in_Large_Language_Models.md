# LLM 可能成为危险的劝说者：关于大型语言模型劝说安全的实证研究

发布时间：2025年04月14日

`LLM应用` `对话安全`

> LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models

# 摘要

> 大型语言模型（LLMs）近期的突破性进展使其具备了接近人类水平的劝说能力。然而，这种能力的提升也带来了LLM驱动劝说的安全风险，尤其是其可能通过操纵、欺骗、利用弱点等不道德手段施加不当影响。本研究从两个关键维度系统考察了LLM的劝说安全问题：(1) LLMs能否恰当拒绝不道德劝说任务，并在执行过程中规避不道德策略，包括初始劝说目标看似中立的情形；(2) 个性特质和外部压力等影响因素如何塑造其行为模式。为此，我们推出了PersuSafety——首个全面的劝说安全评估框架，包含劝说场景创建、对话模拟和安全评估三大阶段。该框架覆盖了6大类不道德劝说主题及15种常见不道德策略。通过对8个广泛应用的LLMs进行大规模实验，我们发现绝大多数模型存在显著安全风险，表现为无法识别有害劝说任务并滥用多种不道德策略。本研究呼吁业界更加关注提升劝说等目标导向对话场景中的安全对齐性。

> Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.

[Arxiv](https://arxiv.org/abs/2504.10430)