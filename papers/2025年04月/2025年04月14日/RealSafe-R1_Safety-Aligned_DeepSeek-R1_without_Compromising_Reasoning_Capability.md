# RealSafe-R1：安全对齐的DeepSeek-R1，同时保持推理能力不妥协

发布时间：2025年04月14日

`LLM应用` `模型安全` `人工智能应用`

> RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability

# 摘要

> 大型推理模型（LRMs）如 OpenAI o1 和 DeepSeek-R1 正快速发展，并在数学和编程等复杂推理任务中取得了突破性表现。然而，开源的 R1 模型在广泛应用中引发了安全担忧，例如倾向于响应恶意查询，这严重影响了这些强大模型在实际应用中的效果。本文中，我们介绍了 RealSafe-R1，这是基于 DeepSeek-R1 蒸馏模型的安全对齐版本。为了训练这些模型，我们构建了一个包含 15,000 个安全意识推理轨迹的数据集，这些轨迹由 DeepSeek-R1 生成，并遵循了预期的拒绝行为指令。定量实验和定性案例研究表明，这些模型在安全性方面有了显著提升，具体表现在其对有害查询和越狱攻击的安全护栏上。值得注意的是，与以往经常损害推理性能的安全对齐方法不同，我们的方法通过保持训练数据在原始生成分布内，保留了模型的推理能力。RealSafe-R1 的模型权重在 https://huggingface.co/RealSafe 上开源。

> Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been rapidly progressing and achieving breakthrough performance on complex reasoning tasks such as mathematics and coding. However, the open-source R1 models have raised safety concerns in wide applications, such as the tendency to comply with malicious queries, which greatly impacts the utility of these powerful models in their applications. In this paper, we introduce RealSafe-R1 as safety-aligned versions of DeepSeek-R1 distilled models. To train these models, we construct a dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1, under explicit instructions for expected refusal behavior. Both quantitative experiments and qualitative case studies demonstrate the models' improvements, which are shown in their safety guardrails against both harmful queries and jailbreak attacks. Importantly, unlike prior safety alignment efforts that often compromise reasoning performance, our method preserves the models' reasoning capabilities by maintaining the training data within the original distribution of generation. Model weights of RealSafe-R1 are open-source at https://huggingface.co/RealSafe.

[Arxiv](https://arxiv.org/abs/2504.10081)