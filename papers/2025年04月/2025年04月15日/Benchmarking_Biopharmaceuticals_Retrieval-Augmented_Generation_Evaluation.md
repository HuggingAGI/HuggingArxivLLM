# # 生物制药检索增强生成评估的基准测试
本研究致力于构建一个全面的评估体系，用于衡量生物制药领域中检索增强生成（RAG）模型的表现。通过将大规模语言模型与外部知识库相结合，RAG 模型在处理专业领域问题时展现出显著的优势。本研究将从多个维度对 RAG 模型进行系统性评估，包括生成内容的准确性、相关性以及实用性。

发布时间：2025年04月15日

`RAG` `生物制药`

> Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation

# 摘要

> 检索增强的大型语言模型（LLMs）在生物制药等特定领域中的应用日益受到关注。然而，针对生物制药领域的LLMs评估，目前尚缺乏专门的基准。本文介绍了首个专注于生物制药领域LLMs查询与参考理解能力（QRUC）评估的基准——生物制药检索增强生成评估（BRAGE），支持英语、法语、德语和中文版本。传统问答（QA）指标如准确性和精确匹配在开放式的检索增强QA场景中表现有限。为此，我们提出了一种基于引用的分类方法，用于评估LLMs的QRUC，以深入理解查询与参考之间的关系。通过该方法，我们对主流LLMs在BRAGE上的表现进行了评估。实验结果揭示，主流LLMs在生物制药领域的QRUC方面存在显著提升空间，亟需进一步优化。

> Recently, the application of the retrieval-augmented Large Language Models (LLMs) in specific domains has gained significant attention, especially in biopharmaceuticals. However, in this context, there is no benchmark specifically designed for biopharmaceuticals to evaluate LLMs. In this paper, we introduce the Biopharmaceuticals Retrieval-Augmented Generation Evaluation (BRAGE) , the first benchmark tailored for evaluating LLMs' Query and Reference Understanding Capability (QRUC) in the biopharmaceutical domain, available in English, French, German and Chinese. In addition, Traditional Question-Answering (QA) metrics like accuracy and exact match fall short in the open-ended retrieval-augmented QA scenarios. To address this, we propose a citation-based classification method to evaluate the QRUC of LLMs to understand the relationship between queries and references. We apply this method to evaluate the mainstream LLMs on BRAGE. Experimental results show that there is a significant gap in the biopharmaceutical QRUC of mainstream LLMs, and their QRUC needs to be improved.

[Arxiv](https://arxiv.org/abs/2504.12342)