# 基于大型语言模型的特征发现助力提升视觉内容可信度感知的预测与解释能力。

发布时间：2025年04月15日

`LLM应用

理由：这篇论文讨论了如何将大型语言模型应用于视觉内容可信度评估和特征发现，属于具体的应用场景，因此归类为LLM应用。` `社交媒体` `媒体传播`

> Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content

# 摘要

> 在视觉主导的社交媒体环境中，预测视觉内容的可信度并理解人类判断的驱动力，对于打击虚假信息至关重要。然而，视觉特征的多样性和复杂性使得这些任务充满挑战。我们提出了一种基于大型语言模型（LLM）的特征发现框架，利用多模态LLM（如GPT-4o）来评估内容可信度并解释其推理过程。通过设计特定的提示语，我们提取并量化可解释的特征，并将其整合到机器学习模型中以提升可信度预测。我们在科学、健康和政治领域的八个主题下，对4,191条视觉社交媒体帖子进行了测试，采用了5,355名众包工作者的可信度评分。我们的方法在R²值上比零-shot GPT预测高出13%，并揭示了关键特征，如信息具体性和图像格式。我们探讨了这一方法对虚假信息缓解、视觉可信度以及LLM在社会科学中作用的启示。

> In today's visually dominated social media landscape, predicting the perceived credibility of visual content and understanding what drives human judgment are crucial for countering misinformation. However, these tasks are challenging due to the diversity and richness of visual features. We introduce a Large Language Model (LLM)-informed feature discovery framework that leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and explain its reasoning. We extract and quantify interpretable features using targeted prompts and integrate them into machine learning models to improve credibility predictions. We tested this approach on 4,191 visual social media posts across eight topics in science, health, and politics, using credibility ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot GPT-based predictions by 13 percent in R2, and revealed key features like information concreteness and image format. We discuss the implications for misinformation mitigation, visual credibility, and the role of LLMs in social science.

[Arxiv](https://arxiv.org/abs/2504.10878)