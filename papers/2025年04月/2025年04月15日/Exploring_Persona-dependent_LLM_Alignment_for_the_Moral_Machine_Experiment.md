# 基于角色的大型语言模型对齐在道德机器实验中的探索

发布时间：2025年04月15日

`LLM理论` `社会学`

> Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment

# 摘要

> # 摘要
将具有自主性的大型语言模型（LLMs）应用于现实世界，引发了关于其行为方式的关键问题。特别是在面对道德困境时，这些模型的决策如何与人类保持一致？本研究探讨了LLM驱动的决策与人类判断在道德机器实验中的不同情境下的一致性，包括反映不同社会人口统计学特征的人格类型。研究发现，LLMs的道德决策因人格而异，尤其在关键任务中，其道德决策的变化幅度远超人类。我们的数据还揭示了一种引人注目的政治分拣现象，其中政治倾向主导了LLM决策的方向和程度。我们进一步探讨了在涉及道德决策的应用中部署这些模型所引发的伦理影响和潜在风险。

> Deploying large language models (LLMs) with agency in real-world applications raises critical questions about how these models will behave. In particular, how will their decisions align with humans when faced with moral dilemmas? This study examines the alignment between LLM-driven decisions and human judgment in various contexts of the moral machine experiment, including personas reflecting different sociodemographics. We find that the moral decisions of LLMs vary substantially by persona, showing greater shifts in moral decisions for critical tasks than humans. Our data also indicate an interesting partisan sorting phenomenon, where political persona predominates the direction and degree of LLM decisions. We discuss the ethical implications and risks associated with deploying these models in applications that involve moral decisions.

[Arxiv](https://arxiv.org/abs/2504.10886)