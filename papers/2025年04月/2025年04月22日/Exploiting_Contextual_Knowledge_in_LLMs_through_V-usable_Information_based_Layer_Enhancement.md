# # 利用 V-usable 信息层增强，挖掘 LLM 中的深层语境知识

发布时间：2025年04月22日

`LLM应用` `问答系统`

> Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement

# 摘要

> 大型语言模型（LLMs）在各类任务中表现卓越，但生成上下文忠实内容时仍面临挑战，难以准确反映上下文知识。现有方法虽专注于优化解码策略，却忽略了LLMs内部状态中上下文信息处理机制。因此，LLMs在充分利用上下文知识方面仍有不足。本文提出了一种全新干预方法——上下文感知层增强（CaLE），旨在提升LLMs内部表示中上下文知识的利用效率。通过V-可用信息分析，CaLE能够战略性地增强上下文信息在最优层的增长，从而丰富最终层的表示。实验结果表明，CaLE在问答任务中显著提升了上下文忠实生成的表现，尤其在涉及未知或冲突上下文知识的场景中效果尤为突出。

> Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet they often struggle with context-faithfulness generations that properly reflect contextual knowledge. While existing approaches focus on enhancing the decoding strategies, they ignore the fundamental mechanism of how contextual information is processed within LLMs' internal states. As a result, LLMs remain limited in their ability to fully leverage contextual knowledge. In this paper, we propose Context-aware Layer Enhancement (CaLE), a novel intervention method that enhances the utilization of contextual knowledge within LLMs' internal representations. By employing V-usable information analysis, CaLE strategically amplifies the growth of contextual information at an optimal layer, thereby enriching representations in the final layer. Our experiments demonstrate that CaLE effectively improves context-faithful generation in Question-Answering tasks, particularly in scenarios involving unknown or conflicting contextual knowledge.

[Arxiv](https://arxiv.org/abs/2504.15630)