# PHYBench：大型语言模型的物理感知与推理能力的全面评估

发布时间：2025年04月22日

`LLM应用` `人工智能`

> PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models

# 摘要

> 我们推出PHYBench——一个专为评估大型语言模型（LLMs）在物理场景中推理能力打造的高质量基准。该基准包含500个基于现实物理场景精心筛选的问题，覆盖力学、电磁学、热力学等多领域，难度从高中到大学再到物理竞赛不等。我们还提出了Expression Edit Distance (EED) Score这一创新评估指标，通过数学表达式编辑距离，更细致地衡量模型推理差异。测试结果显示，即使是最先进的模型在复杂物理推理上仍逊色于人类专家，凸显提升空间。PHYBench的完整数据集及结果已开放访问。

> We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.

[Arxiv](https://arxiv.org/abs/2504.16074)