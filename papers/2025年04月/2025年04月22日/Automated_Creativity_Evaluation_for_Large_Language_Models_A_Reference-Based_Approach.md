# 大型语言模型的自动化创造力评估方法：基于参考的评估方法

发布时间：2025年04月22日

`LLM应用` `文学创作` `创意写作`

> Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach

# 摘要

> 创意写作是大型语言模型（LLMs）的核心能力，其应用范围涵盖文学创作、故事叙述及众多创意领域。然而，评估机器生成文本的创意性仍是重大挑战，现有方法或依赖昂贵的人工标注，或难以与人类评估保持一致。本文提出了一种基于Torrance创意写作测试（TTCW）的自动化评估方法，将创意性视为产品进行评估。我们的方法采用基于参考的Likert量表式评分，通过对比生成文本与高质量参考文本在各项测试中的表现进行评分。实验结果表明，我们的方法显著提升了LLM评估与人类评估的一致性，成对准确率达到0.75（+15%）。

> Creative writing is a key capability of Large Language Models (LLMs), with potential applications in literature, storytelling, and various creative domains. However, evaluating the creativity of machine-generated texts remains a significant challenge, as existing methods either rely on costly manual annotations or fail to align closely with human assessments. In this paper, we propose an effective automated evaluation method based on the Torrance Test of Creative Writing (TTCW), which evaluates creativity as product. Our method employs a reference-based Likert-style approach, scoring generated creative texts relative to high-quality reference texts across various tests. Experimental results demonstrate that our method significantly improves the alignment between LLM evaluations and human assessments, achieving a pairwise accuracy of 0.75 (+15\%).

[Arxiv](https://arxiv.org/abs/2504.15784)