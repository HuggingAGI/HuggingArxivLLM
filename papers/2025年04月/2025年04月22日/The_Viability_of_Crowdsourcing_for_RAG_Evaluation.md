# 众包在 RAG 评估中的可行性研究

发布时间：2025年04月22日

`RAG` `信息检索` `数据科学`

> The Viability of Crowdsourcing for RAG Evaluation

# 摘要

> 人类在 RAG 场景中撰写和评估回复的能力如何？为解答这一问题，我们通过两个互补研究——回复撰写与效用评估——探究了众包在 RAG 中的效果。我们推出了 CrowdRAG-25 数据集，包含针对 TREC RAG'24 赛道 301 个主题的 903 条人工撰写和 903 条 LLM 生成的回复，涵盖'列表'、'散文'和'新闻'三种风格。在精选的 65 个主题中，数据集还包含了 47,320 条人类和 10,556 条 LLM 的两两比较判断，覆盖七个效用维度。我们的分析揭示了人类在 RAG 中的写作行为以及众包在评估中的可行性。与基于 LLM 或混合判断方式相比，人类的两两比较判断更可靠且经济，同时也可与人工参考回复的自动化比较相媲美。我们的所有数据和工具均免费开放。

> How good are humans at writing and judging responses in retrieval-augmented generation (RAG) scenarios? To answer this question, we investigate the efficacy of crowdsourcing for RAG through two complementary studies: response writing and response utility judgment. We present the Crowd RAG Corpus 2025 (CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated responses for the 301 topics of the TREC RAG'24 track, across the three discourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65 topics, the corpus further contains 47,320 pairwise human judgments and 10,556 pairwise LLM judgments across seven utility dimensions (e.g., coverage and coherence). Our analyses give insights into human writing behavior for RAG and the viability of crowdsourcing for RAG evaluation. Human pairwise judgments provide reliable and cost-effective results compared to LLM-based pairwise or human/LLM-based pointwise judgments, as well as automated comparisons with human-written reference responses. All our data and tools are freely available.

[Arxiv](https://arxiv.org/abs/2504.15689)