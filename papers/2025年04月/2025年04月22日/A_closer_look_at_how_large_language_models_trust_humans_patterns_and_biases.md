# 大型语言模型如何信任人类：模式与偏见的探秘

发布时间：2025年04月22日

`Agent` `社会学`

> A closer look at how large language models trust humans: patterns and biases

# 摘要

> 随着大型语言模型（LLMs）和基于LLM的智能体在决策情境中的广泛应用，理解人类与AI智能体之间的信任动态变得至关重要。尽管已有大量研究探讨人类如何信任AI智能体，但关于基于LLM的智能体如何有效信任人类的研究却相对匮乏。在信任相关场景（如评估贷款申请）中，LLM驱动的智能体可能依赖某种隐性的有效信任来提供决策支持。基于行为理论，我们提出了一种研究方法，旨在探讨LLM的信任是否建立在人类主体的三大可信度维度——能力、善意和正直之上。同时，我们还分析了人口统计变量对有效信任的影响。通过对五种流行语言模型进行的43,200次模拟实验，涵盖五种不同场景，我们发现LLM信任的发展模式与人类信任的发展模式总体相似。研究发现，在大多数情况下，LLM的信任程度强烈受可信度影响，而在某些场景中（尤其是金融场景），年龄、宗教和性别等因素也会产生偏见。这一现象在文献中常见的场景和新模型中尤为明显。尽管整体模式与人类信任形成机制相似，但不同模型在信任评估方式上存在差异：在某些情况下，可信度和人口统计因素对有效信任的预测作用较弱。这些发现强调了深入理解AI与人类之间的信任动态的重要性，并呼吁对偏见和信任发展模式进行持续监控，以避免在信任敏感型AI应用中出现意外的负面影响。

> As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.

[Arxiv](https://arxiv.org/abs/2504.15801)