# EVOLVE：借由 LLM 评估实现情感与视觉输出学习

发布时间：2024年12月29日

`LLM应用` `社交机器人` `人机交互`

> EVOLVE: Emotion and Visual Output Learning via LLM Evaluation

# 摘要

> 人类对社交机器人的接受程度深受同理心和感知理解的影响。这要求能对用户的各类输入数据做出精准且灵活的回应。虽然此类系统会因纳入更多状态或响应类型而愈发复杂，但将大型语言模型应用于人机交互的新研究，已能实现更精简的感知与反应流程。LLM 所选的动作和情感表达有助于增强所展现同理心的真实性，促进机器人与用户间的交流改善。除了在口头或书面回应中体现同理心，这还展现了在实际场景中运用 LLM 的可能。在本研究中，我们借助视觉语言模型的新进展，考虑更开放的情感响应选择，以及情感协调的动作和颜色模式选择，以强化意义和同理心的传达，从而拓展了 LLM 驱动的社交机器人非语言行为的研究。

> Human acceptance of social robots is greatly effected by empathy and perceived understanding. This necessitates accurate and flexible responses to various input data from the user. While systems such as this can become increasingly complex as more states or response types are included, new research in the application of large language models towards human-robot interaction has allowed for more streamlined perception and reaction pipelines. LLM-selected actions and emotional expressions can help reinforce the realism of displayed empathy and allow for improved communication between the robot and user. Beyond portraying empathy in spoken or written responses, this shows the possibilities of using LLMs in actuated, real world scenarios. In this work we extend research in LLM-driven nonverbal behavior for social robots by considering more open-ended emotional response selection leveraging new advances in vision-language models, along with emotionally aligned motion and color pattern selections that strengthen conveyance of meaning and empathy.

[Arxiv](https://arxiv.org/abs/2412.20632)