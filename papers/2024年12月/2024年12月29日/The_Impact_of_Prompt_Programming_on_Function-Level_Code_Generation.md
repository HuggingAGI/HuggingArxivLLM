# 提示编程对函数级别代码生成的作用

发布时间：2024年12月29日

`LLM应用` `软件工程` `代码生成`

> The Impact of Prompt Programming on Function-Level Code Generation

# 摘要

> 大型语言模型（LLMs）在软件工程师进行代码生成时的应用愈发广泛。然而，LLMs 存在诸如生成不相关或错误代码等局限，这凸显了提示编程（或提示工程）的必要性，即工程师运用特定的提示技术（比如思维链或输入输出示例）来优化生成的代码。尽管如此，不同提示技术及其组合对代码生成的影响仍有待深入探究。在本研究中，我们推出了 CodePromptEval，这是一个拥有 7072 个提示的数据集，旨在评估五种提示技术（少样本、角色、思维链、函数签名、包列表）以及它们对三个 LLMs（GPT-4o、Llama3 和 Mistral）所生成完整函数的正确性、相似性和质量的作用。我们的发现表明，虽然某些提示技术对生成的代码有显著影响，但多种技术的组合未必能提升效果。此外，我们还观察到在使用提示技术时，正确性和质量之间存在一种平衡。我们的数据集和复制包为未来改进 LLM 生成的代码以及评估新提示技术的研究提供了支持。

> Large Language Models (LLMs) are increasingly used by software engineers for code generation. However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code. Despite this, the impact of different prompt techniques -- and their combinations -- on code generation remains underexplored. In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome. Additionally, we observed a trade-off between correctness and quality when using prompt techniques. Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.

[Arxiv](https://arxiv.org/abs/2412.20545)