# 《对齐、生成、学习：跨语言情境学习的全新闭环框架》

发布时间：2024年12月12日

`LLM应用` `语言处理` `多语言任务`

> Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning

# 摘要

> 跨语言上下文学习（XICL）已成为借助大型语言模型（LLMs）处理多语言任务（尤其是低资源语言任务）的一种变革范式。然而，现有的方法通常依赖外部检索器或特定任务的微调，这限制了其可扩展性和通用性。在本文中，我们提出了一个新颖的自监督框架，利用 LLMs 的生成能力在内部选择和运用与任务相关的示例。我们的方法设定了两个关键目标：一是检索生成对齐损失，用于优化所选示例的质量；二是语义一致性损失，以保障跨语言的一致性。通过在多语言基准上的大量实验，我们的方法取得了最先进的性能，大幅超越现有基线。进一步的分析表明，其在不同语言家族中具有稳健性，且对未见过的任务具备泛化能力。人类评估证实，我们方法生成的输出在流畅性、相关性和语义正确性方面表现出色。此项工作为跨语言上下文学习提供了一个可扩展、有效且通用的解决方案。

> Cross-lingual in-context learning (XICL) has emerged as a transformative paradigm for leveraging large language models (LLMs) to tackle multilingual tasks, especially for low-resource languages. However, existing approaches often rely on external retrievers or task-specific fine-tuning, limiting their scalability and generalizability. In this paper, we propose a novel self-supervised framework that harnesses the generative capabilities of LLMs to internally select and utilize task-relevant examples. Our method introduces two key objectives: a retrieval-generation alignment loss to optimize the quality of selected examples and a semantic coherence loss to ensure cross-lingual consistency. Through extensive experiments on multilingual benchmarks, our approach achieves state-of-the-art performance, significantly outperforming existing baselines. Further analysis highlights its robustness across diverse language families and its ability to generalize to unseen tasks. Human evaluations confirm the superior fluency, relevance, and semantic correctness of outputs generated by our method. This work provides a scalable, effective, and generalizable solution for cross-lingual in-context learning.

[Arxiv](https://arxiv.org/abs/2412.08955)