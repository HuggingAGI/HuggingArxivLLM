# 让讽刺不再有趣：借助生成式大型语言模型降低讽刺语料库的文体偏差

发布时间：2024年12月12日

`LLM应用` `讽刺检测`

> Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by Utilizing Generative LLMs

# 摘要

> 讽刺检测对于从文本数据中精确提取观点以及在线打击不实信息极为重要。然而，由于缺乏多样化的讽刺语料库，导致出现文体偏见问题，影响了模型的检测性能。本研究提出一种针对讽刺检测的去偏方法，重点在于利用生成式大型语言模型来减少训练数据中的偏差。该方法在跨领域（反讽检测）和跨语言（英语）的场景中进行了评估。结果显示，去偏方法增强了模型在土耳其语和英语中讽刺和反讽检测任务的稳健性和泛化能力。不过，其对诸如 Llama-3.1 之类的因果语言模型的影响有限。此外，此项工作整理并展示了带有详细人工标注的土耳其讽刺新闻数据集，并针对分类、去偏和可解释性开展了案例研究。

> Satire detection is essential for accurately extracting opinions from textual data and combating misinformation online. However, the lack of diverse corpora for satire leads to the problem of stylistic bias which impacts the models' detection performances. This study proposes a debiasing approach for satire detection, focusing on reducing biases in training data by utilizing generative large language models. The approach is evaluated in both cross-domain (irony detection) and cross-lingual (English) settings. Results show that the debiasing method enhances the robustness and generalizability of the models for satire and irony detection tasks in Turkish and English. However, its impact on causal language models, such as Llama-3.1, is limited. Additionally, this work curates and presents the Turkish Satirical News Dataset with detailed human annotations, with case studies on classification, debiasing, and explainability.

[Arxiv](https://arxiv.org/abs/2412.09247)