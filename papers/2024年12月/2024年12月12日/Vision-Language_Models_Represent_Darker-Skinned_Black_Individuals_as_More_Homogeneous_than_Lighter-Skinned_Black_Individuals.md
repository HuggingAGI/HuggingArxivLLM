# 视觉-语言模型倾向于将深色皮肤的黑人个体描绘得比浅色皮肤的黑人个体更加单一化。

发布时间：2024年12月12日

`LLM应用

**理由**：这篇论文主要探讨了视觉-语言模型（VLMs）在生成图像描述和文本时可能存在的肤色偏见问题。虽然涉及到了大型语言模型（LLM）的能力，但研究的核心是LLM在多模态任务（图像和文本结合）中的应用，特别是如何在这些应用中检测和减少偏见。因此，这篇论文应归类为LLM应用。` `人工智能` `社会科学`

> Vision-Language Models Represent Darker-Skinned Black Individuals as More Homogeneous than Lighter-Skinned Black Individuals

# 摘要

> # 视觉-语言模型（VLMs）结合了大型语言模型（LLM）的能力与图像处理，使得图像描述和文本生成等任务成为可能。然而，人们对其可能放大人类偏见的担忧依然存在，尤其是肤色偏见。肤色偏见，即肤色较深的个体比肤色较浅的个体更容易遭受负面刻板印象，这一现象在社会科学中已有充分研究，但在AI领域，尤其是VLMs中，仍缺乏深入探讨。我们利用GAN Face Database，生成了不同肤色的黑人男性和女性图像，并保持其他特征不变。随后，我们让VLMs为这些图像生成故事，并比较其同质性。结果显示，在四个模型中的三个中，VLMs为肤色较深的黑人个体生成的故事更具同质性，且在所有模型中，黑人女性的故事比黑人男性更具同质性。交互效应表明，在两个VLMs中，肤色对女性的影响更大，而另外两个模型则未显示出显著差异，这与已知的刻板印象模式一致。这些发现揭示了单模态AI系统中的偏见如何传播到多模态模型中，并强调了进一步研究AI中交叉偏见的必要性。

> Vision-Language Models (VLMs) combine Large Language Model (LLM) capabilities with image processing, enabling tasks like image captioning and text-to-image generation. Yet concerns persist about their potential to amplify human-like biases, including skin tone bias. Skin tone bias, where darker-skinned individuals face more negative stereotyping than lighter-skinned individuals, is well-documented in the social sciences but remains under-explored in Artificial Intelligence (AI), particularly in VLMs. While well-documented in the social sciences, this bias remains under-explored in AI, particularly in VLMs. Using the GAN Face Database, we sampled computer-generated images of Black American men and women, controlling for skin tone variations while keeping other features constant. We then asked VLMs to write stories about these faces and compared the homogeneity of the generated stories. Stories generated by VLMs about darker-skinned Black individuals were more homogeneous than those about lighter-skinned individuals in three of four models, and Black women were consistently represented more homogeneously than Black men across all models. Interaction effects revealed a greater impact of skin tone on women in two VLMs, while the other two showed nonsignificant results, reflecting known stereotyping patterns. These findings underscore the propagation of biases from single-modality AI systems to multimodal models and highlight the need for further research to address intersectional biases in AI.

[Arxiv](https://arxiv.org/abs/2412.09668)