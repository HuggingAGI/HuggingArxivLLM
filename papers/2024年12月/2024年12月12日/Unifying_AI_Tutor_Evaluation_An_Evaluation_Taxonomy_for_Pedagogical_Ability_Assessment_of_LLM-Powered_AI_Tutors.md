# 统一 AI 导师评估：针对 LLM 驱动的 AI 导师教学能力评估的分类法

发布时间：2024年12月12日

`LLM应用` `人工智能`

> Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors

# 摘要

> 在本文中，我们探究当下最前沿的大型语言模型（LLMs）能否胜任人工智能导师一职，以及它们在教育对话中是否展现出优质人工智能辅导所需的教学能力。此前的评估工作仅局限于主观协议和基准。为填补这一空缺，我们依据关键的学习科学原则，提出了涵盖八个教学维度的统一评估分类体系，旨在评估基于学生在数学领域的错误或困惑而产生的LLM驱动的人工智能导师回应的教学价值。我们发布了MRBench——这一新的评估基准包含了来自七个最先进的基于LLM和人类导师的192次对话及1596个回应，并针对八个教学维度提供了权威注释。我们评估了热门的Prometheus2 LLM作为评估者的可靠性，还分析了每位导师的教学能力，指明了哪些LLM是优秀的导师，哪些更适合作为问答系统。我们坚信，所提出的分类体系、基准以及人工标注的标签将会简化评估流程，并助力追踪人工智能导师发展的进程。

> In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues. Previous efforts towards evaluation have been limited to subjective protocols and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusion in the mathematical domain. We release MRBench -- a new evaluation benchmark containing 192 conversations and 1,596 responses from seven state-of-the-art LLM-based and human tutors, providing gold annotations for eight pedagogical dimensions. We assess reliability of the popular Prometheus2 LLM as an evaluator and analyze each tutor's pedagogical abilities, highlighting which LLMs are good tutors and which ones are more suitable as question-answering systems. We believe that the presented taxonomy, benchmark, and human-annotated labels will streamline the evaluation process and help track the progress in AI tutors' development.

[Arxiv](https://arxiv.org/abs/2412.09416)