# AI 预测通用人工智能（AGI）：借助 AGI 预测和同行评审来探究大型语言模型（LLM）的复杂推理能力

发布时间：2024年12月12日

`LLM应用` `人工智能` `预测分析`

> AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities

# 摘要

> 我们让 16 个顶尖的大型语言模型（LLMs）估算到 2030 年通用人工智能（AGI）出现的可能性。为评估这些预测的质量，我们推行了自动同行评审流程（LLM-PR）。这些语言模型的估算结果差异显著，从 3%（Reka-Core）到 47.6%（GPT-4o）不等，中位数为 12.5%。这些估算结果与近期一项专家调查颇为接近，该调查预计到 2027 年 AGI 出现的可能性为 10%，凸显了大型语言模型在预测复杂、推测性场景方面的重要性。LLM-PR 流程展现出了很强的可靠性，其组内相关系数（ICC = 0.79）颇高，反映出各模型评分的高度一致性。在这些模型中，Pplx-70b-online 表现最为出色，而 Gemini-1.5-pro-api 排名垫底。与外部基准（如 LMSYS 聊天机器人竞技场）的交叉对比表明，不同评估方法下的 LLM 排名保持一致，这意味着现有的基准或许未涵盖某些与 AGI 预测相关的技能。我们进一步探索了基于外部基准的加权方案的运用，以优化大型语言模型的预测与人类专家预测的一致性。这一分析推动了一个新的“AGI 基准”的形成，旨在突显 AGI 相关任务中的性能差异。我们的发现为大型语言模型在推测性、跨学科的预测任务中的能力提供了见解，同时也强调了在复杂、不确定的现实世界场景中评估人工智能性能时，对创新评估框架的需求与日俱增。

> We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts. This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.

[Arxiv](https://arxiv.org/abs/2412.09385)