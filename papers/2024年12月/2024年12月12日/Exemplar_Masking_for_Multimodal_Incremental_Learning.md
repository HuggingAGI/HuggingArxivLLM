# 多模态增量学习中的示例掩码

发布时间：2024年12月12日

`LLM应用` `多模态学习` `增量学习`

> Exemplar Masking for Multimodal Incremental Learning

# 摘要

> 多模态增量学习既要消化来自多个模态的信息，又要同步学习新知识，还不能遗忘先前所学。此任务面临诸多挑战，主要有基于示例的方法中多模态数据存储量大，以及对庞大的多模态模型进行微调的计算需求高。本文中，我们采用参数高效的调整方案来减轻微调负担，并提出示例掩码框架来有效重放旧知识。具体来说，依据注意力权重和不同模态间的相关性来遮蔽不重要的标记，大幅减小示例的存储规模，从而在相同内存缓冲区下节省更多示例。另外，我们设计了一种多模态数据增强技术，让用于重放先验知识的示例更加多样化。在实验中，我们不仅在现有的多模态数据集中评估了我们的方法，还把 ImageNet-R 数据集拓展为多模态数据集作为实际应用，其中通过查询多模态大型语言模型（如 InstructBLIP）来生成标题。大量实验表明，在相同有限的内存缓冲区下，我们的示例掩码框架在应对灾难性遗忘方面更高效、更稳健。代码可在 https://github.com/YiLunLee/Exemplar_Masking_MCIL 获取。

> Multimodal incremental learning needs to digest the information from multiple modalities while concurrently learning new knowledge without forgetting the previously learned information. There are numerous challenges for this task, mainly including the larger storage size of multimodal data in exemplar-based methods and the computational requirement of finetuning on huge multimodal models. In this paper, we leverage the parameter-efficient tuning scheme to reduce the burden of fine-tuning and propose the exemplar masking framework to efficiently replay old knowledge. Specifically, the non-important tokens are masked based on the attention weights and the correlation across different modalities, significantly reducing the storage size of an exemplar and consequently saving more exemplars under the same memory buffer. Moreover, we design a multimodal data augmentation technique to diversify exemplars for replaying prior knowledge. In experiments, we not only evaluate our method in existing multimodal datasets but also extend the ImageNet-R dataset to a multimodal dataset as a real-world application, where captions are generated by querying multimodal large language models (e.g., InstructBLIP). Extensive experiments show that our exemplar masking framework is more efficient and robust to catastrophic forgetting under the same limited memory buffer. Code is available at https://github.com/YiLunLee/Exemplar_Masking_MCIL.

[Arxiv](https://arxiv.org/abs/2412.09549)