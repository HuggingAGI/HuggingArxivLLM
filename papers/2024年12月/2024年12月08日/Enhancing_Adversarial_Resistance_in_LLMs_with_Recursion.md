# 通过递归增强大型语言模型（LLMs）的对抗抗性

发布时间：2024年12月08日

`LLM应用` `AI 安全` `语言模型`

> Enhancing Adversarial Resistance in LLMs with Recursion

# 摘要

> 随着大型语言模型（LLMs）在社会中的融合程度不断加深，必须针对越狱和对抗性提示所带来的漏洞构建强有力的防御体系。本项目提出了一个递归框架，借助提示简化技术来增强LLMs抵御操纵的能力。通过提高复杂且令人迷惑的对抗性提示的透明度，该方法能够更可靠地检测和防范恶意输入。我们的研究成果试图解决AI安全保障中的关键问题，为开发能够区分无害输入与含恶意意图提示的系统奠定基础。由于LLMs在各类应用中的持续运用，此类保障措施的重要性只会与日俱增。

> The increasing integration of Large Language Models (LLMs) into society necessitates robust defenses against vulnerabilities from jailbreaking and adversarial prompts. This project proposes a recursive framework for enhancing the resistance of LLMs to manipulation through the use of prompt simplification techniques. By increasing the transparency of complex and confusing adversarial prompts, the proposed method enables more reliable detection and prevention of malicious inputs. Our findings attempt to address a critical problem in AI safety and security, providing a foundation for the development of systems able to distinguish harmless inputs from prompts containing malicious intent. As LLMs continue to be used in diverse applications, the importance of such safeguards will only grow.

[Arxiv](https://arxiv.org/abs/2412.06181)