# 利用索引梯度实现大型语言模型基于优化的越狱

发布时间：2024年12月11日

`LLM应用` `语言模型` `安全攻击`

> Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models

# 摘要

> 尽管在运用对齐技术训练大型语言模型（LLMs）以增强生成内容的安全性方面有所进展，但这些模型仍易遭受越狱这种能暴露LLMs安全漏洞的对抗性攻击。值得注意的是，贪婪坐标梯度（GCG）方法已展现出能自动生成可突破最先进LLMs的对抗性后缀。然而，GCG涉及的优化过程极为耗时，致使越狱流程效率低下。在本文中，我们探究了GCG的过程，发现了间接效应这一关键瓶颈问题。为此，我们提出了模型攻击梯度指数GCG（MAGIC），它利用后缀标记的梯度信息解决间接效应，从而减少计算量和迭代次数以加速流程。我们在AdvBench上的实验表明，MAGIC能实现高达1.5倍的提速，同时攻击成功率（ASR）与其他基线相当甚至更高。我们的MAGIC在Llama-2上的ASR达74%，对GPT-3.5进行转移攻击时的ASR为54%。代码可在https://github.com/jiah-li/magic获取。

> Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at https://github.com/jiah-li/magic.

[Arxiv](https://arxiv.org/abs/2412.08615)