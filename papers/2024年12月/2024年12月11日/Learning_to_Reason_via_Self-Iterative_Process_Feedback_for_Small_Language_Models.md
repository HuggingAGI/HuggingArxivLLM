# 通过自迭代过程反馈来让小型语言模型学会推理

发布时间：2024年12月11日

`LLM应用` `语言模型`

> Learning to Reason via Self-Iterative Process Feedback for Small Language Models

# 摘要

> 小型语言模型（SLMs）比大型语言模型（LLMs）效率更高、成本更低、可定制性更强，不过在推理等特定领域表现欠佳。以往增强 SLMs 推理能力的方法，像监督微调与蒸馏，常依赖高价的外部信号，致使 SLMs 在有限监督信号下过度自信，限制了其能力。所以，本研究让 SLMs 能从自我迭代反馈中学习推理。结合优势比偏好优化（ORPO），利用其自身生成的正负信号对 SLMs 进行微调与对齐。另外，通过基于抽样的推理模拟和过程奖励模型为偏好对齐中的奖励引入过程监督。和监督微调（SFT）相比，我们的方法让 Gemma-2B 在 GSM8K 上的性能提升了 12.43（Acc），在 MBPP 上提升了 3.95（Pass@1）。而且，所提方法在 MMLU_Math 和 HumanEval 上也展现出出色的域外泛化能力。

> Small language models (SLMs) are more efficient, cost-effective, and customizable than large language models (LLMs), though they often underperform in specific areas like reasoning. Past methods for enhancing SLMs' reasoning, such as supervised fine-tuning and distillation, often depend on costly external signals, resulting in SLMs being overly confident with limited supervision signals, thus limiting their abilities. Therefore, this study enables SLMs to learn to reason from self-iterative feedback. By combining odds ratio preference optimization (ORPO), we fine-tune and align SLMs using positive and negative signals generated by themselves. Additionally, we introduce process supervision for rewards in preference alignment by sampling-based inference simulation and process reward models. Compared to Supervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B by 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed method also demonstrated superior out-of-domain generalization capabilities on MMLU_Math and HumanEval.

[Arxiv](https://arxiv.org/abs/2412.08393)