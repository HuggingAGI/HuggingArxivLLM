# 神经交互证明

发布时间：2024年12月11日

`Agent` `人工智能` `安全系统`

> Neural Interactive Proofs

# 摘要

> 我们探讨这样一个问题：可信但算力有限的代理（“验证者”）怎样学会与一个或多个强大却不可信的代理（“证明者”）交互，从而完成给定任务。具体而言，我们研究了用神经网络来表征代理的情形，并将此问题的解决办法称作神经交互证明。首先，我们引入基于证明者 - 验证者博弈的统一框架，它对先前提出的交互协议进行了概括。接着，我们描述了若干生成神经交互证明的新协议，并对新方法和既有方法展开理论对比。最后，我们通过在两个领域的实验来支撑这一理论：一个能阐释关键理念的玩具图同构问题，以及运用大型语言模型的代码验证任务。如此一来，我们期望为神经交互证明的后续工作及其在构建更安全的人工智能系统中的应用筑牢根基。

> We consider the problem of how a trusted, but computationally bounded agent (a 'verifier') can learn to interact with one or more powerful but untrusted agents ('provers') in order to solve a given task. More specifically, we study the case in which agents are represented using neural networks and refer to solutions of this problem as neural interactive proofs. First we introduce a unifying framework based on prover-verifier games, which generalises previously proposed interaction protocols. We then describe several new protocols for generating neural interactive proofs, and provide a theoretical comparison of both new and existing approaches. Finally, we support this theory with experiments in two domains: a toy graph isomorphism problem that illustrates the key ideas, and a code validation task using large language models. In so doing, we aim to create a foundation for future work on neural interactive proofs and their application in building safer AI systems.

[Arxiv](https://arxiv.org/abs/2412.08897)