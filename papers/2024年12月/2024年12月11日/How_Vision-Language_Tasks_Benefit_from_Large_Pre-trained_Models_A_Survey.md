# 大型预训练模型如何助力视觉语言任务：一项调研

发布时间：2024年12月11日

`LLM应用` `人工智能` `视觉语言任务`

> How Vision-Language Tasks Benefit from Large Pre-trained Models: A Survey

# 摘要

> 对诸如视觉字幕生成、视觉问答和视觉常识推理等各类视觉语言任务的探索，在人工智能领域至关重要，始终吸引着研究界的目光。尽管整体性能有所提升，但视觉语言任务中的传统难题依旧存在，阻碍着该领域的发展。近些年来，预训练模型的兴起推动着视觉语言任务的研究。凭借大规模的训练数据和模型参数，预训练模型在众多下游任务中表现卓越。受预训练模型强大能力的启发，新的范式应运而生，以应对经典挑战。此类方法已成为当下研究的主流，备受关注且发展迅速。在本文中，我们全面阐述了视觉语言任务如何从预训练模型中获益。首先，我们回顾了视觉语言任务中的几个主要挑战，并探讨了预训练时代之前原有解决方案的局限性。接着，我们总结了将预训练模型用于解决视觉语言任务挑战的最新进展。最后，我们分析了与预训练模型固有局限性相关的潜在风险，并讨论了可能的解决方案，试图为未来研究指明方向。

> The exploration of various vision-language tasks, such as visual captioning, visual question answering, and visual commonsense reasoning, is an important area in artificial intelligence and continuously attracts the research community's attention. Despite the improvements in overall performance, classic challenges still exist in vision-language tasks and hinder the development of this area. In recent years, the rise of pre-trained models is driving the research on vision-language tasks. Thanks to the massive scale of training data and model parameters, pre-trained models have exhibited excellent performance in numerous downstream tasks. Inspired by the powerful capabilities of pre-trained models, new paradigms have emerged to solve the classic challenges. Such methods have become mainstream in current research with increasing attention and rapid advances. In this paper, we present a comprehensive overview of how vision-language tasks benefit from pre-trained models. First, we review several main challenges in vision-language tasks and discuss the limitations of previous solutions before the era of pre-training. Next, we summarize the recent advances in incorporating pre-trained models to address the challenges in vision-language tasks. Finally, we analyze the potential risks associated with the inherent limitations of pre-trained models and discuss possible solutions, attempting to provide future research directions.

[Arxiv](https://arxiv.org/abs/2412.08158)