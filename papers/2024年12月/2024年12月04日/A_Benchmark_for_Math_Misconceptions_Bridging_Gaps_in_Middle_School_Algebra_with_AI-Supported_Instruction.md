# 一个针对数学误解的基准：借助人工智能支持的教学来填补中学代数的差距

发布时间：2024年12月04日

`LLM应用` `中学代数`

> A Benchmark for Math Misconceptions: Bridging Gaps in Middle School Algebra with AI-Supported Instruction

# 摘要

> 本研究为中学代数引入了一个用于人工智能（AI）教育平台的评估基准。旨在支持设计能依据学习者当前代数理解水平来增强其代数概念理解的AI系统。该数据集涵盖55个代数误解、常见错误以及220个此前经同行评审研究确定的诊断示例。我们给出了一个运用大型语言模型的示例应用，观察到一系列依主题和实验设置而定的精度与召回分数，在纳入教育者反馈并按主题限制时可达83.9%。我们发现像比例这类主题，对LLMs而言和对学生一样棘手。我们纳入了对LLMs结果的人工评估，以及五位中学数学教育工作者对于数据集中误解的清晰程度、出现情况以及AI结合数据集的潜在用途的反馈。多数教育工作者（80%及以上）称在学生中碰到过这些误解，表明该数据集与中学代数教学相关。尽管对AI工具的熟悉程度各异，但五分之四的教育工作者表示有兴趣借助含AI的数据集来诊断学生的误解或培训教师。研究结果凸显了主题受限测试的重要性、多模式方法的需求，以及在将AI用于人类学习时人类专业知识获取实用见解的相关性。

> This study introduces an evaluation benchmark for middle school algebra to be used in artificial intelligence(AI) based educational platforms. The goal is to support the design of AI systems that can enhance learner conceptual understanding of algebra by taking into account their current level of algebra comprehension. The data set comprises 55 misconceptions about algebra, common errors, and 220 diagnostic examples identified in previous peer-reviewed studies. We provide an example application using a large language model, observing a range of precision and recall scores depending on the topic and experimental setup that reaches 83.9% when including educator feedback and restricting it by topic. We found that topics such as ratios and proportions prove as difficult for LLMs as they are for students. We included a human assessment of LLMs results and feedback from five middle school math educators on the clarity and occurrence of misconceptions in the dataset and the potential use of AI in conjunction with the dataset. Most educators (80% or more) indicated that they encounter these misconceptions among their students, suggesting the relevance of the data set to teaching middle school algebra. Despite varying familiarity with AI tools, four out of five educators expressed interest in using the data set with AI to diagnose student misconceptions or train teachers. The results emphasize the importance of topic-constrained testing, the need for multimodal approaches, and the relevance of human expertise to gain practical insights when using AI for human learning.

[Arxiv](https://arxiv.org/abs/2412.03765)