# MIDI：用于实现从单幅图像到 3D 场景生成的多实例扩散

发布时间：2024年12月04日

`其他` `3D 生成` `图像生成`

> MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation

# 摘要

> 摘要：本文引入了 MIDI 这一全新范式，用于从单张图像生成组合式 3D 场景。现有的方法要么依赖重建或检索技术，要么采用多阶段逐个对象生成的方式，而 MIDI 则将预训练的图像到 3D 对象生成模型拓展为多实例扩散模型，能够同时生成多个具有精确空间关系和高通用性的 3D 实例。其核心在于，MIDI 融入了新颖的多实例注意力机制，能在生成过程中直接有效地捕捉对象间的相互作用和空间连贯性，无需繁杂的多步骤流程。该方法以部分对象图像和全局场景上下文为输入，在 3D 生成时直接对对象的完成情况进行建模。在训练中，我们利用少量的场景级数据有效监管 3D 实例间的交互，同时结合单对象数据进行正则化，以此保持预训练的泛化能力。MIDI 在图像到场景的生成中展现出了顶尖水平的性能，这在对合成数据、真实世界场景数据以及由文本到图像扩散模型生成的风格化场景图像的评估中得到了验证。

> 
Abstract:This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.
    

[Arxiv](https://arxiv.org/pdf/2412.03558)