# 针对图像字幕的多模态大型语言模型个性化：一项实验分析

发布时间：2024年12月04日

`LLM应用` `多模态`

> Personalizing Multimodal Large Language Models for Image Captioning: An Experimental Analysis

# 摘要

> 图像描述这一任务需要一种算法来生成针对视觉输入的自然语言描述。近来的发展使得图像描述研究与大型语言模型（LLMs）以及多模态LLMs（如GPT-4V和Gemini）的发展相互融合，这类多模态LLMs将纯文本LLMs的能力拓展到了多种模态。本文通过在各类图像描述基准上评估其性能，探究多模态LLMs能否取代传统的图像描述网络。我们既探索了这些模型的零样本能力，也研究了它们通过提示学习、前缀调整和低秩适应等微调方法对不同语义领域的适应情况。我们的结果显示，尽管多模态LLMs的零样本性能出色，但在保持其泛化能力的前提下针对特定领域进行微调仍颇具挑战。我们探讨了这些发现对于图像描述未来研究以及更具适应性的多模态LLMs发展的意义。

> The task of image captioning demands an algorithm to generate natural language descriptions of visual inputs. Recent advancements have seen a convergence between image captioning research and the development of Large Language Models (LLMs) and Multimodal LLMs -- like GPT-4V and Gemini -- which extend the capabilities of text-only LLMs to multiple modalities. This paper investigates whether Multimodal LLMs can supplant traditional image captioning networks by evaluating their performance on various image description benchmarks. We explore both the zero-shot capabilities of these models and their adaptability to different semantic domains through fine-tuning methods, including prompt learning, prefix tuning, and low-rank adaptation. Our results demonstrate that while Multimodal LLMs achieve impressive zero-shot performance, fine-tuning for specific domains while maintaining their generalization capabilities intact remains challenging. We discuss the implications of these findings for future research in image captioning and the development of more adaptable Multimodal LLMs.

[Arxiv](https://arxiv.org/abs/2412.03665)