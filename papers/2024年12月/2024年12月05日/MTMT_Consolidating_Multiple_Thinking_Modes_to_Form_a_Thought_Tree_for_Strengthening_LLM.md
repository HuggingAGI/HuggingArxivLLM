# MTMT：通过整合多种思维模式形成思想树以增强 LLM

发布时间：2024年12月05日

`LLM应用` `人工智能`

> MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM

# 摘要

> 大型语言模型（LLMs）在复杂逻辑推理和多步骤问题解决这类任务中存在局限性。为应对此挑战，研究人员运用精心设计的提示和流程图，模拟人类认知过程来提升LLM性能，像思维链方法就是如此。在本文里，我们推出了MTMT（多思维模式树）这一新方法，它能与LLM交互构建思维树，模拟多种高级认知过程，像联想、反事实思维、任务分解和比较等。通过把原本复杂的任务拆解为更简单的子问题，MTMT让LLM更易解决问题，能更有效地利用LLM中的潜在知识。我们以GPT-4o mini为基础模型，在不同参数配置下评估MTMT的性能。我们的成果表明，融合多种思维模式能显著增强LLM处理复杂任务的能力。

> Large language models (LLMs) have shown limitations in tasks requiring complex logical reasoning and multi-step problem-solving. To address these challenges, researchers have employed carefully designed prompts and flowcharts, simulating human cognitive processes to enhance LLM performance, such as the Chain of Thought approach. In this paper, we introduce MTMT (Multi-thinking Modes Tree), a novel method that interacts with LLMs to construct a thought tree, simulating various advanced cognitive processes, including but not limited to association, counterfactual thinking, task decomposition, and comparison. By breaking down the original complex task into simpler sub-questions, MTMT facilitates easier problem-solving for LLMs, enabling more effective utilization of the latent knowledge within LLMs. We evaluate the performance of MTMT under different parameter configurations, using GPT-4o mini as the base model. Our results demonstrate that integrating multiple modes of thinking significantly enhances the ability of LLMs to handle complex tasks.

[Arxiv](https://arxiv.org/abs/2412.03987)