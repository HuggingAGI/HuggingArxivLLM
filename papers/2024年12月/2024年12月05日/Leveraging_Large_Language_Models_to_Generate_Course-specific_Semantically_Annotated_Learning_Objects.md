# 借助大型语言模型来生成课程专属的语义标注学习对象

发布时间：2024年12月05日

`LLM应用` `计算机科学`

> Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects

# 摘要

> 背景：过去数十年，自动问题生成（AQG）的流程与方法发生了显著变化。生成式自然语言模型的最新进展为教育内容的生成带来了新机遇。
  目标：本文探究了大型语言模型（LLMs）在生成计算机科学问题方面的潜力，这些问题有充足注释可用于自动学习者模型更新，完全契合特定课程情境，并解决认知维度的理解问题。
  方法：不同于以往可能使用 ChatGPT 这类基础方法的尝试，我们的方法采用了更具针对性的策略，如检索增强生成（RAG），以产出与上下文相关且具有教学意义的学习对象。
  结果和结论：我们的结果显示，生成结构、语义注释效果不错。但这种成功在关系注释方面未得到体现。生成问题的质量通常达不到教育标准，这表明尽管 LLMs 能为学习资料库添砖加瓦，但其当前的性能水平需要大量人工干预来优化和验证生成的内容。

> Background: Over the past few decades, the process and methodology of automated question generation (AQG) have undergone significant transformations. Recent progress in generative natural language models has opened up new potential in the generation of educational content.
  Objectives: This paper explores the potential of large language models (LLMs) for generating computer science questions that are sufficiently annotated for automatic learner model updates, are fully situated in the context of a particular course, and address the cognitive dimension understand.
  Methods: Unlike previous attempts that might use basic methods like ChatGPT, our approach involves more targeted strategies such as retrieval-augmented generation (RAG) to produce contextually relevant and pedagogically meaningful learning objects.
  Results and Conclusions: Our results show that generating structural, semantic annotations works well. However, this success was not reflected in the case of relational annotations. The quality of the generated questions often did not meet educational standards, highlighting that although LLMs can contribute to the pool of learning materials, their current level of performance requires significant human intervention to refine and validate the generated content.

[Arxiv](https://arxiv.org/abs/2412.04185)