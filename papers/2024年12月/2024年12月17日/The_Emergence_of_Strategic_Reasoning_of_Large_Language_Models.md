# 大型语言模型战略推理的涌现

发布时间：2024年12月17日

`LLM应用` `行为经济学` `战略推理`

> The Emergence of Strategic Reasoning of Large Language Models

# 摘要

> 随着大型语言模型（LLMs）在各类复杂关键任务中的应用愈发广泛，评估其在战略环境下的逻辑能力变得至关重要。本文探究了它们的战略推理能力，即通过预测和适应其他主体行为来选择最优行动方案的过程。借助六个LLMs，我们对来自行为经济学中的经典游戏（如p-选美比赛、11-20金钱请求游戏和猜谜游戏）的游戏响应进行了分析，并运用推理分层模型（level-$k$理论和认知层次理论）评估了其性能。研究发现，尽管LLMs能理解游戏，但多数在高阶战略推理上存在困境。虽然大多数LLMs在涉及重复互动的游戏中展现出了学习能力，可它们始终达不到人类受试者典型行为所呈现的推理水平。不过，OpenAI的GPT-o1是个例外，它专门接受过解决复杂推理任务的训练，始终优于其他LLMs和人类受试者。这些发现从行为经济学的视角凸显了推动LLMs走向强大战略推理所面临的挑战与路径。

> As Large Language Models (LLMs) are increasingly used for a variety of complex and critical tasks, it is vital to assess their logical capabilities in strategic environments. This paper examines their ability in strategic reasoning -- the process of choosing an optimal course of action by predicting and adapting to other agents' behavior. Using six LLMs, we analyze responses from play in classical games from behavioral economics (p-Beauty Contest, 11-20 Money Request Game, and Guessing Game) and evaluate their performance through hierarchical models of reasoning (level-$k$ theory and cognitive hierarchy theory). Our findings reveal that while LLMs show understanding of the games, the majority struggle with higher-order strategic reasoning. Although most LLMs did demonstrate learning ability with games involving repeated interactions, they still consistently fall short of the reasoning levels demonstrated by typical behavior from human subjects. The exception to these overall findings is with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning tasks -- which consistently outperforms other LLMs and human subjects. These findings highlight the challenges and pathways in advancing LLMs toward robust strategic reasoning from the perspective of behavioral economics.

[Arxiv](https://arxiv.org/abs/2412.13013)