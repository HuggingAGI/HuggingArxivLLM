# CoMT：针对大型视觉语言模型多模态思维链的全新基准

发布时间：2024年12月17日

`LLM应用` `多模态` `视觉推理`

> CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models

# 摘要

> 大型视觉语言模型（LVLMs）近来在多模态任务中成果斐然，在多模态思维链（MCoT）推理方面也有进步。然而，当下的基准仍遵循多模态输入与文本模态输出的传统范式，这带来了明显弊端，比如缺少视觉操作和表达含混。鉴于此，我们推出了新颖的多模态思维链（CoMT）基准来克服这些局限。与传统的 MCoT 基准不同，CoMT 要求多模态输入和多模态推理输出，旨在效仿人类那种天然融合视觉操作的推理。具体而言，CoMT 涵盖四个类别：（1）视觉创造，（2）视觉删除，（3）视觉更新，（4）视觉选择，从而全面探究真实场景中的复杂视觉操作和简洁表达。我们在 CoMT 上对各类 LVLMs 和策略进行了评估，揭示了当前方法在能力和局限性方面的一些关键洞察。我们期望 CoMT 能激发更多关于将多模态生成引入推理过程的研究。

> Large Vision-Language Models (LVLMs) have recently demonstrated amazing success in multi-modal tasks, including advancements in Multi-modal Chain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks still follow a traditional paradigm with multi-modal input and text-modal output, which leads to significant drawbacks such as missing visual operations and vague expressions. Motivated by this, we introduce a novel Chain of Multi-modal Thought (CoMT) benchmark to address these limitations. Different from the traditional MCoT benchmark, CoMT requires both multi-modal input and multi-modal reasoning output, aiming to mimic human-like reasoning that inherently integrates visual operation. Specifically, CoMT consists of four categories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and (4) Visual Selection to comprehensively explore complex visual operations and concise expression in real scenarios. We evaluate various LVLMs and strategies on CoMT, revealing some key insights into the capabilities and limitations of the current approaches. We hope that CoMT can inspire more research on introducing multi-modal generation into the reasoning process.

[Arxiv](https://arxiv.org/abs/2412.12932)