# 大型语言模型与图神经网络的交汇：图挖掘新视角

发布时间：2024年12月26日

`LLM应用

**理由**：这篇论文主要讨论了大型语言模型（LLMs）与图神经网络（GNNs）的结合与应用技术，重点在于如何利用LLMs来增强图挖掘任务的效果。虽然涉及了图神经网络（GNNs），但核心内容仍然是LLMs在图挖掘中的应用，因此应归类为LLM应用。` `图挖掘` `机器学习`

> Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining

# 摘要

> # 摘要
图挖掘是数据挖掘和机器学习中的重要领域，专注于从图结构数据中提取有价值的信息。近年来，图神经网络（GNNs）的发展推动了这一领域的显著进步。然而，GNNs 在泛化多样化图数据方面仍有不足。针对这一问题，大型语言模型（LLMs）凭借其强大的语义理解能力，为图挖掘任务提供了新的解决方案。本文系统回顾了 LLMs 和 GNNs 的结合与应用技术，并提出了这一跨学科领域的新分类法，涵盖三大类：GNN 驱动 LLM、LLM 驱动 GNN 以及 GNN 和 LLM 协同驱动。在此框架下，我们展示了 LLMs 在增强图特征提取和提升下游任务（如节点分类、链接预测和社区检测）效果方面的潜力。尽管 LLMs 在处理图结构数据上展现了巨大前景，但其高计算需求和复杂性仍是挑战。未来研究需继续探索如何高效融合 LLMs 和 GNNs，以实现更强大的图学习与推理能力，为图挖掘技术的发展注入新动力。

> Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, Large Language Models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graph-structured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.

[Arxiv](https://arxiv.org/abs/2412.19211)