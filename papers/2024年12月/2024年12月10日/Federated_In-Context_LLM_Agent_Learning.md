# 联邦式上下文内 LLM 代理学习

发布时间：2024年12月10日

`Agent` `联邦学习` `语言模型`

> Federated In-Context LLM Agent Learning

# 摘要

> 大型语言模型（LLMs）凭借实现逻辑推理、工具运用以及与外部系统的交互，让智能服务发生了革命性的变化。然而，LLMs 的发展常受高质量数据稀缺的制约，其中不少数据本身就很敏感。联邦学习（FL）为解决这一问题提供了可能，它能促进分布式 LLMs 的协同训练，同时保障私有数据安全。但 FL 框架面临着巨大的带宽和计算需求，还面临着异构数据分布带来的挑战。LLMs 新出现的上下文学习能力，通过聚合自然语言而非庞大的模型参数，提供了一种很有前景的方法。不过，这种方法有隐私泄露的风险，因为在聚合过程中需要收集和展示来自不同客户端的数据样本。在本文中，我们提出了一种新颖的隐私保护联邦上下文 LLMs 代理学习（FICAL）算法，据我们所知，这是首次通过 FL 释放上下文学习的力量来训练各类 LLMs 代理。在我们的设计中，由新颖的 LLMs 增强型知识纲要生成（KCG）模块生成的知识纲要在客户端和服务器之间传输，而非像以往 FL 方法中的模型参数。此外，我们还设计了一个基于令人惊叹的检索增强生成（RAG）的工具学习和利用（TLU）模块，并将聚合的全局知识纲要当作老师，教导 LLMs 代理使用工具。我们开展了大量实验，结果显示 FICAL 与其他最先进的基线相比表现出色，通信成本大幅降低了 3.33×10^5 倍。

> Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents. The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data. However, FL frameworks face significant bandwidth and computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitates the collection and presentation of data samples from various clients during aggregation. In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL. In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\mathbf{3.33\times10^5}$ times.

[Arxiv](https://arxiv.org/abs/2412.08054)