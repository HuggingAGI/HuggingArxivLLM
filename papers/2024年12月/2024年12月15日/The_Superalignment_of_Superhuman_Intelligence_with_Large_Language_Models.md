# 超级人类智能与大型语言模型的完美对齐

发布时间：2024年12月15日

`LLM理论

理由：这篇论文主要探讨了大型语言模型和多模态语言模型在超人类智能背景下的对齐问题，特别是如何确保这些模型的安全、可靠性和与人类价值观的一致性。论文提出了超对齐的概念框架，并讨论了相关的核心研究问题，如弱到强泛化、可扩展监督和评估等。这些内容主要涉及对大型语言模型的理论研究和算法设计，因此归类为“LLM理论”。` `人工智能` `机器学习`

> The Superalignment of Superhuman Intelligence with Large Language Models

# 摘要

> 随着大型语言模型和多模态语言模型的飞速发展，我们见证了超人类智能的崛起。然而，随着这些超人类模型的广泛应用，一个关键问题浮出水面：如何确保这些模型依然安全、可靠且与人类价值观保持一致？在这篇立场论文中，我们从学习的角度探讨了超对齐的概念，通过梳理从大规模预训练、监督微调到对齐训练的学习范式转变，试图回答这一问题。我们将超对齐定义为：在任务复杂到人类专家难以标注且模型能力超越人类时，设计高效的对齐算法，从噪声标记的数据（如点样本或成对偏好数据）中以可扩展的方式学习。我们重点探讨了超对齐中的几个核心研究问题，包括弱到强泛化、可扩展监督和评估。接着，我们提出了一个超对齐的概念框架，包含三个模块：攻击者生成对抗性查询，试图暴露学习者模型的弱点；学习者通过批评模型生成的可扩展反馈和少量人类专家的指导进行自我改进；批评者则为查询-响应对生成批评或解释，旨在通过批评提升学习者的表现。我们讨论了该框架中各组件的重要研究问题，并提出了与之相关的有趣研究思路，如自对齐、自玩、自改进等。最后，我们展望了超对齐的未来研究方向，包括识别新的涌现风险和多维对齐。

> We have witnessed superhuman intelligence thanks to the fast development of large language models and multimodal language models. As the application of such superhuman models becomes more and more common, a critical question rises here: how can we ensure superhuman models are still safe, reliable and aligned well to human values? In this position paper, we discuss the concept of superalignment from the learning perspective to answer this question by outlining the learning paradigm shift from large-scale pretraining, supervised fine-tuning, to alignment training. We define superalignment as designing effective and efficient alignment algorithms to learn from noisy-labeled data (point-wise samples or pair-wise preference data) in a scalable way when the task becomes very complex for human experts to annotate and the model is stronger than human experts. We highlight some key research problems in superalignment, namely, weak-to-strong generalization, scalable oversight, and evaluation. We then present a conceptual framework for superalignment, which consists of three modules: an attacker which generates adversary queries trying to expose the weaknesses of a learner model; a learner which will refine itself by learning from scalable feedbacks generated by a critic model along with minimal human experts; and a critic which generates critics or explanations for a given query-response pair, with a target of improving the learner by criticizing. We discuss some important research problems in each component of this framework and highlight some interesting research ideas that are closely related to our proposed framework, for instance, self-alignment, self-play, self-refinement, and more. Last, we highlight some future research directions for superalignment, including identification of new emergent risks and multi-dimensional alignment.

[Arxiv](https://arxiv.org/abs/2412.11145)