# 100% 借助 Acurai 消除幻觉

发布时间：2024年12月06日

`LLM应用` `人工智能`

> 100% Hallucination Elimination Using Acurai

# 摘要

> 大型语言模型（LLMs）中的幻觉问题一直是人工智能在企业和其他高风险应用中得以采用的重大阻碍。尽管检索增强生成（RAG）系统有所发展，但就算提供了相关且准确的上下文，当下最先进的方法在生成忠实且事实无误的输出时，准确率仍达不到 80%以上。在本次研究中，我们推出了 Acurai，这一全新的系统方法通过在输入前重新整理查询和上下文数据，在 LLMs 中达成了 100%无幻觉响应。凭借对 LLM 内部表征的深刻理解、名词短语主导地位的重要性以及离散功能单元（DFUs）的作用，Acurai 保证了输入上下文与生成输出的一致性。我们借助 RAGTruth 语料库对该方法进行了验证，表明其能够为 GPT-4 和 GPT-3.5 Turbo 彻底消除幻觉。Acurai 为实现一致、准确且忠实的人工智能响应树立了新标杆，在可信人工智能系统的开发进程中迈出了关键的一步。

> The issue of hallucinations in large language models (LLMs) remains a critical barrier to the adoption of AI in enterprise and other high-stakes applications. Despite advancements in retrieval-augmented generation (RAG) systems, current state-of-the-art methods fail to achieve more than 80% accuracy in generating faithful and factually correct outputs, even when provided with relevant and accurate context. In this work, we introduce Acurai, a novel systematic approach that achieves 100% hallucination-free responses in LLMs by reformatting queries and context data prior to input. Leveraging a deep understanding of LLM internal representations, the importance of noun-phrase dominance, and the role of discrete functional units (DFUs), Acurai ensures alignment between input context and generated output. We validate this method using the RAGTruth corpus, demonstrating its ability to eliminate 100% hallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for achieving consistent, accurate, and faithful AI responses, marking a significant step forward in the development of trustworthy AI systems.

[Arxiv](https://arxiv.org/abs/2412.05223)