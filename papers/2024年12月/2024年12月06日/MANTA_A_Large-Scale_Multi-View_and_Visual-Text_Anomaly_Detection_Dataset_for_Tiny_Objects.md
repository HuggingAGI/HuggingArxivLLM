# MANTA：一个针对微小物体的大规模多视图与视觉文本异常检测数据集

发布时间：2024年12月06日

`其他` `视觉检测` `数据集`

> MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection Dataset for Tiny Objects

# 摘要

> 摘要：我们推出了 MANTA，这是一个针对微小物体的视觉-文本异常检测数据集。视觉部分涵盖 38 个对象类别，跨越五个典型领域，包含超过 137.3 万张图像，其中 8.6 万张图像被标注为异常，且带有像素级注释。每张图像均从五个不同视角拍摄，以实现对象的全面覆盖。文本部分包含两个子集：陈述性知识，涵盖 875 个描述不同领域和特定类别常见异常的单词，并对<什么、为什么、如何>进行了详尽解释，包括成因和视觉特征；建构主义学习，提供 2000 个不同难度等级的选择题，每个都与图像配对，并附有答案解释。我们还为视觉-文本任务设定了一条基线，并开展了大量的基准测试实验，以评估不同设置下的先进方法，凸显了我们数据集面临的挑战和有效性。

> 
Abstract:We present MANTA, a visual-text anomaly detection dataset for tiny objects. The visual component comprises over 137.3K images across 38 object categories spanning five typical domains, of which 8.6K images are labeled as anomalous with pixel-level annotations. Each image is captured from five distinct viewpoints to ensure comprehensive object coverage. The text component consists of two subsets: Declarative Knowledge, including 875 words that describe common anomalies across various domains and specific categories, with detailed explanations for < what, why, how>, including causes and visual characteristics; and Constructivist Learning, providing 2K multiple-choice questions with varying levels of difficulty, each paired with images and corresponded answer explanations. We also propose a baseline for visual-text tasks and conduct extensive benchmarking experiments to evaluate advanced methods across different settings, highlighting the challenges and efficacy of our dataset.
    

[Arxiv](https://arxiv.org/pdf/2412.04867)