# Explingo：借助大型语言模型阐释人工智能的预测

发布时间：2024年12月06日

`LLM应用` `机器学习`

> Explingo: Explaining AI Predictions using Large Language Models

# 摘要

> 机器学习（ML）模型预测的解释由像 SHAP 这类可解释人工智能（XAI）技术生成，这对使用 ML 输出做决策的人来说极为重要。我们探究大型语言模型（LLMs）把这些解释转化为契合自然交流的人类可读叙述格式的潜力。我们处理了两个关键研究问题：（1）LLMs 能否可靠地将传统解释转变为高质量叙述？（2）我们怎样有效评估叙述性解释的质量？为回答这些问题，我们引入了 Explingo，它包含两个基于 LLM 的子系统，即叙述者和评分者。叙述者接收 ML 解释并将其转化为自然语言描述。评分者依据包括准确性、完整性、流畅性和简洁性等一组指标给这些叙述打分。
  我们的实验表明，LLMs 能够生成在所有指标上都得分很高的高质量叙述，尤其是在少量人工标注和自举示例的引导下。我们还找出了仍具挑战性的方面，特别是在复杂领域中有效地给叙述评分。这项工作的成果已整合进一个开源工具，让叙述性解释能用于更多应用。

> Explanations of machine learning (ML) model predictions generated by Explainable AI (XAI) techniques such as SHAP are essential for people using ML outputs for decision-making. We explore the potential of Large Language Models (LLMs) to transform these explanations into human-readable, narrative formats that align with natural communication. We address two key research questions: (1) Can LLMs reliably transform traditional explanations into high-quality narratives? and (2) How can we effectively evaluate the quality of narrative explanations? To answer these questions, we introduce Explingo, which consists of two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML explanations and transforms them into natural-language descriptions. The Grader scores these narratives on a set of metrics including accuracy, completeness, fluency, and conciseness.
  Our experiments demonstrate that LLMs can generate high-quality narratives that achieve high scores across all metrics, particularly when guided by a small number of human-labeled and bootstrapped examples. We also identified areas that remain challenging, in particular for effectively scoring narratives in complex domains. The findings from this work have been integrated into an open-source tool that makes narrative explanations available for further applications.

[Arxiv](https://arxiv.org/abs/2412.05145)