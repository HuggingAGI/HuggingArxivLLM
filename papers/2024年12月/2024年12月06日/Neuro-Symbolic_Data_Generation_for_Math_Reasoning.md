# 用于数学推理的神经符号数据生成

发布时间：2024年12月06日

`LLM应用` `数据生成`

> Neuro-Symbolic Data Generation for Math Reasoning

# 摘要

> 关于大型语言模型（LLMs），有一个关键问题：它们在数学推理上的明显不足是与生俱来的，还是只因未充分接触优质数学数据所致。为探究此问题，我们研发了一种自动生成高质量、有监督数学数据集的方法。此方法对现有数学问题精心变异，保证新生成问题的多样性和有效性。这通过一个神经符号数据生成框架达成，该框架融合了LLMs直观的非正式化长处、数学求解器的精确符号推理，以及在高度不规则符号空间中的投影马尔可夫链蒙特卡罗抽样。实证实验表明，所提方法生成的数据质量颇高，而且LLMs，尤其是LLaMA-2和Mistral，在与生成的数据重新匹配后，超越了其最先进的同类产品。

> A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space. Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.

[Arxiv](https://arxiv.org/abs/2412.04857)