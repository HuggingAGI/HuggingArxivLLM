# 大型语言模型的安全：全面综述

发布时间：2024年12月23日

`LLM应用` `人工智能` `语言模型`

> Large Language Model Safety: A Holistic Survey

# 摘要

> 大型语言模型（LLMs）的迅猛发展与部署，在人工智能领域开拓了新前沿，展现出自然语言理解与生成方面前所未有的能力。然而，这些模型在关键应用中的不断融合，引发了重大安全忧虑，故而有必要对其潜在风险及相关缓解策略进行深入探究。
  此次调研对 LLM 安全的现状进行了全面梳理，涵盖四大类别：价值偏差、对抗攻击的稳健性、滥用以及自主人工智能风险。除了对这四个方面的缓解办法和评估资源予以全面审视外，我们还进一步探讨了与 LLM 安全相关的四个主题：LLM 代理的安全影响、可解释性在提升 LLM 安全方面的作用、一系列人工智能公司和机构为 LLM 安全提出并遵循的技术路线图，以及针对 LLM 安全的人工智能治理，包括国际合作、政策提议和未来监管方向的探讨。
  我们的发现凸显了对 LLM 安全采取积极、多面策略的必要性，强调技术解决方案、伦理考量和强大治理框架的融合。本次调研意在为学术研究人员、行业从业者和政策制定者提供基础参考，就 LLMs 安全融入社会所涉及的挑战与机遇提供见解。最终，致力于为 LLMs 的安全、有益发展贡献力量，与利用人工智能推动社会进步和福祉的总体目标相契合。相关论文的精选列表已在 https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers 公开。

> The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies.
  This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions.
  Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.

[Arxiv](https://arxiv.org/abs/2412.17686)