# 揭开语言模型中视觉感知的面纱：一种注意力头分析途径

发布时间：2024年12月23日

`LLM应用` `多模态` `人工智能`

> Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach

# 摘要

> 近期，多模态大型语言模型（MLLMs）取得了显著进展，在视觉理解方面表现出色。这一惊人的飞跃引发了一个引人深思的问题：最初仅基于语言数据训练的语言模型，怎样才能有效地解读和处理视觉内容？本文通过对 4 个模型家族和 4 种模型规模展开系统研究，旨在解决此问题，揭示出一类专门聚焦于视觉内容的独特注意力头。我们的分析显示，这些注意力头的行为、注意力权重的分布以及它们对输入中视觉标记的关注程度之间存在着紧密的关联。这些发现加深了我们对于大型语言模型如何适应多模态任务的认识，展现出其弥合文本与视觉理解之间鸿沟的潜力。此项工作为能够应对多种模态的人工智能系统的发展铺平了道路。

> Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on linguistic data, effectively interpret and process visual content? This paper aims to address this question with systematic investigation across 4 model families and 4 model scales, uncovering a unique class of attention heads that focus specifically on visual content. Our analysis reveals a strong correlation between the behavior of these attention heads, the distribution of attention weights, and their concentration on visual tokens within the input. These findings enhance our understanding of how LLMs adapt to multimodal tasks, demonstrating their potential to bridge the gap between textual and visual understanding. This work paves the way for the development of AI systems capable of engaging with diverse modalities.

[Arxiv](https://arxiv.org/abs/2412.18108)