# GenAI 内容检测任务 2：AI 与人类——学术论文的真实性挑战

发布时间：2024年12月24日

`LLM应用` `文本检测`

> GenAI Content Detection Task 2: AI vs. Human -- Academic Essay Authenticity Challenge

# 摘要

> 本文对作为 COLING 2025 同期 GenAI 内容检测共享任务一部分所组织的学术论文真实性挑战第一版进行了全面综述。该挑战重点在于检测用于学术目的的机器生成和人类撰写的论文，其任务定义为：“给定一篇论文，判别它是由机器生成还是由人类创作。”此挑战涵盖英语和阿拉伯语两种语言。在评估阶段，25 支团队为英语提交了系统，21 支团队为阿拉伯语提交了系统，足见大家对该任务的浓厚兴趣。最终，7 支团队提交了系统描述论文。多数提交成果运用了基于微调的 Transformer 模型，有一支团队采用了像 Llama 2 和 Llama 3 这样的大型语言模型（LLMs）。本文阐述了任务设定，详述了数据集构建流程，并阐释了评估框架。另外，我们还呈现了参赛团队所采用方法的汇总。几乎所有提交的系统都比基于 n-gram 的基线表现出色，顶尖系统在两种语言上的 F1 分数均超 0.98，这表明在检测机器生成文本方面有了重大进展。

> This paper presents a comprehensive overview of the first edition of the Academic Essay Authenticity Challenge, organized as part of the GenAI Content Detection shared tasks collocated with COLING 2025. This challenge focuses on detecting machine-generated vs. human-authored essays for academic purposes. The task is defined as follows: "Given an essay, identify whether it is generated by a machine or authored by a human.'' The challenge involves two languages: English and Arabic. During the evaluation phase, 25 teams submitted systems for English and 21 teams for Arabic, reflecting substantial interest in the task. Finally, seven teams submitted system description papers. The majority of submissions utilized fine-tuned transformer-based models, with one team employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This paper outlines the task formulation, details the dataset construction process, and explains the evaluation framework. Additionally, we present a summary of the approaches adopted by participating teams. Nearly all submitted systems outperformed the n-gram-based baseline, with the top-performing systems achieving F1 scores exceeding 0.98 for both languages, indicating significant progress in the detection of machine-generated text.

[Arxiv](https://arxiv.org/abs/2412.18274)