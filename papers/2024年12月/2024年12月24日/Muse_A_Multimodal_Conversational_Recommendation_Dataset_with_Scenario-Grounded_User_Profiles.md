# Muse：一个具备基于场景的用户画像的多模态会话推荐数据集

发布时间：2024年12月24日

`LLM应用` `推荐系统`

> Muse: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles

# 摘要

> 当前的对话推荐系统主要聚焦于文本。然而，现实中的推荐场景通常是多模态的，这致使现有研究与实际应用存在显著差距。为应对此问题，我们推出了 Muse，这是首个多模态对话推荐数据集。Muse 涵盖了来自 7000 个围绕服装领域的对话中的 83148 条话语。每个对话均包含全面的多模态交互、丰富元素以及自然的对话。Muse 中的数据由多模态大型语言模型（MLLMs）驱动的多智能体框架自动合成。它创新性地从现实场景推导用户画像，而非依赖手动设计和历史数据，以获得更好的可扩展性，进而完成对话模拟与优化。人类和 LLM 的评估均显示 Muse 中的对话质量颇高。此外，在三个 MLLMs 上的微调实验表明 Muse 在推荐和响应方面具有可学习模式，证实了其对多模态对话推荐的价值。我们的数据集和代码可在 url{https://anonymous.4open.science/r/Muse-0086}获取。

> Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational recommendation dataset. Muse comprises 83,148 utterances from 7,000 conversations centered around the Clothing domain. Each conversation contains comprehensive multimodal interactions, rich elements, and natural dialogues. Data in Muse are automatically synthesized by a multi-agent framework powered by multimodal large language models (MLLMs). It innovatively derives user profiles from real-world scenarios rather than depending on manual design and history data for better scalability, and then it fulfills conversation simulation and optimization. Both human and LLM evaluations demonstrate the high quality of conversations in Muse. Additionally, fine-tuning experiments on three MLLMs demonstrate Muse's learnable patterns for recommendations and responses, confirming its value for multimodal conversational recommendation. Our dataset and codes are available at url{https://anonymous.4open.science/r/Muse-0086}.

[Arxiv](https://arxiv.org/abs/2412.18416)