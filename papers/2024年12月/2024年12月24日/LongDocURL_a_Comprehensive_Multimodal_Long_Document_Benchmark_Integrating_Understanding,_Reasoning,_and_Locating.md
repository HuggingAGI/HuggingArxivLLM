# LongDocURL：一个融合了理解、推理和定位的综合性多模态长文档基准

发布时间：2024年12月24日

`LLM应用` `文档理解` `模型评估`

> LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating

# 摘要

> 大型视觉语言模型（LVLMs）大幅提升了文档理解能力，能够应对复杂文档元素、更长的上下文以及更广泛的任务。然而，现有的文档理解基准测试仅能处理少量页面，且未能对布局元素定位进行全面剖析。在本文中，我们首先界定了三个主要任务类别：长文档理解、数值推理和跨元素定位，接着提出了一个综合性基准 LongDocURL，它整合了上述三个主要任务，包含基于不同主要任务分类的 20 个子任务以及答案依据。此外，我们开发了一条半自动构建管道，收集了 2325 个高质量问答对，涵盖超过 33000 页文档，明显优于现有的基准。随后，我们针对 26 种不同配置的开源和闭源模型开展了全面评估实验，揭示出该领域的关键性能差距。

> Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark, LongDocURL, integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field.

[Arxiv](https://arxiv.org/abs/2412.18424)