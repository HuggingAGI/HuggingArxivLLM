# 走向模态泛化：一项基准与前瞻性分析

发布时间：2024年12月24日

`其他` `多模态` `机器学习`

> Towards Modality Generalization: A Benchmark and Prospective Analysis

# 摘要

> 多模态学习整合了来自各种模态的信息，成果斐然，在识别和检索等任务中的表现优于单模态方法。然而，受资源和隐私所限，现实场景中常有训练时未见过的新模态出现，这是当前方法难以应对的挑战。本文引入了模态泛化（MG），致力于让模型能泛化到未见过的模态。我们定义了两种情形：弱 MG，即已见和未见模态都能通过现有感知器映射到联合嵌入空间；强 MG，即不存在此类映射。为推动发展，我们提出了一个涵盖多模态算法的综合基准，并对专注于泛化的现有方法进行了调整。大量实验凸显了 MG 的复杂性，暴露了现有方法的不足，指明了未来研究的关键方向。我们的工作为推进稳健且适应性强的多模态模型奠定了基础，使其能够应对现实场景中的未见模态。

> Multi-modal learning has achieved remarkable success by integrating information from various modalities, achieving superior performance in tasks like recognition and retrieval compared to uni-modal approaches. However, real-world scenarios often present novel modalities that are unseen during training due to resource and privacy constraints, a challenge current methods struggle to address. This paper introduces Modality Generalization (MG), which focuses on enabling models to generalize to unseen modalities. We define two cases: weak MG, where both seen and unseen modalities can be mapped into a joint embedding space via existing perceptors, and strong MG, where no such mappings exist. To facilitate progress, we propose a comprehensive benchmark featuring multi-modal algorithms and adapt existing methods that focus on generalization. Extensive experiments highlight the complexity of MG, exposing the limitations of existing methods and identifying key directions for future research. Our work provides a foundation for advancing robust and adaptable multi-modal models, enabling them to handle unseen modalities in realistic scenarios.

[Arxiv](https://arxiv.org/abs/2412.18277)