# VLM 分类能否从 LLM 描述语义中获益？

发布时间：2024年12月16日

`LLM应用

理由：这篇论文探讨了如何利用LLMs生成的描述来提升视觉-语言模型（VLMs）的分类性能，并提出了一种方法来区分描述的实际判别能力与可能依赖集成效应的性能提升。这涉及到LLMs在实际应用中的使用，特别是与视觉-语言模型的结合，因此属于LLM应用的范畴。` `计算机视觉` `人工智能`

> Does VLM Classification Benefit from LLM Description Semantics?

# 摘要

> # 摘要
通过文本准确描述图像是可解释AI的基石。CLIP等视觉-语言模型（VLMs）通过将图像和文本对齐到共享嵌入空间，解决了这一问题。LLMs生成的描述可以提升VLM分类性能，但性能提升可能源于语义无关的集成效应。为此，我们探讨如何区分描述的实际判别能力与可能依赖集成效应的性能提升。我们提出了一种替代评估场景，用于检测描述是否具有判别能力。此外，我们提出了一种无需训练的方法，选择独立于类名集成效应的判别性描述。该方法通过测试图像的局部CLIP标签邻域（即前$k$个标签预测），在小选择集中提取能有效区分各类的描述。实验表明，使用这些描述在七个数据集上提升了分类准确性，并深入分析了基于描述的VLM图像分类的可解释性。

> Accurately describing images via text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect. Considering this, we ask how to distinguish the actual discriminative power of descriptions from performance boosts that potentially rely on an ensembling effect. To study this, we propose an alternative evaluation scenario that shows a characteristic behavior if the used descriptions have discriminative power. Furthermore, we propose a training-free method to select discriminative descriptions that work independently of classname ensembling effects. The training-free method works in the following way: A test image has a local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then, w.r.t. to a small selection set, we extract descriptions that distinguish each class well in the local neighborhood. Using the selected descriptions, we demonstrate improved classification accuracy across seven datasets and provide in-depth analysis and insights into the explainability of description-based image classification by VLMs.

[Arxiv](https://arxiv.org/abs/2412.11917)