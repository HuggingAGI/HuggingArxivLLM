# QUENCH：衡量大型语言模型中印度语和非印度语上下文通用推理的差距

发布时间：2024年12月16日

`LLM应用` `基准测试`

> QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning in LLMs

# 摘要

> 大型语言模型（LLMs）的崛起使得超越传统设置的先进基准测试系统成为必需。为此，我们推出了 QUENCH，这是一种新颖的基于文本的英语测验基准，由 YouTube 测验视频手动策划并转录而成。QUENCH 具有被遮蔽的实体和原理，以供 LLMs 通过生成进行预测。在地理背景与常识推理的交汇处，QUENCH 通过零样本、开放领域的测验设置，助力评估 LLMs 的世界知识和推理能力。我们针对 7 个 LLMs 和 4 项指标展开了广泛评估，探究了模型规模、提示风格、地理背景以及有标注的原理生成所产生的影响。基准测试以对 LLMs 易犯错误的分析告终。

> The rise of large language models (LLMs) has created a need for advanced benchmarking systems beyond traditional setups. To this end, we introduce QUENCH, a novel text-based English Quizzing Benchmark manually curated and transcribed from YouTube quiz videos. QUENCH possesses masked entities and rationales for the LLMs to predict via generation. At the intersection of geographical context and common sense reasoning, QUENCH helps assess world knowledge and deduction capabilities of LLMs via a zero-shot, open-domain quizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics, investigating the influence of model size, prompting style, geographical context, and gold-labeled rationale generation. The benchmarking concludes with an error analysis to which the LLMs are prone.

[Arxiv](https://arxiv.org/abs/2412.11763)