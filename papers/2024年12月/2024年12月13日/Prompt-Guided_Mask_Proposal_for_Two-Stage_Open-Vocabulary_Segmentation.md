# 用于两阶段开放词汇分割的提示引导掩码提议

发布时间：2024年12月13日

`LLM应用` `计算机视觉` `图像分割`

> Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation

# 摘要

> 我们致力于应对开放词汇分割的挑战，即要在不同环境中从众多类别里识别对象，并以文本提示作为输入。为攻克此挑战，现有的方法常采用诸如 CLIP 这样的多模态模型，其在共享嵌入空间中将图像和文本特征相结合，以缩小有限和广泛词汇识别之间的差距，形成了两阶段的方法：第一阶段，掩码生成器接收输入图像来生成掩码提议；第二阶段依据查询选取目标掩码。然而，预期的目标掩码可能不在生成的掩码提议中，从而导致意外的输出掩码。在我们的工作中，提出了一种名为提示引导掩码提议（PMP）的新方法，掩码生成器接收输入文本提示，并在其引导下生成掩码。与无输入提示生成的掩码提议相比，PMP 生成的掩码与输入提示更契合。为实现 PMP，我们设计了文本标记和查询标记之间的交叉注意力机制，每次解码后都能生成提示引导的掩码提议。我们将 PMP 与几个采用基于查询的分割骨干的现有工作相结合，在五个基准数据集上的实验表明了该方法的有效性，相较于当前的两阶段模型有显著改进（在 mIOU 方面有 1%~3%的绝对性能提升）。这些基准测试中性能的稳定提升，表明我们所提出的轻量级提示感知方法具有有效的泛化能力。

> We tackle the challenge of open-vocabulary segmentation, where we need to identify objects from a wide range of categories in different environments, using text prompts as our input. To overcome this challenge, existing methods often use multi-modal models like CLIP, which combine image and text features in a shared embedding space to bridge the gap between limited and extensive vocabulary recognition, resulting in a two-stage approach: In the first stage, a mask generator takes an input image to generate mask proposals, and the in the second stage the target mask is picked based on the query. However, the expected target mask may not exist in the generated mask proposals, which leads to an unexpected output mask. In our work, we propose a novel approach named Prompt-guided Mask Proposal (PMP) where the mask generator takes the input text prompts and generates masks guided by these prompts. Compared with mask proposals generated without input prompts, masks generated by PMP are better aligned with the input prompts. To realize PMP, we designed a cross-attention mechanism between text tokens and query tokens which is capable of generating prompt-guided mask proposals after each decoding. We combined our PMP with several existing works employing a query-based segmentation backbone and the experiments on five benchmark datasets demonstrate the effectiveness of this approach, showcasing significant improvements over the current two-stage models (1% ~ 3% absolute performance gain in terms of mIOU). The steady improvement in performance across these benchmarks indicates the effective generalization of our proposed lightweight prompt-aware method.

[Arxiv](https://arxiv.org/abs/2412.10292)