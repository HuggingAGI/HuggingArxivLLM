# 分析并可视化大型语言模型中的语言结构：BERT 中动词 - 小品词结构的神经表达

发布时间：2024年12月19日

`LLM理论` `计算语言学` `语言分析`

> Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT

# 摘要

> 本研究深入探究了基于转换器的大型语言模型（LLMs）里动词-小品词组合的内部表征，着重考察这些模型在不同神经网络层对词汇和句法细微差异的捕捉情况。运用 BERT 架构，我们对诸如“agree on”“come back”“give up”等各类动词-小品词结构，分析了其各层的表征效力。我们的研究方法涵盖了从英国国家语料库精心准备详细的数据集，接着通过多维缩放（MDS）和广义判别值（GDV）计算等技术展开广泛的模型训练及输出分析。研究结果显示，BERT 的中间层在捕捉句法结构方面最为有效，不同动词类别在表征准确性上存在显著的差异。这些发现对神经网络处理语言元素时所假定的传统一致性提出了挑战，也表明网络架构与语言表征之间存在复杂的相互作用。我们的研究有助于加深对深度学习模型理解和处理语言方式的认识，为当前神经方法在语言分析中的潜力与局限提供了深刻见解。此项研究不但增进了我们在计算语言学领域的知识，还推动了对优化神经架构以提升语言精准度的进一步探索。

> This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network layers. Employing the BERT architecture, we analyse the representational efficacy of its layers for various verb-particle constructions such as 'agree on', 'come back', and 'give up'. Our methodology includes a detailed dataset preparation from the British National Corpus, followed by extensive model training and output analysis through techniques like multi-dimensional scaling (MDS) and generalized discrimination value (GDV) calculations. Results show that BERT's middle layers most effectively capture syntactic structures, with significant variability in representational accuracy across different verb categories. These findings challenge the conventional uniformity assumed in neural network processing of linguistic elements and suggest a complex interplay between network architecture and linguistic representation. Our research contributes to a better understanding of how deep learning models comprehend and process language, offering insights into the potential and limitations of current neural approaches to linguistic analysis. This study not only advances our knowledge in computational linguistics but also prompts further research into optimizing neural architectures for enhanced linguistic precision.

[Arxiv](https://arxiv.org/abs/2412.14670)