# 利用来自测试和静态分析的反馈助力大型语言模型（LLMs）优化代码生成

发布时间：2024年12月19日

`LLM应用` `软件工程` `代码开发`

> Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis

# 摘要

> 大型语言模型（LLMs）是人工智能领域极具前景的发展成果之一，软件工程界很快就注意到了其在软件开发生命周期中的潜在作用。开发人员常让 LLMs 生成代码片段，这虽提高了生产力，却也可能带来所有权、隐私、正确性和安全性等问题。以往的工作指出，主流商业 LLMs 生成的代码往往不安全，存在漏洞、错误和代码异味。本文中，我们提出了一个框架，借助测试和静态分析来评估通用开源 LLMs 生成的代码质量，并引导其自我改进。
  首先，我们让 LLMs 生成 C 代码来解决一些编程任务。接着，运用基准测试评估生成代码的正确性，并用静态分析工具检测潜在安全漏洞。随后，通过让模型检测错误和漏洞来评估其评估生成代码的能力。最后，测试模型修复生成代码的能力，将静态分析和不正确性评估阶段产生的报告作为反馈。
  我们的结果显示，模型常生成错误代码，生成的代码可能存在安全问题。而且，它们在检测这些问题时表现很差。好的方面是，当提供失败测试或潜在漏洞的相关信息时，我们发现模型修复有缺陷代码的能力较强，这为提升基于 LLM 的代码生成工具的安全性开辟了一条充满希望的道路。

> Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.
  First, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.
  Our results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.

[Arxiv](https://arxiv.org/abs/2412.14841)