# 直至层崩塌：从批标准化层的视角压缩深度神经网络

发布时间：2024年12月19日

`其他` `计算机视觉`

> Till the Layers Collapse: Compressing a Deep Neural Network through the Lenses of Batch Normalization Layers

# 摘要

> 如今，深度神经网络因其能够处理各类复杂任务而被广泛运用。其通用性使之成为现代科技中强有力的工具。然而，深度神经网络常常存在参数过多的问题。运用这些大型模型会耗费大量计算资源。在本文中，我们引入了一种名为	extbf{T}ill the 	extbf{L}ayers 	extbf{C}ollapse (TLC)的方法，它从批归一化层的角度对深度神经网络进行压缩。通过降低网络深度，我们的方法减少了深度神经网络的计算需求和整体延迟。我们在诸如Swin-T、MobileNet-V2和RoBERTa等流行模型上验证了该方法，涵盖了图像分类和自然语言处理（NLP）任务。

> Today, deep neural networks are widely used since they can handle a variety of complex tasks. Their generality makes them very powerful tools in modern technology. However, deep neural networks are often overparameterized. The usage of these large models consumes a lot of computation resources. In this paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers \textbf{C}ollapse (TLC), which compresses deep neural networks through the lenses of batch normalization layers. By reducing the depth of these networks, our method decreases deep neural networks' computational requirements and overall latency. We validate our method on popular models such as Swin-T, MobileNet-V2, and RoBERTa, across both image classification and natural language processing (NLP) tasks.

[Arxiv](https://arxiv.org/abs/2412.15077)