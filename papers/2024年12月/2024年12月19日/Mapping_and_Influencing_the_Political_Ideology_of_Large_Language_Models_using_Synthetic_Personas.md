# 通过合成角色来映射并影响大型语言模型的政治意识形态

发布时间：2024年12月19日

`LLM应用` `语言模型`

> Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas

# 摘要

> 对于大型语言模型（LLMs）中的政治偏见，分析主要将其视作具有固定观点的单一实体。尽管有多种测量此类偏见的方法，但基于角色提示对LLMs政治取向的影响尚未被探究。在本研究中，我们借助PersonaHub（一组合成的角色描述），通过政治罗盘测试（PCT）绘制基于角色提示的LLMs的政治分布。接着，我们考察这些初始的罗盘分布能否通过对完全相反的政治取向（右翼威权主义和左翼自由主义）进行明确的意识形态提示来操控。实验发现，合成角色大多集中在左翼自由主义象限，当用明确的意识形态描述提示时，模型表现出不同程度的响应。虽然所有模型都显著地转向右翼威权主义立场，但它们向左翼自由主义立场的转变较为有限，这表明对意识形态操纵的响应不对称，或许反映了模型训练中的固有偏差。

> The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.

[Arxiv](https://arxiv.org/abs/2412.14843)