# 对自然语言生成中的不确定性估计进行重新思考

发布时间：2024年12月19日

`LLM理论` `不确定性估计`

> Rethinking Uncertainty Estimation in Natural Language Generation

# 摘要

> 大型语言模型（LLMs）在现实应用中的运用日益广泛，这使得评估其生成文本的可信度变得尤为重要，因此可靠的不确定性估计必不可少。当前的 LLMs 是通过随机过程自回归生成文本的，相同的提示可能会产生不同的输出。正因如此，主流的不确定性估计方法会生成并分析多个输出序列来确定 LLMs 的不确定性。然而，生成输出序列的计算开销巨大，导致这些方法难以大规模应用。在本研究中，我们审视了主流方法的理论基础，并探索了提升其计算效率的新方向。基于恰当评分规则的框架，我们发现最有可能的输出序列的负对数似然构成了一种有理论依据的不确定性度量。为了近似这种替代度量，我们提出了 G-NLL，其优势在于仅通过贪心解码生成的单个输出序列就能获取。这让不确定性估计更高效、更直接，同时还保持了理论的严谨性。实证结果显示，G-NLL 在各类 LLMs 和任务中均达到了领先水平。我们的工作为自然语言生成中高效可靠的不确定性估计奠定了基础，对当前该领域主流的、计算复杂的方法的必要性提出了挑战。

> Large Language Models (LLMs) are increasingly employed in real-world applications, driving the need to evaluate the trustworthiness of their generated text. To this end, reliable uncertainty estimation is essential. Since current LLMs generate text autoregressively through a stochastic process, the same prompt can lead to varying outputs. Consequently, leading uncertainty estimation methods generate and analyze multiple output sequences to determine the LLM's uncertainty. However, generating output sequences is computationally expensive, making these methods impractical at scale. In this work, we inspect the theoretical foundations of the leading methods and explore new directions to enhance their computational efficiency. Building on the framework of proper scoring rules, we find that the negative log-likelihood of the most likely output sequence constitutes a theoretically grounded uncertainty measure. To approximate this alternative measure, we propose G-NLL, which has the advantage of being obtained using only a single output sequence generated by greedy decoding. This makes uncertainty estimation more efficient and straightforward, while preserving theoretical rigor. Empirical results demonstrate that G-NLL achieves state-of-the-art performance across various LLMs and tasks. Our work lays the foundation for efficient and reliable uncertainty estimation in natural language generation, challenging the necessity of more computationally involved methods currently leading the field.

[Arxiv](https://arxiv.org/abs/2412.15176)