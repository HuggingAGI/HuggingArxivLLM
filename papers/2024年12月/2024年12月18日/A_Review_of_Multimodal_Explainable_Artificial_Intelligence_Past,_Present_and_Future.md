# 关于多模态可解释人工智能的回顾：过去、当下与未来

发布时间：2024年12月18日

`LLM应用` `人工智能`

> A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future

# 摘要

> 人工智能（AI）借助计算能力的提升和大规模数据集的增多得以迅猛发展。然而，这一发展也使得解读 AI 模型的“黑箱”特性面临更大挑战。为应对此类问题，注重透明度和可解释性的可解释人工智能（XAI）应运而生，以增进人类对 AI 决策过程的理解与信任。在多模态数据融合及复杂推理场景下，多模态可解释人工智能（MXAI）的提出将多种模态整合用于预测和解释任务。同时，大型语言模型（LLMs）的出现给自然语言处理带来显著突破，但其复杂性也让 MXAI 的问题愈发严重。为深入洞悉 MXAI 方法的发展，为构建更透明、公平且可信的 AI 系统提供关键指引，我们从历史视角对 MXAI 方法进行回顾，并将其划分为四个时期：传统机器学习、深度学习、判别式基础模型和生成式 LLMs。我们还对 MXAI 研究中使用的评估指标和数据集进行了回顾，最后探讨了未来的挑战与方向。与本综述相关的项目已在 https://github.com/ShilinSun/mxai_review 建立。

> Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the "black-box" nature of AI models. To address these concerns, eXplainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal eXplainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions. A project related to this review has been created at https://github.com/ShilinSun/mxai_review.

[Arxiv](https://arxiv.org/abs/2412.14056)