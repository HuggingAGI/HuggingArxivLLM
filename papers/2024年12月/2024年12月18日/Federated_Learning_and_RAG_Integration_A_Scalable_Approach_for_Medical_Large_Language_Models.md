# 联邦学习与 RAG 集成：医疗大型语言模型的可扩展之法

发布时间：2024年12月18日

`RAG` `联邦学习`

> Federated Learning and RAG Integration: A Scalable Approach for Medical Large Language Models

# 摘要

> 本研究在联邦学习框架中融入检索增强生成（RAG）系统，对医疗领域的特定大型语言模型（LLMs）性能展开分析。借助联邦学习在保护数据隐私、支持分布式计算等方面的固有优势，本研究探索将 RAG 系统与不同客户端配置下训练的模型相整合，以优化性能。实验结果显示，基于联邦学习且与 RAG 系统集成的模型在所有评估指标上均始终优于未集成的同类模型。本研究凸显了将联邦学习与 RAG 系统相结合用于开发医疗领域特定 LLMs 的潜力，为提升文本生成能力提供了一种可扩展且能保护隐私的解决方案。

> This study analyzes the performance of domain-specific Large Language Models (LLMs) for the medical field by integrating Retrieval-Augmented Generation (RAG) systems within a federated learning framework. Leveraging the inherent advantages of federated learning, such as preserving data privacy and enabling distributed computation, this research explores the integration of RAG systems with models trained under varying client configurations to optimize performance. Experimental results demonstrate that the federated learning-based models integrated with RAG systems consistently outperform their non-integrated counterparts across all evaluation metrics. This study highlights the potential of combining federated learning and RAG systems for developing domain-specific LLMs in the medical field, providing a scalable and privacy-preserving solution for enhancing text generation capabilities.

[Arxiv](https://arxiv.org/abs/2412.13720)