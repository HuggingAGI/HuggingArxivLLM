# 《空间思考：多模态大型语言模型怎样看待、铭记和回想起空间》

发布时间：2024年12月18日

`LLM应用` `视觉空间智能`

> Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces

# 摘要

> 人类具备通过连续的视觉观察来记住空间的视觉空间智能。然而，在百万规模视频数据集上训练的多模态大型语言模型（MLLMs）能否从视频里“在空间中思考”呢？我们推出了一个全新的基于视频的视觉空间智能基准（VSI-Bench），其中有超 5000 个问答对，并且发现 MLLMs 展现出了有竞争力——但逊于人类——的视觉空间智能。我们探索模型，看它们如何以语言和视觉形式在空间中思考，发现尽管空间推理能力仍是 MLLMs 达到更高基准性能的主要阻碍，但局部世界模型和空间意识确实在这些模型中有所显现。值得注意的是，流行的语言推理技术（比如思维链、自我一致性、思维树）无法提升性能，而在问答期间明确生成认知地图能增强 MLLMs 的空间距离能力。

> Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also ``think in space'' from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance ability.

[Arxiv](https://arxiv.org/abs/2412.14171)