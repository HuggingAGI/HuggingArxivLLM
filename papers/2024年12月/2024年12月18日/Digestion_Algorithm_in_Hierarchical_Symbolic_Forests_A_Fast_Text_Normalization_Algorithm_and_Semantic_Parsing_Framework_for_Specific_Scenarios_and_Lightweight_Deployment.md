# 层次符号森林中的消化算法：适用于特定场景的快速文本规范化算法与语义解析框架以及轻量级部署

发布时间：2024年12月18日

`其他` `特定场景领域`

> Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios and Lightweight Deployment

# 摘要

> 文本规范化和语义解析在自然语言处理中应用广泛，像自然语言编程、释义、数据增强、构建专家系统、文本匹配等方面都有涉及。尽管深度学习在大型语言模型（LLMs）中成果斐然，但神经网络架构的可解释性依旧不佳，这影响了其可信度，进而限制了在风险敏感场景中的部署。在某些特定场景领域，数据稀缺，迅速获取大量有监督学习标签困难重重，手动标注数据的工作量更是巨大。神经网络中的灾难性遗忘还致使数据利用率低下。在对响应速度要求高的情况下，模型的密度导致本地部署困难，响应时间长，不利于这些领域在本地的应用。受组合数学的乘法规则和人类思维模式的启发，提出了一种多层框架及其算法——分层符号森林中的消化算法（DAHSF），用于解决上述问题，将文本规范化和语义解析工作流相结合。中文脚本语言“火兔智能开发平台 V2.0”是本文所讨论技术的重要测试和应用。DAHSF 能够在特定场景领域的小数据集上本地运行，模型大小和内存使用量至少优化两个数量级，从而提升了执行速度，有着良好的优化前景。

> Text Normalization and Semantic Parsing have numerous applications in natural language processing, such as natural language programming, paraphrasing, data augmentation, constructing expert systems, text matching, and more. Despite the prominent achievements of deep learning in Large Language Models (LLMs), the interpretability of neural network architectures is still poor, which affects their credibility and hence limits the deployments of risk-sensitive scenarios. In certain scenario-specific domains with scarce data, rapidly obtaining a large number of supervised learning labels is challenging, and the workload of manually labeling data would be enormous. Catastrophic forgetting in neural networks further leads to low data utilization rates. In situations where swift responses are vital, the density of the model makes local deployment difficult and the response time long, which is not conducive to local applications of these fields. Inspired by the multiplication rule, a principle of combinatorial mathematics, and human thinking patterns, a multilayer framework along with its algorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is proposed to address these above issues, combining text normalization and semantic parsing workflows. The Chinese Scripting Language "Fire Bunny Intelligent Development Platform V2.0" is an important test and application of the technology discussed in this paper. DAHSF can run locally in scenario-specific domains on little datasets, with model size and memory usage optimized by at least two orders of magnitude, thus improving the execution speed, and possessing a promising optimization outlook.

[Arxiv](https://arxiv.org/abs/2412.14054)