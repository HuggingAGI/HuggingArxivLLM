# 探究大型语言模型合成数据中质量、多样性与复杂性所产生的影响

发布时间：2024年12月03日

`LLM应用` `数据生成` `模型评估`

> Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models

# 摘要

> 利用大型语言模型生成合成数据，是在近乎无限的任务范围内增强自然数据的一种极有前景的模式。鉴于其多样性，合成数据生成算法间的直接对比稀缺，导致难以知晓改进源自何处以及存在何种瓶颈。我们提议依据各算法生成的合成数据的构成，从数据质量、多样性及复杂性的角度来评估算法。我们选取这三个特性，是因其在开放式流程中的重要性以及对下游模型能力的影响。我们发现，质量对于分布内模型的泛化至关重要，多样性对于分布外泛化不可或缺，复杂性对两者均有益处。此外，我们着重指出训练数据中存在质量 - 多样性的权衡以及其对模型性能的下游影响。接着，我们探究了合成数据管道中的各类组件对每个数据特征的作用。这种考察让我们能够依据所利用的组件以及对数据 QDC 构成的结果影响，对合成数据生成算法进行分类和比较。此分析延伸至关于在合成数据中平衡 QDC 对于高效强化学习和自我改进算法的重要性的探讨。类似于训练数据中的 QD 权衡，通常在模型输出质量和输出多样性之间存在权衡，这会影响合成数据的构成。我们观察到，当前许多模型仅针对输出质量进行评估和优化，从而限制了输出多样性和自我改进的潜力。我们认为，平衡这些权衡对于未来自我改进算法的发展至关重要，并强调了在此方向上取得进展的若干工作。

> Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.

[Arxiv](https://arxiv.org/abs/2412.02980)