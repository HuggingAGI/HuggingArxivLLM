# 人类的变异性与机器的一致性：关于人类和大型语言模型所生成文本的语言分析

发布时间：2024年12月03日

`LLM应用` `文本分析`

> Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts Generated by Humans and Large Language Models

# 摘要

> 大型语言模型（LLMs）发展迅猛，其生成自然语言的能力大幅提升，致使 LLMs 生成的文本和人类创作的文本愈发难以分辨。近期研究多侧重于借助 LLMs 来区分文本是人类所写还是机器生成。在我们的研究里，我们另辟蹊径，依据 250 种独特的语言特征对涵盖四个领域的文本进行剖析。我们从 SemEval 2024 任务 8 的子任务 B 中选取了 M4 数据集。我们用 LFTK 工具自动算出各类语言特征，还为每份文档测量了平均句法深度、语义相似度和情感内容。接着，我们对所有算出的特征进行二维 PCA 降维。我们的分析表明，人类创作的文本和 LLMs 生成的文本存在显著差异，尤其是在这些特征的变异性上，人类创作的文本变异性要高得多。这种差异在语言风格约束较宽松的文本类型中尤为突出。我们的发现指出，与 LLMs 生成的文本相比，人类创作的文本认知要求更低，语义内容更丰富，情感内容更饱满。这些发现强调了纳入有意义的语言特征的必要性，以增进对 LLMs 文本输出的理解。

> The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. Recent research has predominantly focused on using LLMs to classify text as either human-written or machine-generated. In our study, we adopt a different approach by profiling texts spanning four domains based on 250 distinct linguistic features. We select the M4 dataset from the Subtask B of SemEval 2024 Task 8. We automatically calculate various linguistic features with the LFTK tool and additionally measure the average syntactic depth, semantic similarity, and emotional content for each document. We then apply a two-dimensional PCA reduction to all the calculated features. Our analyses reveal significant differences between human-written texts and those generated by LLMs, particularly in the variability of these features, which we find to be considerably higher in human-written texts. This discrepancy is especially evident in text genres with less rigid linguistic style constraints. Our findings indicate that humans write texts that are less cognitively demanding, with higher semantic content, and richer emotional content compared to texts generated by LLMs. These insights underscore the need for incorporating meaningful linguistic features to enhance the understanding of textual outputs of LLMs.

[Arxiv](https://arxiv.org/abs/2412.03025)