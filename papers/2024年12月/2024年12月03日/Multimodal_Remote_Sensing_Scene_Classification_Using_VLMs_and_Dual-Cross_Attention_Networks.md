# 利用 VLMs 和双交叉注意力网络进行多模态遥感场景分类

发布时间：2024年12月03日

`LLM应用` `资源管理`

> Multimodal Remote Sensing Scene Classification Using VLMs and Dual-Cross Attention Networks

# 摘要

> 遥感场景分类（RSSC）是一项关键任务，在土地利用和资源管理领域有着广泛应用。单模态图像方法虽有前景，但常受类内方差大、类间相似度高等限制。融入文本信息能通过提供额外的上下文和语义理解来强化分类，然而手动文本标注既费力又昂贵。在本研究中，我们提出了一个新颖的 RSSC 框架，将大型视觉语言模型（VLMs）生成的文本描述作为辅助模态整合进来，无需高昂的手动标注成本。为充分利用视觉与文本数据间的潜在互补性，我们提出了基于双交叉注意力的网络，将这些模态融合为统一的表征。在五个 RSSC 数据集上开展的大量定量和定性评估实验表明，我们的框架始终优于基线模型。我们还验证了 VLM 生成的文本描述相较于人工标注描述的有效性。此外，我们设计了零样本分类场景，表明所学到的多模态表征能有效用于未见过的类别分类。本研究为在 RSSC 任务中利用文本信息开辟了新新机遇，提供了有前景的多模态融合结构，为未来研究带来了启示和灵感。代码可在：https://github.com/CJR7/MultiAtt-RSSC 获取。

> Remote sensing scene classification (RSSC) is a critical task with diverse applications in land use and resource management. While unimodal image-based approaches show promise, they often struggle with limitations such as high intra-class variance and inter-class similarity. Incorporating textual information can enhance classification by providing additional context and semantic understanding, but manual text annotation is labor-intensive and costly. In this work, we propose a novel RSSC framework that integrates text descriptions generated by large vision-language models (VLMs) as an auxiliary modality without incurring expensive manual annotation costs. To fully leverage the latent complementarities between visual and textual data, we propose a dual cross-attention-based network to fuse these modalities into a unified representation. Extensive experiments with both quantitative and qualitative evaluation across five RSSC datasets demonstrate that our framework consistently outperforms baseline models. We also verify the effectiveness of VLM-generated text descriptions compared to human-annotated descriptions. Additionally, we design a zero-shot classification scenario to show that the learned multimodal representation can be effectively utilized for unseen class classification. This research opens new opportunities for leveraging textual information in RSSC tasks and provides a promising multimodal fusion structure, offering insights and inspiration for future studies. Code is available at: https://github.com/CJR7/MultiAtt-RSSC

[Arxiv](https://arxiv.org/abs/2412.02531)