# 破解密码：评估为提供编程反馈的零样本提示方法

发布时间：2024年12月20日

`LLM应用` `反馈评估`

> Cracking the Code: Evaluating Zero-Shot Prompting Methods for Providing Programming Feedback

# 摘要

> 尽管大型语言模型（LLMs）在提供反馈方面的应用越来越多，但关于如何获取高质量反馈的研究却很有限。本案例研究引入了一个评估框架，用于评估不同的零样本提示工程方法。我们对提示进行了系统性的改变，并对 R 中编程错误所提供的反馈进行了分析。结果显示，建议分步操作的提示能提高精度，而省略关于分析哪些提供数据的明确说明则有助于提高错误识别能力。

> Despite the growing use of large language models (LLMs) for providing feedback, limited research has explored how to achieve high-quality feedback. This case study introduces an evaluation framework to assess different zero-shot prompt engineering methods. We varied the prompts systematically and analyzed the provided feedback on programming errors in R. The results suggest that prompts suggesting a stepwise procedure increase the precision, while omitting explicit specifications about which provided data to analyze improves error identification.

[Arxiv](https://arxiv.org/abs/2412.15702)