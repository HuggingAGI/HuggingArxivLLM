# 在集成开发环境（IDEs）中的信任校准：为人工智能重构的广泛应用铺平道路

发布时间：2024年12月20日

`LLM应用` `代码重构`

> Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring

# 摘要

> 在软件行业，添加新功能的冲动常常盖过了改进现有代码的必要。大型语言模型（LLMs）借助人工智能辅助重构，为大规模改进代码库带来了新途径。但 LLMs 存在固有风险，比如造成破坏性更改和引入安全漏洞。我们提倡在集成开发环境（IDEs）中封装与模型的交互，并运用可靠的保障手段来验证重构尝试。然而，对于人工智能重构的应用而言，关于信任发展的研究同样重要。在这篇立场文件中，我们依据自动化中人为因素研究的既定模型来规划未来工作。我们简述了在 CodeScene 内的行动研究，涵盖 1）新型 LLM 保障措施的开发以及 2）能传递恰当信任程度的用户交互。行业合作能够开展大规模的存储库分析和 A/B 测试，持续引导我们研究干预措施的设计。

> In the software industry, the drive to add new features often overshadows the need to improve existing code. Large Language Models (LLMs) offer a new approach to improving codebases at an unprecedented scale through AI-assisted refactoring. However, LLMs come with inherent risks such as braking changes and the introduction of security vulnerabilities. We advocate for encapsulating the interaction with the models in IDEs and validating refactoring attempts using trustworthy safeguards. However, equally important for the uptake of AI refactoring is research on trust development. In this position paper, we position our future work based on established models from research on human factors in automation. We outline action research within CodeScene on development of 1) novel LLM safeguards and 2) user interaction that conveys an appropriate level of trust. The industry collaboration enables large-scale repository analysis and A/B testing to continuously guide the design of our research interventions.

[Arxiv](https://arxiv.org/abs/2412.15948)