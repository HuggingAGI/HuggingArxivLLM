# 对拥有大型语言模型的新闻出版商可靠性标准的评估

发布时间：2024年12月20日

`LLM应用`

> Evaluation of Reliability Criteria for News Publishers with Large Language Models

# 摘要

> 在本研究中，我们探究了运用大型语言模型来辅助评估众多现存在线新闻出版商的可靠性，解决了仅依赖人类专家标注员来完成此项任务的不现实问题。在意大利新闻媒体市场的情境下，我们首先让模型用新闻文章的代表性样本评估专家设计的可靠性标准，接着将模型的答案和人类专家的答案进行对比。该数据集包含 340 篇新闻文章，每篇均由两名人类专家和 LLM 进行标注。考虑了六个标准，总计 6120 个标注。我们发现，在六个评估标准中的三个里，LLM 与人类标注员的结果具有良好一致性，其中包括检测文本负面针对实体或个人实例的关键能力。对于另外两个标准，比如检测耸人听闻的语言和识别新闻内容中的偏见，LLM 给出了还算合理的标注，不过存在一定的权衡。此外，我们表明 LLM 能够助力解决人类专家之间的分歧，特别是在识别负面针对等任务中。

> In this study, we investigate the use of a large language model to assist in the evaluation of the reliability of the vast number of existing online news publishers, addressing the impracticality of relying solely on human expert annotators for this task. In the context of the Italian news media market, we first task the model with evaluating expert-designed reliability criteria using a representative sample of news articles. We then compare the model's answers with those of human experts. The dataset consists of 340 news articles, each annotated by two human experts and the LLM. Six criteria are taken into account, for a total of 6,120 annotations. We observe good agreement between LLM and human annotators in three of the six evaluated criteria, including the critical ability to detect instances where a text negatively targets an entity or individual. For two additional criteria, such as the detection of sensational language and the recognition of bias in news content, LLMs generate fair annotations, albeit with certain trade-offs. Furthermore, we show that the LLM is able to help resolve disagreements among human experts, especially in tasks such as identifying cases of negative targeting.

[Arxiv](https://arxiv.org/abs/2412.15896)