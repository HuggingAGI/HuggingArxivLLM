# 小型语言与大型模型：对挪威语言持续训练的探究

发布时间：2024年12月09日

`LLM应用` `语言模型` `挪威语`

> Small Languages, Big Models: A Study of Continual Training on Languages of Norway

# 摘要

> 训练大型语言模型需要海量数据，这给挪威语这类使用范围不广的语言带来了难题，对于萨米语这类低资源语言更是如此。为应对此问题，我们提出了一种新颖的三阶段持续训练方法。我们还尝试将因果语言建模与掩码语言建模相结合，以获取更灵活的模型。基于我们的研究成果，我们训练、评估并公开推出了一个新的拥有 114 亿参数的挪威语（书面挪威语、新挪威语和北萨米语）大型生成语言模型：NorMistral-11B。

> Training large language models requires vast amounts of data, posing a challenge for less widely spoken languages like Norwegian and even more so for truly low-resource languages like Sámi. To address this issue, we present a novel three-stage continual training approach. We also experiment with combining causal and masked language modeling to get more flexible models. Based on our findings, we train, evaluate, and openly release a new large generative language model for Norwegian Bokmål, Nynorsk, and Northern Sámi with 11.4 billion parameters: NorMistral-11B.

[Arxiv](https://arxiv.org/abs/2412.06484)