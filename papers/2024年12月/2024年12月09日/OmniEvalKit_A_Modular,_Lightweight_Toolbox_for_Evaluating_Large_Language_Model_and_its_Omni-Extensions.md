# OmniEvalKit：一个用于评估大型语言模型及其全方位扩展的模块化、轻量型工具箱

发布时间：2024年12月09日

`LLM应用` `人工智能` `基准测试`

> OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions

# 摘要

> 大型语言模型（LLMs）发展迅速，其应用范围大幅拓展，涵盖了从多语言支持到特定领域任务以及多模态集成等方面。在本文中，我们推出了 OmniEvalKit 这一全新的基准测试工具箱，用于评估 LLMs 及其在多语言、多领域和多模态能力上的全方位拓展。现有的基准往往只聚焦于单个方面，而 OmniEvalKit 则提供了一个模块化、轻量且自动化的评估系统。它采用由静态构建器和动态数据流构成的模块化架构，推动新模型和数据集的无缝融合。OmniEvalKit 支持超 100 个 LLMs 和 50 个评估数据集，涵盖了数千种模型 - 数据集组合的全面评估。OmniEvalKit 致力于打造一个超轻量且可快速部署的评估框架，让 AI 社区的下游应用更加便捷和多样。

> The rapid advancements in Large Language Models (LLMs) have significantly expanded their applications, ranging from multilingual support to domain-specific tasks and multimodal integration. In this paper, we present OmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their omni-extensions across multilingual, multidomain, and multimodal capabilities. Unlike existing benchmarks that often focus on a single aspect, OmniEvalKit provides a modular, lightweight, and automated evaluation system. It is structured with a modular architecture comprising a Static Builder and Dynamic Data Flow, promoting the seamless integration of new models and datasets. OmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering comprehensive evaluations across thousands of model-dataset combinations. OmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable evaluation framework, making downstream applications more convenient and versatile for the AI community.

[Arxiv](https://arxiv.org/abs/2412.06693)