# MuMu-LLaMA：借助大型语言模型达成多模态音乐的理解与生成

发布时间：2024年12月09日

`LLM应用` `多模态`

> MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models

# 摘要

> 有关大型语言模型的研究在文本、语音、图像和视频领域已取得显著进步。但因缺乏优质注释的数据集，多模态音乐的理解与生成仍有待深入探索。为此，我们引入了一个拥有 167.69 小时多模态数据的数据集，涵盖文本、图像、视频及音乐注释。基于此数据集，我们提出了 MuMu-LLaMA 模型，它借助了音乐、图像和视频的预训练编码器。在音乐生成方面，我们整合了 AudioLDM 2 和 MusicGen。通过对音乐理解、文本转音乐生成、基于提示的音乐编辑以及多模态音乐生成这四个任务的评估表明，MuMu-LLaMA 优于前沿模型，展现出其在多模态音乐应用方面的潜力。

> Research on large language models has advanced significantly across text, speech, images, and videos. However, multi-modal music understanding and generation remain underexplored due to the lack of well-annotated datasets. To address this, we introduce a dataset with 167.69 hours of multi-modal data, including text, images, videos, and music annotations. Based on this dataset, we propose MuMu-LLaMA, a model that leverages pre-trained encoders for music, images, and videos. For music generation, we integrate AudioLDM 2 and MusicGen. Our evaluation across four tasks--music understanding, text-to-music generation, prompt-based music editing, and multi-modal music generation--demonstrates that MuMu-LLaMA outperforms state-of-the-art models, showing its potential for multi-modal music applications.

[Arxiv](https://arxiv.org/abs/2412.06660)