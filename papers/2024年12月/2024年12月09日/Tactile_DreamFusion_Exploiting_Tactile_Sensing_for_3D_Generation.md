# 《触觉 DreamFusion：借助触觉传感实现 3D 生成》

发布时间：2024年12月09日

`其他` `3D 生成` `触觉传感`

> Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation

# 摘要

> 3D 生成方法在扩散图像先验的助力下，已呈现出极具视觉吸引力的成果。但它们常常难以生成逼真的几何细节，致使表面过于平滑，或者几何细节在反照率图中烘焙得不准确。为应对此问题，我们引入了一种新方法，将触摸作为额外模态来优化生成的 3D 资产的几何细节。我们设计了轻量级的 3D 纹理场来合成视觉和触觉纹理，并由视觉和触觉领域的 2D 扩散模型先验加以引导。我们以高分辨率触觉法线为条件生成视觉纹理，并用定制的 TextureDreambooth 引导基于补丁的触觉纹理细化。我们还提出了一个多部分生成管线，能够在不同区域合成不同纹理。据我们所知，我们是首个借助高分辨率触觉传感来增强 3D 生成任务几何细节的。我们在文本转 3D 和图像转 3D 的设定中评估了我们的方法。我们的实验表明，我们的方法能提供定制且逼真的精细几何纹理，同时保持视觉和触摸两种模态的准确对齐。

> 3D generation methods have shown visually compelling results powered by diffusion image priors. However, they often fail to produce realistic geometric details, resulting in overly smooth surfaces or geometric details inaccurately baked in albedo maps. To address this, we introduce a new method that incorporates touch as an additional modality to improve the geometric details of generated 3D assets. We design a lightweight 3D texture field to synthesize visual and tactile textures, guided by 2D diffusion model priors on both visual and tactile domains. We condition the visual texture generation on high-resolution tactile normals and guide the patch-based tactile texture refinement with a customized TextureDreambooth. We further present a multi-part generation pipeline that enables us to synthesize different textures across various regions. To our knowledge, we are the first to leverage high-resolution tactile sensing to enhance geometric details for 3D generation tasks. We evaluate our method in both text-to-3D and image-to-3D settings. Our experiments demonstrate that our method provides customized and realistic fine geometric textures while maintaining accurate alignment between two modalities of vision and touch.

[Arxiv](https://arxiv.org/abs/2412.06785)