# 具有 EDA 感知的大型语言模型 RTL 生成

发布时间：2024年11月20日

`LLM应用` `芯片设计` `代码生成`

> EDA-Aware RTL Generation with Large Language Models

# 摘要

> 大型语言模型（LLMs）在生成 RTL 代码方面日益受到青睐。然而，即便是最前沿的 LLMs，于零样本设定下生成毫无差错的 RTL 代码依旧困难重重，常常引发需要手动且反复优化的问题。这一额外的调试流程会大幅增加验证工作的负担，凸显出从起始阶段就需要强大且自动化的纠错机制来保障代码的正确性。
    在本次研究中，我们推出了 AIvril2，这是一个具备自我验证能力且与 LLM 无关的智能框架，旨在通过对语法和功能错误的反复修正来提升 RTL 代码的生成效果。我们的方法借助了一个协同的多智能体系统，该系统融合了 EDA 工具生成的错误日志所提供的反馈，能够自动识别并解决设计瑕疵。在 VerilogEval-Human 基准套件上开展的实验结果显示，我们的框架显著提升了代码质量，较之前的方法提升近 3.4 倍。在最佳情形下，Verilog 的功能通过率达 77%，VHDL 的功能通过率达 66%，极大地增强了 LLM 驱动的 RTL 代码生成的可靠性。

> Large Language Models (LLMs) have become increasingly popular for generating RTL code. However, producing error-free RTL code in a zero-shot setting remains highly challenging for even state-of-the-art LLMs, often leading to issues that require manual, iterative refinement. This additional debugging process can dramatically increase the verification workload, underscoring the need for robust, automated correction mechanisms to ensure code correctness from the start.
  In this work, we introduce AIvril2, a self-verifying, LLM-agnostic agentic framework aimed at enhancing RTL code generation through iterative corrections of both syntax and functional errors. Our approach leverages a collaborative multi-agent system that incorporates feedback from error logs generated by EDA tools to automatically identify and resolve design flaws. Experimental results, conducted on the VerilogEval-Human benchmark suite, demonstrate that our framework significantly improves code quality, achieving nearly a 3.4$\times$ enhancement over prior methods. In the best-case scenario, functional pass rates of 77% for Verilog and 66% for VHDL were obtained, thus substantially improving the reliability of LLM-driven RTL code generation.

[Arxiv](https://arxiv.org/abs/2412.04485)