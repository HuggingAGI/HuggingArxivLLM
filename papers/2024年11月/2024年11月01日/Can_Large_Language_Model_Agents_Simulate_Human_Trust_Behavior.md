# 大型语言模型会模拟人类的信任行为吗？

发布时间：2024年11月01日

`LLM应用` `社会科学` `人工智能`

> Can Large Language Model Agents Simulate Human Trust Behavior?

# 摘要

> 大型语言模型（LLM）智能体在社会科学和角色扮演应用中被越来越多地用作模拟工具，但一个核心问题仍然存在：LLM智能体能否真正模拟人类行为？本文聚焦于人类互动中一个关键且基础的行为——信任，探讨LLM智能体是否能够模拟人类的信任行为。研究发现，在行为经济学中广受认可的信任博弈框架下，LLM智能体确实表现出信任行为，我们称之为智能体信任。进一步研究表明，GPT-4智能体在信任行为上与人类高度一致，证明了LLM智能体模拟人类信任行为的可行性。此外，我们还深入探讨了智能体信任的偏见、智能体对其他LLM智能体和人类的信任行为差异，以及在外部操控和高级推理策略等条件下的内在属性。本研究不仅为理解LLM智能体的行为提供了新视角，还揭示了LLM与人类之间超越价值对齐的基本类比关系。我们的发现为信任至关重要的应用场景提供了重要启示。

> Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.

[Arxiv](https://arxiv.org/abs/2402.04559)