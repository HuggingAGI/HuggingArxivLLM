# 可解释的音频编辑评估：基于多模态大型语言模型的思维链差异-共性推理

发布时间：2025年09月21日

`LLM应用` `媒体与娱乐`

> Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs

# 摘要

> 自动平均意见分（MOS）预测为客观指标提供了更具感知性的替代方案，能更深入地洞察被评估模型。随着多模态大型语言模型（MLLMs）的迅猛发展，其强大的感知与推理能力为更全面、可解释的音频质量评估提供了可能。本研究聚焦音频编辑评估这一难点任务，首次提出了基于MLLMs的自然语言自动化评估框架。该方法通过两项微调任务提升多音频理解能力，并结合思维链提示与轻量级指令微调，进一步强化逐步推理过程。实验结果显示，该框架能生成准确、可解释的文本化编辑评估结果，不仅与人类判断及客观指标高度吻合，还大幅超越了现有基线模型。相关代码与演示已开源，地址为https://github.com/NKU-HLT/Eval_Reasoning。

> Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at https://github.com/NKU-HLT/Eval_Reasoning.

[Arxiv](https://arxiv.org/abs/2509.16975)