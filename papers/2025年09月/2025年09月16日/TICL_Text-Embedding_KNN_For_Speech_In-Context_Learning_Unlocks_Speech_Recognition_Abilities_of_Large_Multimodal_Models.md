# TICL：基于文本嵌入KNN的语音上下文学习解锁大型多模态模型的语音识别能力

发布时间：2025年09月16日

`LLM应用` `基础理论`

> TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models

# 摘要

> 语音基础模型最近具备了语音上下文学习（SICL）能力。选择有效的上下文示例对SICL性能至关重要，但其选择方法尚未得到充分研究。本研究提出了适用于SICL的文本嵌入KNN方法（TICL）——这是一种简单流程，无需微调即可利用语义上下文提升现成大型多模态模型的语音识别能力。在带口音英语、多语言语音、儿童语音等挑战性自动语音识别任务中，我们的方法可使模型超越零样本性能，相对词错误率（WER）降幅高达84.7%。我们通过消融研究验证了该方法的鲁棒性和高效性。

> Speech foundation models have recently demonstrated the ability to perform Speech In-Context Learning (SICL). Selecting effective in-context examples is crucial for SICL performance, yet selection methodologies remain underexplored. In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline that uses semantic context to enhance off-the-shelf large multimodal models' speech recognition ability without fine-tuning. Across challenging automatic speech recognition tasks, including accented English, multilingual speech, and children's speech, our method enables models to surpass zero-shot performance with up to 84.7% relative WER reduction. We conduct ablation studies to show the robustness and efficiency of our method.

[Arxiv](https://arxiv.org/abs/2509.13395)