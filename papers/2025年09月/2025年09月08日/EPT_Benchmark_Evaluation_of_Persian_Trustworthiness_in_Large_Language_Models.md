# EPT Benchmark：大型语言模型波斯语可信度评估

发布时间：2025年09月08日

`LLM应用` `基础理论`

> EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models

# 摘要

> 大型语言模型（LLMs）依托先进深度学习架构，在海量数据上训练而成，不仅在各类语言任务中表现卓越，更已成为现代AI技术的核心基石。然而，如何确保其可信度仍是一大核心挑战——毕竟可靠性不仅关乎性能精准，更与伦理、文化及社会价值观的坚守密不可分。因此，精心校准训练数据、建立植根于文化的评估标准，是构建负责任AI系统的关键所在。本研究中，我们提出了EPT（波斯可信度评估）指标——这是一套融入文化视角的基准体系，专门用于从六个核心维度评估LLMs的可信度，包括真实性、安全性、公平性、鲁棒性、隐私性及伦理一致性。我们构建了带标签数据集，并结合基于LLM的自动化评估与人工评估两种方式，对ChatGPT、Claude、DeepSeek、Gemini、Grok、LLaMA、Mistral、Qwen等主流模型展开评测。结果显示，这些模型在安全维度存在明显短板，这也凸显了必须优先聚焦模型行为这一核心环节。不仅如此，研究还深入揭示了这些模型与波斯伦理文化价值观的契合程度，为推动可信且具备文化责任感的AI发展，指明了关键差距与未来机遇。相关数据集已公开，获取链接：https://github.com/Rezamirbagheri110/EPT-Benchmark。

> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.

[Arxiv](https://arxiv.org/abs/2509.06838)