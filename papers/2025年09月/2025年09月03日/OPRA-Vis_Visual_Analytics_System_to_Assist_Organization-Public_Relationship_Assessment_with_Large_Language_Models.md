# OPRA-Vis：基于大型语言模型的组织-公众关系评估辅助可视化分析系统

发布时间：2025年09月03日

`LLM应用` `媒体与娱乐`

> OPRA-Vis: Visual Analytics System to Assist Organization-Public Relationship Assessment with Large Language Models

# 摘要

> 对数字媒体中公众意见的分析，能帮助组织与公众维持良好关系。这类公共关系（PR）分析常需评估公众意见，例如衡量人们对组织的信任强度。预训练大型语言模型（LLMs）在支持组织-公众关系评估（OPRA）方面潜力巨大——它们可将非结构化公众文本映射到OPRA维度，并通过提示生成推理依据。但将LLMs应用于PR分析时，通常需在大规模标注数据集上微调，这既耗费人力又依赖专业知识，导致PR研究者难以实际运用。为此，本文提出OPRA-Vis——一个无需大规模标注数据、基于LLMs的OPRA可视化分析系统。该框架采用思维链（Chain-of-Thought）提示法，将PR专业知识直接融入推理过程，引导LLMs分析公众意见数据。此外，OPRA-Vis通过可视化呈现LLMs的分析线索与推理路径，助力用户探索、审视并优化模型决策。我们通过两个真实案例验证了其有效性，并从定量（与其他LLMs及提示策略对比）和定性（可用性、有效性评估及专家反馈）两方面展开评估。

> Analysis of public opinions collected from digital media helps organizations maintain positive relationships with the public. Such public relations (PR) analysis often involves assessing opinions, for example, measuring how strongly people trust an organization. Pre-trained Large Language Models (LLMs) hold great promise for supporting Organization-Public Relationship Assessment (OPRA) because they can map unstructured public text to OPRA dimensions and articulate rationales through prompting. However, adapting LLMs for PR analysis typically requires fine-tuning on large labeled datasets, which is both labor-intensive and knowledge-intensive, making it difficult for PR researchers to apply these models. In this paper, we present OPRA-Vis, a visual analytics system that leverages LLMs for OPRA without requiring extensive labeled data. Our framework employs Chain-of-Thought prompting to guide LLMs in analyzing public opinion data by incorporating PR expertise directly into the reasoning process. Furthermore, OPRA-Vis provides visualizations that reveal the clues and reasoning paths used by LLMs, enabling users to explore, critique, and refine model decisions. We demonstrate the effectiveness of OPRA-Vis through two real-world use cases and evaluate it quantitatively, through comparisons with alternative LLMs and prompting strategies, and qualitatively, through assessments of usability, effectiveness, and expert feedback.

[Arxiv](https://arxiv.org/abs/2509.03164)