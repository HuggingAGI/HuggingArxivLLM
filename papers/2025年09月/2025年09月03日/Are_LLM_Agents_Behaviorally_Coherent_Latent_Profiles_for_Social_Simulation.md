# # LLM智能体的行为是否具有一致性？社会模拟的潜在轮廓

发布时间：2025年09月03日

`Agent` `基础理论`

> Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation

# 摘要

> 大型语言模型（LLMs）的惊人能力引发了这样一种设想：合成智能体有望替代人类受试者，成为人类研究中的参与者。为验证这一说法是否成立，社会科学研究者大多聚焦于LLM生成的调查数据是否与LLM被设定模仿的人类对象的数据一致。而我们则提出了一个更核心的问题：智能体是否具备内部一致性？在不同实验场景下接受测试时，它们能否保持行为的稳定性？为此，我们设计了一项研究，一方面揭示智能体的内部状态，另一方面在基础对话场景中观察其行为。通过这种设计，我们得以验证一系列行为假设，判断智能体的对话行为是否与其被揭示的内部状态相符。研究结果表明，无论是不同模型家族还是不同规模的LLMs，均存在显著的内部不一致性。关键发现是：尽管智能体能够生成与人类对象相似的回答，但其内部一致性的缺失，成为它们无法准确替代人类受试者参与研究的致命短板。我们的模拟代码与数据已公开。

> The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.

[Arxiv](https://arxiv.org/abs/2509.03736)