# SinhalaMMLU：用于评估僧伽罗语多任务语言理解的综合基准

发布时间：2025年09月03日

`LLM应用` `教育科技`

> SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala

# 摘要

> 大型语言模型（LLMs）虽具备令人惊叹的通用知识与推理能力，但其评估却主要聚焦于全球性或以英语为中心的主题，往往忽略了低资源语言及文化特定内容。尽管近年来的多语言基准尝试填补这一空白，却多依赖自动翻译，可能导致错误并扭曲原始文化语境。为此，我们提出SinhalaMMLU——首个专为僧伽罗语（一种低资源语言）设计的多选题问答基准。该数据集包含7000余道题目，覆盖中学至大学教育阶段，与斯里兰卡国家课程体系保持一致，涉及6个领域、30个学科，既包含通用学术主题，也涵盖文化特定知识。我们在SinhalaMMLU上对26个LLM进行了评估，结果显示Claude 3.5 Sonnet和GPT-4o的平均准确率最高，分别为67%和62%，但所有模型的整体性能仍显不足。尤其是在人文这类文化底蕴深厚的领域，模型表现欠佳，这凸显出LLM在适应低资源语言及文化特定语境方面仍有巨大提升空间。

> Large Language Models (LLMs) demonstrate impressive general knowledge and reasoning abilities, yet their evaluation has predominantly focused on global or anglocentric subjects, often neglecting low-resource languages and culturally specific content. While recent multilingual benchmarks attempt to bridge this gap, many rely on automatic translation, which can introduce errors and misrepresent the original cultural context. To address this, we introduce SinhalaMMLU, the first multiple-choice question answering benchmark designed specifically for Sinhala, a low-resource language. The dataset includes over 7,000 questions spanning secondary to collegiate education levels, aligned with the Sri Lankan national curriculum, and covers six domains and 30 subjects, encompassing both general academic topics and culturally grounded knowledge. We evaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and GPT-4o achieve the highest average accuracies at 67% and 62% respectively, overall model performance remains limited. In particular, models struggle in culturally rich domains such as the Humanities, revealing substantial room for improvement in adapting LLMs to low-resource and culturally specific contexts.

[Arxiv](https://arxiv.org/abs/2509.03162)