# VulnRepairEval：一个基于漏洞利用的大型语言模型漏洞修复能力评估框架

发布时间：2025年09月03日

`LLM应用` `工业与制造`

> VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities

# 摘要

> 将大型语言模型（LLMs）应用于自动软件漏洞修复，在精心构建的评估数据集上展现出良好前景。然而，现有数据集大多依赖表面验证方法，而非基于漏洞利用的验证，这导致其在安全敏感应用中的性能被高估。本文提出VulnRepairEval——一个以功能性概念验证（PoC）漏洞利用为核心的评估框架。该框架提供全面的容器化评估流程，支持可复现的差异化评估，其修复成功的标准是原始漏洞利用在修改后的代码上无法执行。在基准构建过程中，我们进行了大量数据整理：处理了400多个CVE和约2500个潜在来源，从中提取出23个真实的Python漏洞实例，这些实例可通过有效的PoC进行自动化测试。通过VulnRepairEval，我们对12个主流LLM展开全面评估，结果发现其性能存在显著不足：即便是表现最佳的模型也仅成功修复了5/23个漏洞（约21.7%），暴露出其在安全敏感应用中的严重缺陷。失败分析显示，大多数修复失败源于漏洞定位不准确以及补丁存在语法或语义错误。而改进的提示策略和多智能体方法仅带来微小提升，整体效果基本未变。这项工作提出了一个严格且实用的LLM驱动漏洞修复评估框架，并强调了评估协议需真实反映现实漏洞利用场景的必要性。

> The adoption of Large Language Models (LLMs) for automated software vulnerability patching has shown promising outcomes on carefully curated evaluation sets. Nevertheless, existing datasets predominantly rely on superficial validation methods rather than exploit-based verification, leading to overestimated performance in security-sensitive applications. This paper introduces VulnRepairEval, an evaluation framework anchored in functional Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive, containerized evaluation pipeline that enables reproducible differential assessment, where repair success requires the original exploit to fail execution against the modified code. The benchmark construction involved extensive data curation: we processed over 400 CVEs and approximately 2,500 potential sources to extract a collection of authentic vulnerability instances (23 Python CVEs) amenable to automated testing with working PoCs. Through VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and observe a significant performance deficit: even the top-performing model successfully addresses merely 5/23 instances (about 21.7%), exposing critical weaknesses in security-focused applications. Our failure analysis reveals that most unsuccessful attempts stem from imprecise vulnerability identification and patches containing syntactic or semantic errors. Enhanced prompting strategies and multi-agent approaches yield minimal improvements, with overall effectiveness remaining largely unaffected. This work contributes a stringent, practical evaluation framework for LLM-driven vulnerability remediation and underscores the necessity for assessment protocols that authentically reflect real-world exploitation scenarios.

[Arxiv](https://arxiv.org/abs/2509.03331)