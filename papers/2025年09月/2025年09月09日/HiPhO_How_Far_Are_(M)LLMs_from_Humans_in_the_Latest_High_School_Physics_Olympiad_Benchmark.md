# HiPhO：最新高中物理奥赛基准测试中，（多模态）大型语言模型与人类差距几何？

发布时间：2025年09月09日

`LLM应用` `教育科技`

> HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?

# 摘要

> 最近，（多模态）大型语言模型（(M)LLMs）的物理推理能力日益受到关注。然而，现有的物理领域基准测试存在两大不足：一是未能系统、及时地覆盖现实中的物理竞赛（如物理奥赛），二是无法直接与人类表现对比。为此，我们推出了HiPhO——首个专注于高中物理奥赛、采用人类对齐评估方式的基准测试。具体来说，HiPhO有三大创新亮点。（1）全面的数据集：整合了2024-2025年最新的13套奥赛试题，覆盖国际及地区赛事，且包含混合模态，题目类型从纯文本到图表题均有涉及。（2）专业的评估机制：采用官方评分标准，从答案和步骤双层面进行细粒度打分，与人类考官评判标准完全一致，确保评估的高质量与领域针对性。（3）与人类参赛者的对标：依据官方奖牌分数线为模型颁发金、银、铜牌，实现（多模态）大型语言模型与人类参赛者的直接对标。我们对30个最先进的（多模态）大型语言模型开展了大规模评估，结果显示：在13套试题中，开源多模态大型语言模型大多处于铜牌水平及以下；开源大型语言模型进步显著，偶尔能摘得金牌；闭源推理型多模态大型语言模型则能斩获6至12枚金牌；但多数模型与满分仍相去甚远。这些结果表明：开源模型与顶尖学生的性能差距显著，闭源推理模型具备强大的物理推理能力，而（多模态）大型语言模型的整体性能仍有巨大提升空间。

> Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight a substantial performance gap between open-source models and top students, the strong physical reasoning capabilities of closed-source reasoning models, and the fact that there is still significant room for improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused benchmark for advancing multimodal physical reasoning, is open-source and available at https://github.com/SciYu/HiPhO.

[Arxiv](https://arxiv.org/abs/2509.07894)