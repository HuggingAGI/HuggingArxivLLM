# 借助反问题揭示大型语言模型的规模定律

发布时间：2025年09月09日

`LLM理论` `基础理论`

> Uncovering Scaling Laws for Large Language Models via Inverse Problems

# 摘要

> 大型语言模型（LLMs）作为大规模预训练模型，已在多个领域大放异彩，成就斐然。这些成就的背后，是数据与计算在复杂性和规模上的空前突破。但由于训练此类模型成本高昂，靠蛮力试错来优化LLMs显然不切实际。鉴于逆问题在揭示基础科学定律上的成功案例，本立场文件提出，逆问题同样能高效挖掘缩放定律，从而指导LLMs的构建，以更优的成本效益达成理想性能。

> Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.

[Arxiv](https://arxiv.org/abs/2509.07909)