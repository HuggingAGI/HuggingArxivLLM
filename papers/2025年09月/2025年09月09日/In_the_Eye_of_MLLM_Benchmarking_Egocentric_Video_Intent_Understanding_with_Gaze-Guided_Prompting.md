# MLLM之眼：基于注视引导提示的第一视角视频意图理解基准测试

发布时间：2025年09月09日

`LLM应用` `媒体与娱乐`

> In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting

# 摘要

> 先进的多模态大型语言模型（MLLMs）的出现，极大地提升了AI助手跨模态处理复杂信息的能力。近期，以自我为中心的视频通过在统一坐标系中直接捕捉用户的注意力、动作与上下文，为借助MLLMs实现主动且个性化的AI用户体验带来了新机遇。然而，现有基准忽视了凝视作为用户意图指标的关键作用。为此，我们提出EgoGazeVQA——一个以自我为中心的凝视引导视频问答基准，旨在通过凝视信息增强对更长日常视频的理解。该基准包含由MLLMs生成并经人工标注员优化的基于凝视的问答对。实验结果显示，现有MLLMs难以准确解读用户意图；相比之下，我们提出的凝视引导意图提示方法通过整合空间、时间及意图相关线索，显著提升了性能。我们还开展了凝视相关的微调实验，并分析了凝视估计准确性对提示效果的影响。这些结果凸显了在以自我为中心的场景中，凝视对打造更个性化、更高效AI助手的重要价值。

> The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistants' ability to process complex information across modalities. Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent. To address this gap, we introduce EgoGazeVQA, an egocentric gaze-guided video question answering benchmark that leverages gaze information to improve the understanding of longer daily-life videos. EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, our gaze-guided intent prompting methods significantly enhance performance by integrating spatial, temporal, and intent-related cues. We further conduct experiments on gaze-related fine-tuning and analyze how gaze estimation accuracy impacts prompting effectiveness. These results underscore the value of gaze for more personalized and effective AI assistants in egocentric settings.

[Arxiv](https://arxiv.org/abs/2509.07447)