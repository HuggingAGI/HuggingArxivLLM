# 从数字不信任走向编码化诚实：生成式AI于信任品市场的实验证据

发布时间：2025年09月07日

`LLM应用` `基础理论`

> From Digital Distrust to Codified Honesty: Experimental Evidence on Generative AI in Credence Goods Markets

# 摘要

> 生成式人工智能正在重塑专家服务的供给模式。本文通过一系列一次性实验，量化了大型语言模型（LLMs）对AI-AI、人-人、人-AI及人-AI-人专家市场在行为、福利与分配层面的影响。基于专家掌握消费者最优服务私人信息的信任品框架，我们发现，借助亲社会的专家偏好和更高的消费者信任，人-人市场的效率水平普遍高于AI-AI和人-AI市场。值得注意的是，LLM专家获得的剩余显著高于人类专家——却以消费者剩余为代价——这暗示可能存在刺激LLMs有害应用的不良激励。同时，在人-AI-人市场中，当有机会时，大多数人类专家会选择依赖LLM智能体，尤其是当他们能控制LLM的（社会）目标函数时。在此场景下，相当一部分专家将追求效率的偏好优先于纯粹的自身利益。向消费者披露这些偏好，通过边缘化自利的LLM专家和人类专家，可显著提升效率。因此，在透明度规则下，人-AI-人市场的表现优于人-人市场。然而，若信息模糊，效率提升便会消失，专家的不良激励依然存在。我们的研究结果揭示了在专家服务领域应用LLMs的潜在机遇与风险，也引发了诸多监管挑战。一方面，在信息不对称时，LLMs可能削弱人类信任，并通过自动化部分挤出专家的利他偏好；另一方面，LLMs使专家能够对其目标函数进行编码和传达，从而减少信息不对称并提高效率。

> Generative AI is transforming the provision of expert services. This article uses a series of one-shot experiments to quantify the behavioral, welfare and distribution consequences of large language models (LLMs) on AI-AI, Human-Human, Human-AI and Human-AI-Human expert markets. Using a credence goods framework where experts have private information about the optimal service for consumers, we find that Human-Human markets generally achieve higher levels of efficiency than AI-AI and Human-AI markets through pro-social expert preferences and higher consumer trust. Notably, LLM experts still earn substantially higher surplus than human experts -- at the expense of consumer surplus - suggesting adverse incentives that may spur the harmful deployment of LLMs. Concurrently, a majority of human experts chooses to rely on LLM agents when given the opportunity in Human-AI-Human markets, especially if they have agency over the LLM's (social) objective function. Here, a large share of experts prioritizes efficiency-loving preferences over pure self-interest. Disclosing these preferences to consumers induces strong efficiency gains by marginalizing self-interested LLM experts and human experts. Consequently, Human-AI-Human markets outperform Human-Human markets under transparency rules. With obfuscation, however, efficiency gains disappear, and adverse expert incentives remain. Our results shed light on the potential opportunities and risks of disseminating LLMs in the context of expert services and raise several regulatory challenges. On the one hand, LLMs can negatively affect human trust in the presence of information asymmetries and partially crowd-out experts' other-regarding preferences through automation. On the other hand, LLMs allow experts to codify and communicate their objective function, which reduces information asymmetries and increases efficiency.

[Arxiv](https://arxiv.org/abs/2509.06069)