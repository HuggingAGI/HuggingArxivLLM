# 人工智能与学术同行评审的未来

发布时间：2025年09月17日

`LLM应用` `基础理论`

> AI and the Future of Academic Peer Review

# 摘要

> 同行评审作为科学质量控制的核心机制，其履职能力却日益捉襟见肘。实证研究揭示了其严重缺陷：出版延迟漫长、审稿人负担集中于少数学者且持续加重、评审质量参差不齐、审稿人间共识度低，还存在基于性别、语言和机构声望的系统性偏见。数十年来的人为改革收效甚微。与此同时，人工智能（尤其是大型语言模型LLMs）正被期刊、资助机构和个体审稿人在同行评审全流程中试点应用。早期研究显示，AI辅助不仅能生成质量媲美人类的评审意见，加快审稿人遴选与反馈速度，减少部分偏见，却也带来了幻觉生成、保密风险、投机取巧、创新识别不足及信任危机等独特隐忧。本文将同行评审的目标与现存问题对应到具体LLM应用场景，系统分析相关争议及可行的风险防控措施。基于最新证据，我们提出：在不取代人类判断的前提下，有针对性的受监督LLM辅助或能有效提升错误检测能力、缩短评审周期、减轻审稿人负担。我们还指出，微调、检索增强及多智能体等先进架构，有望实现更可靠、可审计的跨学科评审。值得强调的是，伦理与实践考量并非旁枝末节，而是核心要素——AI辅助同行评审的合法性，既取决于技术水平，更依赖治理决策。未来之路不应是全盘接纳或一味排斥，而需开展目标明确、指标清晰、透明可问责的试点探索。

> Peer review remains the central quality-control mechanism of science, yet its ability to fulfill this role is increasingly strained. Empirical studies document serious shortcomings: long publication delays, escalating reviewer burden concentrated on a small minority of scholars, inconsistent quality and low inter-reviewer agreement, and systematic biases by gender, language, and institutional prestige. Decades of human-centered reforms have yielded only marginal improvements. Meanwhile, artificial intelligence, especially large language models (LLMs), is being piloted across the peer-review pipeline by journals, funders, and individual reviewers. Early studies suggest that AI assistance can produce reviews comparable in quality to humans, accelerate reviewer selection and feedback, and reduce certain biases, but also raise distinctive concerns about hallucination, confidentiality, gaming, novelty recognition, and loss of trust. In this paper, we map the aims and persistent failure modes of peer review to specific LLM applications and systematically analyze the objections they raise alongside safeguards that could make their use acceptable. Drawing on emerging evidence, we show that targeted, supervised LLM assistance can plausibly improve error detection, timeliness, and reviewer workload without displacing human judgment. We highlight advanced architectures, including fine-tuned, retrieval-augmented, and multi-agent systems, that may enable more reliable, auditable, and interdisciplinary review. We argue that ethical and practical considerations are not peripheral but constitutive: the legitimacy of AI-assisted peer review depends on governance choices as much as technical capacity. The path forward is neither uncritical adoption nor reflexive rejection, but carefully scoped pilots with explicit evaluation metrics, transparency, and accountability.

[Arxiv](https://arxiv.org/abs/2509.14189)