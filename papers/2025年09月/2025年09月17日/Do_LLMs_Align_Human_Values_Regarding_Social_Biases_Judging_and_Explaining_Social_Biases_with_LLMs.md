# # 大型语言模型（LLMs）是否与人类在社会偏见上的价值观对齐？利用大型语言模型评判并解释社会偏见

发布时间：2025年09月17日

`LLM应用` `基础理论`

> Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs

# 摘要

> 大型语言模型（LLMs）若与人类价值观相悖，可能引发不良后果，尤其在涉及复杂敏感社会偏见的场景中。以往研究通过专家设计或基于智能体的模拟偏见场景，揭示了LLMs与人类价值观的不一致问题。然而，目前尚不清楚LLMs与人类价值观的一致性在不同类型场景中是否存在差异（例如，包含负面问题与非负面问题的场景）。本研究旨在探究不同类型偏见场景下，LLMs与人类社会偏见价值观（HVSB）的一致性。通过对4个模型家族的12个LLMs及4个数据集的深入分析，我们发现参数规模大的LLMs未必具有更低的不一致率和攻击成功率。此外，LLMs对特定类型场景表现出一定的一致性偏好，且同一家族的LLMs往往具有更高的判断一致性。我们还研究了LLMs对HVSB解释的理解能力，结果显示不同LLMs对HVSB的理解无显著差异，同时发现LLMs更偏好自身生成的解释。此外，我们赋予小型语言模型（LMs）解释HVSB的能力，生成结果表明，微调后的小型LMs生成的解释更易读，但模型一致性相对较低。

> Large language models (LLMs) can lead to undesired consequences when misaligned with human values, especially in scenarios involving complex and sensitive social biases. Previous studies have revealed the misalignment of LLMs with human values using expert-designed or agent-based emulated bias scenarios. However, it remains unclear whether the alignment of LLMs with human values differs across different types of scenarios (e.g., scenarios containing negative vs. non-negative questions). In this study, we investigate the alignment of LLMs with human values regarding social biases (HVSB) in different types of bias scenarios. Through extensive analysis of 12 LLMs from four model families and four datasets, we demonstrate that LLMs with large model parameter scales do not necessarily have lower misalignment rate and attack success rate. Moreover, LLMs show a certain degree of alignment preference for specific types of scenarios and the LLMs from the same model family tend to have higher judgment consistency. In addition, we study the understanding capacity of LLMs with their explanations of HVSB. We find no significant differences in the understanding of HVSB across LLMs. We also find LLMs prefer their own generated explanations. Additionally, we endow smaller language models (LMs) with the ability to explain HVSB. The generation results show that the explanations generated by the fine-tuned smaller LMs are more readable, but have a relatively lower model agreeability.

[Arxiv](https://arxiv.org/abs/2509.13869)