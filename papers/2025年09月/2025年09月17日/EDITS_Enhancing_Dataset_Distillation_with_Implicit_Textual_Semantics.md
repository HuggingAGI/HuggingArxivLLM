# EDITS：借助隐式文本语义提升数据集蒸馏

发布时间：2025年09月17日

`LLM应用` `基础理论`

> EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics

# 摘要

> 数据集蒸馏旨在从原始大规模数据集中提炼出紧凑数据集，在保持模型性能竞争力的同时实现高效学习。然而，传统技术多聚焦于捕捉低级视觉特征，却忽略了图像中蕴含的高级语义与结构信息。为此，本文提出EDITS——一个利用图像数据中隐含文本语义实现增强蒸馏的全新框架。首先，视觉语言模型（VLM）生成的外部文本经全局语义查询模块与图像特征融合，构建先验聚类缓冲区；接着，局部语义感知从缓冲区筛选代表性样本，构建图像与文本原型，其中文本原型通过精心设计的提示引导大型语言模型（LLM）生成；最后，双原型引导策略借助扩散模型生成最终合成数据集。大量实验验证了该方法的有效性，源代码详见：https://github.com/einsteinxia/EDITS。

> Dataset distillation aims to synthesize a compact dataset from the original large-scale one, enabling highly efficient learning while preserving competitive model performance. However, traditional techniques primarily capture low-level visual features, neglecting the high-level semantic and structural information inherent in images. In this paper, we propose EDITS, a novel framework that exploits the implicit textual semantics within the image data to achieve enhanced distillation. First, external texts generated by a Vision Language Model (VLM) are fused with image features through a Global Semantic Query module, forming the prior clustered buffer. Local Semantic Awareness then selects representative samples from the buffer to construct image and text prototypes, with the latter produced by guiding a Large Language Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype Guidance strategy generates the final synthetic dataset through a diffusion model. Extensive experiments confirm the effectiveness of our method.Source code is available in: https://github.com/einsteinxia/EDITS.

[Arxiv](https://arxiv.org/abs/2509.13858)