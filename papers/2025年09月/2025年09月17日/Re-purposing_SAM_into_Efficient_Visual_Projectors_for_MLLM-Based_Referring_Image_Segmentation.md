# 重新赋能SAM：打造高效视觉投影器，助力基于MLLM的指称图像分割

发布时间：2025年09月17日

`LLM应用` `基础理论`

> Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation

# 摘要

> 近年来，将多模态大型语言模型（MLLM）与分割一切模型（SAM）相结合的指代图像分割（RIS）框架已取得显著成效。但将MLLM应用于分割任务时计算开销极大，这主要源于视觉token的冗余问题。我们发现，传统基于补丁的视觉投影器难以在减少视觉token数量与保留语义清晰度间找到平衡，为避免性能下降，通常会保留过长的token序列。受文本分词器启发，我们提出一种新型语义视觉投影器，它利用SAM生成的语义超像素识别图像中的“视觉词”。通过将语义超像素压缩并投影为视觉token，我们的方法能根据场景复杂度自适应缩短token序列，同时最大程度减少压缩过程中的语义损失。为减轻信息损失，我们提出语义超像素位置嵌入以增强MLLM对超像素几何形状和位置的感知；同时设计语义超像素聚合器，以保留超像素内部的细粒度细节和外部的全局上下文。实验结果显示，我们的方法在不损失性能的前提下将视觉token数量减少93%，显著提升了MLLM的训练和推理速度，且在RIS任务上性能超越现有压缩视觉投影器。

> Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive, primarily due to visual token redundancy. We observe that traditional patch-wise visual projectors struggle to strike a balance between reducing the number of visual tokens and preserving semantic clarity, often retaining overly long token sequences to avoid performance drops. Inspired by text tokenizers, we propose a novel semantic visual projector that leverages semantic superpixels generated by SAM to identify "visual words" in an image. By compressing and projecting semantic superpixels as visual tokens, our approach adaptively shortens the token sequence according to scene complexity while minimizing semantic loss in compression. To mitigate loss of information, we propose a semantic superpixel positional embedding to strengthen MLLM's awareness of superpixel geometry and position, alongside a semantic superpixel aggregator to preserve both fine-grained details inside superpixels and global context outside. Experiments show that our method cuts visual tokens by 93% without compromising performance, notably speeding up MLLM training and inference, and outperforming existing compressive visual projectors on RIS.

[Arxiv](https://arxiv.org/abs/2509.13676)