# 基于确定性可复制性的大型语言模型易处理非对称验证

发布时间：2025年09月13日

`Agent` `基础理论`

> Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability

# 摘要

> 大型语言模型（LLMs）的发展格局正迅速向动态多智能体系统转变。这在建立计算信任方面带来了一个核心挑战：一个智能体如何验证另一个智能体的输出确实由其声称的LLM生成，而非伪造或由更廉价、劣质的模型生成。为应对这一挑战，本文提出一种验证框架，实现了可处理的非对称成本——验证计算的成本远低于执行计算的成本。我们的方法基于确定性可复制性原则——这是自回归模型的固有属性，它严格要求计算环境同构，即所有智能体运行在完全一致的软硬件栈上。在此背景下，我们的框架允许多个验证者对LLM输出的随机小片段进行概率审计，并高效分配验证任务。模拟结果显示，目标验证的速度可比完全重新生成快12倍以上，且可通过调整参数来控制检测概率。通过为可审计LLM系统构建可处理的机制，我们的研究为负责任AI奠定了基础层，并成为未来探索更复杂异构多智能体系统的基石。

> The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic, multi-agent systems. This introduces a fundamental challenge in establishing computational trust, specifically how one agent can verify that another's output was genuinely produced by a claimed LLM, and not falsified or generated by a cheaper or inferior model. To address this challenge, this paper proposes a verification framework that achieves tractable asymmetric effort, where the cost to verify a computation is substantially lower than the cost to perform it. Our approach is built upon the principle of deterministic replicability, a property inherent to autoregressive models that strictly necessitates a computationally homogeneous environment where all agents operate on identical hardware and software stacks. Within this defined context, our framework enables multiple validators to probabilistically audit small, random segments of an LLM's output and it distributes the verification workload effectively. The simulations demonstrated that targeted verification can be over 12 times faster than full regeneration, with tunable parameters to adjust the detection probability. By establishing a tractable mechanism for auditable LLM systems, our work offers a foundational layer for responsible AI and serves as a cornerstone for future research into the more complex, heterogeneous multi-agent systems.

[Arxiv](https://arxiv.org/abs/2509.11068)