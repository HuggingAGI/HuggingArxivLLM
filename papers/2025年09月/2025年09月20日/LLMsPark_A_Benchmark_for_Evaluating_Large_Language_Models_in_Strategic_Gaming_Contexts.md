# LLMsPark：战略游戏场景下评估大型语言模型的基准测试

发布时间：2025年09月20日

`LLM应用` `基础理论`

> LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts

# 摘要

> 随着大型语言模型（LLMs）在各类任务中持续发展，仅靠单一指标已难以满足全面评估的需求，因此超越单一指标的综合评估愈发关键。要全面评估LLM的智能水平，考察其交互动态与策略行为是核心环节。为此，我们推出了LLMsPark——一个基于博弈论的评估平台，它在经典博弈论场景中衡量LLMs的决策策略与社会行为，并构建了多智能体环境以深入探索其策略深度。该系统通过排行榜排名与评分机制，对15个主流LLMs（涵盖商业及开源模型）进行了交叉评估。分数越高表明模型的推理与策略能力越强，同时也揭示了不同模型间鲜明的行为模式与性能差异。这项研究为评估LLMs的策略智能提供了全新视角，不仅丰富了现有评估基准，还拓展了其在交互式博弈论场景下的评估维度。相关基准与排名已在https://llmsparks.github.io/公开。

> As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.

[Arxiv](https://arxiv.org/abs/2509.16610)