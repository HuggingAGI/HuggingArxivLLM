# # 大型语言模型（LLMs）能否应对心理健康问题？——与人类治疗师的对比

发布时间：2025年09月15日

`LLM应用` `医疗健康`

> Can LLMs Address Mental Health Questions? A Comparison with Human Therapists

# 摘要

> 心理健康服务获取困难，推动了由大型语言模型（LLMs）驱动的数字工具和对话代理的应用，但其质量与接受度尚不明晰。为此，我们开展了一项研究，对比治疗师撰写的回复与ChatGPT、Gemini、Llama针对真实患者问题生成的内容。文本分析表明，LLMs生成的内容篇幅更长、可读性更佳、词汇更丰富，且语气更积极；相比之下，治疗师的回复则更多使用第一人称。在针对150名用户和23名持证治疗师的调查中，参与者评价LLM生成的内容比治疗师撰写的回复更清晰、更尊重、也更具支持性。但两组参与者均更倾向于接受人类治疗师的支持。这些发现既展现了LLMs在心理健康领域的潜力，也揭示了其局限性，同时强调需设计出平衡其沟通优势与信任、隐私、责任顾虑的方案。

> Limited access to mental health care has motivated the use of digital tools and conversational agents powered by large language models (LLMs), yet their quality and reception remain unclear. We present a study comparing therapist-written responses to those generated by ChatGPT, Gemini, and Llama for real patient questions. Text analysis showed that LLMs produced longer, more readable, and lexically richer responses with a more positive tone, while therapist responses were more often written in the first person. In a survey with 150 users and 23 licensed therapists, participants rated LLM responses as clearer, more respectful, and more supportive than therapist-written answers. Yet, both groups of participants expressed a stronger preference for human therapist support. These findings highlight the promise and limitations of LLMs in mental health, underscoring the need for designs that balance their communicative strengths with concerns of trust, privacy, and accountability.

[Arxiv](https://arxiv.org/abs/2509.12102)