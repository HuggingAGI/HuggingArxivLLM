# # 不止于个人可识别信息：用户如何尝试评估与缓解隐式LLM推理

发布时间：2025年09月15日

`LLM应用` `基础理论`

> Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference

# 摘要

> 像ChatGPT这样的大型语言模型（LLMs）能从看似无关紧要的文本中推断出个人属性，由此引发的隐私风险已远超记忆数据泄露的范畴。尽管已有研究证实了这些风险，但用户如何评估并应对此类风险仍有待探索。我们对240名美国参与者开展了调查，让他们评估文本片段的推断风险、报告担忧程度，并尝试通过重写来阻止推断。我们将参与者的重写内容与ChatGPT及最先进的文本净化工具Rescriber生成的重写内容进行了对比。结果表明，参与者很难预判推断风险，表现仅略优于随机水平。用户重写的有效率仅为28%——虽优于Rescriber，但不及ChatGPT。我们分析了参与者的重写策略，发现释义虽是最常用的方法，效果却最差；而抽象化和增加模糊性的策略则更为有效。本研究强调，在与LLM交互时，具备推断感知的设计至关重要。

> Large Language Models (LLMs) such as ChatGPT can infer personal attributes from seemingly innocuous text, raising privacy risks beyond memorized data leakage. While prior work has demonstrated these risks, little is known about how users estimate and respond. We conducted a survey with 240 U.S. participants who judged text snippets for inference risks, reported concern levels, and attempted rewrites to block inference. We compared their rewrites with those generated by ChatGPT and Rescriber, a state-of-the-art sanitization tool. Results show that participants struggled to anticipate inference, performing a little better than chance. User rewrites were effective in just 28\% of cases - better than Rescriber but worse than ChatGPT. We examined our participants' rewriting strategies, and observed that while paraphrasing was the most common strategy it is also the least effective; instead abstraction and adding ambiguity were more successful. Our work highlights the importance of inference-aware design in LLM interactions.

[Arxiv](https://arxiv.org/abs/2509.12152)