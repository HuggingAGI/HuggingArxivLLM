# JustEva：用于评估法律知识推理中LLM公平性的工具包

发布时间：2025年09月15日

`LLM应用` `法律科技`

> JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference

# 摘要

> 将大型语言模型（LLMs）融入法律实践引发了对司法公平的紧迫担忧，这尤其源于其“黑箱”运作特性。本研究推出JustEva——一个全面的开源评估工具包，专门用于衡量LLMs在法律任务中的公平性。JustEva的优势包括：(1)一套涵盖65项法外因素的结构化标签体系；(2)三大核心公平性指标——不一致性、偏见及不平衡误差；(3)可靠的统计推断方法；(4)直观的可视化呈现。该工具包支持两种实验类型，可实现完整的评估流程：(1)基于给定数据集生成LLMs的结构化输出；(2)通过回归等统计方法对LLMs的输出结果进行统计分析与推断。实证应用表明，JustEva揭示了当前LLMs存在显著的公平性缺陷，凸显了公平可信的LLM法律工具的缺失。JustEva为评估和改进法律领域的算法公平性提供了便捷工具和方法基础。

> The integration of Large Language Models (LLMs) into legal practice raises pressing concerns about judicial fairness, particularly due to the nature of their "black-box" processes. This study introduces JustEva, a comprehensive, open-source evaluation toolkit designed to measure LLM fairness in legal tasks. JustEva features several advantages: (1) a structured label system covering 65 extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and imbalanced inaccuracy; (3) robust statistical inference methods; and (4) informative visualizations. The toolkit supports two types of experiments, enabling a complete evaluation workflow: (1) generating structured outputs from LLMs using a provided dataset, and (2) conducting statistical analysis and inference on LLMs' outputs through regression and other statistical methods. Empirical application of JustEva reveals significant fairness deficiencies in current LLMs, highlighting the lack of fair and trustworthy LLM legal tools. JustEva offers a convenient tool and methodological foundation for evaluating and improving algorithmic fairness in the legal domain.

[Arxiv](https://arxiv.org/abs/2509.12104)