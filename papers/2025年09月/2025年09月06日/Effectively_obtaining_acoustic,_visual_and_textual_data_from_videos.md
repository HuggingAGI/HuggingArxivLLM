# 从视频中有效获取音频、视觉与文本数据

发布时间：2025年09月06日

`其他` `基础理论`

> Effectively obtaining acoustic, visual and textual data from videos

# 摘要

> 随着机器学习模型的应用日益广泛，对高质量、大规模多模态数据集的需求也随之激增。然而，这类数据集的获取仍存在局限，尤其是那些融合了声学、视觉与文本数据的数据集。为此，本文提出了一种从视频中提取相关音频-图像-文本观测数据的方法，以填补这一空白。我们详细阐述了选择合适视频、提取相关数据对，并利用图像转文本模型生成描述性文本的完整流程。该方法确保了不同模态间的语义关联紧密可靠，从而提升了所构建数据集在各类应用中的实用价值。我们还探讨了研究过程中遇到的挑战，并提出了提升数据质量的解决方案。最终构建的数据集已公开，旨在为多模态数据分析及机器学习领域的研究提供支持并推动其发展。

> The increasing use of machine learning models has amplified the demand for high-quality, large-scale multimodal datasets. However, the availability of such datasets, especially those combining acoustic, visual and textual data, remains limited. This paper addresses this gap by proposing a method to extract related audio-image-text observations from videos. We detail the process of selecting suitable videos, extracting relevant data pairs, and generating descriptive texts using image-to-text models. Our approach ensures a robust semantic connection between modalities, enhancing the utility of the created datasets for various applications. We also discuss the challenges encountered and propose solutions to improve data quality. The resulting datasets, publicly available, aim to support and advance research in multimodal data analysis and machine learning.

[Arxiv](https://arxiv.org/abs/2509.05786)