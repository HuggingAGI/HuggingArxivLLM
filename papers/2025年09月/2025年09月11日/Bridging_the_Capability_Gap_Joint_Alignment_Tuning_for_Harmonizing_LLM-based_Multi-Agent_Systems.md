# 弥合能力鸿沟：联合对齐调优助力协调基于LLM的多智能体系统

发布时间：2025年09月11日

`Agent` `基础理论`

> Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems

# 摘要

> 大型语言模型（LLMs）的进步推动了多智能体系统的构建，这些系统通过将职责分配给专门智能体（如生成子目标的规划智能体、执行工具操作的落地智能体）来解决复杂任务。然而现有方法多独立微调各智能体，导致智能体间能力脱节、协同效率低下。为此，我们提出MOAT——多智能体联合对齐调优框架，通过迭代对齐提升智能体协作能力。MOAT交替进行两个关键阶段：（1）规划智能体对齐阶段，优化规划智能体生成更易引导落地智能体的子目标序列；（2）落地智能体改进阶段，利用智能体自身生成的多样化子目标-动作对微调落地智能体，增强其泛化能力。理论分析证实，MOAT可确保训练过程非递减且渐进收敛。在六项基准测试中，MOAT性能超越现有最优基线，在保留任务上平均提升3.1%，在留出任务上平均提升4.4%。

> The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.

[Arxiv](https://arxiv.org/abs/2509.09629)