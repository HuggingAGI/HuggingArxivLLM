# Hunyuan-MT 技术报告

发布时间：2025年09月05日

`LLM应用` `基础理论`

> Hunyuan-MT Technical Report

# 摘要

> 本报告介绍了我们首个开源多语言翻译模型Hunyuan-MT-7B，它支持33种主要语言的双向互译，尤其聚焦于普通话与多种少数民族语言及方言的翻译。此外，为满足多样化翻译场景需求并提升测试性能，我们推出了受慢思考模式启发的Hunyuan-MT-Chimera-7B翻译模型。该模型整合Hunyuan-MT-7B在不同参数配置下生成的多份输出，性能超越传统基于思维链（CoT）的慢思考模型。模型开发采用专为多语言翻译设计的全流程训练方案：先通过通用及机器翻译导向的预训练夯实基础能力，再经监督微调（SFT）适配特定任务，最终通过强化学习（RL）与弱到强强化学习实现深度对齐。实验表明，Hunyuan-MT-7B与Hunyuan-MT-Chimera-7B显著优于所有同参数规模的专业翻译模型及多数最先进（SOTA）大型模型，尤其在普通话与少数民族语言及方言互译任务上表现卓越。在WMT2025共享任务（通用机器翻译）中，模型展现顶尖性能，31个语言对中有30个位列第一。这一结果充分印证了模型在多语言场景下的稳健性，覆盖中文、英文、日文等高资源语言，以及捷克语、马拉地语、爱沙尼亚语、冰岛语等低资源语言。

> In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic.

[Arxiv](https://arxiv.org/abs/2509.05209)