# FreezeVLA：对视觉-语言-动作模型的动作冻结攻击

发布时间：2025年09月24日

`Agent` `工业与制造`

> FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models

# 摘要

> 视觉-语言-动作（VLA）模型让智能体能够解读多模态输入并执行复杂的长时任务，由此推动机器人领域迅猛发展。然而，其针对对抗性攻击的安全性与鲁棒性却鲜少被深入探究。本研究识别并形式化定义了一个关键对抗性漏洞——对抗性图像可“冻结”VLA模型，使其无视后续指令。这种威胁会切断机器人“数字大脑”与物理动作的联系，可能在关键干预时造成“不作为”风险。为系统研究该漏洞，我们提出新型攻击框架FreezeVLA，它通过最小-最大双层优化生成并评估动作冻结攻击。在三个最先进VLA模型和四个机器人基准测试上的实验显示，FreezeVLA平均攻击成功率达76.2%，显著超越现有方法。此外，FreezeVLA生成的对抗性图像迁移能力极强，单张图像即可在多种语言指令下稳定引发瘫痪。研究结果揭示了VLA模型存在的关键安全隐患，同时凸显了构建鲁棒防御机制的迫切性。

> Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.

[Arxiv](https://arxiv.org/abs/2509.19870)