# SynchroRaMa：借助多模态情感嵌入的唇同步与情感感知说话人脸生成

发布时间：2025年09月24日

`LLM应用` `媒体与娱乐`

> SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding

# 摘要

> 音频驱动的说话人脸生成近年来备受关注，尤其适用于需要生动自然的人机交互场景。然而，现有多数情感感知方法仅依赖单一模态（音频或图像）提取情感嵌入，难以捕捉细腻的情感线索。此外，多数方法仅以单张参考图像为条件，无法充分展现动作或属性随时间的动态变化。为此，我们提出了SynchroRaMa——一个全新框架，它融合文本（情感分析）和音频（语音情感识别及音频导出的效价-唤醒特征）的情感信号，构建多模态情感嵌入，进而生成情感更丰富、更真实的说话人脸视频。为实现自然的头部动作与精准的唇同步，SynchroRaMa内置音频到运动（A2M）模块，可生成与输入音频同步的运动帧。最后，SynchroRaMa还将大型语言模型（LLM）生成的场景描述作为额外文本输入，以捕捉动态动作和高级语义属性。通过视觉与文本线索的双重约束，模型的时间一致性和视觉真实感得到显著提升。在基准数据集上的定量与定性实验显示，SynchroRaMa性能超越现有最先进技术，在图像质量、表情还原度和运动真实感上均有提升。用户研究进一步证实，SynchroRaMa在整体自然度、运动多样性及视频流畅度上的主观评分均高于其他竞争方法。项目页面详见<https://novicemm.github.io/synchrorama>。

> Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <https://novicemm.github.io/synchrorama>.

[Arxiv](https://arxiv.org/abs/2509.19965)