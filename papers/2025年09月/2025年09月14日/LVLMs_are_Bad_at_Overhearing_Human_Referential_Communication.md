# LVLMs 不擅长理解人类指称性交流

发布时间：2025年09月14日

`Agent` `基础理论`

> LVLMs are Bad at Overhearing Human Referential Communication

# 摘要

> 自然对话中，说话者会共同构建新的指称表达，并在后续对话中重复使用。理解这类表达是具身智能体的核心能力，使其能够在现实世界中完成任务——这需要整合语言、视觉与对话交互，并加以理解。我们以参与协作物体匹配任务的成对人类对话者的自然对话为语料，研究了七个最先进的大型视觉语言模型（LVLMs）作为“偷听者”的能力。结果发现，当前LVLMs在这类任务上仍面临挑战：即便偷听了同一对话者重复多轮同一任务的更多对话，它们也未能展现出稳定的性能提升。为此，我们已公开相关语料库与代码，旨在保障研究的可复现性并推动未来相关探索。

> During spontaneous conversations, speakers collaborate on novel referring expressions, which they can then re-use in subsequent conversations. Understanding such referring expressions is an important ability for an embodied agent, so that it can carry out tasks in the real world. This requires integrating and understanding language, vision, and conversational interaction. We study the capabilities of seven state-of-the-art Large Vision Language Models (LVLMs) as overhearers to a corpus of spontaneous conversations between pairs of human discourse participants engaged in a collaborative object-matching task. We find that such a task remains challenging for current LVLMs and they all fail to show a consistent performance improvement as they overhear more conversations from the same discourse participants repeating the same task for multiple rounds. We release our corpus and code for reproducibility and to facilitate future research.

[Arxiv](https://arxiv.org/abs/2509.11514)