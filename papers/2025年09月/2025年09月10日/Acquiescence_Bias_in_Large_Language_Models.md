# 大型语言模型中的顺从偏见

发布时间：2025年09月10日

`LLM应用` `基础理论`

> Acquiescence Bias in Large Language Models

# 摘要

> 默许偏差——即人类在调查中无论真实想法如何，都倾向于同意陈述的现象——已得到充分研究和证实。鉴于大型语言模型（LLMs）容易受输入中细微变化的影响，且训练数据来源于人类生成内容，我们有理由推测它们可能也存在类似倾向。为此，我们开展了一项研究，探究不同模型、任务及语言（英语、德语、波兰语）下LLMs是否存在默许偏差。研究结果显示，与人类不同，LLMs存在一种“否定倾向”——无论问题本身表示同意还是不同意，它们都更倾向于回答“否”。

> Acquiescence bias, i.e. the tendency of humans to agree with statements in surveys, independent of their actual beliefs, is well researched and documented. Since Large Language Models (LLMs) have been shown to be very influenceable by relatively small changes in input and are trained on human-generated data, it is reasonable to assume that they could show a similar tendency. We present a study investigating the presence of acquiescence bias in LLMs across different models, tasks, and languages (English, German, and Polish). Our results indicate that, contrary to humans, LLMs display a bias towards answering no, regardless of whether it indicates agreement or disagreement.

[Arxiv](https://arxiv.org/abs/2509.08480)