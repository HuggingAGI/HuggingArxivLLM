# 角色扮演对话中LLM生成与人类撰写回应的评估

发布时间：2025年09月22日

`LLM应用` `教育科技`

> Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues

# 摘要

> 在长篇、知识驱动的角色扮演对话中评估大型语言模型（LLMs）一直颇具挑战。本研究通过人工评估（N=38）与自动化的LLM评判评估，对比了多轮专业培训模拟中LLM生成与人类撰写的对话回应。人工评估结果显示，LLM生成的回应质量随轮次增加显著下降，尤其在自然度、上下文连贯性和整体质量上表现突出，而人类撰写的回应质量则逐步提升。与此发现一致，参与者也明确表示持续偏好人类撰写的对话。这些人工判断通过我们的自动化LLM评判评估得到了验证——Gemini 2.0 Flash在零样本 pairwise 偏好和随机6样本构念评分上均与人工评估者高度一致，证实了LLM与人类回应的质量差距随时间逐渐拉大。本研究构建了一个多轮评估基准，揭示了LLM在知识驱动角色扮演对话中的质量衰减问题，并提供了经过验证的混合评估框架，为LLMs在培训模拟中的可靠集成提供指导。

> Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation ($N=38$) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.

[Arxiv](https://arxiv.org/abs/2509.17694)