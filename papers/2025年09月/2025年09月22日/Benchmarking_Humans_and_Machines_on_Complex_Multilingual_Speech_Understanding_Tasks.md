# 复杂多语言语音理解任务中人类与机器的基准测试

发布时间：2025年09月22日

`LLM应用` `基础理论`

> Benchmarking Humans and Machines on Complex Multilingual Speech Understanding Tasks

# 摘要

> 听觉注意力与选择性相位锁定是人类在复杂声学场景及鸡尾酒会环境中理解语音的核心机制，但多语言被试的这些能力目前仍未被充分研究。尽管近年来机器对自然语音的理解已取得进展，但在重叠语音和混合通道语音的理解方面仍存在疑问。我们提出了一种系统范式，用于在多语言环境中研究人类与机器在清晰及混合通道语音下的语音问答任务。对人类听者而言，其对目标说话者的选择性注意力在母语（L1）中显著强于第二语言（L2）。在机器听辨方面，基于语音的大型语言模型（LLMs）在清晰单说话者条件下的表现已达到或超越人类水平，但在双说话者场景中却难以实现选择性注意。这些结果揭示了一个关键差异：人类依赖于母语中更高效的注意力线索，而LLMs则默认采用并行信息提取，其能力已超越人类。

> Auditory attention and selective phase-locking are central to human speech understanding in complex acoustic scenes and cocktail party settings, yet these capabilities in multilingual subjects remain poorly understood. While machine understanding of natural speech has advanced in recent years, questions persist about comprehension of overlapped and mixed-channel speech. We propose a systematic paradigm for studying humans and machines in speech question-answering tasks in multilingual settings with clean and mixed-channel speech. For human listeners, selective attention to a target speaker was significantly better in their native language (L1) than in their second language (L2). For machine listening, speech-based large language models (LLMs) match or exceed human performance in clean, single-speaker conditions but often struggle to selectively attend in two-speaker settings. These results reveal a key divergence: humans rely on attentional cues that are more streamlined in their native language, whereas LLMs default to parallel information extraction which exceed human skills.

[Arxiv](https://arxiv.org/abs/2509.17965)