# PalmX 2025：首个阿拉伯语与伊斯兰文化大型语言模型（LLMs）基准测试共享任务

发布时间：2025年09月02日

`LLM应用` `基础理论`

> PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture

# 摘要

> 大型语言模型（LLMs）本质上是其预训练阶段所接触海量数据分布的体现。由于这些数据主要源自网络，因此很可能偏向资源丰富的语言和文化，如西方文化。LLMs对部分群体的理解能力因此大打折扣，这种认知差距在阿拉伯与伊斯兰文化领域尤为突出——而对于那些代表性愈发不足的主题，这一问题则更为严重。为应对这一核心挑战，我们推出了PalmX 2025——首个针对LLMs在特定领域文化能力的基准评测共享任务。该任务包含两个子任务，均采用现代标准阿拉伯语（MSA）的多项选择题（MCQs）形式，分别聚焦阿拉伯通用文化与伊斯兰通用文化。子任务覆盖主题广泛，囊括22个阿拉伯国家的传统习俗、饮食文化、历史脉络、宗教实践及语言表达。该项目引发广泛关注，26支团队报名参与子任务1，19支团队参与子任务2，最终分别收到9份和6份有效提交。研究结果显示，特定任务微调能大幅提升模型性能，显著优于基线模型：表现最优的系统在文化类问题上准确率达72.15%，在伊斯兰知识类问题上则高达84.22%。参数高效微调成为参与者采用的主流且最有效的方法，而数据增强的效果则因领域而异。

> Large Language Models (LLMs) inherently reflect the vast data distributions they encounter during their pre-training phase. As this data is predominantly sourced from the web, there is a high chance it will be skewed towards high-resourced languages and cultures, such as those of the West. Consequently, LLMs often exhibit a diminished understanding of certain communities, a gap that is particularly evident in their knowledge of Arabic and Islamic cultures. This issue becomes even more pronounced with increasingly under-represented topics. To address this critical challenge, we introduce PalmX 2025, the first shared task designed to benchmark the cultural competence of LLMs in these specific domains. The task is composed of two subtasks featuring multiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General Arabic Culture and General Islamic Culture. These subtasks cover a wide range of topics, including traditions, food, history, religious practices, and language expressions from across 22 Arab countries. The initiative drew considerable interest, with 26 teams registering for Subtask 1 and 19 for Subtask 2, culminating in nine and six valid submissions, respectively. Our findings reveal that task-specific fine-tuning substantially boosts performance over baseline models. The top-performing systems achieved an accuracy of 72.15% on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient fine-tuning emerged as the predominant and most effective approach among participants, while the utility of data augmentation was found to be domain-dependent.

[Arxiv](https://arxiv.org/abs/2509.02550)