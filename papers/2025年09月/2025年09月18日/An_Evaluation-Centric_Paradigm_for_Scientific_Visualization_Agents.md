# # 以评估为核心的科学可视化智能体范式

发布时间：2025年09月18日

`Agent` `基础理论`

> An Evaluation-Centric Paradigm for Scientific Visualization Agents

# 摘要

> 近年来，多模态大型语言模型（MLLMs）的进步催生了日益复杂的自主可视化智能体，它们能将用户意图转化为数据可视化。然而，衡量其进展并比较不同智能体仍颇具挑战，尤其在科学可视化（SciVis）领域——因为缺乏评估实际能力的全面、大规模基准。本文作为立场文件，探讨了科学可视化智能体所需的各类评估，概述了相关挑战，提供了一个简单的概念验证评估示例，并讨论了评估基准如何助力智能体实现自我提升。我们倡导开展更广泛的合作，共同开发科学可视化智能体评估基准——该基准不仅能评估现有能力，还能推动创新并助力该领域的未来发展。

> Recent advances in multi-modal large language models (MLLMs) have enabled increasingly sophisticated autonomous visualization agents capable of translating user intentions into data visualizations. However, measuring progress and comparing different agents remains challenging, particularly in scientific visualization (SciVis), due to the absence of comprehensive, large-scale benchmarks for evaluating real-world capabilities. This position paper examines the various types of evaluation required for SciVis agents, outlines the associated challenges, provides a simple proof-of-concept evaluation example, and discusses how evaluation benchmarks can facilitate agent self-improvement. We advocate for a broader collaboration to develop a SciVis agentic evaluation benchmark that would not only assess existing capabilities but also drive innovation and stimulate future development in the field.

[Arxiv](https://arxiv.org/abs/2509.15160)