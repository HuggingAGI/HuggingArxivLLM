# 基于强化学习与人工智能的交通网络分布偏移问题

发布时间：2025年09月18日

`强化学习` `交通运输`

> The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI

# 摘要

> 近年来，机器学习（ML）与人工智能（AI）在智能交通网络中的应用呈显著增长趋势。在众多机器学习与人工智能方法中，强化学习（RL）被多位研究者认为极具应用前景。然而，在交通信号控制场景中应用强化学习时，输入数据分布会随训练数据分布动态变化，这导致训练后的RL智能体可靠性不足。这不仅给训练后的AI智能体网络带来了严峻挑战和可靠性隐患，若缺乏有效解决方案，甚至可能引发严重的不良后果。为此，研究者们尝试了多种方法来攻克这一难题。其中，元强化学习（Meta RL）被认为是极具潜力的解决方案。本文评估并分析了当前最先进的元强化学习方法MetaLight，结果显示：尽管在特定条件下MetaLight能取得较好效果，但在其他条件下性能表现欠佳（误差最高达22%）。这表明，元强化学习方案的稳健性不足，甚至可能引发严重的可靠性问题。

> The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart transportation networks has increased significantly in the last few years. Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of AI agents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular, Meta Reinforcement Learning (Meta RL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that Meta RL schemes are often not robust enough and can even pose major reliability problems.

[Arxiv](https://arxiv.org/abs/2509.15291)