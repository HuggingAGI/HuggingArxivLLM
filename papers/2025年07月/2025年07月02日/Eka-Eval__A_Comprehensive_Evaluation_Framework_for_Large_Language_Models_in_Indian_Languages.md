# # Eka-Eval：针对印度语言的大型语言模型综合评估框架
Eka-Eval 是一个全面的评估框架，专为印度语言中的大型语言模型（LLMs）设计。

发布时间：2025年07月02日

`LLM应用` `人工智能`

> Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages

# 摘要

> 大型语言模型（LLMs）的快速发展进一步凸显了对超越以英语为中心的基准、并满足印度等语言多样性地区需求的评估框架的必要性。我们提出了EKA-EVAL，一个统一且生产级的评估框架，整合了35多个基准测试，其中包括10个印度语言特定的数据集，覆盖推理、数学、工具使用、长上下文理解以及阅读理解等多个领域。与现有的印度语言评估工具相比，EKA-EVAL提供了更广泛的基准覆盖范围，并内置支持分布式推理、量化和多GPU使用。我们的系统性对比表明，EKA-EVAL是首个专为全球和印度语言LLMs设计的端到端、可扩展的评估套件，显著降低了多语言基准测试的门槛。该框架是开源的，可在https://github.com/lingo-iitgn/ eka-eval获取，并作为正在进行的EKA计划（https://eka.soket.ai）的一部分，旨在扩展至100多个基准测试，并为LLMs建立一个强大、多语言的评估生态系统。

> The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

[Arxiv](https://arxiv.org/abs/2507.01853)