# 内部失调：大型语言模型再现人类双环学习盲点

发布时间：2025年07月02日

`LLM理论

摘要讨论了大型语言模型（LLMs）在对齐问题中的行为和潜在影响，属于理论层面的探讨。` `组织管理` `人工智能`

> Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness

# 摘要

> 本文探讨了一个关键却未被深入研究的AI对齐问题：大型语言模型（LLMs）可能继承并放大人类宣称的理论与实际使用理论之间的不一致。基于行动科学的发现，我们发现经过人类文本训练的LLMs很可能吸收并复制了Model 1的实际使用理论——这种防御性推理模式会抑制学习，并在个体、群体和组织层面产生持续的反学习动态。通过一个LLM扮演人力资源顾问的案例研究，我们发现其建议虽然看似专业，却系统性地强化了低效的问题解决方法，阻碍了组织深入学习的路径。这揭示了一个AI对齐问题的独特实例：AI系统成功模仿了人类行为，却继承了我们的认知盲点。如果将LLMs应用于组织决策，可能固化反学习实践并为之赋予权威，带来特殊风险。本文最后探讨了开发能够促进Model 2学习的LLMs的可能性——这是一种更高效的理论使用方式——并认为这一努力可能推动AI对齐研究和行动科学实践的进步。这种分析揭示了对齐挑战中一个意想不到的对称性：开发与人类价值观恰当对齐的AI系统的过程，可能产生帮助人类自身更好地践行这些价值观的工具。

> This paper examines a critical yet unexplored dimension of the AI alignment problem: the potential for Large Language Models (LLMs) to inherit and amplify existing misalignments between human espoused theories and theories-in-use. Drawing on action science research, we argue that LLMs trained on human-generated text likely absorb and reproduce Model 1 theories-in-use - a defensive reasoning pattern that both inhibits learning and creates ongoing anti-learning dynamics at the dyad, group, and organisational levels. Through a detailed case study of an LLM acting as an HR consultant, we show how its advice, while superficially professional, systematically reinforces unproductive problem-solving approaches and blocks pathways to deeper organisational learning. This represents a specific instance of the alignment problem where the AI system successfully mirrors human behaviour but inherits our cognitive blind spots. This poses particular risks if LLMs are integrated into organisational decision-making processes, potentially entrenching anti-learning practices while lending authority to them. The paper concludes by exploring the possibility of developing LLMs capable of facilitating Model 2 learning - a more productive theory-in-use - and suggests this effort could advance both AI alignment research and action science practice. This analysis reveals an unexpected symmetry in the alignment challenge: the process of developing AI systems properly aligned with human values could yield tools that help humans themselves better embody those same values.

[Arxiv](https://arxiv.org/abs/2507.02283)