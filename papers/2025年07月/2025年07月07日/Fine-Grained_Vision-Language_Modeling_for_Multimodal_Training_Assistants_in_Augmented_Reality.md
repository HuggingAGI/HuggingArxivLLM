# 细粒度视觉-语言建模赋能增强现实中的多模态训练助手

发布时间：2025年07月07日

`LLM应用

理由：这篇论文探讨了视觉语言模型（VLMs）在增强现实（AR）训练中的应用，属于将大型语言模型应用于特定领域或任务的范畴。` `增强现实` `人工智能`

> Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality

# 摘要

> 视觉语言模型（VLMs）对于实现AI驱动的智能助手在多模态环境中的理解和推理至关重要。然而，它们在增强现实（AR）训练中的应用尚未得到充分探索。在本研究中，我们引入了一个专为AR训练设计的全面数据集，包含系统化的视觉语言任务，并在该数据集上评估了九种最先进的VLMs。结果显示，即使是像GPT-4o这样先进的模型，在精细的组装任务上也表现不佳，状态检测的F1分数最高仅为40.54%。这表明我们需要更好的数据集、基准和进一步研究来提升精细视觉语言对齐性能。除了技术上的贡献，我们的研究还具有更广泛的社会意义，特别是在为盲人和视障用户提供平等的AI驱动学习机会方面。我们提供了所有相关资源，包括数据集、源代码和评估结果，以支持研究社区的发展。

> Vision-language models (VLMs) are essential for enabling AI-powered smart assistants to interpret and reason in multimodal environments. However, their application in augmented reality (AR) training remains largely unexplored. In this work, we introduce a comprehensive dataset tailored for AR training, featuring systematized vision-language tasks, and evaluate nine state-of-the-art VLMs on it. Our results reveal that even advanced models, including GPT-4o, struggle with fine-grained assembly tasks, achieving a maximum F1 score of just 40.54% on state detection. These findings highlight the demand for enhanced datasets, benchmarks, and further research to improve fine-grained vision-language alignment. Beyond technical contributions, our work has broader social implications, particularly in empowering blind and visually impaired users with equitable access to AI-driven learning opportunities. We provide all related resources, including the dataset, source code, and evaluation results, to support the research community.

[Arxiv](https://arxiv.org/abs/2507.05515)