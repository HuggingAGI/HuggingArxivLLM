# 深度研究比较器：用于深度研究智能体精细人工标注的平台

发布时间：2025年07月07日

`LLM应用` `人工智能` `计算机科学`

> Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents

# 摘要

> # 摘要
有效评估具备自主网络搜索、信息分析及报告生成能力的深度研究代理仍是一项重大挑战，特别是在长篇报告评估与中间步骤详细反馈方面。为解决这些难题，我们推出深度研究比较器（Deep Research Comparator），一个集深度研究代理托管、并行对比视图、细致反馈收集与排名计算于一体的综合性平台。针对用户查询，平台不仅展示两个不同代理的最终报告，还呈现其生成过程中的关键步骤。标注人员可基于并行对比评估报告整体质量，亦可针对中间步骤或报告特定内容提供详细反馈。此外，我们开发了Simple Deepresearch，一个端到端代理框架，作为基线，便于各类大型语言模型快速集成，转化为深度研究代理以供评估。为展示平台在深度研究代理开发中的实用价值，我们从17位标注者处收集了针对三个深度研究代理的真实用户偏好数据。平台演示视频可在https://www.youtube.com/watch?v=g4d2dnbdseg观看。

> Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.

[Arxiv](https://arxiv.org/abs/2507.05495)