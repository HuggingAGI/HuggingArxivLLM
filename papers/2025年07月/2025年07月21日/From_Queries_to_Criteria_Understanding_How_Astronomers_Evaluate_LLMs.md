# 从查询到标准，探索天文学家如何评估大型语言模型

发布时间：2025年07月21日

`LLM应用` `天文学`

> From Queries to Criteria: Understanding How Astronomers Evaluate LLMs

# 摘要

> 近年来，利用大型语言模型（LLMs）辅助天文学和其他科学研究的兴趣日益增长，但LLMs的通用评估基准并未跟上人们日益多样化地评估和使用这些模型的方式。本研究旨在通过理解用户如何评估LLMs来改进评估流程。我们聚焦于一个特定的应用场景：一个通过Slack部署的、基于LLMs的检索增强生成式机器人，用于与天文学文献互动。通过对四个月内368次查询的归纳编码，以及对11位天文学家的后续访谈，我们揭示了人类如何评估这一系统，包括提出的问题类型以及判断响应的标准。我们将研究发现整合为构建更好评估基准的具体建议，并据此构建了一个用于评估天文学领域LLMs的示例基准。总体而言，我们的工作为改进LLMs的评估流程和最终的可用性提供了方法，特别是在科学研究中的应用。

> There is growing interest in leveraging LLMs to aid in astronomy and other scientific research, but benchmarks for LLM evaluation in general have not kept pace with the increasingly diverse ways that real people evaluate and use these models. In this study, we seek to improve evaluation procedures by building an understanding of how users evaluate LLMs. We focus on a particular use case: an LLM-powered retrieval-augmented generation bot for engaging with astronomical literature, which we deployed via Slack. Our inductive coding of 368 queries to the bot over four weeks and our follow-up interviews with 11 astronomers reveal how humans evaluated this system, including the types of questions asked and the criteria for judging responses. We synthesize our findings into concrete recommendations for building better benchmarks, which we then employ in constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our work offers ways to improve LLM evaluation and ultimately usability, particularly for use in scientific research.

[Arxiv](https://arxiv.org/abs/2507.15715)