# 揭示变体信息，调整多模态大语言模型生成图像描述的可靠性感知

发布时间：2025年07月21日

`LLM应用` `视障辅助` `人工智能`

> Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions

# 摘要

> 多模态大型语言模型 (MLLMs) 为视障和低视力 (BLV) 人群打开了获取视觉信息的新窗口。然而，这些模型的输出常存在难以察觉的错误，给视障人士在药物识别、服装搭配等场景中带来了安全隐患。尽管 BLV 用户会采用跨工具比对或咨询明眼人等方法，但这些方式往往费时费力。我们研究了如何通过揭示多个 MLLM 响应中的差异，帮助视障用户在无需视觉检查的情况下识别不可靠信息。我们提出了一套设计框架，用于提取和展示 MLLM 描述中的差异，并开发了一个原型系统，实现了三种差异展示方式。通过与 15 名视障用户的合作研究，我们发现：展示差异能显著提升用户识别不可靠信息的能力（与单一描述相比，使用我们的方法提升了 4.9 倍），同时显著降低了用户对 MLLM 响应可靠性的过高期待。14 名参与者更倾向于看到 MLLM 响应的差异展示，而非单一描述，所有人都对将我们的系统应用于从理解龙卷风路径到在社交媒体发图等任务表示兴趣。

> Multimodal large language models (MLLMs) provide new opportunities for blind and low vision (BLV) people to access visual information in their daily lives. However, these models often produce errors that are difficult to detect without sight, posing safety and social risks in scenarios from medication identification to outfit selection. While BLV MLLM users use creative workarounds such as cross-checking between tools and consulting sighted individuals, these approaches are often time-consuming and impractical. We explore how systematically surfacing variations across multiple MLLM responses can support BLV users to detect unreliable information without visually inspecting the image. We contribute a design space for eliciting and presenting variations in MLLM descriptions, a prototype system implementing three variation presentation styles, and findings from a user study with 15 BLV participants. Our results demonstrate that presenting variations significantly increases users' ability to identify unreliable claims (by 4.9x using our approach compared to single descriptions) and significantly decreases perceived reliability of MLLM responses. 14 of 15 participants preferred seeing variations of MLLM responses over a single description, and all expressed interest in using our system for tasks from understanding a tornado's path to posting an image on social media.

[Arxiv](https://arxiv.org/abs/2507.15692)