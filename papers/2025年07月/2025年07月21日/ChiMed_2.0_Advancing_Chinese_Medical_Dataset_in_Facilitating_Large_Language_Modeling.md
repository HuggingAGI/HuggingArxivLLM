# ChiMed 2.0：推动中文医学数据集发展，助力大型语言建模

发布时间：2025年07月21日

`LLM应用

理由：这篇论文主要讨论了中文医学数据集的构建及其在LLM预训练和微调中的应用，属于LLM应用领域。` `中文医学`

> ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling

# 摘要

> 构建高质量的数据资源对于推进特定领域的人工智能研究和应用至关重要，特别是在中文医学领域。现有中文医学数据集规模有限、领域覆盖狭窄，难以满足有效预训练所需的多样化语料需求。此外，现有数据集大多仅用于LLM的微调，不支持预训练和基于人类反馈的强化学习（RLHF）。本文中，我们提出了名为ChiMed 2.0的中文医学数据集，它扩展了我们之前的工作ChiMed，涵盖了从中文医学在线平台收集的数据以及由LLM生成的数据。ChiMed 2.0包含204.4亿个中文字符，涵盖传统中医经典和现代通用医学数据，其中包含16.48万个文档用于预训练，35.16万个问答对用于监督微调（SFT），以及4.17万个偏好数据元组用于RLHF。为了验证我们训练中文医学LLM方法的有效性，我们对具有代表性的通用领域LLM进行了进一步的预训练、SFT和RLHF实验，并在医学基准数据集上评估了它们的性能。结果显示不同模型规模的性能均有提升，验证了数据集的有效性和适用性。

> Building high-quality data resources is crucial for advancing artificial intelligence research and applications in specific domains, particularly in the Chinese medical domain. Existing Chinese medical datasets are limited in size and narrow in domain coverage, falling short of the diverse corpora required for effective pre-training. Moreover, most datasets are designed solely for LLM fine-tuning and do not support pre-training and reinforcement learning from human feedback (RLHF). In this paper, we propose a Chinese medical dataset named ChiMed 2.0, which extends our previous work ChiMed, and covers data collected from Chinese medical online platforms and generated by LLMs. ChiMed 2.0 contains 204.4M Chinese characters covering both traditional Chinese medicine classics and modern general medical data, where there are 164.8K documents for pre-training, 351.6K question-answering pairs for supervised fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the effectiveness of our approach for training a Chinese medical LLM, we conduct further pre-training, SFT, and RLHF experiments on representative general domain LLMs and evaluate their performance on medical benchmark datasets. The results show performance gains across different model scales, validating the dataset's effectiveness and applicability.

[Arxiv](https://arxiv.org/abs/2507.15275)