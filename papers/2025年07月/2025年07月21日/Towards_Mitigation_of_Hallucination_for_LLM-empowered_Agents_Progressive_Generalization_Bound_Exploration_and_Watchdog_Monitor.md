# # 摘要
大型语言模型（LLMs）的最新进展推动了从机器人流程自动化到智能体流程自动化的革命性转变，通过基于LLMs的自动化工作流编排实现了这一目标。

发布时间：2025年07月21日

`Agent` `人工智能` `智能体`

> Towards Mitigation of Hallucination for LLM-empowered Agents: Progressive Generalization Bound Exploration and Watchdog Monitor

# 摘要

> 大型语言模型（LLMs）赋予了智能体与开放环境交互的能力，从而推动了AI的广泛应用。然而，LLMs生成的幻觉——即输出与事实不符——严重威胁了智能体的可信度。只有有效缓解幻觉，智能体才能在现实世界中安全应用。因此，检测和缓解幻觉对于确保智能体的可靠性至关重要。现有方法要么依赖于对LLMs的白盒访问，要么无法准确识别幻觉。为应对这一挑战，我们提出了一种新型的黑盒 watchdog 框架 HalMit，通过建模基于LLMs的智能体的泛化边界，在无需了解LLM架构的情况下检测幻觉。HalMit采用概率分形采样技术，高效生成查询以触发异常响应，从而准确识别智能体的泛化边界。实验结果表明，HalMit在幻觉监控方面显著优于现有方法。凭借其黑盒特性和卓越性能，HalMit为提升基于LLM系统的可靠性提供了有前景的解决方案。

> Empowered by large language models (LLMs), intelligent agents have become a popular paradigm for interacting with open environments to facilitate AI deployment. However, hallucinations generated by LLMs-where outputs are inconsistent with facts-pose a significant challenge, undermining the credibility of intelligent agents. Only if hallucinations can be mitigated, the intelligent agents can be used in real-world without any catastrophic risk. Therefore, effective detection and mitigation of hallucinations are crucial to ensure the dependability of agents. Unfortunately, the related approaches either depend on white-box access to LLMs or fail to accurately identify hallucinations. To address the challenge posed by hallucinations of intelligent agents, we present HalMit, a novel black-box watchdog framework that models the generalization bound of LLM-empowered agents and thus detect hallucinations without requiring internal knowledge of the LLM's architecture. Specifically, a probabilistic fractal sampling technique is proposed to generate a sufficient number of queries to trigger the incredible responses in parallel, efficiently identifying the generalization bound of the target agent. Experimental evaluations demonstrate that HalMit significantly outperforms existing approaches in hallucination monitoring. Its black-box nature and superior performance make HalMit a promising solution for enhancing the dependability of LLM-powered systems.

[Arxiv](https://arxiv.org/abs/2507.15903)