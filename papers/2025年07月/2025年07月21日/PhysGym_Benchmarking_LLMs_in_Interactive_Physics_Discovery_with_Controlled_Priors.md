# # PhysGym: 基于可控先验知识的交互式物理发现中对大型语言模型（LLMs）的基准测试

发布时间：2025年07月21日

`Agent` `科学推理` `智能体`

> PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors

# 摘要

> 评估大型语言模型智能体的科学发现能力，尤其是它们应对不同环境复杂度和利用先验知识的水平，需要专门的基准测试，而目前这一领域尚缺乏此类工具。为填补这一空白，我们推出PhysGym，这是一个全新的基准测试套件和仿真平台，旨在严格评估基于LLM的智能体在交互式物理环境中的科学推理能力。

PhysGym的核心贡献在于其对提供给智能体的先验知识水平进行了精细控制。这使研究人员能够从问题复杂度和先验知识水平等多个维度剖析智能体的表现。该基准测试包含一系列交互式仿真，其中智能体必须积极探测环境，在约束条件下按顺序收集数据，并对潜在的物理规律提出假设。

PhysGym提供了标准化的评估协议和指标，用于衡量假设的准确性以及模型的忠实度。我们通过展示基线LLMs在不同先验和任务复杂度下的结果，证明了该基准测试的实用性，凸显了其区分智能体能力的潜力。

> Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity.

[Arxiv](https://arxiv.org/abs/2507.15550)