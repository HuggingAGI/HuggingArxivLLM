# CEA-LIST在CheckThat! 2025：考察LLMs在识别文本偏见与观点方面的表现

发布时间：2025年07月10日

`LLM应用` `多语言处理`

> CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text

# 摘要

> 本文提出了一种基于大型语言模型（LLMs）和少量样本提示的高效方法，用于多语言主观性检测。我们参与了CheckThat! 2025评估活动的任务1：主观性检测。研究发现，LLMs在与精心设计的提示结合使用时，能够与微调后的小型语言模型（SLMs）相媲美，尤其在处理噪声或低质量数据时表现更优。尽管尝试了包括与LLM辩论和多种示例选择策略在内的先进提示工程技巧，但最终发现，这些方法并未显著超越精心设计的标准少样本提示的效果。在CheckThat! 2025的主观性检测任务中，我们的系统在多种语言中取得了优异成绩，包括阿拉伯语和波兰语的第一名，以及意大利语、英语、德语和多语言赛道的前四名。特别值得注意的是，我们的方法在阿拉伯语数据集上表现尤为稳健，这可能得益于其对标注不一致的强健性。这些发现凸显了基于LLM的少量样本学习在多语言情感任务中的有效性和适应性，为传统的微调方法提供了一种有力的替代方案，尤其是在标注数据稀缺或不一致的情况下。

> This paper presents a competitive approach to multilingual subjectivity detection using large language models (LLMs) with few-shot prompting. We participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation campaign. We show that LLMs, when paired with carefully designed prompts, can match or outperform fine-tuned smaller language models (SLMs), particularly in noisy or low-quality data settings. Despite experimenting with advanced prompt engineering techniques, such as debating LLMs and various example selection strategies, we found limited benefit beyond well-crafted standard few-shot prompts. Our system achieved top rankings across multiple languages in the CheckThat! 2025 subjectivity detection task, including first place in Arabic and Polish, and top-four finishes in Italian, English, German, and multilingual tracks. Notably, our method proved especially robust on the Arabic dataset, likely due to its resilience to annotation inconsistencies. These findings highlight the effectiveness and adaptability of LLM-based few-shot learning for multilingual sentiment tasks, offering a strong alternative to traditional fine-tuning, particularly when labeled data is scarce or inconsistent.

[Arxiv](https://arxiv.org/abs/2507.07539)