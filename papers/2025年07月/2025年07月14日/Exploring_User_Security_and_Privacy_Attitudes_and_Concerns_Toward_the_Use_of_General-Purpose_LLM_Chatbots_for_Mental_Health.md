# 用户对通用LLM聊天机器人用于心理健康的隐私与安全态度及担忧探讨

发布时间：2025年07月14日

`LLM应用

摘要讨论了配备大型语言模型（LLM）的对话式智能体在情感支持中的应用，研究用户隐私担忧和期望，属于LLM的实际应用。` `心理健康` `情感支持`

> Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health

# 摘要

> 人们越来越依赖配备大型语言模型（LLM）的对话式智能体来获得情感支持。尽管先前研究关注了专为心理健康设计的聊天机器人中的隐私和安全问题，但这些产品大多基于规则，未采用生成式AI。目前尚无实证研究评估用户在使用通用型LLM聊天机器人管理心理健康时的隐私担忧、态度和期望。通过21场半结构化访谈，我们发现参与者存在关键性误解和风险意识不足。他们将LLM展现的人类化共情与人类化责任混为一谈，并错误认为与聊天机器人的互动受到与向专业治疗师披露信息相同的法规保护（如《健康保险可移植性和责任法案》（HIPAA））。我们提出“无形脆弱性”概念，即情感或心理信息披露的价值低于财务或位置数据等具体信息形式。为此，我们建议采取措施，更有效地保护用户在使用通用型LLM聊天机器人时的心理健康信息披露。

> Individuals are increasingly relying on large language model (LLM)-enabled conversational agents for emotional support. While prior research has examined privacy and security issues in chatbots specifically designed for mental health purposes, these chatbots are overwhelmingly "rule-based" offerings that do not leverage generative AI. Little empirical research currently measures users' privacy and security concerns, attitudes, and expectations when using general-purpose LLM-enabled chatbots to manage and improve mental health. Through 21 semi-structured interviews with U.S. participants, we identified critical misconceptions and a general lack of risk awareness. Participants conflated the human-like empathy exhibited by LLMs with human-like accountability and mistakenly believed that their interactions with these chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures with a licensed therapist. We introduce the concept of "intangible vulnerability," where emotional or psychological disclosures are undervalued compared to more tangible forms of information (e.g., financial or location-based data). To address this, we propose recommendations to safeguard user mental health disclosures with general-purpose LLM-enabled chatbots more effectively.

[Arxiv](https://arxiv.org/abs/2507.10695)