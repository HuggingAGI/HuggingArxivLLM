# 大型语言模型中的文化偏见：通过道德问卷评估AI代理的能力

发布时间：2025年07月14日

`LLM理论` `人工智能` `社会科学`

> Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires

# 摘要

> AI系统是真正代表了人类的价值观，还是仅仅进行了简单的平均？我们的研究揭示了一个令人担忧的现实：尽管大型语言模型（LLMs）在语言能力上表现出色，但它们未能准确反映多样化的文化道德框架。通过在19种文化背景下应用道德基础问卷，我们揭示了AI生成与人类道德直觉之间的显著差距。将多个最先进的LLMs的输出与人类基线数据进行比较后，我们发现这些模型系统性地消除了道德多样性。令人惊讶的是，模型规模的增加并不总是能一致地改善文化表现的真实性。我们的发现不仅挑战了将LLMs作为合成群体用于社会科学研究所日益增长的趋势，还突显了当前AI对齐方法中的一个根本性限制。如果不超越提示工程，仅依赖数据驱动的对齐方法，这些系统就无法捕捉到细微且具有文化特定性的道德直觉。我们的研究结果呼吁制定更切实的对齐目标和评估指标，以确保AI系统能够代表多样化的人类价值观，而不是抹平道德的多样性。

> Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.

[Arxiv](https://arxiv.org/abs/2507.10073)