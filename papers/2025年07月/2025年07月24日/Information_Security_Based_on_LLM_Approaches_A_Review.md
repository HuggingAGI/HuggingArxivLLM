# 基于LLM方法的信息安全研究综述

发布时间：2025年07月24日

`LLM应用

理由：这篇论文主要探讨了大型语言模型（LLMs）在信息安全领域的应用，包括恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别以及密码算法优化等方面。论文分析了LLMs在这些应用场景中的潜力和优势，并讨论了当前应用成果和面临的挑战。因此，这篇论文属于“LLM应用”类别。` `信息安全`

> Information Security Based on LLM Approaches: A Review

# 摘要

> 信息安全正面临前所未有的挑战，传统防护手段难以应对日益复杂多变的威胁。近年来，大型语言模型（LLMs）作为一项新兴智能技术，在信息安全领域展现出巨大潜力。本文聚焦于大型语言模型在信息安全中的关键作用，系统梳理其在恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别以及密码算法优化等方面的应用进展，并深入探讨其提升安全防护性能的潜力。基于神经网络和Transformer架构，本文分析了大型语言模型的技术基础及其在自然语言处理任务中的独特优势。研究发现，引入大型语言模型能够显著提升安全系统的检测精度并有效降低误报率。最后，本文总结了当前应用成果，并指出其在模型透明度、可解释性以及场景适配性等方面仍面临诸多挑战。未来，我们需要进一步探索模型结构优化与泛化能力提升，以构建更加智能、精准的信息安全防护体系。

> Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.

[Arxiv](https://arxiv.org/abs/2507.18215)