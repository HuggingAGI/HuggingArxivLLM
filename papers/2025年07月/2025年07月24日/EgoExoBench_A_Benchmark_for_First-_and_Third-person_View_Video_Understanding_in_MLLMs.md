# EgoExoBench：用于多模态大语言模型的第一人称与第三人称视角视频理解基准数据集

发布时间：2025年07月24日

`LLM应用` `视频处理` `智能助手`

> EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs

# 摘要

> 在第一人称视角（自我中心视角）和第三人称视角（外部视角）之间迁移和整合知识是人类智能的核心，使人类能够从他人身上学习，并传达自己经验中的见解。尽管多模态大型语言模型（MLLMs）取得了迅速进展，但它们在跨视角推理方面的能力尚未得到探索。为了解决这一问题，我们引入了EgoExoBench，这是首个针对自我中心-外部视频理解和推理的基准测试。EgoExoBench基于公开可用的数据集构建，包含超过7,300个问题-答案对，涵盖11个子任务，分为三个核心挑战：语义对齐、视角关联和时间推理。我们评估了13个最先进的MLLMs，并发现尽管这些模型在单视角任务上表现出色，但它们难以在不同视角之间对齐语义，准确关联视角，并在自我中心-外部背景下推断时间动态。我们希望EgoExoBench能成为研究具身代理和智能助手寻求类人跨视角智能的宝贵资源。


> Transferring and integrating knowledge across first-person (egocentric) and third-person (exocentric) viewpoints is intrinsic to human intelligence, enabling humans to learn from others and convey insights from their own experiences. Despite rapid progress in multimodal large language models (MLLMs), their ability to perform such cross-view reasoning remains unexplored. To address this, we introduce EgoExoBench, the first benchmark for egocentric-exocentric video understanding and reasoning. Built from publicly available datasets, EgoExoBench comprises over 7,300 question-answer pairs spanning eleven sub-tasks organized into three core challenges: semantic alignment, viewpoint association, and temporal reasoning. We evaluate 13 state-of-the-art MLLMs and find that while these models excel on single-view tasks, they struggle to align semantics across perspectives, accurately associate views, and infer temporal dynamics in the ego-exo context. We hope EgoExoBench can serve as a valuable resource for research on embodied agents and intelligent assistants seeking human-like cross-view intelligence.

[Arxiv](https://arxiv.org/abs/2507.18342)