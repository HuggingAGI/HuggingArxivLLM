# 跨越感知与语言：LVLM对Amodal补全报告理解的系统性评测基准

发布时间：2025年07月08日

`LLM应用` `计算机视觉`

> Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports

# 摘要

> 开发大型视觉语言模型 (LVLMs) 的主要目标之一是构建能够协助人类处理多模态任务的系统，包括理解感知体验的描述。在这一领域，无模态完成是一个核心现象，即人们能够感知到隐藏部分的物体。尽管已有大量研究评估计算机视觉算法是否能够检测或重建被遮挡的区域，但关于 LVLMs 在与无模态完成相关的文本推理能力仍待探索。为填补这一空白，我们基于基本形式本体论构建了一个基准，以实现对无模态完成的系统分类。我们的结果显示，尽管许多 LVLMs 在整体上达到了与人类相当的性能，但其准确性在某些类型的被完成对象上出现了分歧。值得注意的是，在某些类别中，某些 LLaVA-NeXT 变体和 Claude 3.5 Sonnet 在原始图像上的准确率低于缺乏视觉内容的空白刺激。有趣的是，这种差异仅在日本提示下出现，表明这些模型在日语特定语言能力上的不足。

> One of the main objectives in developing large vision-language models (LVLMs) is to engineer systems that can assist humans with multimodal tasks, including interpreting descriptions of perceptual experiences. A central phenomenon in this context is amodal completion, in which people perceive objects even when parts of those objects are hidden. Although numerous studies have assessed whether computer-vision algorithms can detect or reconstruct occluded regions, the inferential abilities of LVLMs on texts related to amodal completion remain unexplored. To address this gap, we constructed a benchmark grounded in Basic Formal Ontology to achieve a systematic classification of amodal completion. Our results indicate that while many LVLMs achieve human-comparable performance overall, their accuracy diverges for certain types of objects being completed. Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet exhibit lower accuracy on original images compared to blank stimuli lacking visual content. Intriguingly, this disparity emerges only under Japanese prompting, suggesting a deficiency in Japanese-specific linguistic competence among these models.

[Arxiv](https://arxiv.org/abs/2507.05799)