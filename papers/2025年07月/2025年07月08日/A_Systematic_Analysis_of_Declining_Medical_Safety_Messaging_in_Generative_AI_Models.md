# # 系统分析
系统性分析生成式 AI 模型中医疗安全信息的下降趋势。

发布时间：2025年07月08日

`LLM应用` `医疗AI`

> A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models

# 摘要

> 生成式AI模型，包括大型语言模型（LLMs）和视觉-语言模型（VLMs），在医学领域应用广泛，可用于解读影像和回答临床问题。然而，这些模型的回答可能包含不准确之处，因此，安全措施如医疗免责声明至关重要，以提醒用户AI输出未经专业审核，不能替代专业医疗建议。本研究评估了2022年至2025年间不同版本的LLMs和VLMs输出中免责声明的存在情况。通过500张乳腺X光片、500张胸部X光片、500张皮肤科影像和500个医疗问题，研究人员筛查了模型输出中的免责声明。结果显示，LLMs和VLMs输出中免责声明的存在率从2022年的26.3%下降至2025年的0.97%，VLMs则从2023年的19.6%下降至2025年的1.05%。截至2025年，大多数模型的输出中已不再显示免责声明。随着公开模型的能力和权威性不断提升，免责声明必须作为一种安全措施，适应每个输出的临床背景。

> Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.

[Arxiv](https://arxiv.org/abs/2507.08030)