# 统一感知-语言-行动框架：自适应自动驾驶新方案

发布时间：2025年07月31日

`Agent` `自动驾驶` `认知科学`

> A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving

# 摘要

> 自动驾驶系统在复杂开放环境中的类人适应性、鲁棒性和可解释性方面面临重大挑战。这些问题源于架构分散、对新场景的泛化能力有限以及感知中的语义提取不足。为了解决这些问题，我们提出了一种统一的感知-语言-行动（PLA）框架，该框架将多传感器融合（摄像头、LiDAR、雷达）与大型语言模型（LLM）增强的视觉-语言-行动（VLA）架构相结合，具体采用GPT-4.1支持的推理核心。此框架将低级感官处理与高级情境推理相结合，紧密连接感知与基于自然语言的语义理解和决策，从而实现情境感知、可解释且安全边界内的自动驾驶。在城市路口施工区域场景下的评估显示，该框架在轨迹跟踪、速度预测和自适应规划方面表现出色。结果突显了语言增强型认知框架在提升自动驾驶系统安全性、可解释性和可扩展性方面的潜力。

> Autonomous driving systems face significant challenges in achieving human-like adaptability, robustness, and interpretability in complex, open-world environments. These challenges stem from fragmented architectures, limited generalization to novel scenarios, and insufficient semantic extraction from perception. To address these limitations, we propose a unified Perception-Language-Action (PLA) framework that integrates multi-sensor fusion (cameras, LiDAR, radar) with a large language model (LLM)-augmented Vision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered reasoning core. This framework unifies low-level sensory processing with high-level contextual reasoning, tightly coupling perception with natural language-based semantic understanding and decision-making to enable context-aware, explainable, and safety-bounded autonomous driving. Evaluations on an urban intersection scenario with a construction zone demonstrate superior performance in trajectory tracking, speed prediction, and adaptive planning. The results highlight the potential of language-augmented cognitive frameworks for advancing the safety, interpretability, and scalability of autonomous driving systems.

[Arxiv](https://arxiv.org/abs/2507.23540)