# PhysicsEval：增强大型语言模型解决物理问题推理能力的推理技巧

发布时间：2025年07月31日

`Agent` `人工智能`

> PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems

# 摘要

> 物理学作为人类智慧的基石，推动了科技的发展，也加深了我们对宇宙基本原理的理解。在自然语言推理领域，解决物理问题是一项重要任务，而当代文献中已有一些相关研究。本文将评估前沿大型语言模型（LLMs）在数学和描述性物理问题上的表现。我们采用了多种推理时技术以及智能体框架来提升模型性能，包括通过其他小型LLM智能体以累积方式验证提出的解决方案。比较分析表明，当多智能体框架应用于模型最初表现不佳的问题时，性能有了显著提升。此外，我们还引入了一个新的物理问题评估基准——${m P{\small HYSICS}E{\small VAL}}$，该基准包含19,609道物理题目，这些题目来自各种物理教科书，对应的正确解答则从物理论坛和教育网站中抓取。我们的代码和数据已在GitHub公开，地址为https://github.com/areebuzair/PhysicsEval。

> The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.

[Arxiv](https://arxiv.org/abs/2508.00079)