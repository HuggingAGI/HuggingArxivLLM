# CogDual：采用隐式规则强化学习提升大型语言模型的双认知能力

发布时间：2025年07月22日

`Agent` `人工智能` `服务行业`

> CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards

# 摘要

> 角色扮演语言代理（RPLAs）已成为LLM的重要应用方向。现有方法通常依赖提示工程或监督微调，使模型能够在特定场景中模仿角色行为，但往往忽视了驱动这些行为的底层认知机制。受认知心理学启发，我们提出了	extbf{CogDual}，这是一种采用	extit{认知-回应}推理范式的新型RPLA。通过联合建模外部情境感知和内部自我认知，CogDual能够生成一致性更强、上下文对齐度更高的回应。为了进一步优化性能，我们采用了强化学习，并设计了两种通用奖励方案，专门针对开放领域文本生成。在CoSER基准测试以及Cross-MR和LifeChoice上的广泛实验表明，CogDual始终显著超越现有基线，并在多样化的角色扮演任务中表现出良好的泛化能力。

> Role-Playing Language Agents (RPLAs) have emerged as a significant application direction for Large Language Models (LLMs). Existing approaches typically rely on prompt engineering or supervised fine-tuning to enable models to imitate character behaviors in specific scenarios, but often neglect the underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a \textit{cognize-then-respond } reasoning paradigm. By jointly modeling external situational awareness and internal self-awareness, CogDual generates responses with improved character consistency and contextual alignment. To further optimize the performance, we employ reinforcement learning with two general-purpose reward schemes designed for open-domain text generation. Extensive experiments on the CoSER benchmark, as well as Cross-MR and LifeChoice, demonstrate that CogDual consistently outperforms existing baselines and generalizes effectively across diverse role-playing tasks.

[Arxiv](https://arxiv.org/abs/2507.17147)