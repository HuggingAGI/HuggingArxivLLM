# 探讨大型语言模型作为裁判在代码生成与总结中的有效性

发布时间：2025年07月22日

`LLM应用` `软件工程`

> On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization

# 摘要

> 大型语言模型近期被用于评估复杂自然语言处理任务，如问答。其核心理念是将评估任务委托给大型语言模型（LLM），以评估自动化技术在以下任务中的输出质量：(i) 定量指标只能反映部分情况；(ii) 大规模人工评估成本过高。若证明LLMs作为评估工具在特定任务中有效，它将解锁新的自动化可能性：多个LLMs针对同一任务实例提出解决方案，其他LLMs进行评估并决定最佳输出。我们研究了LLMs作为评估工具在代码生成和代码总结两个任务中的有效性。选择这两个任务的原因有二：首先，定量指标通常不足以评估代码总结器/生成器的表现。例如，像BLEU这样的指标对生成摘要质量的衡量相当有限。其次，即使是当前最先进的技术在处理这些任务的复杂实例时仍然存在困难，这使得它们成为受益于更高级解决方案的优秀候选者，这些解决方案设想了LLMs之间的协作。对于代码生成，我们检查了八种LLMs是否能够评估由相同LLMs生成或由人类实现的1,405个Java方法和1,281个Python函数的正确性。对于代码总结，我们比较了五种LLMs与九位人类提供的评估结果，涉及约1.2k个与Java和Python函数相关的摘要。研究发现，GPT-4-turbo在评估能力和代码总结任务方面表现最佳，而具有数十亿参数的“较小”LLMs无法应对评估任务。然而，即使是表现最好的LLM也会经常错误地评估代码和摘要的质量。

> Large Language Models have been recently exploited as judges for complex natural language processing tasks, such as Q&A. The basic idea is to delegate to an LLM the assessment of the "quality" of the output provided by an automated technique for tasks for which: (i) quantitative metrics would only tell part of the story, and; (ii) a large-scale human-based evaluation would be too expensive. LLMs-as-a-judge, if proven effective for a specific task, can also unlock new possibilities for automation, with several LLMs proposing a solution for a given instance of the task and others judging and deciding what is the best output to show the user. We study the effectiveness of LLMs-as-a-judge for two code-related tasks, namely code generation and code summarization. The rationale for choosing these tasks is two-fold. First, quantitative metrics are usually not enough for the assessment of code summarizers/generators. For example, it is well documented that metrics such as BLEU are quite weak proxies for the quality of the generated summaries. Second, even state-of-the-art techniques still struggle with handling complex instances of these tasks, making them good candidates for benefiting from more advanced solutions envisioning collaboration among LLMs. For code generation, we check whether eight LLMs are able to judge the correctness of 1,405 Java methods and 1,281 Python functions generated by the same LLMs or implemented by humans. For code summarization, we compare the judgment of five LLMs to those provided by nine humans for ~1.2k summaries, related to both Java and Python functions. Our findings show that GPT-4-turbo is the best LLM in terms of judging capabilities for both tasks, with "smaller" LLMs featuring tens of billions parameters not being able to cope with judging tasks. However, even the best-performing LLM frequently misjudges the correctness of the code and summary quality.

[Arxiv](https://arxiv.org/abs/2507.16587)