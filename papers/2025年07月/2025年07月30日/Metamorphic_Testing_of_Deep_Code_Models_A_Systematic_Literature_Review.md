# # 摘要  
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年07月30日

`LLM应用` `软件工程` `软件测试`

> Metamorphic Testing of Deep Code Models: A Systematic Literature Review

# 摘要

> 大型语言模型和专为代码智能设计的深度学习模型彻底革新了软件工程领域。这些模型在代码补全、缺陷检测和代码总结等任务中能够高精度处理源代码和软件制品，有望成为现代软件工程实践的重要组成部分。尽管具备这些能力，稳健性仍是深度代码模型的关键质量属性，因为它们可能在不同条件（如变量重命名）下产生差异性结果。变形测试作为一种评估模型稳健性的常用方法，通过在输入程序上应用保持语义的变换并分析模型输出的稳定性来实现。尽管先前研究探索了对深度学习模型的测试，但本次系统性文献综述专门针对深度代码模型的变形测试。通过对45篇主要论文的研究，我们分析了用于评估稳健性的变换、技术和评估方法。本综述总结了当前研究现状，识别了常用模型、编程任务、数据集、目标语言和评估指标，并强调了推动该领域发展的关键挑战和未来方向。

> Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate models' robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.

[Arxiv](https://arxiv.org/abs/2507.22610)