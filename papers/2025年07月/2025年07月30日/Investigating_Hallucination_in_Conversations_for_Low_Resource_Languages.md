# # 探索低资源语言对话中的幻觉生成现象
在对话中研究低资源语言中的幻觉现象

发布时间：2025年07月30日

`LLM应用

理由：该论文研究了大型语言模型在不同语言中的幻觉现象，分析了模型在不同语言中的表现，属于对LLMs应用的实证研究，因此归类为LLM应用。` `多语言处理`

> Investigating Hallucination in Conversations for Low Resource Languages

# 摘要

> 大型语言模型（LLMs）在生成与人类书写极为相似的文本方面展现出卓越的能力。然而，它们却常常生成事实错误的陈述，这一问题通常被称为‘幻觉’。解决幻觉问题对提升LLMs的可靠性和有效性至关重要。尽管许多研究聚焦于英语中的幻觉现象，本研究将这一调查扩展到三种语言的对话数据：印地语、波斯语和普通话。我们对一个数据集进行了全面分析，以检查GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1和Qwen-3在这些语言中的事实和语言错误。研究发现，LLMs在普通话中生成的幻觉响应非常少，但在印地语和波斯语中生成的幻觉数量显著增加。

> Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.

[Arxiv](https://arxiv.org/abs/2507.22720)