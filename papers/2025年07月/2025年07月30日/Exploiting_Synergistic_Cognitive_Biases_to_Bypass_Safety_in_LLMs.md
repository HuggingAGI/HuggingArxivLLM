# 巧妙利用协同认知偏差，绕过大型语言模型的安全机制

发布时间：2025年07月30日

`LLM理论` `AI安全`

> Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs

# 摘要

> 大型语言模型（LLMs）在各项任务中表现卓越，但其安全机制仍易受利用认知偏见的对抗攻击影响——即系统性偏离理性判断的情况。与以往专注于提示工程或算法操控的越狱方法不同，本研究揭示了多偏见交互在破坏LLM安全防护中的潜在力量。我们提出了一种名为CognitiveAttack的新型红队框架，系统性地利用个体及组合认知偏见。通过融合监督微调与强化学习，CognitiveAttack生成嵌入优化偏见组合的提示，成功绕过安全协议的同时保持高攻击成功率。实验结果揭示了30个不同LLMs的重大漏洞，尤其在开源模型中表现突出。与最先进的黑盒方法PAP（60.1% vs. 31.6%）相比，CognitiveAttack实现了显著更高的攻击成功率，暴露出当前防御机制的关键局限。这些发现凸显了多偏见交互作为强大但未充分探索的攻击向量。本研究通过将认知科学与LLM安全相结合，开创了一种新颖的跨学科视角，为构建更 robust 且符合人类价值观的AI系统铺平了道路。

> Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.

[Arxiv](https://arxiv.org/abs/2507.22564)