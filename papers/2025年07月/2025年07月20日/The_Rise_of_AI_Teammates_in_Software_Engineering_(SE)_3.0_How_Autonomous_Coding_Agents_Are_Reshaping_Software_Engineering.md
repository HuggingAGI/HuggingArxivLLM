# AI队友正在崛起，重塑软件工程3.0：自主编码代理如何改变软件工程

发布时间：2025年07月20日

`Agent` `软件工程` `人工智能`

> The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering

# 摘要

> 软件工程的未来——SE 3.0——正在随着AI队友的崛起而展开：这些自主的、目标驱动的系统与人类开发者协同工作。其中，自主编码代理尤其具有变革性，它们现在正积极主动地在大规模范围内发起、审查和演进代码。本文介绍了AIDev，这是首个大规模数据集，记录了这些代理在实际应用中是如何运作的。该数据集覆盖了五个领先AI代理——OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code——在61,000个代码仓库和47,000名开发者中发起的超过456,000次拉取请求。AIDev为研究软件开发中的自主队友提供了前所未有的实证基础。

与此前主要从理论角度探讨AI原生软件工程的研究不同，AIDev提供了结构化的开放数据，支持在基准测试、代理准备度、优化、协作建模和AI治理等领域的研究。该数据集包含了丰富的元数据，涵盖拉取请求、作者信息、审查时间线、代码更改和集成结果——这些内容使研究超越了合成基准（如SWE-bench）。例如，尽管代理在速度上通常优于人类，但它们的拉取请求被接受的频率更低，这揭示了信任与效用之间的差距。此外，虽然代理加速了代码提交——一位开发者在三天内提交的拉取请求数量相当于过去三年的总和——但这些请求在结构上更为简单（通过代码复杂度指标衡量）。

我们设想AIDev作为一个动态发展的资源：可扩展、可分析，并为SE和AI社区做好准备。通过将SE 3.0扎根于现实世界的证据，AIDev推动了新一代AI原生工作流程的研究，并支持构建下一个人机协同的新时代。该数据集可在https://github.com/SAILResearch/AI_Teammates_in_SE3公开获取。
> AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent

> The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent

[Arxiv](https://arxiv.org/abs/2507.15003)