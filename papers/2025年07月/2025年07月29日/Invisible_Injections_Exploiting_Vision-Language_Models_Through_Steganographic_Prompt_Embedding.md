# 隐形注入：利用隐写提示嵌入攻击视觉-语言模型

发布时间：2025年07月29日

`LLM应用` `多模态AI`

> Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding

# 摘要

> 视觉语言模型（VLMs）彻底改变了多模态AI应用，但也带来了尚未被充分研究的安全隐患。我们首次系统性地研究了针对VLMs的隐写术提示注入攻击，其中恶意指令通过先进隐写技术被秘密嵌入图像中。我们的研究表明，现有VLM架构在处理图像时可能无意中提取并执行这些隐藏提示，导致行为被暗中操控。我们开发的多领域嵌入框架结合空间、频率和神经隐写方法，在GPT-4V、Claude和LLaVA等主流VLMs上实现了24.3%（±3.2%，95%置信区间）的整体攻击成功率，其中神经隐写方法的成功率高达31.8%，同时保持了良好的视觉不可知性（PSNR>38 dB，SSIM>0.94）。通过对12个数据集和8个先进模型的系统评估，我们发现当前VLM架构存在适度但值得注意的安全漏洞，并提出了有效的防御方案。我们的研究结果对安全敏感场景下的VLM部署具有重要启示，并凸显了构建相称的多模态AI安全框架的迫切需求。

> Vision-language models (VLMs) have revolutionized multimodal AI applications but introduce novel security vulnerabilities that remain largely unexplored. We present the first comprehensive study of steganographic prompt injection attacks against VLMs, where malicious instructions are invisibly embedded within images using advanced steganographic techniques. Our approach demonstrates that current VLM architectures can inadvertently extract and execute hidden prompts during normal image processing, leading to covert behavioral manipulation. We develop a multi-domain embedding framework combining spatial, frequency, and neural steganographic methods, achieving an overall attack success rate of 24.3% (plus or minus 3.2%, 95% CI) across leading VLMs including GPT-4V, Claude, and LLaVA, with neural steganography methods reaching up to 31.8%, while maintaining reasonable visual imperceptibility (PSNR greater than 38 dB, SSIM greater than 0.94). Through systematic evaluation on 12 diverse datasets and 8 state-of-the-art models, we reveal moderate but meaningful vulnerabilities in current VLM architectures and propose effective countermeasures. Our findings have significant implications for VLM deployment in security-critical applications and highlight the need for proportionate multimodal AI security frameworks.

[Arxiv](https://arxiv.org/abs/2507.22304)