# LLM-众包：一种无需基准的大型语言模型相互评估新范式

发布时间：2025年07月29日

`LLM应用` `人工智能` `计算机科学`

> LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models

# 摘要

> 大型语言模型（LLMs）在各类任务中展现了卓越的能力，但评估这些能力仍是一项颇具挑战性的任务。现有的评估方法往往受限于数据污染、黑箱操作以及主观偏好等问题，难以全面准确地衡量LLMs的真实水平。为应对这些挑战，我们提出了一种创新的无基准评估范式——LLM-Crowdsourced。该方法巧妙地利用LLMs自动生成问题、独立作答并相互评估，同时整合了动态性、透明性、客观性和专业性四大关键评估标准，这些标准在传统评估方法中难以同时满足。通过在数学和编程领域对八种主流LLMs的实验，我们验证了该方法在区分模型性能方面的显著优势。此外，我们的研究还揭示了传统方法难以察觉的若干新发现，包括但不限于：（1）Gemini在问题设计的原创性和专业性方面表现最为突出；（2）某些LLMs通过误将问题识别为结构相似的熟悉问题而表现出“基于记忆的回答”；（3）LLM评估结果展现出高度一致性（鲁棒性）。

> Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness).

[Arxiv](https://arxiv.org/abs/2507.22359)