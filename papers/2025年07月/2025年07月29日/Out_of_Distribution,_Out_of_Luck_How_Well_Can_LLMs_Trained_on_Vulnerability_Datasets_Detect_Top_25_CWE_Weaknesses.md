# 模型失准，霉运临头？看LLMs能否精准识别25种常见漏洞！

发布时间：2025年07月29日

`LLM应用` `软件安全` `软件工程`

> Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?

# 摘要

> 自动漏洞检测研究虽然取得了显著进展，但实际应用效果仍有待提升。现有漏洞数据集存在标签不准确率（20-71%）、大量重复以及对关键 CWE 类型覆盖不足等问题，这些缺陷造成了显著的“泛化差距”。具体而言，模型往往通过虚假关联而非真实漏洞模式实现自我测试的高性能，但在独立数据集上的表现却大幅下滑，甚至不如随机猜测（性能下降高达 40.6%）。

为解决这些局限，我们提出了一套三管齐下的解决方案：
1. **BenchVul 测试集**：涵盖 MITRE 最危险的 Top 25 CWE 类型，为漏洞检测提供高标准的评估基准。
2. **TitanVul 训练集**：聚合七个公开来源，借助新型多智能体 LLM 框架进行去重和验证，包含 35,045 个高质量函数。
3. **现实漏洞生成（RVG）框架**：通过模拟开发工作流程，为关键但代表性不足的 CWE 类型生成上下文感知的漏洞示例。

评估结果表明：
- **BenchVul 测试集**揭示了现有模型的局限性：基于 BigVul 和 PrimeVul 训练的模型在 BenchVul 上性能大幅下降（从 0.776 降至 0.519，从 0.567 降至 0.337）。
- **TitanVul 训练集**显著提升了模型的泛化能力：在 BenchVul 上的性能从 0.584 提升至 0.767。
- **RVG 补充数据**进一步优化了模型表现：性能提升 14.0%，达到 0.874。


> Automated vulnerability detection research has made substantial progress, yet its real-world impact remains limited. Current vulnerability datasets suffer from issues including label inaccuracy rates of 20-71%, extensive duplication, and poor coverage of critical CWE types. These issues create a significant "generalization gap" where models achieve misleading self-testing performance (measured on held-out data from same dataset for training) by exploiting spurious correlations rather than learning true vulnerability patterns. Our analysis reveals that many models experience substantial performance drops of up to 40.6% when evaluated on independent data, sometimes underperforming random guessing.
  To address these limitations, we present a three-part solution. First, we introduce a manually curated test dataset, BenchVul, covering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a high-quality training dataset, TitanVul, comprising 35,045 functions by aggregating seven public sources and applying deduplication and validation using a novel multi-agent LLM framework. Third, we propose a Realistic Vulnerability Generation (RVG) framework, which synthesizes context-aware vulnerability examples for underrepresented but critical CWE types through simulated development workflows.
  Our evaluation shows the strengths of each component in closing the generalization gap. First, BenchVul shows the limitations of self-testing: models trained on existing datasets, such as BigVul and PrimeVul, experience performance drops on BenchVul (from 0.776 to 0.519 and from 0.567 to 0.337). Second, training models on TitanVul demonstrates improved generalization, with model performance increasing from 0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated data yields further gains, increasing model performance by 14.0% to 0.874.

[Arxiv](https://arxiv.org/abs/2507.21817)