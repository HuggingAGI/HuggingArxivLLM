# MSGCoOp: 多语义引导上下文优化方法用于小样本学习

发布时间：2025年07月29日

`LLM应用` `视觉语言模型` `模型优化`

> MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning

# 摘要

> 视觉语言预训练模型（VLMs）如CLIP展现了卓越的零样本泛化能力，而提示学习作为一种高效替代方案，为全微调提供了新的选择。然而，现有方法在面对新类别时往往难以实现良好泛化，这一现象通常归因于对已见类别的过拟合以及对通用知识的遗忘。此外，近期提升泛化能力的方法通常会引入复杂的架构设计或带来沉重的计算负担。本文中，我们提出了一种多语义引导上下文优化（MSGCoOp）框架，旨在在保持计算效率的同时增强少量样本下的泛化能力。我们的方法利用一组并行可学习上下文向量的集合，以捕捉多样化的语义特征。为了丰富这些提示，我们引入了一种语义引导机制，使其能够与大型语言模型（LLM）自动生成的全面类别描述自动对齐。此外，一种多样性正则化损失被引入，以鼓励提示学习互补且正交的特征，防止其陷入冗余表示。在11个基准数据集上的广泛实验表明，MSGCoOp在基类到新类的泛化性能上有显著提升，较强大的KgCoOp基线平均调和均值提升了1.10%。我们的方法在跨领域泛化任务中也展现了更强的鲁棒性。我们的代码可在以下地址获取：\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}。

> Vision-language pre-trained models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, and prompt learning has emerged as an efficient alternative to full fine-tuning. However, existing methods often struggle with generalization to novel classes, a phenomenon attributed to overfitting on seen classes and forgetting general knowledge. Furthermore, recent approaches that improve generalization often introduce complex architectures or heavy computational overhead. In this paper, we propose a Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance few-shot generalization while maintaining computational efficiency. Our approach leverages an ensemble of parallel learnable context vectors to capture diverse semantic aspects. To enrich these prompts, we introduce a semantic guidance mechanism that aligns them with comprehensive class descriptions automatically generated by a Large Language Model (LLM). Furthermore, a diversity regularization loss encourages the prompts to learn complementary and orthogonal features, preventing them from collapsing into redundant representations. Extensive experiments on 11 benchmark datasets show that MSGCoOp significantly improves performance on base-to-novel generalization, achieving an average harmonic mean improvement of 1.10\% over the strong KgCoOp baseline. Our method also demonstrates enhanced robustness in cross-domain generalization tasks. Our code is avaliable at: \href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.

[Arxiv](https://arxiv.org/abs/2507.21786)