# MCIF：源自科学演讲的多模态跨语言指令遵循基准

发布时间：2025年07月25日

`LLM应用` `跨语言`

> MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks

# 摘要

> 大型语言模型的突破性进展催生了多模态大型语言模型（MLLMs），这些模型能够在统一框架中整合文本、语音和视觉信息。随着MLLMs从单一语言、特定任务的系统发展为通用的指令遵循模型，评估其在长上下文和短上下文中的多语言和多模态能力成为关键研究方向。然而，现有基准测试在联合评估这些维度时存在局限：它们通常局限于英语、一次仅关注单一模态、依赖短上下文，或缺乏人工标注，这阻碍了对模型跨语言、跨模态及任务复杂性方面的全面性能评估。

为解决这些局限，我们推出了MCIF（多模态跨语言指令遵循），这是首个基于科学演讲的多语言人工标注基准测试。MCIF旨在评估模型在跨语言和多模态设置下，处理短上下文和长上下文输入时的指令遵循能力。它涵盖语音、视觉和文本三个核心模态，并支持英语、德语、意大利语和中文四种语言，为全面评估MLLMs的跨语言理解和多模态上下文整合能力提供了可能。MCIF在CC-BY 4.0许可证下开放，以促进MLLMs领域的开放研究与技术进步。

> Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.

[Arxiv](https://arxiv.org/abs/2507.19634)