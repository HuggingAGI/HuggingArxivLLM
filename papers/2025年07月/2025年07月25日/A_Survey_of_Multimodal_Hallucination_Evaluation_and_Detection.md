# 多模态生成内容评估与检测综述

发布时间：2025年07月25日

`LLM理论` `生成内容` `图像处理`

> A Survey of Multimodal Hallucination Evaluation and Detection

# 摘要

> # 摘要
多模态大型语言模型（MLLMs）作为一种强大的范式，能够整合视觉和文本信息，支持多种多模态任务。然而，这些模型常常会出现幻觉现象，生成看似合理却与输入内容或常识相悖的内容。本文综述了图像到文本（I2T）和文本到图像（T2I）生成任务中幻觉的评估基准和检测方法。具体而言，我们首先基于忠实度和事实性提出了幻觉的分类体系，并纳入了实践中常见幻觉类型。接着，我们概述了现有针对T2I和I2T任务的幻觉评估基准，重点介绍其构建过程、评估目标及所用指标。此外，我们总结了近期在幻觉检测方法上的进展，旨在从实例层面识别幻觉内容，并作为基于基准评估的实用补充。最后，我们指出现有基准和检测方法的关键局限性，并展望了未来研究的方向。

> Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm for integrating visual and textual information, supporting a wide range of multi-modal tasks. However, these models often suffer from hallucination, producing content that appears plausible but contradicts the input content or established world knowledge. This survey offers an in-depth review of hallucination evaluation benchmarks and detection methods across Image-to-Text (I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose a taxonomy of hallucination based on faithfulness and factuality, incorporating the common types of hallucinations observed in practice. Then we provide an overview of existing hallucination evaluation benchmarks for both T2I and I2T tasks, highlighting their construction process, evaluation objectives, and employed metrics. Furthermore, we summarize recent advances in hallucination detection methods, which aims to identify hallucinated content at the instance level and serve as a practical complement of benchmark-based evaluation. Finally, we highlight key limitations in current benchmarks and detection methods, and outline potential directions for future research.

[Arxiv](https://arxiv.org/abs/2507.19024)