# LitBench：创意写作可靠评估的基准与数据集

发布时间：2025年07月01日

`LLM应用` `创意写作`

> LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing

# 摘要

> 评估大型语言模型生成的创意写作仍然是一个难题，因为开放式的叙事缺乏客观标准。在缺乏有效的自动化评估方法的情况下，现成的语言模型被用作零-shot 评判工具，但它们在这种情境下的可靠性尚不明确。为了实现创意写作的稳健评估，我们推出了 LitBench——首个标准化的创意写作验证基准测试和配对数据集。LitBench 包含一个从 Reddit 采集的 2,480 个去偏见、人工标注的故事比较测试集，以及一个包含 43,827 对人类偏好标签的训练语料库。

借助 LitBench，我们完成了三项工作：(i) 对零-shot LLM 评判工具进行基准测试，(ii) 训练 Bradley Terry 模型和生成奖励模型，(iii) 开展在线人类研究，以验证奖励模型在新生成的 LLM 故事中的排名。我们的基准测试发现，Claude-3.7-Sonnet 是最强大的现成评判工具，与人类偏好达成 73% 的一致率；在训练好的奖励模型中，Bradley-Terry 模型和生成奖励模型均达到 78% 的准确率，超越所有现成评判工具。在线人类研究进一步证实，我们的训练奖励模型在新型 LLM 生成故事中始终与人类偏好保持一致。

我们已在 https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461 上发布了 LitBench 和奖励模型，为创意写作系统的可靠自动化评估和优化提供了经过验证的资源。


> Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.

[Arxiv](https://arxiv.org/abs/2507.00769)