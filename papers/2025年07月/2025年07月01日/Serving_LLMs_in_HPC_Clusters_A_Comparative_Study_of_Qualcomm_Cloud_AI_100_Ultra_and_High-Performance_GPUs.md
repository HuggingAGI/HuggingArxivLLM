# LLMs 在 HPC 集群中的部署与运行：高通云 AI 100 Ultra 与高性能 GPU 的对比研究

发布时间：2025年07月01日

`其他` `硬件加速器` `高性能计算`

> Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs

# 摘要

> 本研究对高通云AI 100 Ultra (QAic) 加速器在大型语言模型 (LLM) 推理性能进行了基准测试，评估了其能效（每瓦吞吐量）以及在国家研究平台 (NRP) 生态系统中与英伟达 (A100, H200) 和 AMD (MI300A) GPU 的性能对比。研究采用 vLLM 框架对 15 个开源 LLM 进行了服务，参数规模从 1.17 亿到 900 亿不等。结果显示，QAic 在能效方面表现优异，大多数情况下都展现出良好的能效性能。这些发现为高通云AI 100 Ultra 在国家研究平台 (NRP) 中的高性能计算 (HPC) 应用潜力提供了重要参考。

> This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt) and performance against leading NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform (NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90 billion parameters, are served using the vLLM framework. The QAic inference cards appears to be energy efficient and performs well in the energy efficiency metric in most cases. The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications within the National Research Platform (NRP).

[Arxiv](https://arxiv.org/abs/2507.00418)