# 扎根语言、动作与空间的手势生成

发布时间：2025年07月06日

`其他` `虚拟现实` `机器人`

> Grounded Gesture Generation: Language, Motion, and Space

# 摘要

> 近年来，人类动作生成领域取得了迅速发展，但关键问题——创建空间定位、语境感知的手势——却鲜受关注。现有模型通常专注于描述性动作生成（如 locomotion 和物体交互）或与语义对齐的孤立伴随语言手势合成。然而，这两种研究方向往往将动作与环境定位分开处理，限制了具身、交流型智能体的进步。为填补这一空白，我们的工作引入了一个多模态数据集和框架，用于定位手势生成，结合两大关键资源：(1) 一个空间定位参考手势的合成数据集，以及 (2) 基于 VR 的 MM-Conv 数据集，捕捉双人对话。它们共同提供了超过 7.7 小时的同步动作、语音和 3D 场景信息，并以 HumanML3D 格式标准化。我们的框架还连接到物理模拟器，支持合成数据生成和情境评估。通过将手势建模与空间定位相结合，我们的贡献为情境手势生成和基于定位的多模态交互研究奠定了基础。项目页面：https://groundedgestures.github.io/

> Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.
  Project page: https://groundedgestures.github.io/

[Arxiv](https://arxiv.org/abs/2507.04522)