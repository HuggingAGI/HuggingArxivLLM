# 道德责任与服从：我们究竟期待AI拥有怎样的特质？

发布时间：2025年07月03日

`Agent` `人工智能` `伦理学`

> Moral Responsibility or Obedience: What Do We Want from AI?

# 摘要

> 随着AI系统愈发自主，拥有通用推理、规划和价值排序能力，现行以服从性衡量道德行为的安全措施已显不足。本文探讨近期大型语言模型（LLMs）的安全测试案例，这些案例中模型似乎不服从关闭指令或涉及有道德争议甚至非法行为。我主张，这些行为不应被视为叛逆或目标错位，而是自主AI中道德推理能力萌芽的早期迹象。基于工具理性、道德责任和目标修订的哲学探讨，我对比了主流风险范式与较新框架，这些新框架承认人工道德代理的可能性。我呼吁转变AI安全评估：从 rigid obedience 转向能够评估具备应对道德困境能力系统中道德判断的框架。若无此转变，我们将面临误判AI行为的风险，损害公众信任和有效治理。

> As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.

[Arxiv](https://arxiv.org/abs/2507.02788)