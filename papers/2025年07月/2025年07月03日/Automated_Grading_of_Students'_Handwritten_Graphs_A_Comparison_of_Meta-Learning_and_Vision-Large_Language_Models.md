# 学生手写图表的自动化评分：比较元学习与视觉-大型语言模型的能力

发布时间：2025年07月03日

`LLM应用

理由：论文讨论了视觉大型语言模型（VLLMs）在自动评分学生手写图表和文本方面的应用，并将其与元学习模型进行比较，属于LLM的应用研究。` `图像处理`

> Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models

# 摘要

> 随着在线学习的兴起，过去十年中对数学高效评估的需求显著增加。机器学习（ML），特别是自然语言处理（NLP），已被广泛用于自动评分学生回答，尤其是涉及文本和数学表达式的内容。然而，尽管手写图表在科学、技术、工程和数学（STEM）课程中普遍存在，但关于自动评分学生手写图表的研究仍然有限。本研究实现了用于自动评分包含学生手写图表和文本的图像的多模态元学习模型，并比较了视觉大型语言模型（VLLMs）与这些元学习模型的性能。在真实世界数据集上的评估结果显示，最佳元学习模型在二元分类任务中优于VLLMs，但在三元分类任务中，VLLMs略胜一筹。尽管VLLMs展现出潜力，但其可靠性和实际应用性仍需进一步研究。

> With the rise of online learning, the demand for efficient and consistent assessment in mathematics has significantly increased over the past decade. Machine Learning (ML), particularly Natural Language Processing (NLP), has been widely used for autograding student responses, particularly those involving text and/or mathematical expressions. However, there has been limited research on autograding responses involving students' handwritten graphs, despite their prevalence in Science, Technology, Engineering, and Mathematics (STEM) curricula. In this study, we implement multimodal meta-learning models for autograding images containing students' handwritten graphs and text. We further compare the performance of Vision Large Language Models (VLLMs) with these specially trained metalearning models. Our results, evaluated on a real-world dataset collected from our institution, show that the best-performing meta-learning models outperform VLLMs in 2-way classification tasks. In contrast, in more complex 3-way classification tasks, the best-performing VLLMs slightly outperform the meta-learning models. While VLLMs show promising results, their reliability and practical applicability remain uncertain and require further investigation.

[Arxiv](https://arxiv.org/abs/2507.03056)