# MCPEval: 针对AI智能体模型的基于MCP自动深度评估方法

发布时间：2025年07月17日

`LLM应用

理由：这篇论文专注于开发一个用于评估大型语言模型（LLM）智能体的开源框架，展示了LLM在实际评估中的应用，属于LLM应用类别。` `评估系统` `人工智能`

> MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models

# 摘要

> 大型语言模型（LLMs）智能体的如火如荼发展凸显了建立稳健且可扩展评估框架的迫切需求。现有方法依赖静态基准和劳动密集型数据收集，限制了实际评估的应用。我们提出了\oursystemname，一个基于模型上下文协议（MCP）的开源框架，能够自动化端到端任务生成，并对跨多领域LLM智能体进行深度评估。MCPEval统一了评估指标，与原生智能体工具无缝衔接，消除了构建评估流水线的人工投入。实证结果显示，在五个真实世界领域中，MCPEval能够有效揭示LLM智能体在具体领域中的细微表现差异。我们公开发布了MCPEval（https://github.com/SalesforceAIResearch/MCPEval），以推动LLM智能体评估的可重复性和标准化。

> The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.

[Arxiv](https://arxiv.org/abs/2507.12806)