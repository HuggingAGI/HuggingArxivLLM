# # 大型语言模型推理链中的概率正确性保证

发布时间：2025年07月17日

`LLM理论` `人工智能` `推理系统`

> Probabilistic Soundness Guarantees in LLM Reasoning Chains

# 摘要

> 大型语言模型（LLMs）生成的推理链中，初始错误往往会扩散，影响最终结论的可靠性。现有的LLMs错误检测方法通常无法有效识别这些传播的错误，因为它们未能充分考虑早期错误对后续推理判断的影响。为了更好地解决这一问题，我们提出了自回归推理蕴含稳定框架（ARES），这是一种创新的概率框架。ARES通过仅基于先前评估的合理前提来判断每个主张，从而有效防止错误传播。与传统的二元标签方法不同，ARES采用归纳方法为每一步提供细致评分，并提供其合理性的认证统计保证。ARES在四个基准测试中表现出色，Macro-F1达到72.1%，较现有方法提升了8.2个百分点。尤其在处理非常长的合成推理链时，ARES展现了卓越的鲁棒性，错误检测F1值高达90.3%，较现有方法提升了27.6个百分点。

> In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. Current LLM-based error detection methods often fail to detect propagated errors because they do not properly account for how earlier errors might corrupt judgments of downstream reasoning. To better detect such propagated errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a novel probabilistic framework that prevents error propagation by judging each claim based only on previously-assessed sound premises. This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label. ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points).

[Arxiv](https://arxiv.org/abs/2507.12948)