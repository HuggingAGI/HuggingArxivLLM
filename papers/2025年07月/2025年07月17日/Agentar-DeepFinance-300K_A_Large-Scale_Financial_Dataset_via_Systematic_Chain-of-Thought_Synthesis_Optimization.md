# Agentar-DeepFinance-300K：通过系统性思维链合成优化构建的大型金融数据集

发布时间：2025年07月17日

`LLM应用` `推理模型`

> Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization

# 摘要

> 大型语言模型（LLMs）近期展现出卓越的通用推理能力，尤其在金融领域具有巨大潜力。研究表明，从先进模型中提炼高质量链式思维（CoT）推理路径是构建金融推理模型的高效途径。然而，现有方法在CoT采样深度上仍有不足，如何构建有效的金融推理知识空间尚待探索。本文介绍	extbf{Agentar-DeepFinance-300K}，一个以系统化CoT优化为特色的大型金融推理数据集。我们通过多角度知识提取（MKE）和自我修正重写（SCR）构建了全面的CoT合成管道，生成详尽深入的金融推理轨迹。此外，我们开展CoT Cube研究，分析必要性、长度和合成器等关键因素，为高质量金融CoT的构建提供见解。实验表明，基于Agentar-DeepFinance-300K训练的模型在金融基准测试中表现显著提升。我们公开发布此数据集，助力金融推理模型研究。

> Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.

[Arxiv](https://arxiv.org/abs/2507.12901)