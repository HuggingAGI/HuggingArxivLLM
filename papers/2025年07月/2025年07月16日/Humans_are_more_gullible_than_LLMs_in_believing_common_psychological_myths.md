# 人类比LLM更易受骗，容易相信常见的心理学误区。

发布时间：2025年07月16日

`LLM应用` `心理学` `人工智能`

> Humans are more gullible than LLMs in believing common psychological myths

# 摘要

> 尽管已被广泛辟谣，许多心理误区仍然根深蒂固。本文研究了大型语言模型（LLMs）是否模仿人类对误区的信仰行为，并探索了缓解此类倾向的方法。我们使用50个广受欢迎的心理误区，评估了多个大型语言模型在不同提示策略下的误区信仰程度，包括基于检索增强生成和倾向性提示。研究结果显示，大型语言模型对心理误区的信念程度显著低于人类，尽管用户提示可能会影响模型的回答。RAG（检索增强生成）在降低误区信念方面证明有效，并揭示了大型语言模型内在的潜在去偏见能力。我们的研究为新兴的机器心理学领域做出了贡献，并展示了认知科学方法如何能够指导基于LLM系统的评估与发展。

> Despite widespread debunking, many psychological myths remain deeply entrenched. This paper investigates whether Large Language Models (LLMs) mimic human behaviour of myth belief and explores methods to mitigate such tendencies. Using 50 popular psychological myths, we evaluate myth belief across multiple LLMs under different prompting strategies, including retrieval-augmented generation and swaying prompts. Results show that LLMs exhibit significantly lower myth belief rates than humans, though user prompting can influence responses. RAG proves effective in reducing myth belief and reveals latent debiasing potential within LLMs. Our findings contribute to the emerging field of Machine Psychology and highlight how cognitive science methods can inform the evaluation and development of LLM-based systems.

[Arxiv](https://arxiv.org/abs/2507.12296)