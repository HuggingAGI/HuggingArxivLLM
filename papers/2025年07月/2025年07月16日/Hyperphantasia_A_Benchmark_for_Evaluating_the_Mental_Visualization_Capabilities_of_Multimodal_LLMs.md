# 超幻觉：评测多模态大语言模型心理可视化能力的基准

发布时间：2025年07月16日

`LLM应用` `心理学` `人工智能`

> Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs

# 摘要

> 心理可视化能力是人类认知的核心，它让我们能够在内心构建和操作视觉表征，这对推理、预测和抽象等任务至关重要。尽管多模态大语言模型（MLLMs）发展迅速，现有评估主要集中在被动视觉感知上，而对主动构建视觉模式以支持问题解决的能力关注不足。然而，心理可视化是人类的关键认知技能，支持空间导航、物理轨迹预测和复杂视觉问题求解。为填补这一空白，我们推出了Hyperphantasia，一个通过四个精心设计谜题评估MLLMs心理可视化能力的合成基准。每个任务程序化生成，分为三个难度级别，便于在复杂性递增的情况下分析模型性能。对当前最先进模型的全面评估显示，人类与MLLMs之间存在显著差距。此外，我们还探索了强化学习在提升视觉模拟能力方面的潜力。研究发现，尽管某些模型在识别视觉模式方面表现部分能力，但实现稳健的心理可视化仍是当前MLLMs的开放挑战。

> Mental visualization, the ability to construct and manipulate visual representations internally, is a core component of human cognition and plays a vital role in tasks involving reasoning, prediction, and abstraction. Despite the rapid progress of Multimodal Large Language Models (MLLMs), current benchmarks primarily assess passive visual perception, offering limited insight into the more active capability of internally constructing visual patterns to support problem solving. Yet mental visualization is a critical cognitive skill in humans, supporting abilities such as spatial navigation, predicting physical trajectories, and solving complex visual problems through imaginative simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic benchmark designed to evaluate the mental visualization abilities of MLLMs through four carefully constructed puzzles. Each task is procedurally generated and presented at three difficulty levels, enabling controlled analysis of model performance across increasing complexity. Our comprehensive evaluation of state-of-the-art models reveals a substantial gap between the performance of humans and MLLMs. Additionally, we explore the potential of reinforcement learning to improve visual simulation capabilities. Our findings suggest that while some models exhibit partial competence in recognizing visual patterns, robust mental visualization remains an open challenge for current MLLMs.

[Arxiv](https://arxiv.org/abs/2507.11932)