# # 摘要  
    最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年07月09日

`LLM应用

这篇论文探讨了大型语言模型在数据提取和审查过程中的应用，特别是通过审查协议来加速数据提取。它详细描述了使用Claude 3.5 Sonnet进行实验，评估了不同方法的性能，并讨论了LLM反馈对审查协议的改进。因此，它属于LLM应用类别。` `审查领域` `数据提取`

> Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review

# 摘要

> 审查的数据提取阶段资源消耗较大，研究人员可能会寻求利用在线大型语言模型（LLMs）和审查协议来加快数据提取速度。Claude 3.5 Sonnet被用于试验两种方法，这些方法使用审查协议从案例研究范围审查中包含的10个证据来源中提取数据。还使用了基于协议的方法来审查提取的数据。性能评估结果显示，当提取简单、定义明确的引用细节时，两种提取方法的准确率分别为83.3%和100%；而提取复杂、主观数据项时，准确率下降至9.6%和15.8%。总体来看，两种方法的精确度均超过90%，但召回率低于25%，F1分数低于40%。复杂范围审查的背景、开放的响应类型和方法学方法可能影响了性能，导致数据遗漏和错误归属。LLM反馈认为基线提取准确，并建议进行小幅修改：15个引用细节中的4个（26.7%）和38个关键发现数据项中的8个（21.1%）被认为可能增加价值。然而，当使用包含故意错误的数据集重复该过程时，仅检测到39个错误中的2个（5%）。基于审查协议的快速方法需要在各种LLMs和审查背景下进行全面的性能评估，并与传统的提示工程方法进行比较。我们建议研究人员在使用LLMs进行数据提取或审查提取数据时评估并报告LLM的性能。LLM反馈有助于协议适应，并可能协助未来审查协议的起草。

> The data extraction stages of reviews are resource-intensive, and researchers may seek to expediate data extraction using online (large language models) LLMs and review protocols. Claude 3.5 Sonnet was used to trial two approaches that used a review protocol to prompt data extraction from 10 evidence sources included in a case study scoping review. A protocol-based approach was also used to review extracted data. Limited performance evaluation was undertaken which found high accuracy for the two extraction approaches (83.3% and 100%) when extracting simple, well-defined citation details; accuracy was lower (9.6% and 15.8%) when extracting more complex, subjective data items. Considering all data items, both approaches had precision >90% but low recall (<25%) and F1 scores (<40%). The context of a complex scoping review, open response types and methodological approach likely impacted performance due to missed and misattributed data. LLM feedback considered the baseline extraction accurate and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of 38 (21.1%) to key findings data items were considered to potentially add value. However, when repeating the process with a dataset featuring deliberate errors, only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for expediency require more robust performance evaluation across a range of LLMs and review contexts with comparison to conventional prompt engineering approaches. We recommend researchers evaluate and report LLM performance if using them similarly to conduct data extraction or review extracted data. LLM feedback contributed to protocol adaptation and may assist future review protocol drafting.

[Arxiv](https://arxiv.org/abs/2507.06623)