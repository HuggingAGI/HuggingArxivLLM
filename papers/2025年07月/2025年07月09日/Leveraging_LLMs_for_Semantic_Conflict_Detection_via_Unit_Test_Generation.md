# 借助LLMs生成单元测试，实现语义冲突检测

发布时间：2025年07月09日

`LLM应用` `软件工程` `软件测试`

> Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation

# 摘要

> 当开发人员在代码库中引入更改时，无意中影响了其他开发人员并行集成的更改行为，就会产生语义冲突。传统合并工具无法检测这种语义冲突，因此提出了互补工具如SMAT。SMAT通过生成和执行单元测试来工作：如果一个测试在基线版本上失败，在开发人员修改后的版本上通过，但在与另一位开发人员的更改合并后再次失败，则表明存在语义冲突。虽然SMAT在检测冲突方面有效，但它存在较高的漏报率，部分原因在于Randoop和Evosuite等单元测试生成工具的局限性。为了探究大型语言模型（LLMs）是否能克服这些限制，我们提出并将在SMAT中集成一种基于Code Llama 70B的新测试生成工具。我们探讨了该模型在不同交互策略、提示内容和参数配置下生成测试的能力。我们的评估使用了两个样本：一个基于相关工作中更简单系统的基准，以及一个基于复杂真实系统的重要样本。我们评估了SMAT新扩展在检测冲突方面的有效性。结果显示，尽管在复杂场景下基于LLM的测试生成仍然具有挑战性且计算成本高昂，但在改进语义冲突检测方面存在令人鼓舞的潜力。

> Semantic conflicts arise when a developer introduces changes to a codebase that unintentionally affect the behavior of changes integrated in parallel by other developers. Traditional merge tools are unable to detect such conflicts, so complementary tools like SMAT have been proposed. SMAT relies on generating and executing unit tests: if a test fails on the base version, passes on a developer's modified version, but fails again after merging with another developer's changes, a semantic conflict is indicated. While SMAT is effective at detecting conflicts, it suffers from a high rate of false negatives, partly due to the limitations of unit test generation tools such as Randoop and Evosuite. To investigate whether large language models (LLMs) can overcome these limitations, we propose and integrate a new test generation tool based on Code Llama 70B into SMAT. We explore the model's ability to generate tests using different interaction strategies, prompt contents, and parameter configurations. Our evaluation uses two samples: a benchmark with simpler systems from related work, and a more significant sample based on complex, real-world systems. We assess the effectiveness of the new SMAT extension in detecting conflicts. Results indicate that, although LLM-based test generation remains challenging and computationally expensive in complex scenarios, there is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudanças em uma base de código que afetam, de forma n~ao intencional, o comportamento de alteraç~oes integradas em paralelo por outros desenvolvedores. Ferramentas tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso ferramentas complementares como o SMAT foram propostas. O SMAT depende da geraç~ao e execuç~ao de testes de unidade: se um teste falha na vers~ao base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar após o merge com as mudanças de outro desenvolvedor, um conflito sem^antico é identificado. Embora o SMAT seja eficaz na detecç~ao de conflitos, apresenta alta taxa de falsos negativos, em parte devido às limitaç~oes das ferramentas de geraç~ao de testes como Randoop e Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem superar essas limitaç~oes, propomos e integramos ao SMAT uma nova ferramenta de geraç~ao de testes baseada no Code Llama 70B. Exploramos a capacidade do modelo de gerar testes utilizando diferentes estratégias de interaç~ao, conteúdos de prompts e configuraç~oes de par^ametros. Nossa avaliaç~ao utiliza duas amostras: um benchmark com sistemas mais simples, usados em trabalhos relacionados, e uma amostra mais significativa baseada em sistemas complexos e reais. Avaliamos a eficácia da nova extens~ao do SMAT na detecç~ao de conflitos. Os resultados indicam que, embora a geraç~ao de testes por LLM em cenários complexos ainda seja desafiadora e custosa computacionalmente, há potencial promissor para aprimorar a detecç~ao de conflitos sem^anticos.

[Arxiv](https://arxiv.org/abs/2507.06762)