# 通过领域自适应持续预训练实现高效工业 sLLMs：方法、评估与应用

发布时间：2025年07月09日

`LLM应用`

> Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications

# 摘要

> 开源大型语言模型（LLMs）为企业应用开辟了新的可能，但许多组织仍受限于基础设施，难以部署大规模模型。于是，尽管性能有限，小型LLMs（sLLMs）成为了一种实用选择。虽然领域自适应持续预训练（DACP）在学术领域已有研究，但其商业价值尚待挖掘。本研究验证了DACP方案在各类基础模型和服务领域中的有效性。通过大量实验和实际评估，我们发现，应用DACP的小型LLMs不仅在目标领域表现出色，还保持了通用能力，为企业的规模化部署提供了一种高效、可扩展的解决方案。

> The emergence of open-source large language models (LLMs) has expanded opportunities for enterprise applications; however, many organizations still lack the infrastructure to deploy and maintain large-scale models. As a result, small LLMs (sLLMs) have become a practical alternative, despite their inherent performance limitations. While Domain Adaptive Continual Pretraining (DACP) has been previously explored as a method for domain adaptation, its utility in commercial applications remains under-examined. In this study, we validate the effectiveness of applying a DACP-based recipe across diverse foundation models and service domains. Through extensive experiments and real-world evaluations, we demonstrate that DACP-applied sLLMs achieve substantial gains in target domain performance while preserving general capabilities, offering a cost-efficient and scalable solution for enterprise-level deployment.

[Arxiv](https://arxiv.org/abs/2507.06795)