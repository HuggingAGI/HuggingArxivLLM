# # 摘要  
清单工程学赋能多语言LLM评估系统

清单工程学（Checklist Engineering）是一种系统化的方法论，通过构建详尽的任务清单来确保大型语言模型（LLMs）在多语言场景下的评估全面性与准确性。这种方法不仅能够显著提升模型的评估效率，还能确保不同语言环境下的评估结果具有可比性。

发布时间：2025年07月09日

`LLM应用` `多语言`

> Checklist Engineering Empowers Multilingual LLM Judges

# 摘要

> 自动化文本评估一直是自然语言处理（NLP）领域的核心问题。最近，该领域逐渐转向将大型语言模型（LLMs）作为评估器，这一趋势被称为“LLM作为裁判”范式。尽管这一方法在跨任务应用中展现出巨大潜力且易于适应，但在多语言环境下却鲜有探索。现有的多语言研究通常依赖专有模型，或者需要大量训练数据进行微调，这引发了关于成本、时间和效率的担忧。在本文中，我们提出了一种基于检查清单工程的“LLM作为裁判”框架（CE-Judge），这是一个无需训练的开放源代码模型框架，利用检查清单直觉进行多语言评估。在多种语言和三个基准数据集上进行的点对点和对对比较实验表明，我们的方法通常优于基线方法，并且与GPT-4o模型的表现持平。

> Automated text evaluation has long been a central issue in Natural Language Processing (NLP). Recently, the field has shifted toward using Large Language Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While promising and easily adaptable across tasks, this approach has seen limited exploration in multilingual contexts. Existing multilingual studies often rely on proprietary models or require extensive training data for fine-tuning, raising concerns about cost, time, and efficiency. In this paper, we propose Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free framework that uses checklist intuition for multilingual evaluation with an open-source model. Experiments across multiple languages and three benchmark datasets, under both pointwise and pairwise settings, show that our method generally surpasses the baselines and performs on par with the GPT-4o model.

[Arxiv](https://arxiv.org/abs/2507.06774)