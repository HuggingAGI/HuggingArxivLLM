# 面向多模态大型语言模型的视觉丰富文档理解综述：方法、挑战与新兴趋势

发布时间：2025年07月13日

`LLM应用` `文档分析与理解`

> A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends

# 摘要

> 视觉丰富文档理解（VRDU）已成为一个关键领域，推动这一领域发展的是对自动处理包含复杂视觉、文本和版式信息文档的需求。近期，多模态大型语言模型（MLLMs）在这一领域展现了巨大潜力，它们结合基于光学字符识别（OCR）依赖和无OCR框架，从文档图像中提取和解释信息。本综述回顾了基于MLLM的VRDU领域的最新进展，重点突出三个核心组件：（1）用于编码和融合文本、视觉和版式特征的方法；（2）训练范式，包括预训练策略、指令-响应微调以及不同模型模块的可训练性；（3）用于预训练、指令微调和监督微调的数据集。最后，我们探讨了这一不断发展的领域所面临的挑战和机遇，并提出了未来的研究方向，以提升VRDU系统的效率、泛化能力和鲁棒性。

> Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.

[Arxiv](https://arxiv.org/abs/2507.09861)