# 语音转文本 × 大型语言模型 × 文本转语音组合在 AI 面试系统中的应用评估

发布时间：2025年07月15日

`LLM应用` `人工智能` `语音技术`

> Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems

# 摘要

> 基于语音的对话式AI系统正越来越多地采用级联架构，整合语音转文本（STT）、大型语言模型（LLMs）和文本转语音（TTS）组件。然而，生产环境中不同组件组合的系统性评估仍待深入。我们基于30余万次AI面试数据，对STT x LLM x TTS组件组合进行了大规模实证对比。通过开发一个自动化评估框架，利用LLM作为评估者，我们从对话质量、技术准确性和技能评估能力三个维度进行了全面评估。研究发现，Google STT与GPT-4.1的组合在对话和质量指标上表现尤为突出。值得注意的是，客观质量指标与用户满意度评分的相关性较弱，这表明基于语音的AI系统用户体验不仅受技术性能影响。我们的研究为多模态对话式AI系统的组件选择提供了实用指导，并为语音交互评估贡献了一种经过验证的方法论。

> Voice-based conversational AI systems increasingly rely on cascaded architectures combining speech-to-text (STT), large language models (LLMs), and text-to-speech (TTS) components. However, systematic evaluation of different component combinations in production settings remains understudied. We present a large-scale empirical comparison of STT x LLM x TTS stacks using data from over 300,000 AI-conducted job interviews. We develop an automated evaluation framework using LLM-as-a-Judge to assess conversational quality, technical accuracy, and skill assessment capabilities. Our analysis of four production configurations reveals that Google STT paired with GPT-4.1 significantly outperforms alternatives in both conversational and technical quality metrics. Surprisingly, we find that objective quality metrics correlate weakly with user satisfaction scores, suggesting that user experience in voice-based AI systems depends on factors beyond technical performance. Our findings provide practical guidance for selecting components in multimodal conversational AI systems and contribute a validated evaluation methodology for voice-based interactions.

[Arxiv](https://arxiv.org/abs/2507.16835)