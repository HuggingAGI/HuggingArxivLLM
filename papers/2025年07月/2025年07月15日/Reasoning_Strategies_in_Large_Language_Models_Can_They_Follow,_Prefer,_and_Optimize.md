# 大型语言模型的推理策略：能否遵循、偏好并进行优化？

发布时间：2025年07月15日

`LLM应用` `人工智能` `推理系统`

> Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?

# 摘要

> 人类推理涉及多种策略，每种策略针对特定问题。先前研究表明，大型语言模型（LLMs）往往偏好单一推理策略，这可能限制了它们在多样化推理任务中的效果。本研究探讨了提示（prompting）是否能控制LLMs的推理策略，并评估其对逻辑问题解决的影响。实验表明，单一策略难以持续提升准确率，但如果模型能自适应选择最优策略，性能将显著提升。我们提出了一些方法来指导LLMs选择策略，揭示了提升其推理能力的新途径。

> Human reasoning involves different strategies, each suited to specific problems. Prior work shows that large language model (LLMs) tend to favor a single reasoning strategy, potentially limiting their effectiveness in diverse reasoning challenges. In this work, we investigate whether prompting can control LLMs reasoning strategies and assess its impact on logical problem-solving. While our experiments show that no single strategy consistently improves accuracy, performance could be enhanced if models could adaptively choose the optimal strategy. We propose methods to guide LLMs in strategy selection, highlighting new ways to refine their reasoning abilities.

[Arxiv](https://arxiv.org/abs/2507.11423)