# 面具背后的魔鬼：扩散式大型语言模型中潜藏的安全漏洞

发布时间：2025年07月15日

`LLM理论` `人工智能安全`

> The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs

# 摘要

> 扩散型大语言模型（dLLMs）作为自回归模型的革新替代方案，凭借并行解码和双向建模特性，实现了更快的推理速度和更高的交互性。然而，尽管在代码生成和文本填充任务中表现优异，我们揭示了一个关键安全问题：现有对齐机制未能有效防御上下文感知的带掩码输入对抗性提示，导致dLLMs面临全新漏洞。为此，我们推出DIJA——首个系统性研究并针对dLLMs独特安全弱点的越狱攻击框架。DIJA通过构建对抗性交错掩码文本提示，巧妙利用dLLMs的双向建模与并行解码机制。双向建模使模型倾向于为掩码内容生成上下文一致的输出，即便内容有害；而并行解码则限制了模型对不安全内容的动态过滤与拒绝采样。这种机制导致传统对齐机制失效，使得经过对齐优化的dLLMs仍可能生成有害内容，即便提示中直接暴露了危险行为或不安全指令。通过详尽实验，我们证实DIJA显著超越现有越狱方法，揭示了dLLM架构中一个此前未被重视的威胁面。值得注意的是，DIJA在Dream-Instruct上实现了100%的关键词ASR，在JailbreakBench上基于评估器的ASR较最强基线ReNeLLM高出78.5%，同时在StrongREJECT得分上领先37.7分，且无需在提示中重写或隐藏有害内容。这一发现凸显了重新审视并优化新兴语言模型安全对齐机制的迫切需求。代码已开源，地址为https://github.com/ZichenWen1/DIJA。

> Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.

[Arxiv](https://arxiv.org/abs/2507.11097)