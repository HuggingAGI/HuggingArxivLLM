# AI语言熟练度监测系统 -- 追踪LLMs在多语言基准上的发展

发布时间：2025年07月11日

`LLM应用` `多语言模型`

> The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks

# 摘要

> 为了公平共享大型语言模型 (LLMs) 的优势，评估其在世界各地语言中的能力至关重要。我们推出 AI 语言熟练度监测器，这是一个全面的多语言基准测试，系统性地评估 LLM 在多达 200 种语言中的表现，尤其关注低资源语言。我们的基准测试整合了多样化的任务，包括翻译、问答、数学和推理，采用 FLORES+、MMLU、GSM8K、TruthfulQA 和 ARC 等数据集。我们提供开源且自动更新的排行榜和仪表盘，支持研究人员、开发者和政策制定者识别模型性能的优势与不足。除了对模型进行排名，该平台还提供描述性见解，如全球熟练度地图和随时间变化的趋势。通过补充和扩展先前的多语言基准测试，我们的工作旨在促进多语言 AI 中的透明度、包容性和进步。该系统可在 https://huggingface.co/spaces/fair-forward/evals-for-every-language 访问。

> To ensure equitable access to the benefits of large language models (LLMs), it is essential to evaluate their capabilities across the world's languages. We introduce the AI Language Proficiency Monitor, a comprehensive multilingual benchmark that systematically assesses LLM performance across up to 200 languages, with a particular focus on low-resource languages. Our benchmark aggregates diverse tasks including translation, question answering, math, and reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We provide an open-source, auto-updating leaderboard and dashboard that supports researchers, developers, and policymakers in identifying strengths and gaps in model performance. In addition to ranking models, the platform offers descriptive insights such as a global proficiency map and trends over time. By complementing and extending prior multilingual benchmarks, our work aims to foster transparency, inclusivity, and progress in multilingual AI. The system is available at https://huggingface.co/spaces/fair-forward/evals-for-every-language.

[Arxiv](https://arxiv.org/abs/2507.08538)