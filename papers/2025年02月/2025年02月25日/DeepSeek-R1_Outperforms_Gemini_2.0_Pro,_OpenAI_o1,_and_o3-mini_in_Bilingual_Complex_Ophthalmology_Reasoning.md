# DeepSeek-R1 在双语复杂眼科推理领域超越 Gemini 2.0 Pro、OpenAI o1 与 o3-mini

发布时间：2025年02月25日

`LLM应用` `人工智能`

> DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning

# 摘要

> # 研究目的
本研究旨在评估DeepSeek-R1与其他三种新发布的大型语言模型（LLMs）在双语复杂眼科病例中的准确性和推理能力。

# 研究方法
我们从中国眼科高级职称考试中收集了130道与诊断（n=39）和治疗（n=91）相关的多项选择题（MCQs），并将其分为六个主题。使用DeepSeek-R1将这些MCQs翻译成英文。DeepSeek-R1、Gemini 2.0 Pro、OpenAI o1和o3-mini在2025年2月15日至2月20日之间在默认配置下生成回答。准确率计算为正确回答问题的比例，遗漏和额外答案被视为错误。推理能力通过分析推理逻辑和推理错误的原因进行评估。

# 研究结果
DeepSeek-R1在整体准确率上表现最优，在中文MCQs中达到0.862，在英文MCQs中达到0.808。Gemini 2.0 Pro、OpenAI o1和OpenAI o3-mini在中文MCQs中的准确率分别为0.715、0.685和0.692（与DeepSeek-R1相比，所有P<0.001），在英文MCQs中分别为0.746（P=0.115）、0.723（P=0.027）和0.577（P<0.001）。DeepSeek-R1在中英文MCQs的五个主题中均达到最高准确率。它在中文治疗问题中也表现出色（所有P<0.05）。推理能力分析显示，四个LLMs的推理逻辑相似。忽略关键阳性病史、忽略关键阳性体征、误读医学数据和过于激进是推理错误最常见的原因。

# 结论
与另外三个先进的LLMs相比，DeepSeek-R1在双语复杂眼科推理任务中表现更优。尽管其临床应用仍具挑战性，但它在支持诊断和临床决策方面显示出潜力。

> Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and management (n = 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making.

[Arxiv](https://arxiv.org/abs/2502.17947)