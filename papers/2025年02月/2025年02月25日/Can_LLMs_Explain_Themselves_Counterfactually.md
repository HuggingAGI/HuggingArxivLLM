# <翻译失败>

发布时间：2025年02月25日

`LLM应用` `人工智能` `机器学习`

> Can LLMs Explain Themselves Counterfactually?

# 摘要

> 解释对于理解机器学习模型的行为、建立用户信任以及确保合规性至关重要。过去几年见证了大量生成模型解释的后验方法的涌现，其中许多依赖于计算模型梯度或解决专门设计的优化问题。然而，得益于大型语言模型（LLMs）卓越的推理能力，自解释——即通过提示模型解释其输出——作为一种全新的范式应运而生。本研究聚焦于自解释中的一种特定类型：自生成反事实解释（SCEs）。我们设计了一系列测试来评估LLMs生成SCEs的能力。通过对不同LLM家族、模型规模、温度设置以及数据集的分析，我们发现LLMs有时难以生成有效的SCEs。即便能够生成，其预测结果也常常与其自身的反事实推理相矛盾。

> Explanations are an important tool for gaining insights into the behavior of ML models, calibrating user trust and ensuring regulatory compliance. Past few years have seen a flurry of post-hoc methods for generating model explanations, many of which involve computing model gradients or solving specially designed optimization problems. However, owing to the remarkable reasoning abilities of Large Language Model (LLMs), self-explanation, that is, prompting the model to explain its outputs has recently emerged as a new paradigm. In this work, we study a specific type of self-explanations, self-generated counterfactual explanations (SCEs). We design tests for measuring the efficacy of LLMs in generating SCEs. Analysis over various LLM families, model sizes, temperature settings, and datasets reveals that LLMs sometimes struggle to generate SCEs. Even when they do, their prediction often does not agree with their own counterfactual reasoning.

[Arxiv](https://arxiv.org/abs/2502.18156)