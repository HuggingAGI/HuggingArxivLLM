# 对比调查受访者与训练数据，解密LLMs在美国最高法院案件中的政治倾向

发布时间：2025年02月25日

`LLM应用` `伦理评估`

> Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases

# 摘要

> 大型语言模型（LLMs）的广泛应用及其对公众舆论的潜在影响，引发了对其政治倾向评估的兴趣。在先前研究的基础上，我们进一步探究了系统响应中政治偏见的潜在原因，通过实证研究来考察训练语料库中嵌入的价值观和偏见如何影响模型输出。具体而言，我们提出了一种方法，用于定量评估大型预训练语料库中嵌入的政治倾向。进而，我们研究了LLMs的政治倾向与它们的预训练语料库或调查中的人类意见之间更一致。作为一个案例研究，我们专注于探究LLMs在美国最高法院32个案件中的政治倾向，涉及有争议的话题，如堕胎和投票权。我们的研究结果表明，LLMs强烈地反映了其训练数据中的政治倾向，并且没有观察到它们与调查中表达的人类意见之间的强烈相关性。这些结果强调了负责任地整理训练数据的重要性，以及需要稳健的评估指标，以确保LLMs与以人为本的价值观保持一致。

> The increased adoption of Large Language Models (LLMs) and their potential to shape public opinion have sparked interest in assessing these models' political leanings. Building on previous research that compared LLMs and human opinions and observed political bias in system responses, we take a step further to investigate the underlying causes of such biases by empirically examining how the values and biases embedded in training corpora shape model outputs. Specifically, we propose a method to quantitatively evaluate political leanings embedded in the large pretraining corpora. Subsequently we investigate to whom are the LLMs' political leanings more aligned with, their pretrainig corpora or the surveyed human opinions. As a case study, we focus on probing the political leanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics such as abortion and voting rights. Our findings reveal that LLMs strongly reflect the political leanings in their training data, and no strong correlation is observed with their alignment to human opinions as expressed in surveys. These results underscore the importance of responsible curation of training data and the need for robust evaluation metrics to ensure LLMs' alignment with human-centered values.

[Arxiv](https://arxiv.org/abs/2502.18282)