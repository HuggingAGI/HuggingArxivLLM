# EgoSpeak：对话代理何时发声的艺术

发布时间：2025年02月16日

`Agent` `视频处理` `对话系统`

> EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild

# 摘要

> 在现实环境中预测何时开始对话仍是对话代理的核心挑战。我们提出EgoSpeak，一个专为以自我为中心的流媒体视频设计的实时说话时机预测新框架。该框架从说话者的视角建模对话，使对话代理能够像人类一样持续观察环境并动态决定发言时机。EgoSpeak通过融合四项关键能力，成功弥合了实验室简化场景与真实复杂对话间的鸿沟：第一人称视角、RGB图像处理、在线处理以及未剪辑视频处理。我们还发布了YT-Conversation，一个来自YouTube的多样化真实场景对话视频数据集，为大规模预训练提供资源支持。在EasyCom和Ego4D上的实验表明，EgoSpeak在实时场景中显著优于随机和静默基线。研究结果还凸显了多模态输入和上下文长度在决定何时发言中的关键作用。

> Predicting when to initiate speech in real-world environments remains a fundamental challenge for conversational agents. We introduce EgoSpeak, a novel framework for real-time speech initiation prediction in egocentric streaming video. By modeling the conversation from the speaker's first-person viewpoint, EgoSpeak is tailored for human-like interactions in which a conversational agent must continuously observe its environment and dynamically decide when to talk. Our approach bridges the gap between simplified experimental setups and complex natural conversations by integrating four key capabilities: (1) first-person perspective, (2) RGB processing, (3) online processing, and (4) untrimmed video processing. We also present YT-Conversation, a diverse collection of in-the-wild conversational videos from YouTube, as a resource for large-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that EgoSpeak outperforms random and silence-based baselines in real time. Our results also highlight the importance of multimodal input and context length in effectively deciding when to speak.

[Arxiv](https://arxiv.org/abs/2502.14892)