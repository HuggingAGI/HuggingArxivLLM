# 基于分布假设的无需人工评判的LLM开放性生成任务评估基准

发布时间：2025年02月13日

`LLM应用` `生成式AI`

> A Judge-free LLM Open-ended Generation Benchmark Based on the Distributional Hypothesis

# 摘要

> 评估大型语言模型 (LLMs) 的开放文本生成面临两大挑战：缺乏明确的基准答案，以及高昂的人工或基于LLM的评估成本。为此，我们提出了一种创新的评估基准，通过n-gram统计和规则对LLMs进行评估，无需依赖人工判断或以LLM为裁判的方法。基于50个问题和参考答案集，我们引入了三个全新的评估指标：流畅性、真实性和 helpfulness。我们的基准不仅与基于GPT-4的评估结果高度相关，还显著降低了计算资源需求，证明了其作为评估LLMs开放生成能力的高效替代方案的潜力。

> Evaluating the open-ended text generation of large language models (LLMs) is challenging because of the lack of a clear ground truth and the high cost of human or LLM-based assessments. We propose a novel benchmark that evaluates LLMs using n-gram statistics and rules, without relying on human judgement or LLM-as-a-judge approaches. Using 50 question and reference answer sets, we introduce three new metrics based on n-grams and rules: Fluency, Truthfulness, and Helpfulness. Our benchmark strongly correlates with GPT-4o-based evaluations while requiring significantly fewer computational resources, demonstrating its effectiveness as a scalable alternative for assessing LLMs' open-ended generation capabilities.

[Arxiv](https://arxiv.org/abs/2502.09316)