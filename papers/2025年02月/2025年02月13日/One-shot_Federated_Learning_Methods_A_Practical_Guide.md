# # 摘要
一触即发：联邦学习实用指南

发布时间：2025年02月13日

`其他` `人工智能` `联邦学习`

> One-shot Federated Learning Methods: A Practical Guide

# 摘要

> 一次性联邦学习（OFL）是一种将客户端-服务器通信限制为单轮交互的分布式机器学习范式，有效解决了传统联邦学习中多轮数据交换带来的隐私和通信开销问题。OFL在与需要协作训练模型的未来方法（如大型语言模型（LLMs））集成方面展现了巨大潜力。然而，现有的OFL方法面临两大挑战：数据异质性和模型异质性，这导致其性能远逊于传统FL方法。尽管已有大量研究试图解决这些限制，但仍然缺乏全面的总结。本文对OFL所面临的技术挑战进行了系统性分析，并对现有方法进行了深入综述。我们还提出了一种创新的分类方法，并分析了各种技术的权衡。此外，我们探讨了最有前景的未来研究方向以及应与OFL领域整合的技术。本研究旨在为未来的相关研究提供指导和见解。

> One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs). However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an innovative categorization method and analyze the trade-offs of various techniques. Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and insights for future research.

[Arxiv](https://arxiv.org/abs/2502.09104)