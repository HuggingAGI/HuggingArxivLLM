# 亮出你的工作：事实核查员对可解释自动化事实核查的要求

发布时间：2025年02月13日

`LLM应用` `事实核查`

> Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking

# 摘要

> 大型语言模型和生成式人工智能在在线媒体中的普及，凸显了有效自动事实核查的重要性。面对日益增长和复杂的错误信息，事实核查人员需要自动事实核查系统提供能够帮助其审查输出的解释。然而，如何使这些解释与事实核查人员的决策和推理过程相匹配，以便有效整合到工作流程中，仍是一个待解决的问题。

通过与事实核查专业人士的半结构化访谈，我们从三个方面填补了这一研究空白：首先，我们详细描述了事实核查人员在评估证据、做出决策以及解释其过程时的具体方法；其次，我们深入探讨了他们在实际工作中如何运用自动化工具；最后，我们明确了事实核查人员对自动事实核查工具的解释需求。研究发现，现有的解释功能尚无法满足这些需求，并提出了构建可复制事实核查解释的重要标准：包括清晰展示模型的推理路径、准确引用相关证据，以及明确指出潜在的不确定性与信息缺口。

> The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.

[Arxiv](https://arxiv.org/abs/2502.09083)