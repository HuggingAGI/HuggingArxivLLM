# 推理能力的提升并不一定能增强角色扮演能力

发布时间：2025年02月24日

`LLM应用` `角色扮演`

> Reasoning Does Not Necessarily Improve Role-Playing Ability

# 摘要

> 角色扮演的大语言模型 (LLMs) 在学术界和商业领域的应用正在迅速扩展，对高精度角色扮演模型的需求日益增长。与此同时，推理技术的快速发展不断推动着 LLMs 的性能边界。这种角色扮演需求与推理能力的结合，提出了一个重要研究问题：'推理技术能否提升 LLMs 的角色扮演能力？' 我们通过 6 个角色扮演基准测试、24 个 LLMs 和 3 种不同的角色扮演策略进行了全面研究，比较了直接零-shot 角色扮演、思维链 (CoT) 辅助的角色扮演以及推理优化 LLMs 的效果。研究发现：CoT 可能削弱角色扮演性能，推理优化的 LLMs 不适合角色扮演，推理能力扰乱了角色扮演的规模定律，大型模型在高级角色扮演方面仍不熟练，且中文角色扮演表现优于英文。基于实验结果，我们提出两个未来研究方向：面向角色感知的 CoT 以改进角色扮演 LLMs，以及强化学习应用于角色扮演 LLMs，旨在提升其适应性、一致性和有效性，助力研究与实际应用。

> The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: "Can reasoning techniques enhance the role-playing capabilities of LLMs?" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.

[Arxiv](https://arxiv.org/abs/2502.16940)