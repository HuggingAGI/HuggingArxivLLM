# LLM解释的不确定性解析：从推理拓扑视角探讨

发布时间：2025年02月24日

`LLM理论

摘要讨论了大型语言模型（LLM）解释中的不确定性，并提出了一种新的框架来量化这种不确定性。研究通过推理拓扑视角，设计了一种结构化提取策略，将LLM的解释转化为图拓扑结构，从而在语义和推理路径两个层面进行量化。这属于对LLM理论的深入研究，特别是解释性和评估方法的探讨。因此，这篇论文属于LLM理论类别。` `人工智能`

> Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology

# 摘要

> 理解大型语言模型（LLM）解释中的不确定性对于评估其忠实度和推理一致性至关重要，从而为判断LLM对某一问题的回答可靠性提供见解。本研究提出了一种新颖的框架，通过推理拓扑视角量化LLM解释中的不确定性。我们设计了一种结构化提取策略，引导LLM将答案的解释框架化为图拓扑结构。这一过程将解释分解为与知识相关的子问题和基于拓扑的推理结构，使我们不仅能够在语义层面量化不确定性，还能从推理路径的角度进行量化。这进一步为评估知识冗余提供了便利，并对推理过程提供了可解释的洞察。我们的方法为解释LLM推理提供了一种系统化的方式，能够分析其局限性，并为提升稳健性和忠实度提供指导。本研究率先在LLM解释中引入基于图结构的不确定性测量，并展示了基于拓扑量化的潜力。

> Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.

[Arxiv](https://arxiv.org/abs/2502.17026)