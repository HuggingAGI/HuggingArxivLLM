# 通过模型合并，将文本偏好融入视觉语言理解

发布时间：2025年02月19日

`LLM应用` `人工智能` `多模态`

> Transferring Textual Preferences to Vision-Language Understanding through Model Merging

# 摘要

> 大型视觉语言模型（LVLMs）在多模态任务中表现出色，但其评估生成内容的能力仍有待提升，且基于偏好数据训练视觉语言奖励模型（VLRMs）的计算成本较高。本文提出了一种无训练的解决方案，通过融合基于文本的奖励模型（RMs）与LVLMs来构建VLRMs。实验结果表明，这种整合不仅提升了LVLMs的评分能力，还优化了基于文本的RMs的表现，为在LVLMs中高效融入文本偏好提供了新思路。

> Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.

[Arxiv](https://arxiv.org/abs/2502.13487)