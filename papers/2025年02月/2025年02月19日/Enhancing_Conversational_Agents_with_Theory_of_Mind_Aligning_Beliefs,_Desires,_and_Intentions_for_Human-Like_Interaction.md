# 借助心智理论，提升对话代理的类人交互能力：对齐信念、欲望与意图

发布时间：2025年02月19日

`LLM应用` `人工智能` `智能体`

> Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction

# 摘要

> 基于大型语言模型 (LLMs) 的智能体人工智能 (AI) 与自然语言交互有望继续保持主导地位。尽管人类本能地能够调整沟通以适应心理状态 -- 这种能力被称为心智理论 (ToM)，但目前基于 LLM 的系统在这一方面存在明显局限。本研究探讨开源语言模型 (LLaMA) 在捕获和保留 ToM 相关信息方面的潜力，以及这些信息如何有效提升生成响应中的 ToM 推理一致性。我们进一步研究了对信念、欲望和意图等 ToM 相关组件进行显式操作是否能增强响应对齐效果。实验结果表明，在两种 LLaMA 3 变体中引入 ToM 指导的对齐方法显著提升了响应质量，使 3B 和 8B 模型的胜率分别达到了 67% 和 63%。这些发现凸显了 ToM 驱动策略在提升基于 LLM 的对话代理对齐方面的潜力。

> Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.

[Arxiv](https://arxiv.org/abs/2502.14171)