# 理论物理基准 (TPBench) —— AI 在理论物理推理能力的数据集与研究

发布时间：2025年02月19日

`LLM应用

摘要：这项研究展示了将大型语言模型应用于理论物理问题解决的基准测试，评估了模型在不同难度级别上的表现，并探讨了自动验证与评分的挑战，属于LLM的应用层面。` `理论物理` `AI应用`

> Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics

# 摘要

> 我们推出了一项基准测试，专注于评估 AI 在高能理论和宇宙学等理论物理领域的问题解决能力。该基准测试的首个版本包含 57 个难度从本科生到研究水平的问题，这些问题均为全新设计，未见于现有公开题库。我们在包括 o3-mini、o1、DeepSeek-R1、GPT-4o 以及 Llama 和 Qwen 系列在内的多种开源与闭源语言模型上进行了评估。尽管我们观察到最新模型在性能上有显著提升，但研究级别的难题仍大多未能攻克。我们还探讨了自动验证与评分的挑战，并分析了常见的失败模式。虽然当前最先进的模型对研究人员的实际帮助仍有限，但我们的研究结果表明，AI 辅助的理论物理研究有望在不久的将来成为现实。我们进一步讨论了实现这一目标的主要障碍，并提出了可能的解决方案。所有公开问题及其解答、各模型的测试结果以及数据集的更新和得分分布，均可访问 tpbench.org 查阅。

> We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics, focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including o3-mini, o1, DeepSeek-R1, GPT-4o and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted theoretical physics research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset tpbench.org.

[Arxiv](https://arxiv.org/abs/2502.15815)