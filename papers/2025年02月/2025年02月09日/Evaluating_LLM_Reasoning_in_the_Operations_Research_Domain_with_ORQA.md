# 通过 ORQA 系统评估 LLM 在运筹学领域的推理能力

发布时间：2025年02月09日

`LLM应用` `运筹学` `数学建模`

> Evaluating LLM Reasoning in the Operations Research Domain with ORQA

# 摘要

> # 摘要
本文推出全新基准测试——运筹学问答（ORQA），旨在评估大型语言模型（LLMs）在运筹学专业领域的泛化能力。该基准测试通过真实世界优化问题，考察LLMs能否像运筹学专家一样运用知识和推理能力解决复杂问题。由运筹学专家精心打造的数据集，要求模型通过多步推理构建【数学公式】。我们对LLaMA 3.1、DeepSeek和Mixtral等开源模型的评估结果表明，现有模型在专业领域应用中的表现仍有提升空间。这项研究为LLMs泛化能力的探讨提供了新视角，并为未来研究指明了方向。数据集和评估代码现已公开。

> In this paper, we introduce and apply Operations Research Question Answering (ORQA), a new benchmark designed to assess the generalization capabilities of Large Language Models (LLMs) in the specialized technical domain of Operations Research (OR). This benchmark evaluates whether LLMs can emulate the knowledge and reasoning skills of OR experts when confronted with diverse and complex optimization problems. The dataset, developed by OR experts, features real-world optimization problems that demand multistep reasoning to construct their mathematical models. Our evaluations of various open source LLMs, such as LLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting a gap in their ability to generalize to specialized technical domains. This work contributes to the ongoing discourse on LLMs generalization capabilities, offering valuable insights for future research in this area. The dataset and evaluation code are publicly available.

[Arxiv](https://arxiv.org/abs/2412.17874)