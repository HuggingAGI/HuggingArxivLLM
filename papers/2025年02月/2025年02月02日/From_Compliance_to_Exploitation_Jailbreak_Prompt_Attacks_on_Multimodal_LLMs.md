# 从合规到利用：多模态LLMs的越狱提示攻击

发布时间：2025年02月02日

`LLM应用

**理由**：这篇论文主要讨论了针对多模态大型语言模型（LLMs）的语音越狱攻击，并提出了一种新的攻击策略。研究内容集中在LLMs的应用场景中，特别是如何通过语音输入绕过模型的防御机制。因此，这篇论文应归类为LLM应用。` `人工智能` `网络安全`

> From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs

# 摘要

> # 摘要
大型语言模型（LLMs）因其处理多种输入数据（如文本、音频、图像和视频）的能力不断增强，已在多个领域广泛应用。尽管LLMs在理解和生成不同场景的上下文方面表现出色，但它们容易受到基于提示的攻击，尤其是通过文本输入的攻击。本文首次提出了一种针对多模态LLMs的基于语音的越狱攻击，称为Flanking Attack，该攻击能够同时处理多种输入类型，针对多模态LLMs。我们的研究受到近期单语言语音驱动LLMs进展的启发，这些进展为LLMs引入了超越传统文本漏洞的新攻击面。为了探究这些风险，我们研究了前沿的多模态LLMs，这些模型支持多种输入类型（如音频输入），重点关注对抗性提示如何绕过其防御机制。我们提出了一种新颖的策略，将被禁止的提示包裹在良性的、叙事驱动的提示中，并将其整合到Flanking Attack中，通过虚构的设定人性化交互上下文并执行攻击。为了更好地评估攻击效果，我们设计了一个半自动的自我评估框架，用于检测策略违规。实验表明，Flank Attack能够操纵最先进的LLMs生成不匹配和禁止的输出，在七个禁止场景中平均攻击成功率在0.67到0.93之间。这些发现揭示了语音环境中基于提示的混淆的强大效果，以及当前LLMs审核保障措施的局限性，亟需先进的防御策略来应对日益复杂、上下文丰富的攻击挑战。

> Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the frontier multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. To better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flank Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios. These findings highlight both the potency of prompt-based obfuscation in voice-enabled contexts and the limitations of current LLMs' moderation safeguards and the urgent need for advanced defense strategies to address the challenges posed by evolving, context-rich attacks.

[Arxiv](https://arxiv.org/abs/2502.00735)