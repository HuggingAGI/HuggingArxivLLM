# # 安全规模化：大型模型安全的全面综述

发布时间：2025年02月02日

`其他` `人工智能安全` `模型安全`

> Safety at Scale: A Comprehensive Survey of Large Model Safety

# 摘要

> 大型模型的快速发展，得益于其通过大规模预训练展现出的卓越学习与泛化能力，已经重塑了人工智能（AI）领域的格局。这些模型现已成为多种应用场景的基础，包括对话式AI、推荐系统、自动驾驶、内容生成、医学诊断以及科学发现。然而，这些模型的大规模部署也带来了显著的安全风险，引发了关于鲁棒性、可靠性和伦理影响的担忧。

本研究综述了目前针对大型模型的安全研究现状，涵盖视觉基础模型（VFMs）、大型语言模型（LLMs）、视觉语言预训练（VLP）模型、视觉语言模型（VLMs）、扩散模型（DMs）以及基于大型模型的智能体。我们的贡献总结如下：
（1）我们提出了针对这些模型的安全威胁的全面分类，包括对抗攻击、数据投毒、后门攻击、越狱与提示注入攻击、能量-延迟攻击、数据与模型提取攻击，以及新兴的智能体特定威胁。
（2）我们回顾了针对每种攻击类型所提出的防御策略（如有），并总结了安全研究中常用的数据集与基准测试。
（3）在此基础上，我们识别并探讨了大型模型安全领域的开放挑战，强调了全面安全评估、可扩展且有效的防御机制，以及可持续数据实践的必要性。更重要的是，我们强调了研究界与国际合作的集体努力的必要性。

我们的工作可作为研究人员与从业者的有用参考，助力全面防御系统与平台的持续发展，从而保护AI模型的安全。

> The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.

[Arxiv](https://arxiv.org/abs/2502.05206)