# KazMMLU：评估语言模型的哈萨克语、俄语及哈萨克斯坦区域知识能力。

发布时间：2025年02月18日

`LLM应用` `NLP` `数据集`

> KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan

# 摘要

> 哈萨克斯坦的文化和语言在NLP领域代表性不足，尽管其人口已超过两千万。虽然全球大型语言模型（LLMs）不断发展，但哈萨克语的进步却十分有限，这一点从专用模型和基准评估的稀缺性中可见一斑。为填补这一空白，我们推出了首个专为哈萨克语设计的MMLU风格数据集——KazMMLU。该数据集包含23,000个问题，涵盖从STEM到人文学科和社会科学等多个领域，所有问题均来自真实教育材料，并经过母语者和教育工作者的手动验证。其中包含10,969个哈萨克语问题和12,031个俄语问题，体现了哈萨克斯坦的双语教育体系和丰富的本地语境。我们对多个先进多语言模型（如Llama-3.1、Qwen-2.5、GPT-4和DeepSeek V3）进行了评估，结果表明这些模型在哈萨克语和俄语上的表现仍有显著提升空间，即使是表现最佳的模型也难以达到令人满意的水平。与资源丰富的语言相比，这些结果凸显了明显的性能差距。我们希望这一数据集能够推动以哈萨克语为中心的LLM研究与开发。数据和代码将在论文被接收后公开。

> Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs. Data and code will be made available upon acceptance.

[Arxiv](https://arxiv.org/abs/2502.12829)