# DeepSeek与GPT的推理与信任行为：揭示大型语言模型中的隐性缺陷实验

发布时间：2025年02月18日

`LLM应用` `人工智能`

> Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models

# 摘要

> 在开发应用时，当大型语言模型（LLM）频繁带来性能提升或成本降低，开发者必须在采用新模型与坚守旧模型之间做出抉择。若仅因切换成本低而选择新模型，可能会忽视模型转变可能带来的潜在行为变化。我们的研究通过博弈论信任模型揭示了OpenAI与DeepSeek模型在信任行为上的显著差异。我们发现，o1-mini和o3-mini模型在平衡利润追求与信任回报时出现了信任行为的经济性崩溃，而DeepSeek模型则展现了更为复杂且收益更高的信任行为，这得益于其在前瞻性规划和心智理论等高级概念上的优势。鉴于LLMs已成为高风险商业系统的核心，我们的研究强调了过度依赖狭隘性能基准的潜在风险，并建议在组织的AI战略中纳入对LLM隐藏缺陷的深入分析。

> When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy.

[Arxiv](https://arxiv.org/abs/2502.12825)