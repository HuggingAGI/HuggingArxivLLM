# 大型语言模型携手符号验证器，助力逻辑推理评估

发布时间：2025年02月10日

`LLM应用

理由：这篇论文展示了如何利用大型语言模型（LLMs）生成高质量的一阶逻辑推理数据集，并评估了LLMs在该任务中的表现。它属于LLM的应用层面，因为它涉及LLMs在特定任务中的实际应用和数据集构建。` `人工智能` `逻辑推理`

> Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation

# 摘要

> 一阶逻辑（FOL）推理在智能系统中扮演着关键角色，尤其在链式思维（CoT）情境下，是评估推理能力的重要工具。然而，现有的基准测试往往受限于人工标注或模板的依赖，难以在复杂性、规模和多样性上满足需求。为此，我们提出了一种创新性框架ProverGen，该框架巧妙结合了大型语言模型（LLMs）的生成能力与符号证明器的严谨特性，成功构建了高质量的一阶逻辑推理数据集——ProverQA。ProverQA的亮点在于，每个问题都配备了清晰易懂且逻辑连贯的中间推理步骤。实验结果表明，即使借助CoT提示，当前最先进的LLMs在ProverQA上仍面临挑战，这凸显了该数据集的难度。我们进一步通过框架生成的独立训练集对Llama3.1-8B-Instruct进行了微调。微调后的模型在各类测试集上均表现优异，充分证明了ProverGen框架的有效性。更多代码细节请访问：https://github.com/opendatalab/ProverGen

> First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework. The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: https://github.com/opendatalab/ProverGen

[Arxiv](https://arxiv.org/abs/2502.06563)