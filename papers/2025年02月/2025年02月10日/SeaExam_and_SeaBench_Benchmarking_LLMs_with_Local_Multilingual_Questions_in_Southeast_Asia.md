# SeaExam 和 SeaBench：在东南亚地区使用本地多语言问题对大型语言模型进行基准测试

发布时间：2025年02月10日

`LLM应用` `社区服务`

> SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia

# 摘要

> 这项研究推出了两个创新的基准测试——SeaExam 和 SeaBench，专门设计用于评估大型语言模型（LLMs）在东南亚（SEA）应用场景中的表现。与现有主要基于英语翻译的多语言数据集不同，这两个基准测试立足于SEA地区的实际场景。SeaExam 从区域教育考试中汲取素材，构建了一个涵盖当地历史和文学等多学科的综合性数据集。而 SeaBench 则聚焦于多轮、开放式的任务设计，真实还原SEA社区的日常交流场景。我们的研究表明，相较于翻译基准，SeaExam 和 SeaBench 更能精准反映 LLM 在 SEA 语言任务中的性能差异。这一发现强调了使用真实世界查询评估 LLM 多语言能力的重要性。

> This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.

[Arxiv](https://arxiv.org/abs/2502.06298)