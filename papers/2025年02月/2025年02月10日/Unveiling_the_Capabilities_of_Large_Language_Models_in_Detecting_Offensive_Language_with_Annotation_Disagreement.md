# Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement
探讨大型语言模型在面对标注分歧时检测攻击性语言的潜力

发布时间：2025年02月10日

`LLM应用` `社交媒体`

> Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement

# 摘要

> LLMs 凭借其先进能力在冒犯性语言检测中得到广泛应用。然而，真实数据集中人工标注分歧带来的挑战仍未得到充分研究。这些样本因模糊特性难以检测。此外，LLMs 处理分歧样本的置信度能揭示其与人工标注者的一致性，提供宝贵见解。为填补这一研究空白，我们系统性地评估了 LLMs 在标注存在分歧情况下的冒犯性语言检测能力。我们比较了多个 LLMs 在不同标注一致水平下的二元分类准确率，分析了 LLM 置信度与标注一致性的关系。此外，我们还研究了分歧样本对 LLM 在 few-shot 学习和指令微调过程中的决策影响。我们的研究结果突出了分歧样本带来的挑战，为提升基于 LLM 的冒犯性语言检测提供了指导。

> LLMs are widely used for offensive language detection due to their advanced capability. However, the challenges posed by human annotation disagreement in real-world datasets remain underexplored. These disagreement samples are difficult to detect due to their ambiguous nature. Additionally, the confidence of LLMs in processing disagreement samples can provide valuable insights into their alignment with human annotators. To address this gap, we systematically evaluate the ability of LLMs to detect offensive language with annotation disagreement. We compare the binary accuracy of multiple LLMs across varying annotation agreement levels and analyze the relationship between LLM confidence and annotation agreement. Furthermore, we investigate the impact of disagreement samples on LLM decision-making during few-shot learning and instruction fine-tuning. Our findings highlight the challenges posed by disagreement samples and offer guidance for improving LLM-based offensive language detection.

[Arxiv](https://arxiv.org/abs/2502.06207)