# 计划与执行：LLM代理作为日常助手时用户信任与团队绩效的实证研究

发布时间：2025年02月03日

`Agent

理由：这篇论文主要探讨了LLM代理（Agent）在日常任务中的应用，特别是它们在规划和顺序决策方面的能力。论文通过实证研究分析了LLM代理在不同任务中的表现，并讨论了用户参与对信任和团队协作的影响。因此，这篇论文的核心内容与LLM代理的应用和设计相关，适合归类为Agent。` `日常助手` `人机协作`

> Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant

# 摘要

> # 摘要
自 ChatGPT 爆火以来，大型语言模型（LLMs）持续改变着我们的日常生活。配备了特定功能的外部工具（如航班预订或闹钟），LLM 代理在日常工作中帮助人类的能力不断增强。尽管 LLM 代理作为日常助手展现出了巨大潜力，但我们对它们如何基于规划和顺序决策能力提供日常帮助的理解仍然有限。我们从最近的研究中汲取灵感，这些研究强调了“LLM-modulo”设置与人类在循环中的结合在规划任务中的价值。我们进行了一项实证研究（N = 248），探讨了 LLM 代理在六种常见任务中作为日常助手的情况，这些任务通常具有不同的风险水平（如机票预订和信用卡支付）。为了确保用户对 LLM 代理的控制权，我们采用了“先计划后执行”的方式，代理在模拟环境中进行逐步规划和逐步执行。我们分析了用户在每一阶段的参与如何影响他们的信任和团队协作表现。研究结果表明，LLM 代理可能是一把双刃剑——（1）当有高质量的计划和必要的用户参与执行时，它们可以很好地工作；（2）用户可能会轻易地对看似合理的计划产生不信任。我们总结了使用 LLM 代理作为日常助手的关键见解，以校准用户信任并实现更好的整体任务结果。我们的工作对未来日常助手的设计以及人类与 LLM 代理的协作具有重要影响。

> Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of 'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.

[Arxiv](https://arxiv.org/abs/2502.01390)