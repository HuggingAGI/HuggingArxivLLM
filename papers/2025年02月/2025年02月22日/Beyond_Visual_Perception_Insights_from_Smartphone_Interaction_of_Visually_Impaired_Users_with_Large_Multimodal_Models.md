# 探索视觉感知之外：大型多模态模型与视障用户的智能手机交互洞察

发布时间：2025年02月22日

`其他

理由：这篇论文探讨了大型多模态模型在帮助视障人士方面的应用，属于多模态模型的应用领域，而不仅仅是专注于大型语言模型（LLM）的应用或理论。因此，它更符合“其他”分类。` `无障碍技术` `人机交互`

> Beyond Visual Perception: Insights from Smartphone Interaction of Visually Impaired Users with Large Multimodal Models

# 摘要

> 大型多模态模型 (LMMs) 开启了全新的 AI 驱动应用，帮助视障人士通过听觉文本理解周围环境。我们研究了这一视觉辅助新范式如何重塑视障人士的日常生活。超越简单的可用性评估，我们深入探讨了 LMM 工具在个人和社会环境中的潜力与局限，并为未来设计提供了重要启示。通过与 14 位 Be My AI 应用用户（基于 LMM 的应用）的访谈，以及对其图像描述的分析（来自研究参与者和社会媒体平台），我们发现了两个主要问题。首先，这些系统在识别社交环境、风格和人类身份时存在幻觉和误解。其次，它们往往难以准确捕捉和响应用户的意图。基于这些发现，我们提出了优化人机和 AI 交互的设计策略，为开发更高效、互动和个性化的辅助技术铺平了道路。

> Large multimodal models (LMMs) have enabled new AI-powered applications that help people with visual impairments (PVI) receive natural language descriptions of their surroundings through audible text. We investigated how this emerging paradigm of visual assistance transforms how PVI perform and manage their daily tasks. Moving beyond usability assessments, we examined both the capabilities and limitations of LMM-based tools in personal and social contexts, while exploring design implications for their future development. Through interviews with 14 visually impaired users of Be My AI (an LMM-based application) and analysis of its image descriptions from both study participants and social media platforms, we identified two key limitations. First, these systems' context awareness suffers from hallucinations and misinterpretations of social contexts, styles, and human identities. Second, their intent-oriented capabilities often fail to grasp and act on users' intentions. Based on these findings, we propose design strategies for improving both human-AI and AI-AI interactions, contributing to the development of more effective, interactive, and personalized assistive technologies.

[Arxiv](https://arxiv.org/abs/2502.16098)