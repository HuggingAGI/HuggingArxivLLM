# # 多模态基础模型机制可解释性研究综述

发布时间：2025年02月22日

`LLM理论` `机器学习` `可解释性`

> A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models

# 摘要

> 基础模型的崛起彻底改变了机器学习研究的格局，促使研究者们致力于揭示其内在机制，并开发出更高效、更可靠的模型，以实现更好的控制。尽管在解读大型语言模型（LLMs）方面取得了显著进展，但多模态基础模型（MMFMs）——如对比视觉-语言模型、生成式视觉-语言模型和文本到图像模型——在单模态框架之外带来了独特的可解释性挑战。尽管已有初步研究，但LLMs和MMFMs在可解释性方面仍存在显著差距。本研究探讨了两个关键问题：（1）如何将LLMs的可解释性方法适配到多模态模型中；（2）理解单模态语言模型与跨模态系统之间的机制差异。通过对现有MMFM分析技术的系统性回顾，我们提出了一套结构化的可解释性方法分类法，对比了单模态和多模态架构中的见解，并指出了关键的研究空白。

> The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs) - such as contrastive vision-language models, generative vision-language models, and text-to-image models - pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and crossmodal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.

[Arxiv](https://arxiv.org/abs/2502.17516)