# # 解释
深度语言模型揭示大脑中的语言表征机制。

发布时间：2025年02月20日

`LLM理论` `人工智能` `神经科学`

> Explanations of Deep Language Models Explain Language Representations in the Brain

# 摘要

> 人工智能领域的突破催生了大型语言模型（LLMs），这些模型不仅展现出类人水平的表现，还与大脑的语言处理机制共享计算原则。虽然以往研究主要关注于使LLMs的内部表征与神经活动保持一致，但我们提出了一种创新方法，通过可解释人工智能（XAI）技术在人工智能与神经科学领域之间建立更深层次的联系。我们采用归因方法量化了前文单词对LLM下一个单词预测的影响，并利用这些解释来预测参与者在听同一叙述时的fMRI记录。研究结果表明，归因方法能够稳健地预测语言网络中的大脑活动，其效果超越了传统内部表征在早期语言区域的表现。这种一致性呈现分层特征：早期层的解释对应于大脑语言处理的初始阶段，而后期层则与更高级阶段对齐。值得注意的是，对LLM下一个单词预测影响更大的层——即具有更高归因分数的层——表现出与神经活动更强的对齐。这项研究在人工智能与神经科学之间架起了一座双向桥梁。首先，归因方法为探究语言理解的神经机制提供了一种强大的工具，揭示了意义如何从前文上下文中产生。其次，我们将脑对齐作为一种评估归因方法有效性的指标，为评估其生物合理性提供了框架。

> Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction$\unicode{x2014}$those with higher attribution scores$\unicode{x2014}$exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility.

[Arxiv](https://arxiv.org/abs/2502.14671)