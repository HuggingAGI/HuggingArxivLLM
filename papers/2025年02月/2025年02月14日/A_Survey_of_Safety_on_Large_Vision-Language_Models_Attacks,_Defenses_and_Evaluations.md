# 大型视觉-语言模型的安全性研究综述：攻击、防御与评估

发布时间：2025年02月14日

`LLM理论` `人工智能`

> A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations

# 摘要

> 大型视觉-语言模型（LVLMs）的快速发展使得安全性保障成为研究中的重要课题。本研究综述全面分析了LVLM的安全性，涵盖攻击、防御和评估方法等关键领域。我们提出一个统一框架，整合相关组件，全面解析 LVLM 的潜在漏洞及应对策略。通过对 LVLM 生命周期的分析，我们构建了一个分类框架，区分推理与训练阶段，并进一步细分以深入探讨。此外，我们还指出了现有研究的局限性，并提出了未来研究方向，旨在增强 LVLM 的健壮性。在研究中，我们对最新 LVLM 模型 Deepseek Janus-Pro 进行了安全性评估，并对结果进行了理论分析。我们的发现为提升 LVLM 安全性提供了战略建议，确保其在高风险、现实场景中的安全可靠应用。本文综述旨在成为未来研究的基石，推动开发既突破多模态智能边界，又符合最高安全和伦理标准的模型。此外，为支持该领域的研究发展，我们创建了一个公开存储库，持续更新 LVLM 安全性相关最新成果：https://github.com/XuankunRong/Awesome-LVLM-Safety。

> With the rapid advancement of Large Vision-Language Models (LVLMs), ensuring their safety has emerged as a crucial area of research. This survey provides a comprehensive analysis of LVLM safety, covering key aspects such as attacks, defenses, and evaluation methods. We introduce a unified framework that integrates these interrelated components, offering a holistic perspective on the vulnerabilities of LVLMs and the corresponding mitigation strategies. Through an analysis of the LVLM lifecycle, we introduce a classification framework that distinguishes between inference and training phases, with further subcategories to provide deeper insights. Furthermore, we highlight limitations in existing research and outline future directions aimed at strengthening the robustness of LVLMs. As part of our research, we conduct a set of safety evaluations on the latest LVLM, Deepseek Janus-Pro, and provide a theoretical analysis of the results. Our findings provide strategic recommendations for advancing LVLM safety and ensuring their secure and reliable deployment in high-stakes, real-world applications. This survey aims to serve as a cornerstone for future research, facilitating the development of models that not only push the boundaries of multimodal intelligence but also adhere to the highest standards of security and ethical integrity. Furthermore, to aid the growing research in this field, we have created a public repository to continuously compile and update the latest work on LVLM safety: https://github.com/XuankunRong/Awesome-LVLM-Safety .

[Arxiv](https://arxiv.org/abs/2502.14881)