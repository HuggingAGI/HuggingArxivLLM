# 检索增强型系统或成医疗沟通的危险工具

发布时间：2025年02月17日

`LLM应用` `健康科技`

> Retrieval-augmented systems can be dangerous medical communicators

# 摘要

> 长期以来，患者一直通过网络获取健康信息，如今他们越来越多地借助生成式AI解答健康问题。鉴于医疗领域的高风险性，检索增强生成和引用 grounding 等技术被广泛推广为减少幻觉并提高AI生成回答准确性的方法，并已被搜索引擎广泛采用。然而，本文指出，即便这些方法能够生成源自原始文档且无幻觉的字面准确内容，它们仍可能极具误导性。患者从AI生成结果中得出的理解可能与直接阅读原文甚至咨询专业医生大相径庭。通过针对有争议的诊断和程序安全等主题的大规模查询分析，我们以定量和定性证据支持论点：当前系统导致的次优回答普遍存在。特别地，我们揭示了这些模型如何倾向于去语境化事实、省略关键相关来源并强化患者误解或偏见。为此，我们提出了一系列建议——例如整合沟通语用学和增强对原始文档的理解——这些方法不仅可能缓解这些问题，更可将影响扩展至医疗领域之外。

> Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain.

[Arxiv](https://arxiv.org/abs/2502.14898)