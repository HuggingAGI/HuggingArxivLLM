# EssayJudge：一个多粒度基准测试，用于评估多模态大型语言模型的自动作文评分能力

发布时间：2025年02月17日

`LLM应用` `评估系统`

> EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models

# 摘要

> 自动评分系统（AES）在教育评估中发挥着关键作用，通过提供可扩展且一致的写作任务评估。然而，传统的AES系统面临三大挑战：(1) 对人工设计特征的依赖限制了其泛化能力，(2) 难以捕捉连贯性和论证等细微特征，(3) 无法处理多模态语境。在多模态大型语言模型（MLLMs）时代，我们提出了EssayJudge——首个跨词汇、句子和语篇级别特征的AES能力评估多模态基准。通过利用MLLMs在特定特征评分和多模态语境理解方面的优势，EssayJudge致力于提供精准且语境丰富的评估，无需人工特征工程，从而克服AES长期以来的局限性。我们的实验结果表明，18个代表性MLLMs在AES性能上与人工评估存在差距，特别是在语篇级别特征方面，凸显了进一步推进基于MLLM的AES研究的必要性。我们的数据集和代码将在接受后公开。

> Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.

[Arxiv](https://arxiv.org/abs/2502.11916)