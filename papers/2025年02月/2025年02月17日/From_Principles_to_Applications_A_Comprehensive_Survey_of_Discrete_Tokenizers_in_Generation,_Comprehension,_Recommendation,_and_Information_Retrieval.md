# 从原理到应用：生成、理解、推荐与信息检索中的离散分词器全面综述

发布时间：2025年02月17日

`其他` `电子商务` `个性化推荐`

> From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval

# 摘要

> 离散分词器已成为现代机器学习系统中的核心组件，尤其在自回归建模与大型语言模型（LLMs）中发挥着关键作用。它们作为连接原始数据与模型世界的桥梁，将多模态的无结构数据转化为离散标记，使LLMs能够高效执行各类任务。尽管如此，目前尚未有一份专门针对离散分词器的综合性综述，这正是本文的研究起点。我们从分词器的内部结构入手，深入探讨其工作原理，全面解析其设计思路。在此基础上，我们整合前沿技术，将其划分为多模态生成与理解任务，以及面向个性化推荐的语义标记两大类。同时，我们深入分析现有分词器的局限性，并展望未来研究方向。通过构建一个理解离散分词器的统一框架，本文旨在为研究者与从业者提供指导，助力解决开放问题，推动AI技术的进一步发展，最终助力更强大、更灵活的AI系统构建。

> Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.

[Arxiv](https://arxiv.org/abs/2502.12448)