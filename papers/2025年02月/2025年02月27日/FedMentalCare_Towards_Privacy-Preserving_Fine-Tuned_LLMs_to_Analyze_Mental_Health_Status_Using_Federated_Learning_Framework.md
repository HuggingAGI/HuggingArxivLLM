# # FedMentalCare：利用联邦学习框架，打造隐私保护的微调LLM，助力心理健康分析

发布时间：2025年02月27日

`LLM应用` `隐私保护`

> FedMentalCare: Towards Privacy-Preserving Fine-Tuned LLMs to Analyze Mental Health Status Using Federated Learning Framework

# 摘要

> 随着全球心理健康问题的日益普遍，AI驱动的聊天机器人和对话式智能体已成为支持心理健康的便捷工具。然而，在精神卫生护理应用中部署大型语言模型（LLMs）引发了重大的隐私问题，尤其是涉及HIPAA和GDPR等法规。在此项研究中，我们提出了FedMentalCare，一个结合联邦学习（FL）与低秩适配（LoRA）的隐私保护框架，用于微调LLMs以进行心理健康分析。我们探讨了在FL环境中，不同客户端数据量和模型架构（例如MobileBERT和MiniLM）对性能的影响。我们的框架展示了一种可扩展且注重隐私的解决方案，适用于现实世界中的精神卫生护理场景，有效应对数据安全和计算效率的挑战。

> With the increasing prevalence of mental health conditions worldwide, AI-powered chatbots and conversational agents have emerged as accessible tools to support mental health. However, deploying Large Language Models (LLMs) in mental healthcare applications raises significant privacy concerns, especially regarding regulations like HIPAA and GDPR. In this work, we propose FedMentalCare, a privacy-preserving framework that leverages Federated Learning (FL) combined with Low-Rank Adaptation (LoRA) to fine-tune LLMs for mental health analysis. We investigate the performance impact of varying client data volumes and model architectures (e.g., MobileBERT and MiniLM) in FL environments. Our framework demonstrates a scalable, privacy-aware approach for deploying LLMs in real-world mental healthcare scenarios, addressing data security and computational efficiency challenges.

[Arxiv](https://arxiv.org/abs/2503.05786)