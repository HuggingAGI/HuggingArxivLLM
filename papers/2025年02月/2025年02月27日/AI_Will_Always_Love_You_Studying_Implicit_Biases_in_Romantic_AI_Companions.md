# AI始终爱你：解析浪漫AI伴侣中的隐性偏见

发布时间：2025年02月27日

`LLM应用` `社会学` `人工智能`

> AI Will Always Love You: Studying Implicit Biases in Romantic AI Companions

# 摘要

> 尽管现有研究已经识别出生成模型中的显式偏见，包括职业性别偏见，但对性别刻板印象的细微差别以及用户与AI伴侣之间关系期望的研究仍然不足。与此同时，AI伴侣因其被用户视为朋友或有性别属性的浪漫伴侣而变得越来越受欢迎。本研究通过设计三个针对浪漫型、性别指定AI伴侣及其用户的实验，填补了这一研究空白，有效评估了不同规模的LLM中的隐式偏见。每个实验分别关注不同的维度：隐性联想、情绪反应以及阿谀奉承。本研究旨在通过新型评估指标，定量分析具有性别属性关系设定的AI伴侣模型对基线的反应，从而测量和比较不同伴侣系统中表现的偏见。研究结果引人注目：它们表明，为大型语言模型赋予性别化、关系型的人设设定显著改变了这些模型的回应方式，而且在某些情况下是以一种有偏见、刻板的方式改变的。

> While existing studies have recognised explicit biases in generative models, including occupational gender biases, the nuances of gender stereotypes and expectations of relationships between users and AI companions remain underexplored. In the meantime, AI companions have become increasingly popular as friends or gendered romantic partners to their users. This study bridges the gap by devising three experiments tailored for romantic, gender-assigned AI companions and their users, effectively evaluating implicit biases across various-sized LLMs. Each experiment looks at a different dimension: implicit associations, emotion responses, and sycophancy. This study aims to measure and compare biases manifested in different companion systems by quantitatively analysing persona-assigned model responses to a baseline through newly devised metrics. The results are noteworthy: they show that assigning gendered, relationship personas to Large Language Models significantly alters the responses of these models, and in certain situations in a biased, stereotypical way.

[Arxiv](https://arxiv.org/abs/2502.20231)