# 当持续学习邂逅多模态大型语言模型：综述

发布时间：2025年02月26日

`LLM理论` `人工智能` `多模态模型`

> When Continue Learning Meets Multimodal Large Language Model: A Survey

# 摘要

> 人工智能领域的突破性进展催生了多模态大型语言模型（MLLMs）。然而，如何高效地将这些预训练模型适应动态数据分布和多样化任务仍是一个挑战。针对特定任务对MLLMs进行微调时，模型往往会在原有知识领域内出现性能下降，这一现象被称为“灾难性遗忘”。尽管这一问题在持续学习（CL）领域已得到深入研究，但在MLLMs中仍带来了新的挑战。本文作为首篇针对MLLMs持续学习的综述，对这一领域的440篇研究论文进行了全面概述与分析。综述分为四个部分：第一部分聚焦MLLMs的最新研究进展，涵盖模型创新、基准测试及跨领域应用；第二部分对持续学习的最新研究进行了分类概述，包括非大型语言模型单模态持续学习（Non-LLM Unimodal CL）、非大型语言模型多模态持续学习（Non-LLM Multimodal CL）以及大型语言模型中的持续学习（CL in LLM）；第三部分深入分析了MLLMs持续学习研究的现状，包括基准评估、架构创新以及理论与实证研究的总结。最后一部分探讨了MLLMs持续学习的挑战与未来方向，旨在激发该领域的未来研究与开发。本文将持续学习在多模态大模型中的基础概念、理论见解、方法创新与实际应用相连接，全面阐述了该领域的研究进展与挑战，旨在启发研究人员并推动相关技术的发展。

> Recent advancements in Artificial Intelligence have led to the development of Multimodal Large Language Models (MLLMs). However, adapting these pre-trained models to dynamic data distributions and various tasks efficiently remains a challenge. Fine-tuning MLLMs for specific tasks often causes performance degradation in the model's prior knowledge domain, a problem known as 'Catastrophic Forgetting'. While this issue has been well-studied in the Continual Learning (CL) community, it presents new challenges for MLLMs. This review paper, the first of its kind in MLLM continual learning, presents an overview and analysis of 440 research papers in this area.The review is structured into four sections. First, it discusses the latest research on MLLMs, covering model innovations, benchmarks, and applications in various fields. Second, it categorizes and overviews the latest studies on continual learning, divided into three parts: non-large language models unimodal continual learning (Non-LLM Unimodal CL), non-large language models multimodal continual learning (Non-LLM Multimodal CL), and continual learning in large language models (CL in LLM). The third section provides a detailed analysis of the current state of MLLM continual learning research, including benchmark evaluations, architectural innovations, and a summary of theoretical and empirical studies.Finally, the paper discusses the challenges and future directions of continual learning in MLLMs, aiming to inspire future research and development in the field. This review connects the foundational concepts, theoretical insights, method innovations, and practical applications of continual learning for multimodal large models, providing a comprehensive understanding of the research progress and challenges in this field, aiming to inspire researchers in the field and promote the advancement of related technologies.

[Arxiv](https://arxiv.org/abs/2503.01887)