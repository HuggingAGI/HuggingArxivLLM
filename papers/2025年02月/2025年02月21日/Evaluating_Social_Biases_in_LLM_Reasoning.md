# # 评估大型语言模型推理中的社会偏见问题
我们对大型语言模型 (LLM) 推理过程中存在社会偏见的现象进行了深入评估。

发布时间：2025年02月21日

`LLM理论

理由：这篇论文探讨了大型语言模型在推理过程中出现的偏见问题，属于对模型内在特性的分析和理论研究，因此归类为LLM理论。` `计算机科学` `人工智能`

> Evaluating Social Biases in LLM Reasoning

# 摘要

> 在AI推理的最新进展中，大型语言模型（LLMs）能够自动生成链式推理步骤，在数学和编程任务上表现优异。然而，当推理过程中掺杂偏见，形成有力的逻辑论证时，可能带来更严重的负面影响，并加剧幻觉现象。本文评估了DeepSeek-R1的8B和32B版本与其指令微调版本在BBQ数据集上的表现，深入探讨了推理过程中被激发和放大的偏见。据我们所知，这是首个针对大型语言模型推理中偏见问题的实证研究。

> In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning.

[Arxiv](https://arxiv.org/abs/2502.15361)