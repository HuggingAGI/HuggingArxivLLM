# # PAPI: 借助内存计算系统，挖掘大型语言模型解码中的动态并行性

发布时间：2025年02月21日

`LLM应用

摘要讨论了如何优化大型语言模型（LLMs）的解码过程，提出了一种新的架构PAPI来动态调度计算和内存密集型内核，以提高解码效率。这属于LLM的应用优化，因此归类为LLM应用。` `计算机体系结构` `高性能计算`

> PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System

# 摘要

> 大型语言模型（LLMs）被广泛用于自然语言理解和文本生成。在生成输出标记时，LLM模型依赖一个耗时的步骤，即LLM解码。此前的一些研究工作致力于通过并行技术（如批处理和推测性解码）来提升LLM解码的性能。目前最先进的LLM解码同时涉及计算密集型和内存密集型的内核。一些先前的研究工作通过静态识别和映射这些不同类型的内核，将其分配到包含处理内存单元（PIM）和计算为中心的加速器的异构架构中。我们发现，由于参数变化以满足用户和/或系统需求，LLM解码内核的特性（例如，某个内核是否为内存密集型）可能会动态变化，这使得（1）静态映射内核到PIM单元和计算为中心的加速器变得次优，以及（2）由于内存密集型内核之间存在较大的异构性，设计PIM单元时采用“一刀切”的方法效率低下。

本文中，我们旨在加速LLM解码，同时考虑所涉及内核动态变化的特性。为此，我们提出了PAPI（并行解码与PIM），这是一种支持PIM的异构架构，能够动态调度计算密集型或内存密集型内核到适合的硬件单元。PAPI包含两个关键机制：（1）在线内核特性分析，以在运行时动态将内核调度到最适合的硬件单元；（2）支持PIM的异构计算系统，能够和谐地协调计算为中心的处理单元和不同计算能力的混合PIM单元。我们在三个广泛应用的LLMs上的实验结果表明，与最先进的异构LLM加速器和仅支持PIM的LLM加速器相比，PAPI分别实现了1.8×和11.1×的加速效果。


> Large language models (LLMs) are widely used for natural language understanding and text generation. An LLM model relies on a time-consuming step called LLM decoding to generate output tokens. Several prior works focus on improving the performance of LLM decoding using parallelism techniques, such as batching and speculative decoding. State-of-the-art LLM decoding has both compute-bound and memory-bound kernels. Some prior works statically identify and map these different kernels to a heterogeneous architecture consisting of both processing-in-memory (PIM) units and computation-centric accelerators. We observe that characteristics of LLM decoding kernels (e.g., whether or not a kernel is memory-bound) can change dynamically due to parameter changes to meet user and/or system demands, making (1) static kernel mapping to PIM units and computation-centric accelerators suboptimal, and (2) one-size-fits-all approach of designing PIM units inefficient due to a large degree of heterogeneity even in memory-bound kernels.
  In this paper, we aim to accelerate LLM decoding while considering the dynamically changing characteristics of the kernels involved. We propose PAPI (PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that exploits dynamic scheduling of compute-bound or memory-bound kernels to suitable hardware units. PAPI has two key mechanisms: (1) online kernel characterization to dynamically schedule kernels to the most suitable hardware units at runtime and (2) a PIM-enabled heterogeneous computing system that harmoniously orchestrates both computation-centric processing units and hybrid PIM units with different computing capabilities. Our experimental results on three broadly-used LLMs show that PAPI achieves 1.8$\times$ and 11.1$\times$ speedups over a state-of-the-art heterogeneous LLM accelerator and a state-of-the-art PIM-only LLM accelerator, respectively.

[Arxiv](https://arxiv.org/abs/2502.15470)