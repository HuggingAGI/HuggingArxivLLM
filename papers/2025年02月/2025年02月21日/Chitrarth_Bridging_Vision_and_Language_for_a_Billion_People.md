# Chitrarth：架起视觉与语言的桥梁，造福十亿人

发布时间：2025年02月21日

`LLM应用` `多语言` `视觉语言模型`

> Chitrarth: Bridging Vision and Language for a Billion People

# 摘要

> 近期的多模态基础模型主要基于英语或资源丰富的欧洲语言数据进行训练，这限制了其在其他中低资源语言中的应用。为了解决这一局限性，我们推出了Chitrarth（Chitra: 图像；Artha: 意义），一个包容性的视觉语言模型（VLM），专门针对印度10种主要语言的丰富语言多样性和视觉推理能力。我们的模型成功整合了最先进的（SOTA）多语言大型语言模型（LLM）与视觉模块，主要基于多语言图像-文本数据进行训练。此外，我们还推出了BharatBench，一个全面的框架，用于在各种印度语言中评估VLM，最终为构建更加多样和高效的AI系统做出贡献。我们的模型在低资源语言的基准测试中取得了SOTA结果，同时在英语中保持了高效性。通过我们的研究，我们旨在为多语言多模态能力设定新的基准，与现有模型相比实现显著改进，并为未来在这一领域的进展奠定基础。

> Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena.

[Arxiv](https://arxiv.org/abs/2502.15392)