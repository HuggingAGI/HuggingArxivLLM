# 探索具身多模态大模型：发展、数据集与未来方向

发布时间：2025年02月21日

`Agent` `人工智能` `机器人技术`

> Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions

# 摘要

> # 摘要
具身多模态大模型（EMLMs）因其在复杂真实环境中连接感知、认知与行动的潜力，近年来引发了广泛关注。本文综述了EMLMs的发展历程，包括大型语言模型（LLMs）、大型视觉模型（LVMs）及其他模型，并探讨了其他新兴架构。我们重点分析了EMLMs在具身感知、导航、交互与模拟方面的演进。此外，本文详细探讨了用于训练与评估这些模型的数据集，强调了多样化、高质量数据对有效学习的重要性。文章还指出了EMLMs面临的关键挑战，包括扩展性、泛化能力及实时决策等问题。最后，我们展望了未来发展方向，强调通过整合多模态感知、推理与行动来推动日益自主化系统的发展。通过深入剖析前沿方法并识别关键空白，本文旨在激发EMLMs及其跨领域应用的未来进步。

> Embodied multimodal large models (EMLMs) have gained significant attention in recent years due to their potential to bridge the gap between perception, cognition, and action in complex, real-world environments. This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models, while also examining other emerging architectures. We discuss the evolution of EMLMs, with a focus on embodied perception, navigation, interaction, and simulation. Furthermore, the review provides a detailed analysis of the datasets used for training and evaluating these models, highlighting the importance of diverse, high-quality data for effective learning. The paper also identifies key challenges faced by EMLMs, including issues of scalability, generalization, and real-time decision-making. Finally, we outline future directions, emphasizing the integration of multimodal sensing, reasoning, and action to advance the development of increasingly autonomous systems. By providing an in-depth analysis of state-of-the-art methods and identifying critical gaps, this paper aims to inspire future advancements in EMLMs and their applications across diverse domains.

[Arxiv](https://arxiv.org/abs/2502.15336)