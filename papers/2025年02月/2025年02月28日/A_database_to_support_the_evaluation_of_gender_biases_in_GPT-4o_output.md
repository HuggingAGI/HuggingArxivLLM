# 用于评估 GPT-4o 输出中性别偏见的数据库

发布时间：2025年02月28日

`LLM应用` `社会学` `伦理学`

> A database to support the evaluation of gender biases in GPT-4o output

# 摘要

> 大型语言模型（LLMs）的广泛应用为用户和社会带来了伦理风险。其中，LLMs通过性别偏见生成不公平语言输出，从而强化或加剧对弱势群体伤害的风险尤为突出（Weidinger et al., 2022; Bender et al., 2021; Kotek et al., 2023）。因此，评估LLMs输出的公平性成为研究热点。为了推动这一领域的研究进展，促进关于规范基础和评估方法的讨论，并提高研究的可重复性，我们提出了一种全新的数据库构建方法。这种方法不仅能够评估LLM生成语言中性别相关偏见的中立化程度，还能深入分析其偏见表现。

> The widespread application of Large Language Models (LLMs) involves ethical risks for users and societies. A prominent ethical risk of LLMs is the generation of unfair language output that reinforces or exacerbates harm for members of disadvantaged social groups through gender biases (Weidinger et al., 2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the fairness of LLM outputs with respect to such biases is a topic of rising interest. To advance research in this field, promote discourse on suitable normative bases and evaluation methodologies, and enhance the reproducibility of related studies, we propose a novel approach to database construction. This approach enables the assessment of gender-related biases in LLM-generated language beyond merely evaluating their degree of neutralization.

[Arxiv](https://arxiv.org/abs/2502.20898)