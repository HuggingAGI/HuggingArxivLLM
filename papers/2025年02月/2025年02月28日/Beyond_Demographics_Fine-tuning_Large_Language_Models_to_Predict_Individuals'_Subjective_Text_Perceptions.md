# 超越人口统计学：微调大型语言模型预测个体文本感知

发布时间：2025年02月28日

`LLM理论` `数据科学` `机器学习`

> Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals' Subjective Text Perceptions

# 摘要

> 人们在标注主观问题时往往表现出差异性，这种差异部分被认为与标注者的社会人口统计学特征相关。虽然大型语言模型（LLMs）被用于数据标注，但研究表明，模型在处理社会人口统计学属性时表现欠佳，显示出其内在的社会人口统计学知识有限。本研究探讨了是否能通过训练使LLMs准确反映标注者差异。基于一个包含五个任务的精选数据集，这些任务具有标准化的社会人口统计学特征，我们发现模型在社会人口统计学提示方面确实有所提升，但这种改进主要源于模型学习了标注者特定行为，而非社会人口统计学模式。研究结果表明，模型未能有效建立社会人口统计学与标注之间的有意义联系，这使得当前使用LLMs模拟社会人口统计学差异和行为的做法值得商榷。

> People naturally vary in their annotations for subjective questions and some of this variation is thought to be due to the person's sociodemographic characteristics. LLMs have also been used to label data, but recent work has shown that models perform poorly when prompted with sociodemographic attributes, suggesting limited inherent sociodemographic knowledge. Here, we ask whether LLMs can be trained to be accurate sociodemographic models of annotator variation. Using a curated dataset of five tasks with standardized sociodemographics, we show that models do improve in sociodemographic prompting when trained but that this performance gain is largely due to models learning annotator-specific behaviour rather than sociodemographic patterns. Across all tasks, our results suggest that models learn little meaningful connection between sociodemographics and annotation, raising doubts about the current use of LLMs for simulating sociodemographic variation and behaviour.

[Arxiv](https://arxiv.org/abs/2502.20897)