# HAIC: 利用更优描述提升多模态大型语言模型对人类行为的理解与生成能力

发布时间：2025年02月28日

`LLM应用` `视频理解` `人类动作识别`

> HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models

# 摘要

> 多模态大型语言模型（MLLMs）在视频理解领域取得了显著进展，但受限于高质量数据的缺乏，其在人类动作视频上的表现仍有待提升。为突破这一瓶颈，我们开发了一套两阶段数据标注流程。首先，我们设计策略从互联网收集具有清晰人类动作的视频。其次，采用标准化字幕格式对视频进行标注，利用人类属性区分个体，并详细记录动作与互动的时序信息。通过这一流程，我们构建了两个数据集：**HAICTrain**和**HAICBench**。**HAICTrain**包含126,000个由Gemini-Pro生成并经过验证的视频-字幕配对，专为训练设计。**HAICBench**则包含500个手动标注的视频-字幕配对和1,400个问答配对，用于全面评估人类动作理解能力。实验结果显示，基于HAICTrain的训练不仅显著提升了在4个基准测试中对人类动作的理解能力，还能优化文本到视频生成的效果。目前，这两个数据集已公开发布，访问地址为https://huggingface.co/datasets/KuaishouHAIC/HAIC。

> Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \textbf{HAICBench} includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.

[Arxiv](https://arxiv.org/abs/2502.20811)