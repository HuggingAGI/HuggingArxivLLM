# Evalita-LLM: 意大利语大型语言模型基准测试

发布时间：2025年02月04日

`LLM应用

解释：这篇论文介绍了Evalita-LLM，一个专门用于评估大型语言模型（LLMs）在意大利任务上表现的基准。该基准的设计和开发直接涉及到LLMs的应用，特别是如何评估和改进这些模型在特定语言任务上的表现。因此，这篇论文属于LLM应用类别。` `基准测试`

> Evalita-LLM: Benchmarking Large Language Models on Italian

# 摘要

> 我们推出了 Evalita-LLM，这是一个专为评估大型语言模型（LLMs）在意大利任务上的表现而设计的新基准。Evalita-LLM 的亮点在于：(i) 所有任务均为原生意大利语，避免了翻译带来的问题和文化偏见；(ii) 除了经典的多项选择任务，基准还引入了生成任务，使与 LLMs 的交互更加自然；(iii) 所有任务均通过多个提示进行评估，减少模型对特定提示的依赖，确保评估的公平性和客观性。我们采用了一种迭代方法，候选任务和提示会经过一组开发用 LLMs 的验证。我们展示了基准开发阶段的实验结果，并提供了多款顶尖 LLMs 的性能数据。

> We describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs.

[Arxiv](https://arxiv.org/abs/2502.02289)