# 超越英语：以中文为例，评估非英语话语中道德基础的自动测量

发布时间：2025年02月04日

`LLM应用

理由：该论文主要探讨了大型语言模型（LLMs）在跨语言道德基础（MFs）测量中的应用，特别是其在非英语文本中的表现。研究结果表明，LLMs在跨语言任务中表现出色，尤其是在数据效率方面。因此，该论文属于LLM应用类别，因为它关注的是LLMs在实际任务中的应用和效果。` `跨文化研究`

> Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study

# 摘要

> 本研究探索了在非英语语料库中测量道德基础（MFs）的计算方法。由于多数资源主要针对英语开发，道德基础理论的跨语言应用仍显不足。本文以中文为例，评估了将英语资源应用于机器翻译文本、本地语言词典、多语言语言模型和大型语言模型（LLMs）在非英语文本中测量MFs的效果。结果显示，机器翻译和本地词典方法在复杂道德评估中表现不足，常导致文化信息的大量丢失。相比之下，多语言模型和LLMs通过迁移学习展现了可靠的跨语言性能，其中LLMs在数据效率上尤为突出。重要的是，本研究还强调了自动化MF评估中人工验证的必要性，因为最先进的模型可能忽略跨语言测量中的文化细微差别。研究结果凸显了LLMs在跨语言MF测量及其他复杂多语言演绎编码任务中的潜力。

> This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.

[Arxiv](https://arxiv.org/abs/2502.02451)