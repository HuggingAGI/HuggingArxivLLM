# AIN：阿拉伯语包容性大型多模态模型

发布时间：2025年02月04日

`LLM应用

**理由**：这篇论文主要讨论了阿拉伯语多模态模型（LMM）的开发和应用，特别是AIN模型的推出及其在多领域中的表现。虽然涉及多模态模型，但其核心仍然是基于大型语言模型（LLMs）的应用，特别是在阿拉伯语和英语双语环境中的应用。因此，将其归类为LLM应用是合适的。` `多模态模型` `阿拉伯语`

> AIN: The Arabic INclusive Large Multimodal Model

# 摘要

> 随着大型语言模型（LLMs）的快速发展并演变为大型多模态模型（LMMs），英语和汉语等高资源语言取得了显著进展。尽管阿拉伯语LLMs取得了显著进展，但阿拉伯语LMMs仍未被充分探索，通常仅局限于语言和视觉理解的少数特定方面。为了填补这一空白，我们推出了AIN——阿拉伯包容性多模态模型，旨在在多个领域中表现出色。AIN是一个英语-阿拉伯语双语LMM，基于360万高质量阿拉伯语-英语多模态数据样本，展示了最先进的阿拉伯语性能，同时具备强大的英语视觉能力。在最近的CAMEL-Bench基准测试中，涵盖多图像理解、复杂视觉感知、手写文档理解、视频理解、医学成像、植物病害和基于遥感的地物理解等38个子领域，AIN表现出色，7B模型在八个领域和38个子领域的平均绝对增益比GPT-4o高出3.4%。AIN的卓越能力使其成为为阿拉伯语使用者提供先进多模态生成AI工具的重要一步，适用于多种应用。

> Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.

[Arxiv](https://arxiv.org/abs/2502.00094)