# 语言模型能否应对序列优化问题？从评估到黑格尔式增强

发布时间：2025年02月04日

`LLM应用

理由：这篇论文主要探讨了大型语言模型（LLMs）在序列优化问题（SOPs）中的应用，提出了一个动态框架（WorldGen）来评估LLM的性能，并提出了ACE方法来提升LLMs在SOP中的表现。这些内容属于LLM在实际问题中的应用和优化，因此归类为“LLM应用”。` `优化问题` `人工智能`

> Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement

# 摘要

> 大型语言模型（LLMs）在多个领域展现了卓越能力，为优化问题解决这一关键且复杂的领域带来了革命性机遇。本文研究了LLMs在序列优化问题（SOPs）中的表现。我们提出了WorldGen，一个动态框架，用于生成可控复杂性的SOPs，以评估LLM性能。初步结果显示，LLMs在简单SOPs上表现优异，但随着问题复杂度的提升，性能显著下降。为此，我们重新审视了推理的哲学假设，并受黑格尔辩证法启发，提出了ACE方法，展示了如何在不重新训练或微调的情况下，显著提升LLMs在SOP中的表现。

> Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning.

[Arxiv](https://arxiv.org/abs/2502.02573)