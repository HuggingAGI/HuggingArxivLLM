# EgoTextVQA：探索基于自我视角的场景文本感知视频问答系统

发布时间：2025年02月11日

`LLM应用` `问答系统`

> EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering

# 摘要

> 我们正式推出EgoTextVQA——一个全新且严格构建的基准测试，专注于第一人称视角下的场景文本问答辅助。该数据集包含1.5K第一人称视角视频和7K场景文本感知问题，这些问题反映了真实用户在户外驾驶和室内家务活动中的需求。这些问题经过精心设计，旨在激发模型在第一人称动态环境中对场景文本的识别与推理能力。通过EgoTextVQA，我们对10个顶尖的多模态大型语言模型进行了全面评估。结果显示，所有模型都面临严峻挑战，其中表现最佳的Gemini 1.5 Pro的准确率仅为33%，这凸显了现有技术在第一人称问答辅助任务中的显著局限性。我们的进一步研究表明，精准的时间定位、多帧推理能力，以及高分辨率和辅助场景文本输入，是提升模型性能的关键因素。通过详尽的分析和启发式建议，我们希望EgoTextVQA能够成为第一人称场景文本问答辅助研究的坚实试验平台。

> We introduce EgoTextVQA, a novel and rigorously constructed benchmark for egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K ego-view videos and 7K scene-text aware questions that reflect real-user needs in outdoor driving and indoor house-keeping activities. The questions are designed to elicit identification and reasoning on scene text in an egocentric and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10 prominent multimodal large language models. Currently, all models struggle, and the best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the severe deficiency of these techniques in egocentric QA assistance. Our further investigations suggest that precise temporal grounding and multi-frame reasoning, along with high resolution and auxiliary scene-text inputs, are key for better performance. With thorough analyses and heuristic suggestions, we hope EgoTextVQA can serve as a solid testbed for research in egocentric scene-text QA assistance.

[Arxiv](https://arxiv.org/abs/2502.07411)