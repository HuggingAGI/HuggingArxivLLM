# 跨越评估鸿沟：利用大型语言模型进行主题模型评估

发布时间：2025年02月11日

`LLM应用

摘要讨论了利用大型语言模型（LLMs）进行主题分类体系的自动化评估，属于LLM的应用场景，因此归类为LLM应用。` `信息科学` `数字图书馆`

> Bridging the Evaluation Gap: Leveraging Large Language Models for Topic Model Evaluation

# 摘要

> 本研究提出了一种基于大型语言模型（LLMs）的框架，用于对科学文献中动态演进的主题分类体系进行自动化评估。在数字图书馆系统中，主题建模在高效组织和检索学术内容方面发挥着关键作用，帮助研究人员在复杂的知识领域中找到方向。随着研究领域的不断扩展和变化，传统的以人工为中心的静态评估方法难以保持其相关性。本研究提出的方法利用LLMs来衡量关键质量维度，如连贯性、重复性、多样性和主题与文档的对齐度，而无需过度依赖专家标注者或狭窄的统计指标。定制的提示语引导LLM进行评估，确保在不同数据集和建模技术上的评估结果具有一致性和可解释性。在基准语料库上的实验验证了该方法的鲁棒性、可扩展性和适应性，突显了其作为传统评估策略更全面、动态的替代方案的价值。

> This study presents a framework for automated evaluation of dynamically evolving topic taxonomies in scientific literature using Large Language Models (LLMs). In digital library systems, topic modeling plays a crucial role in efficiently organizing and retrieving scholarly content, guiding researchers through complex knowledge landscapes. As research domains proliferate and shift, traditional human centric and static evaluation methods struggle to maintain relevance. The proposed approach harnesses LLMs to measure key quality dimensions, such as coherence, repetitiveness, diversity, and topic-document alignment, without heavy reliance on expert annotators or narrow statistical metrics. Tailored prompts guide LLM assessments, ensuring consistent and interpretable evaluations across various datasets and modeling techniques. Experiments on benchmark corpora demonstrate the method's robustness, scalability, and adaptability, underscoring its value as a more holistic and dynamic alternative to conventional evaluation strategies.

[Arxiv](https://arxiv.org/abs/2502.07352)