# AL-Bench: 自动日志记录的基准测试

发布时间：2025年02月05日

`LLM应用

理由：这篇论文讨论了基于语言模型的技术在自动生成日志语句中的应用，并提出了一个评估基准AL-Bench。虽然论文主要关注的是日志生成和评估，但其核心是利用语言模型技术来解决软件开发中的实际问题，因此可以归类为LLM应用。` `软件工程` `日志系统`

> AL-Bench: A Benchmark for Automatic Logging

# 摘要

> # 摘要
日志记录，即在源代码中插入日志语句，对提升软件可靠性至关重要。近年来，基于语言模型的技术被开发用于自动生成日志。尽管这些方法在孤立评估中表现不俗，但在面对低质量数据和基于代码相似性的评估时，其效果大打折扣。我们认为，一个全面的评估基准应包含：（1）高质量、多样化且大规模的数据集；（2）对插入日志语句的代码可编译性的评估；（3）面向运行时日志的评估方法。为此，本文推出了AL-Bench，一个专为自动日志工具设计的全面基准。AL-Bench包含从10个知名项目中收集的高质量、多样化数据集，并引入了一种新颖的动态评估方法。与现有日志论文的评估不同，AL-Bench不仅评估了插入日志语句的代码的可编译性，还评估了运行时生成的日志质量，从而更真实地反映日志技术的实际效果。AL-Bench揭示了当前最先进工具的显著不足：由这些工具生成的带有日志语句的代码，在20.1%-83.6%的情况下无法编译；即使表现最佳的工具，其生成的日志与真实日志的余弦相似度也仅为0.213。这些结果表明，自动日志工具的开发仍有巨大提升空间。

> Logging, the practice of inserting log statements into source code, is critical for improving software reliability. Recently, language model-based techniques have been developed to automate log generation based on input code. Although these methods demonstrate promising results in isolated evaluations, their effectiveness diminishes when applied to ad-hoc low-quality data and code similarity-based evaluation methods. We consider a comprehensive evaluation benchmark should include (1) a high-quality, diverse, and large-scale dataset, (2) an assessment of the compilability of the code with inserted log statements, and (3) a runtime log-oriented evaluation method. To this end, this paper introduces AL-Bench, a comprehensive benchmark designed specifically for automatic logging tools. AL-Bench includes a high-quality, diverse dataset collected from 10 widely recognized projects with varying logging requirements and introduces a novel dynamic evaluation approach. Different from the evaluation in existing logging papers, AL-Bench assesses both the compilability of the code with inserted log statements and the quality of the logs generated by them during runtime, which we believe can better reflect the effectiveness of logging techniques in practice. AL-Bench reveals significant limitations in the state-of-the-art tools. The codes with log statements generated by the state-of-the-art tools fail to compile in 20.1%-83.6% cases. In addition, even the best-performing tool did not achieve high similarity between the runtime logs produced by the generated log statements and the ground-truth log statements, demonstrating a 0.213 cosine similarity. The results reveal substantial opportunities to further enhance the development of automatic logging tools.

[Arxiv](https://arxiv.org/abs/2502.03160)