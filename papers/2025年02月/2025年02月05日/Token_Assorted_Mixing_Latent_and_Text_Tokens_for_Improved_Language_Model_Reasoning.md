# Token Assorted: 混合潜在与文本标记，提升语言模型推理能力

发布时间：2025年02月05日

`LLM理论

理由：这篇论文主要探讨了大型语言模型（LLMs）在推理和规划能力方面的改进，提出了一种混合表示法来优化推理过程。论文的核心在于对LLMs的理论改进，特别是通过引入潜在离散标记来缩短推理轨迹，并探索了新的训练方法。这些内容属于对LLMs的理论研究和优化，因此应归类为LLM理论。` `人工智能` `推理系统`

> Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning

# 摘要

> 大型语言模型（LLMs）在链式思维（CoT）数据训练下，展现出卓越的推理和规划能力，其思维过程通过文本标记清晰展现。然而，这种训练方式导致输入冗长，大量词语用于维持文本连贯性而非核心推理信息，且处理这些输入消耗大量计算资源。为此，我们提出了一种混合表示法，利用VQ-VAE生成的潜在离散标记部分抽象初始推理步骤，大幅缩短推理轨迹。我们探索了两种应用场景：1）从头训练模型解决钥匙寻找迷宫问题，2）在混合数据上微调LLMs，扩展词汇表以包含未见过的潜在标记，适用于逻辑和数学推理问题。为促进有效学习，我们引入了一种简单训练方法，随机混合潜在和文本标记，使模型能快速适应新潜在标记。我们的方法在多项基准测试中均优于基线方法。

> Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.

[Arxiv](https://arxiv.org/abs/2502.03275)