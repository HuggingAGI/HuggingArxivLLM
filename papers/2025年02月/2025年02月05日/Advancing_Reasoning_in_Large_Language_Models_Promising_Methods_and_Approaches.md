# 提升大型语言模型的推理能力：方法与途径展望

发布时间：2025年02月05日

`LLM理论

**理由**：这篇论文主要讨论了提升大型语言模型（LLMs）推理能力的前沿技术，包括提示策略、架构创新和学习范式。这些内容属于对LLMs的理论研究和改进方法的探讨，因此归类为LLM理论。` `人工智能`

> Advancing Reasoning in Large Language Models: Promising Methods and Approaches

# 摘要

> 大型语言模型（LLMs）在自然语言处理（NLP）任务中表现出色，但其推理能力仍面临重大挑战。尽管LLMs在流畅性和事实回忆上表现优异，但在复杂推理（如逻辑推理、数学问题解决、常识推理和多步推理）方面，往往难以达到人类水平。本文综述了提升LLMs推理能力的前沿技术，将其分为几类：提示策略（如思维链推理、自我一致性、思维树推理）、架构创新（如检索增强模型、模块化推理网络、神经符号集成）以及学习范式（如基于推理数据集的微调、强化学习、自监督推理目标）。此外，我们还探讨了评估LLMs推理能力的框架，并指出了开放挑战，如幻觉、鲁棒性和跨任务推理泛化。通过总结最新进展，本文旨在为未来研究和推理增强LLMs的实际应用提供方向性见解。

> Large Language Models (LLMs) have succeeded remarkably in various natural language processing (NLP) tasks, yet their reasoning capabilities remain a fundamental challenge. While LLMs exhibit impressive fluency and factual recall, their ability to perform complex reasoning-spanning logical deduction, mathematical problem-solving, commonsense inference, and multi-step reasoning-often falls short of human expectations. This survey provides a comprehensive review of emerging techniques enhancing reasoning in LLMs. We categorize existing methods into key approaches, including prompting strategies (e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought reasoning), architectural innovations (e.g., retrieval-augmented models, modular reasoning networks, and neuro-symbolic integration), and learning paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement learning, and self-supervised reasoning objectives). Additionally, we explore evaluation frameworks used to assess reasoning in LLMs and highlight open challenges, such as hallucinations, robustness, and reasoning generalization across diverse tasks. By synthesizing recent advancements, this survey aims to provide insights into promising directions for future research and practical applications of reasoning-augmented LLMs.

[Arxiv](https://arxiv.org/abs/2502.03671)