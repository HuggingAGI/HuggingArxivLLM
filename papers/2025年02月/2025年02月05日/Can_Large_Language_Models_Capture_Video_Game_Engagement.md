# 大型语言模型能否准确捕捉视频游戏的吸引力？

发布时间：2025年02月05日

`LLM应用` `情感分析`

> Can Large Language Models Capture Video Game Engagement?

# 摘要

> 开箱即用的预训练大型语言模型（LLMs）能否成功识别视频中的人类情感？为解答这一问题，我们首次系统评估了 popular LLMs 在通过文本和视频帧的多模态提示下，对视频进行标注并成功预测连续情感标注的能力。特别地，在本文中，我们测试了 LLMs 准确标记来自 GameVibe 语料库中 20 款第一人称射击游戏的 80 分钟注释视频片段中的游戏投入度变化的能力。我们进行了超过 2,400 次实验，以研究 LLM 架构、模型规模、输入模态、提示策略以及地面真实处理方法对投入度预测的影响。我们的研究结果表明，尽管 LLMs 在多个领域中展现出类人的性能，但总体而言，它们在捕捉人类提供的连续体验标注方面仍然表现欠佳。我们探讨了整体表现相对较差的一些根本原因，强调了 LLMs 超出预期的案例，并为通过 LLMs 进一步探索自动情感标注绘制了路线图。

> Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video? To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the GameVibe corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs.

[Arxiv](https://arxiv.org/abs/2502.04379)