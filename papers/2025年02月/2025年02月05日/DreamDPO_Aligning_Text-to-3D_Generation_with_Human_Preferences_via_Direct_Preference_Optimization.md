# # 摘要  
    最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年02月05日

`LLM应用` `影视制作`

> DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization

# 摘要

> 文本到3D生成技术能够从文本描述自动创建3D内容，为多个领域带来变革性潜力。然而，现有方法难以使生成内容与人类偏好保持一致，限制了其应用范围和灵活性。为解决这些问题，本文提出了一种基于优化的框架——DreamDPO，通过直接偏好优化将人类偏好整合到3D生成过程中。具体而言，DreamDPO首先构建配对示例，然后使用奖励或大型多模态模型比较其与人类偏好的一致性，最后通过偏好驱动的损失函数优化3D表示。通过利用配对比较来反映偏好，DreamDPO减少了对精确点式质量评估的依赖，同时通过偏好引导的优化实现了细粒度的可控性。实验表明，DreamDPO能够实现具有竞争力的结果，并与现有方法相比提供更高质量和更可控的3D内容。代码和模型将开源。

> Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.

[Arxiv](https://arxiv.org/abs/2502.04370)