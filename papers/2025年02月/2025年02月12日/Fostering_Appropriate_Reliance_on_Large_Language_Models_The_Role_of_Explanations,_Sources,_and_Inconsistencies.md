# # 如何培养对大型语言模型的合理信任——解释、来源与不一致的作用

发布时间：2025年02月12日

`LLM应用` `人机交互` `用户体验`

> Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies

# 摘要

> 大型语言模型 (LLMs) 可能会产生流畅且令人信服的错误回答，这增加了用户误将其视为正确答案的风险。缓解这种过度依赖是关键挑战。通过一项让参与者使用 LLM 增强型应用回答客观问题的思维 aloud 研究，我们识别出几个影响用户依赖的 LLM 回答特征：解释（支持答案的细节）、解释中的不一致以及来源。通过一项大规模、预注册、对照实验（N=308），我们分离并研究了这些特征对用户的依赖程度、准确性及其他指标的影响。我们发现，解释的存在会增加对正确和错误回答的依赖程度。然而，当提供来源或解释中存在不一致时，我们观察到用户对错误回答的依赖程度较低。我们讨论了这些发现对培养适当依赖 LLM 的意义。

> Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.

[Arxiv](https://arxiv.org/abs/2502.08554)