# # 摘要  
SARChat-Bench-2M: 用于 SAR 图像理解的多任务视觉-语言基准测试

发布时间：2025年02月12日

`LLM应用` `人工智能`

> SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation

# 摘要

> 在合成孔径雷达 (SAR) 遥感图像解释领域，尽管视觉语言模型 (VLMs) 在自然语言处理和图像理解方面取得了显著进展，但其在专业领域的应用仍因领域专业知识不足而受到限制。本文创新性地提出了首个大规模多模态对话数据集 SARChat-2M，该数据集包含约 200 万条高质量的图像-文本配对，涵盖了多种场景并提供了详细的目标标注。该数据集不仅支持视觉理解、目标检测等多种关键任务，还具有独特的创新点：本研究开发了一个面向 SAR 领域的视觉-语言数据集和基准，评估了 VLMs 在 SAR 图像解释方面的能力，为构建多模态遥感数据集提供了范式框架。通过在 16 种主流 VLM 上进行的实验，充分验证了该数据集的有效性，并成功建立了 SAR 领域首个多任务对话基准。该项目将在 https://github.com/JimmyMa99/SARChat 上发布，旨在推动 SAR 视觉语言模型的深入发展和广泛应用。

> In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.

[Arxiv](https://arxiv.org/abs/2502.08168)