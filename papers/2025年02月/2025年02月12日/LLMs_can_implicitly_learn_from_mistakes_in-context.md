# 大型语言模型（LLMs）可以通过上下文中的错误进行隐式学习。

发布时间：2025年02月12日

`LLM理论` `人工智能`

> LLMs can implicitly learn from mistakes in-context

# 摘要

> 从错误中学习是人类智能的基本特征。先前研究表明，大型语言模型（LLMs）在获得详尽解释说明答案错误原因或纠正方法时，能够从错误答案中学习。本研究探讨了在无解释的情况下，LLMs是否能从数学推理任务的错误中学习。我们发现，当上下文中仅展示错误与正确答案而无解释时，LLMs的表现反而更佳。这一方法在评估中显著优于链式思维提示，并且结果在不同规模和推理能力的LLMs上均一致。深入分析表明，同时呈现错误与正确答案的提示方式，比引入更多样化的问题-答案对更能提升性能和泛化能力。值得注意的是，仅观察过错误与正确答案的模型生成的新解释，其质量与借助示例解释生成的解释不相上下，均获高度评价。这表明LLMs确实具备上下文中的隐式学习能力。


> Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning.

[Arxiv](https://arxiv.org/abs/2502.08550)