# 以任意模态提问：多模态生成增强的综合性研究综述

发布时间：2025年02月12日

`RAG` `多模态` `RAG系统`

> Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation

# 摘要

> 大型语言模型（LLMs）面临着幻觉和知识过时的挑战，这主要源于其对静态训练数据的依赖。检索增强生成（RAG）通过引入外部动态信息，有效提升了生成内容的事实准确性和时效性。近年来，多模态学习的突破性进展催生了多模态RAG，它通过整合文本、图像、音频和视频等多种模态，显著提升了生成效果。然而，跨模态对齐与推理为多模态RAG带来了独特挑战，使其与传统的单模态RAG有所不同。本综述对多模态RAG系统进行了全面而系统的分析，涵盖了数据集、评估指标、基准测试、评估方法、技术路线，以及检索、融合、增强和生成等关键环节的创新。我们详细探讨了训练策略、鲁棒性提升方法和损失函数设计，同时深入分析了多模态RAG在不同场景下的应用潜力。此外，我们还总结了当前面临的开放性挑战，并展望了未来的研究方向，以推动这一快速发展的领域不断向前。本综述为构建更强大、更可靠的AI系统奠定了基础，这些系统能够充分利用多模态动态外部知识库。相关资源可在https://github.com/llm-lab-org/Multimodal-RAG-Survey获取。

> Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.

[Arxiv](https://arxiv.org/abs/2502.08826)