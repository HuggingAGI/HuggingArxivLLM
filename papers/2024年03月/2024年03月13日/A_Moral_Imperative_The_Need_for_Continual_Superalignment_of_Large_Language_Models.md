# 面对大型语言模型的发展，一项不容忽视的道德责任是实现其持续超对齐。这一“道德使命”强调了在模型迭代过程中不断校准其价值观与人类伦理规范的重要性。（注：由于原句较短，经过一步翻译后已经较为通顺，故步骤2在此基础上稍作润色以增强表达效果。）

发布时间：2024年03月13日

`LLM理论` `人工智能` `伦理学`

> A Moral Imperative: The Need for Continual Superalignment of Large Language Models

# 摘要

> 本文聚焦于AI系统，尤其是LLMs在实现终身超级对齐过程中遇到的难题。超级对齐这一概念旨在确保超智能AI遵循人类的价值观和目标，尽管前景诱人，但由于LLMs在理解和应对人类伦理观念及其全球情境动态变化上的固有限制，我们主张要实现超级对齐需对当前LLM架构进行根本性的调整。文章深入剖析了将不断演进的人类价值观融入LLMs的挑战，揭示了静态AI模型与人类社会动态变迁间的矛盾。为了生动展现这些挑战，我们选取了两个典型案例——一个揭示了人类价值观的质变，另一个则展示了其量变的一面。通过这两个实例，我们揭示了受训数据制约的LLMs难以与当下的人类价值观和社会情景保持同步的问题。论文结尾部分探讨了可能缓解和应对这一对齐问题的潜在策略，为构建更具适应性和反应能力的AI系统铺就了一条探索之路。

> This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illustrate how LLMs, constrained by their training data, fail to align with contemporary human values and scenarios. The paper concludes by exploring potential strategies to address and possibly mitigate these alignment discrepancies, suggesting a path forward in the pursuit of more adaptable and responsive AI systems.

[Arxiv](https://arxiv.org/abs/2403.14683)