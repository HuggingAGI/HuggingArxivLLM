# 运用大型语言模型（LLMs）对软件的脆弱性及功能进行全面评估

发布时间：2024年03月13日

`LLM应用` `软件开发` `代码审查`

> Software Vulnerability and Functionality Assessment using LLMs

# 摘要

> 虽然代码审查是软件开发的核心环节，但实际操作却往往耗时费力。本文探究大型语言模型（LLMs）能否助力这一过程，并集中分析了优质审查所不可或缺的两大任务——识别带有安全漏洞的代码及执行软件功能验证（确保代码实现预期功能）。通过零样本学习和链式思维引导的方式，我们对模型在这两项任务上的表现进行了评估，使用的数据集包括标志性的代码生成数据集 HumanEval 和 MBPP，以及包含了专家编写的含 CWE 安全漏洞代码片段。实验涵盖了 OpenAI 提供的三种专有大模型及部分小型开源 LLMs，结果显示前者的性能显著优于后者。鉴于初期研究展现出的潜力，我们进一步让模型详尽阐述安全漏洞信息，实验结果显示，约有 36.7% 的 LLM 输出描述能够准确关联到真实的 CWE 漏洞类别。

> While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.

[Arxiv](https://arxiv.org/abs/2403.08429)