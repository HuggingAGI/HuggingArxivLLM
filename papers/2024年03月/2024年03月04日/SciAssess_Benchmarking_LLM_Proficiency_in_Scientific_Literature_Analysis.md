# SciAssess 是一个针对大型语言模型（LLM）在科学文献分析能力上的基准测试工具，旨在衡量和评估 LLM 在理解和解析科学文献方面的专业水准。

发布时间：2024年03月04日

`LLM应用`

> SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis

# 摘要

> 近年来LLMs的显著进展革新了自然语言处理领域，尤其引发了对应用于科研文献深度分析技术的广泛关注。然而，现存基准尚无法准确衡量LLMs在科研领域，尤其是处理复杂理解和多模态数据场景的专业表现。为此，我们推出了SciAssess这一针对科研文献深度分析定制的基准测试平台，致力于全面检验LLMs在此领域的效能。SciAssess聚焦于测评LLMs在科学情境中的记忆、理解与分析技能，涵盖了诸如基础化学、有机材料及合金材料等多个学科的典型任务。严谨的质量把控机制确保了其在准确性、匿名性和版权合规等方面的高度可靠。SciAssess对GPT-4、GPT-3.5-turbo等前沿LLMs进行评估，揭示其优势与待改进之处，有力推动LLMs在科研文献分析应用的深化发展。您可在https://sci-assess.github.io访问SciAssess及其配套资源，借助这一实用工具助力LLMs在科研文献分析领域能力的提升。

> Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copyright compliance. SciAssess evaluates leading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying their strengths and areas for improvement and supporting the ongoing development of LLM applications in scientific literature analysis. SciAssess and its resources are made available at https://sci-assess.github.io, offering a valuable tool for advancing LLM capabilities in scientific literature analysis.

[Arxiv](https://arxiv.org/abs/2403.01976)