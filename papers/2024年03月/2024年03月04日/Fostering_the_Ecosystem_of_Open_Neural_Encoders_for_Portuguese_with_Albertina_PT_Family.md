# Albertina PT* 系列旨在促进葡萄牙语开放神经编码器生态系统的构建与发展

发布时间：2024年03月04日

`LLM应用`

> Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family

# 摘要

> 为了强化葡萄牙语的神经网络表示，本论文贡献了一组奠基性的编码模型，它们扩充了当前稀少却专注于葡萄牙语的大规模语言模型生态系统，并且全然开源、免费提供给各类用户（涵盖学术研究和商业使用）。鉴于葡萄牙语等非英语语言在关键语言资源上的匮乏现状（现有的仅有9亿参数的Albertina和3.35亿参数的Bertimbau），我们以这两者为基础，推出了面向葡萄牙语的前沿开源编码器生态系统扩展计划，包含了一个追求顶级性能、拥有15亿参数的大模型，以及一个注重高效性、具备1亿参数的小型模型。在达成这一核心目标的过程中，我们还意外收获了一些对这个生态系统同样重要的附加成果，比如基于SuperGLUE基准创建并公开发布的葡萄牙语文本新数据集。

> To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are relevant for this ecosystem were obtained as well, namely new datasets for Portuguese based on the SuperGLUE benchmark, which we also distribute openly.

[Arxiv](https://arxiv.org/abs/2403.01897)