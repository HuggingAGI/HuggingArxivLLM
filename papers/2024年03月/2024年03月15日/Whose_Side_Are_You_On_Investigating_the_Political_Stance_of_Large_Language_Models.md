# 探究大型语言模型所持的政治倾向：它们究竟“站”在哪一方？

发布时间：2024年03月15日

`LLM应用`

> Whose Side Are You On? Investigating the Political Stance of Large Language Models

# 摘要

> LLMs因在各类日常任务中的出色表现而备受瞩目，但随着其广泛应用的不断加速，确保它们产出不含政治偏向的回答至关重要，以遏制信息孤岛、保证公正性和减轻确认偏误。本文提出了一个用于系统探究LLMs政治倾向的量化框架与方法。我们对LLMs在堕胎至LGBTQ权益等八大争议性话题上的政治立场进行了深入分析，发现当用户查询中包含职业、种族或政治背景等信息时，LLMs所给出的回答往往更贴近自由派或左倾观点，而非保守派或右倾观点。这项研究不仅再次证实了LLMs具有左倾特性，还揭示出特定属性如职业等，在即使刻意引导其趋向保守时也易受到此类倾向的影响。因此，为避免模型产生政治化回复，建议用户在构建查询时要审慎考虑，并注意选择中立的提示语言。

> Large Language Models (LLMs) have gained significant popularity for their application in various everyday tasks such as text generation, summarization, and information retrieval. As the widespread adoption of LLMs continues to surge, it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias. In this paper, we propose a quantitative framework and pipeline designed to systematically investigate the political orientation of LLMs. Our investigation delves into the political alignment of LLMs across a spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues. Across topics, the results indicate that LLMs exhibit a tendency to provide responses that closely align with liberal or left-leaning perspectives rather than conservative or right-leaning ones when user queries include details pertaining to occupation, race, or political affiliation. The findings presented in this study not only reaffirm earlier observations regarding the left-leaning characteristics of LLMs but also surface particular attributes, such as occupation, that are particularly susceptible to such inclinations even when directly steered towards conservatism. As a recommendation to avoid these models providing politicised responses, users should be mindful when crafting queries, and exercise caution in selecting neutral prompt language.

[Arxiv](https://arxiv.org/abs/2403.13840)