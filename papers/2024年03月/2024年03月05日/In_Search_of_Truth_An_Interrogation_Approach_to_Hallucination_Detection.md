# 为揭示真相，我们提出了一种通过质询法来探测幻觉的新途径。

发布时间：2024年03月05日

`LLM应用`

> In Search of Truth: An Interrogation Approach to Hallucination Detection

# 摘要

> 尽管LLMs日新月异、成就斐然，但因其容易产生“幻觉”——即给出看似真实实则偏离事实的回答，使其在实际生活中的广泛应用受限。本文创新性地提出一种针对LLMs幻觉检测的方法，为解决该问题及推广LLMs的实际应用提供了可能。我们深入研究了多种最新的LLMs，如Llama-2，在多个数据集上的幻觉程度，并通过全面评估证明了此方法能有效自动化识别幻觉。尤其值得一提的是，在一项特殊实验中，Llama-2模型出现了高达62%的幻觉情况，而我们的方法无需借助外部知识就达到了87%的平衡准确率（B-ACC）。

> Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying on external knowledge.

[Arxiv](https://arxiv.org/abs/2403.02889)