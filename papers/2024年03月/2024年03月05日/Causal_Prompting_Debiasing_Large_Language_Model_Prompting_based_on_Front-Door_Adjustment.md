# 因果提示法：运用前门调整策略校正大型语言模型的提示偏差

发布时间：2024年03月05日

`LLM应用`

> Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment

# 摘要

> 虽然诸如上下文学习和思维链等现有提示技术在LLMs领域取得显著成果，但在应对各类偏见难题上依然存在挑战。传统去偏方法多聚焦于模型训练过程，采用数据增强或重权分配策略，然而对于LLMs复杂偏见的处理能力有限。为此，研究人员借助结构因果模型探究了此类提示方法背后的因果机制，并创新性地提出了基于前门调整的因果提示方法，有效抑制LLMs中的偏见问题。该方法巧妙之处在于，无需触及LLMs的参数和逻辑值，通过精心设计提示，利用LLMs生成的思维链作为中介变量，并运用前门调整计算输入提示与输出答案间的因果影响，从而达到减轻模型偏见的目的。为进一步提升样本表征精度及准确估计因果效应，还引入对比学习对样本编码器进行微调，确保其与LLM的空间一致性。实验证明，所提出的因果提示方法在开源和闭源LLMs平台上的三个自然语言处理数据集上均表现出色。

> Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to obtain the representation of the samples precisely and estimate the causal effect more accurately, contrastive learning is used to fine-tune the encoder of the samples by aligning the space of the encoder with the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source LLMs.

[Arxiv](https://arxiv.org/abs/2403.02738)