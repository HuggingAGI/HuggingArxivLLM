# 内存学习：一种针对大型语言模型设计的声明式学习方案，它提供了一种新颖高效的方法来训练和优化大规模模型。

发布时间：2024年03月05日

`Agent`

> In-Memory Learning: A Declarative Learning Framework for Large Language Models

# 摘要

> 探究代理如何在无需借助人类标注数据的情况下与环境有效对齐，是一个颇具吸引力的研究领域。借鉴生物智能中声明性记忆在整合历史经验中发挥核心作用的现象，我们创新性地构建了一种新的学习框架。在这个框架下，智能体能够巧妙地从过去的经验中萃取智慧，不断优化和更新内在知识结构，从而提高其在环境中适应和表现的能力。整个学习过程完全发生在内存组件内部，并以自然语言形式实施，因此我们将其命名为“内存内学习”。此外，我们还深入剖析了一系列旨在衡量自我完善能力的基准测试的核心特征。通过一系列严谨的实验，我们验证了该框架的有效性，并为解决这一问题提供了富有洞见的观点。

> The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.

[Arxiv](https://arxiv.org/abs/2403.02757)