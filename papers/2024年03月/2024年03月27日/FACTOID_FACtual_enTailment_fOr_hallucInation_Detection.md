# 幻觉识别的实证依据——事实蕴含法（FACTOID: FACE）

发布时间：2024年03月27日

`RAG` `信息检索` `语言模型`

> FACTOID: FACtual enTailment fOr hallucInation Detection

# 摘要

> 随着大型语言模型（LLMs）的普及，我们享受到了诸多便利。但幻觉问题也随之而来，引起了广泛关注。为此，检索增强生成（RAG）技术应运而生，通过将LLMs的输出与事实信息相锚定，以提升其输出质量。RAG利用文本蕴含（TE）等方法，检验LLMs生成的文本与检索到的文档相比，是否得到支持或存在矛盾。本文指出，传统的TE方法在识别LLMs生成内容中的幻觉方面力有不逮。以“美国对乌克兰战争立场”为例，AI生成的文本提到“美国总统奥巴马表示，美国不会向乌克兰派兵...”，然而实际上，战争期间的美国总统是拜登，这明显与事实不符。现有的TE系统也无法精确标注文本并找出矛盾之处。为解决这一问题，我们引入了一种新型文本蕴含——“事实蕴含（FE）”，旨在发现LLMs生成内容中的事实错误，并准确指出与现实相悖的文本部分。我们推出了FACTOID基准数据集，专门用于FE，并提出了一个融合了当前顶尖长文本嵌入技术的多任务学习框架，如e5-mistral-7b-instruct、GPT-3、SpanBERT和RoFormer。这一框架在FACTOID基准测试中，相较于传统TE方法，准确率提升了40%。通过FE的自动幻觉检测功能，我们对15种现代LLMs进行了评估，并依据我们设计的自动幻觉易感性指数（HVI_auto）进行了排名。这一指数为评估和比较LLMs的幻觉倾向提供了量化标准。

> The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted. To address this, we introduces a new type of TE called ``Factual Entailment (FE).'', aims to detect factual inaccuracies in content generated by LLMs while also highlighting the specific text segment that contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL) framework for FE, incorporating state-of-the-art (SoTA) long text embeddings such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The proposed MTL architecture for FE achieves an avg. 40\% improvement in accuracy on the FACTOID benchmark compared to SoTA TE methods. As FE automatically detects hallucinations, we assessed 15 modern LLMs and ranked them using our proposed Auto Hallucination Vulnerability Index (HVI_auto). This index quantifies and offers a comparative scale to evaluate and rank LLMs according to their hallucinations.

[Arxiv](https://arxiv.org/abs/2403.19113)