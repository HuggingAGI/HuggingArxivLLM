# Gemini 1.5 开启新篇章，跨过数百万个上下文标记，释放强大的多模态理解潜力

发布时间：2024年03月08日

`Agent`

> Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

# 摘要

> 本报告推出了最新的双子座系列模型Gemini 1.5 Pro，这款强大的多模态混合专家模型能高效处理大量上下文信息，包括各类长文档、长时间的音视频数据，精准地记忆和推断出细微细节。Gemini 1.5 Pro在跨模态长上下文检索任务上展现近乎完美的表现，刷新了长文档问答、长视频问答和长文本语音识别等多项任务的记录，并且在众多基准测试中媲美甚至超越了Gemini 1.0 Ultra的顶级性能。探究其对长上下文理解的极限，我们发现Gemini 1.5 Pro在下一个令牌预测方面的不断提升，以及在高达至少一千万个令牌的范围内保持超过99%的近乎完美检索能力，这一成果远超诸如Claude 2.1（20万令牌）和GPT-4 Turbo（12.8万令牌）等已有的模型。此外，我们还揭示了大型语言模型在前沿领域所展现出的惊人新技能：当给予一份全球使用者不过200人的Kalamang语语法手册时，Gemini 1.5 Pro竟能如同通过同样内容学习的人一般，以相近水平完成从英语到Kalamang语的翻译任务。

> In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.

[Arxiv](https://arxiv.org/abs/2403.05530)