# 本文对保护LLMs数据隐私的策略进行了一次全面的调查，旨在深入探讨和总结当前针对大型语言模型的数据安全防护措施。

发布时间：2024年03月08日

`LLM理论`

> On Protecting the Data Privacy of Large Language Models (LLMs): A Survey

# 摘要

> LLMs作为强大的人工智能系统，能深入理解和创造人类语言，并在处理海量文本数据中习得语言规律，执行写作、对话、摘要等多种任务。然而，在大规模数据的处理与生成过程中，LLMs可能存在泄露敏感信息的风险，对数据隐私构成威胁。本文致力于揭示LLMs所涉及的数据隐私问题，进行全面剖析。我们深入探究了涵盖被动隐私泄漏与主动隐私攻击在内的LLMs内数据隐私威胁的全貌，接着对LLMs在各环节采用的隐私保护策略进行了评估，细致分析其效能及局限。最后，我们探讨了当前面临的主要挑战，并勾勒出LLM隐私保护领域未来的发展路径。

> Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints. Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection.

[Arxiv](https://arxiv.org/abs/2403.05156)