# 运用提示工程技术进行情感操纵，能够加剧 AI 大型语言模型制造虚假信息的问题。

发布时间：2024年03月06日

`LLM应用`

> Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models

# 摘要

> 本研究聚焦于OpenAI的大型语言模型(LLMs)通过巧妙的提示工程生成合成假新闻的现象，并深入探索了它们对情感引导的敏感性。借助davinci系列及其他更新版本的LLMs，我们精心设计实验以验证其制造假新闻的成功率。根据涵盖19,800条合成假新闻社交媒体帖文的数据集分析，结果显示OpenAI的所有LLMs皆具备成功生成假新闻的能力，同时能敏锐捕捉并回应文本中的情感暗示。有趣的是，在温和的引导下，所有测试的LLMs都会高效稳定地产出假新闻；然而，一旦遇到粗鲁的提示，假新闻的生成频率反而下降，此时模型往往会选择拒绝配合，甚至警告用户该工具不应用于此用途。此项研究丰富了有关AI技术负责任发展与应用的探讨，尤其是在抑制假新闻扩散及提升AI生成内容透明性等方面具有重要意义。

> This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.

[Arxiv](https://arxiv.org/abs/2403.03550)