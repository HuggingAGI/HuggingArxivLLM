# 本研究致力于研发适用于医学领域的安全且高度兼容的大型语言模型，旨在提升其在医疗场景下的表现和可靠性。

发布时间：2024年03月06日

`LLM应用`

> Towards Safe and Aligned Large Language Models for Medicine

# 摘要

> 医疗领域的大型语言模型（LLMs）技术日新月异，其潜在能力和风险深度令开发者亦叹为观止。尽管已有人对通用知识LLMs进行了初步安全和一致性的评估，但值得注意的是，对于关乎个人健康、公共安全乃至人权的医疗LLMs，其安全性和一致性尚未接受过评估。因此，我们率先展开了针对医疗LLMs的安全性评估研究。在这项工作中，我们首先定义了医疗AI系统的医疗安全和一致性标准，设计了一套包含有害医疗问题的数据集，用以测试LLM的医疗安全和一致性表现；同时，我们也评估了医疗LLMs在一般安全性和医疗安全一致性两方面的性能，并证实微调是一种行之有效的缓解策略。此外，我们还探讨了机器学习界广泛应用的多种大规模方法，以期推动开发更为安全、一致的LLMs。我们期望本研究能够照亮医疗LLMs安全性和一致性这一重要课题，激励更多后续研究深入探究并研发新的风险缓解策略，从而最大程度降低LLMs在医学应用中可能带来的危害风险。

> The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs. We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine.

[Arxiv](https://arxiv.org/abs/2403.03744)