# 审慎阅读：大型语言模型在法律实务领域的探索与应用

发布时间：2024年03月14日

`LLM理论`

> Caveat Lector: Large Language Models in Legal Practice

# 摘要

> 如今人们对LLMs趋之若鹜，主要是由于多数用户难以评估生成文本的质量，使得LLMs看似超乎寻常的强大。然而，这种基于流畅度和表层合理性的诱惑，容易让人盲目信任生成文本，并滋生过度依赖的风险，比如完美的法律行话就极具迷惑性。本文结合了最近的技术与法学研究成果，针对LLMs在法律实践中的作用做出了更为审慎的判断。若未能深入理解LLMs的局限性，贸然将其整合到法律业务流程中，轻则降低效率，重则带来隐患。尽管LLMs拥有空前绝后的文本生成技能，但却无法理解文本背后的含义，这意味着它们无法有效运用语言、获取知识或进行复杂推理。LLMs基于随机预测词语构建语言模型，无法分辨真实与虚构之间的界限，其对法律的理解仅停留在存储在参数内的词串层面，既不全面又充满谬误。LLMs主要在词频分布层面运作，而非建立在事实验证基础之上。这一特性导致它们有强烈的“臆想”倾向，即生成看似有用且相关的，实则错误的陈述，在法律服务这类高风险领域尤其令人忧虑。当下，律师们应当谨慎对待由LLMs生成的文本。

> The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text. LLMs may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice. Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform complex reasoning tasks. Trained to model language on the basis of stochastic word predictions, LLMs cannot distinguish fact from fiction. Their knowledge of the law is limited to word strings memorized in their parameters. It is also incomplete and largely incorrect. LLMs operate at the level of word distributions, not at the level of verified facts. The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services. At present, lawyers should beware of relying on text generated by LLMs.

[Arxiv](https://arxiv.org/abs/2403.09163)