# 为了实现有效信息综合，在处理大型语言模型时，逻辑离散图形模型不可或缺。它们能够辅助和增强LLM在整合信息方面的表现。

发布时间：2024年03月14日

`LLM理论` `人工智能` `信息检索`

> Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis

# 摘要

> 随着大型语言模型推理能力的发展，信息检索步入更复杂的阶段，现代系统宣称能依据众多不同文档和冲突数据源进行推理并综合生成答案。然而，我们通过回顾最新研究发现，大型语言模型存在根本性缺陷，妨碍其独立实现通用智能或响应广泛的信息整合需求。本文揭示了大型语言模型面临的四大挑战： hallucinations（即虚假信息生成）、复杂推理、不确定环境下的规划以及复杂计算。同时，我们探讨了逻辑离散图形模型如何一站式解决这些问题，并描绘了一种利用无标注文本训练逻辑离散模型的途径。

> Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests. This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.

[Arxiv](https://arxiv.org/abs/2403.09599)