# 大型语言模型是否能洞悉多意图口语？

发布时间：2024年03月07日

`LLM应用`

> Do Large Language Model Understand Multi-Intent Spoken Language ?

# 摘要

> 这项研究突破性地运用LLMs解决多意图口语理解问题，提出了一个新颖方法，借助LLMs在SLU场景下的强大生成能力。我们的创新技术专为LLMs在处理多意图SLU任务时设计了实体槽结构，并创新性地引入了子意图引导（SII）概念，从而更精准地解析和理解跨多个领域内复杂的多意图对话。基于已有基准数据集，我们构建了新的LM-MixATIS和LM-MixSNIPS数据集。实验证明，LLMs不仅能够与现有最先进多意图SLU模型相媲美，甚至有可能超越它们。同时，我们还探究了LLMs在不同意图组合及数据分布情况下的性能表现，并首次引入了实体槽准确率（ESA）和综合语义准确率（CSA）两项评价指标，以深入评估LLMs在此复杂领域的专业水准。

> This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth analysis of LLM proficiency in this complex field.

[Arxiv](https://arxiv.org/abs/2403.04481)