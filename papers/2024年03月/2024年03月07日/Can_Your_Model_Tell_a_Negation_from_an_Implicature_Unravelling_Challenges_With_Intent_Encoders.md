# 探究模型是否能够有效区分否定表达与蕴含含义，同时揭示在构建意图编码器过程中所遇到的难题。

发布时间：2024年03月07日

`LLM应用`

> Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders

# 摘要

> 对话系统在处理意图分类和聚类时，常常借助嵌入模型的力量。如今，大型语言模型（LLMs）崭露头角，其具备的指令性嵌入能力可以通过提示调整嵌入空间中的语义，被誉为解决一系列下游对话任务的良方。但是，现有的评估标准过于依赖任务指标，未能准确衡量语义理解方面的差距。为此，我们创新提出一个意图语义工具箱，它综合考察三种任务——意图分类、意图聚类及一项独特的三元组任务，从而对意图嵌入模型进行全面评估。其中，三元组任务专注于检测模型在真实对话场景下对否定和蕴含这两个关键语义概念的理解水平。遗憾的是，当前的嵌入模型在此类语义理解上存在短板。为改善这一状况，我们提议采用预训练方法，结合自回归模型生成数据的增强技术及对比损失函数优化嵌入模型。此方法在提升意图嵌入模型对否定和蕴含等语言维度语义理解的同时，对其在下游任务指标上的表现仅产生轻微影响。

> Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks-- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems-- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term. Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics.

[Arxiv](https://arxiv.org/abs/2403.04314)