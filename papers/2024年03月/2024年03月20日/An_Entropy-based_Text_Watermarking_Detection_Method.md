# 一种熵导向的文本水印检测技术

发布时间：2024年03月20日

`Agent` `文本水印` `大型语言模型`

> An Entropy-based Text Watermarking Detection Method

# 摘要

> 当前，一种应对大型语言模型（LLMs）滥用问题的方法是采用文本水印技术，通过向LLMs生成的文本中嵌入隐蔽特征以方便后续检测。尽管现行文本水印算法在高熵情境下表现出色，但其在低熵情境下的性能仍有提升空间。为此，本研究建议在水印检测流程中全面考量令牌熵的作用，即根据各令牌的熵值动态调整其在检测时的权重，而非沿用以往同等对待所有令牌权重的做法。我们创新性地提出了一种基于熵的水印检测方案——EWD，在检测时给予高熵令牌更高的权重，以更精确地体现水印的程度。值得一提的是，这一检测流程无需额外训练，实现了全自动化的运行。在实际应用中，我们利用代理-LLM计算各令牌的熵值，无需直接调用原始LLM。实验证明，这种方法在处理低熵情境时能取得更优的检测效果，且具有广泛的适用性，可应用于不同熵分布的文本。未来我们将公开发布相关的代码和数据资料。

> Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. %In actual detection, we use a proxy-LLM to calculate the entropy of each token, without the need to use the original LLM. In the experiment, we found that our method can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data will be available online.

![一种熵导向的文本水印检测技术](../../../paper_images/2403.13485/intro)

![一种熵导向的文本水印检测技术](../../../paper_images/2403.13485/boxcompare)

![一种熵导向的文本水印检测技术](../../../paper_images/2403.13485/ablation)

[Arxiv](https://arxiv.org/abs/2403.13485)