# ConvBench：为大型视觉-语言模型设计的多轮对话评估基准，具备层级分析能力。

发布时间：2024年03月29日

`LLM应用` `视觉语言模型` `对话系统`

> ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models

# 摘要

> 本文推出了ConvBench，一种专为大型视觉语言模型（LVLMs）设计的创新多轮对话评估标准。ConvBench独树一帜，通过建立三级多模态能力层级，模拟人类认知过程，从感知到推理，再到创造，层层递进。它涵盖了577段精心挑选的多轮对话，覆盖215个真实世界任务，为每轮对话和整体交流提供自动性能评估。这一层次化的方法让ConvBench能精准定位对话失误的具体层面。实验表明，即便如GPT4-V这样的多模态模型，在多轮对话中的表现也与人类有显著差异，特别是在细粒度感知方面的不足，影响了模型的推理和创造能力。ConvBench将成为推动视觉对话研究向前发展的新动力。

> This paper presents ConvBench, a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing benchmarks that assess individual capabilities in single-turn dialogues, ConvBench adopts a three-level multimodal capability hierarchy, mimicking human cognitive processes by stacking up perception, reasoning, and creativity. Each level focuses on a distinct capability, mirroring the cognitive progression from basic perception to logical reasoning and ultimately to advanced creativity. ConvBench comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands. Automatic evaluations quantify response performance at each turn and overall conversation level. Leveraging the capability hierarchy, ConvBench enables precise attribution of conversation mistakes to specific levels. Experimental results reveal a performance gap between multi-modal models, including GPT4-V, and human performance in multi-turn conversations. Additionally, weak fine-grained perception in multi-modal models contributes to reasoning and creation failures. ConvBench serves as a catalyst for further research aimed at enhancing visual dialogues.

[Arxiv](https://arxiv.org/abs/2403.20194)