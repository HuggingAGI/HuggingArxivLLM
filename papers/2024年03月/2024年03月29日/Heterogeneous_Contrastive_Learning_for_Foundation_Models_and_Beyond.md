# 异构对比学习：为构建基础模型及其拓展提供支持

发布时间：2024年03月29日

`RAG` `大数据` `人工智能`

> Heterogeneous Contrastive Learning for Foundation Models and Beyond

# 摘要

> 在大数据与人工智能的时代浪潮中，对比自监督学习正成为一种新兴的力量，用于构建和处理大规模的异构数据模型。众多基础模型得益于对比自监督学习的强大泛化能力，它们学习到了紧凑、高品质的数据表示，而无需依赖任何标签信息。随着自然语言处理、计算机视觉等领域基础模型的飞速发展，我们急需对异构对比学习进行一次深入的审视。本综述性研究批判性地评估了当前基础模型的异构对比学习现状，并着重指出了对比学习面临的主要挑战和未来发展趋势。我们首先展示了最新的对比学习方法如何处理视角异质性，以及如何将对比学习应用于多视角基础模型的训练与调优。接着，我们探讨了针对任务异质性的对比学习方法，包括预训练和下游任务，以及如何根据不同目标将不同任务与对比学习损失结合起来。最终，我们通过讨论现存的挑战并展望对比学习的未来方向，为这项调查画上句号。

> In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.

[Arxiv](https://arxiv.org/abs/2404.00225)