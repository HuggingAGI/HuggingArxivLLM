# 本研究通过土耳其语的事实查证案例，探讨了跨语言学习与低资源微调两种方法的优劣。

发布时间：2024年03月01日

`LLM应用`

> Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish

# 摘要

> 社交媒体上错误信息的迅速蔓延引发公众对其对舆论影响的关注，然而目前相关研究主要集中在英语领域，导致像土耳其语等其他语言的数据集严重不足。为此，我们推出了包含3238条真实陈述的FCTR数据集，这些数据覆盖多领域且融合了三家土耳其事实核查机构的证据。同时，我们也致力于探究跨语言迁移学习在解决低资源语言问题上的效果，特别是针对土耳其语场景。通过展示大型语言模型在此情境下的上下文学习（零样本与小样本）性能，实验结果显示FCTR数据集具备推动土耳其语研究深入发展的潜力。

> The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.

[Arxiv](https://arxiv.org/abs/2403.00411)