# 应对大型语言模型的社会误解：采用HCXAI策略进行解决

发布时间：2024年03月26日

`LLM应用` `人工智能` `社会因素`

> Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach

# 摘要

> 以人为本的可解释AI（HCXAI）强调将社会因素融入AI的解释之中。HCXAI的核心理念是社会透明度（ST）框架，旨在让用户了解AI系统的组织和社会背景。本研究提出扩展ST框架，应对大型语言模型（LLMs）在社会归属方面的误导风险，特别是在心理健康这类敏感领域。LLMs擅长模拟各种角色和身份，但可能造成设计者的初衷与用户对社会属性的理解不一致，从而可能引发情感操控、危险行为、知识不公以及过度信任等问题。为解决这些问题，我们建议在ST框架中增加一个“W问题”，明确LLMs设计者和用户赋予的社会属性。这一补充措施旨在缩小LLM的实际能力和用户认知之间的差异，推动以道德负责任的方式开发和应用基于LLM的技术。

> Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims to bridge the gap between LLM capabilities and user perceptions, promoting the ethically responsible development and use of LLM-based technology.

[Arxiv](https://arxiv.org/abs/2403.17873)