# 视觉问答：从早期发展到最新进展 -- 综述

发布时间：2025年01月07日

`LLM应用

**理由**：这篇论文主要讨论了视觉问答（VQA）这一多模态任务，特别是结合了图像和语言处理技术的大型视觉语言模型（LVLMs）。虽然VQA涉及多模态数据处理，但其核心仍然是利用语言模型（如LLM）来处理和理解自然语言问题，并结合视觉信息生成回答。因此，这篇论文属于LLM应用的范畴。` `视觉问答` `多模态学习`

> Visual question answering: from early developments to recent advances -- a survey

# 摘要

> # 摘要
视觉问答（VQA）是一个快速发展的研究领域，旨在通过结合图像和语言处理技术（如特征提取、目标检测、文本嵌入、自然语言理解和语言生成），使机器能够回答关于视觉内容的问题。随着多模态数据研究的兴起，VQA因其广泛的应用场景（如交互式教育工具、医学图像诊断、客户服务、娱乐和社交媒体字幕生成）而备受关注。此外，VQA在帮助视障人士通过图像生成描述性内容方面也发挥了重要作用。本文提出了VQA架构的分类法，基于设计选择和关键组件进行分类，以便进行对比分析和评估。我们回顾了主要的VQA方法，特别是基于深度学习的技术，并探讨了在VQA等多模态任务中表现出色的大型视觉语言模型（LVLMs）这一新兴领域。文章还详细分析了用于评估VQA系统性能的数据集和指标，并探讨了VQA在现实世界中的应用。最后，我们指出了VQA研究中的挑战和未来方向，提出了开放问题和潜在的发展领域。本文为关注VQA最新进展和未来发展的研究人员和从业者提供了全面的参考。

> Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation. With the growth of multimodal data research, VQA has gained significant attention due to its broad applications, including interactive educational tools, medical image diagnosis, customer service, entertainment, and social media captioning. Additionally, VQA plays a vital role in assisting visually impaired individuals by generating descriptive content from images. This survey introduces a taxonomy of VQA architectures, categorizing them based on design choices and key components to facilitate comparative analysis and evaluation. We review major VQA approaches, focusing on deep learning-based methods, and explore the emerging field of Large Visual Language Models (LVLMs) that have demonstrated success in multimodal tasks like VQA. The paper further examines available datasets and evaluation metrics essential for measuring VQA system performance, followed by an exploration of real-world VQA applications. Finally, we highlight ongoing challenges and future directions in VQA research, presenting open questions and potential areas for further development. This survey serves as a comprehensive resource for researchers and practitioners interested in the latest advancements and future

[Arxiv](https://arxiv.org/abs/2501.03939)