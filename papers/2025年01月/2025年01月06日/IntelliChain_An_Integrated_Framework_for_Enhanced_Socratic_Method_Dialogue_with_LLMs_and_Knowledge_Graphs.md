# IntelliChain: 一个集成框架，旨在通过结合大型语言模型（LLMs）和知识图谱，提升苏格拉底式对话的效果

发布时间：2025年01月06日

`Agent

理由：这篇论文主要讨论了将大型语言模型（LLMs）与知识图谱结合，设计并优化多代理系统，以提升教育应用的准确性和可靠性。重点在于多代理系统的设计与协作，以及如何通过链式思维对话方法优化教育互动。因此，这篇论文的核心内容与“Agent”分类最为相关，因为它涉及到智能代理的设计、协作和优化。`

> IntelliChain: An Integrated Framework for Enhanced Socratic Method Dialogue with LLMs and Knowledge Graphs

# 摘要

> 随着教育技术的飞速发展，大型语言模型（LLMs）作为智能教育代理，在提供个性化学习体验方面的需求日益增长。本研究旨在通过将LLMs与知识图谱结合，采用链式思维对话方法，优化为苏格拉底教学定制的多代理系统设计与协作，从而提升教育应用的准确性和可靠性。引入知识图谱后，LLMs处理特定教育内容的能力得到增强，确保了信息的准确性和相关性。同时，我们开发了高效的多代理协作机制，促进智能代理间的信息交换与链式对话，显著提升了教育互动和学习成果的质量。在数学教育领域的实证研究中，该框架在提高教育互动的准确性和可信度方面展现了显著优势。本研究不仅揭示了LLMs和知识图谱在数学教学中的潜力，还为未来AI驱动的教育解决方案提供了宝贵的见解和方法论。

> With the continuous advancement of educational technology, the demand for Large Language Models (LLMs) as intelligent educational agents in providing personalized learning experiences is rapidly increasing. This study aims to explore how to optimize the design and collaboration of a multi-agent system tailored for Socratic teaching through the integration of LLMs and knowledge graphs in a chain-of-thought dialogue approach, thereby enhancing the accuracy and reliability of educational applications. By incorporating knowledge graphs, this research has bolstered the capability of LLMs to handle specific educational content, ensuring the accuracy and relevance of the information provided. Concurrently, we have focused on developing an effective multi-agent collaboration mechanism to facilitate efficient information exchange and chain dialogues among intelligent agents, significantly improving the quality of educational interaction and learning outcomes. In empirical research within the domain of mathematics education, this framework has demonstrated notable advantages in enhancing the accuracy and credibility of educational interactions. This study not only showcases the potential application of LLMs and knowledge graphs in mathematics teaching but also provides valuable insights and methodologies for the development of future AI-driven educational solutions.

[Arxiv](https://arxiv.org/abs/2502.00010)