# GeAR: 生成增强检索

发布时间：2025年01月06日

`RAG

理由：这篇论文提出了一种名为GeAR（生成增强检索）的新方法，通过生成相关文本来增强文档检索的效果。这种方法结合了生成模型和检索技术，属于检索增强生成（Retrieval-Augmented Generation, RAG）的范畴。RAG通常涉及使用生成模型来增强检索系统的性能，或者通过检索来增强生成模型的效果。这篇论文的核心思想是通过生成细粒度的相关文本来提升检索效果，因此应归类为RAG。` `信息检索`

> GeAR: Generation Augmented Retrieval

# 摘要

> 文档检索技术是构建大规模信息系统的基石。当前主流方法是通过双编码器计算语义相似度，但这种标量相似度难以充分反映信息，且阻碍了对检索结果的理解。此外，现有方法过于关注全局语义，忽略了查询与文档中复杂文本之间的细粒度语义关系。为此，我们提出了一种名为$	extbf{GeAR}$（生成增强检索）的新方法，通过精心设计的融合与解码模块，基于查询与文档的融合表示生成相关文本，从而聚焦于细粒度信息。同时，GeAR作为检索器时，不会增加双编码器的计算负担。为了支持新框架的训练，我们设计了一套高效合成高质量数据的流程，利用大型语言模型生成数据。GeAR在多种场景和数据集中展现了卓越的检索与定位性能，其生成的定性分析结果为检索结果的解释提供了全新视角。代码、数据和模型将在技术审查完成后开源，以推动进一步研究。

> Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called $\textbf{Ge}$neration $\textbf{A}$ugmented $\textbf{R}$etrieval ($\textbf{GeAR}$) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to "focus on" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research.

[Arxiv](https://arxiv.org/abs/2501.02772)