# 战略代理的响应方式：战略分类中分析模型与LLM生成响应的对比

发布时间：2025年01月19日

`LLM应用

理由：这篇论文探讨了在战略分类（SC）场景中使用大型语言模型（LLMs）来模拟人类代理的响应。具体来说，研究者使用LLMs生成的战略建议来模拟人类代理在不同场景中的行为，并将这些响应与现有理论模型进行比较。这表明论文主要关注LLMs在实际应用中的使用，特别是在模拟人类行为和决策过程中的应用。因此，将其分类为“LLM应用”是合适的。` `人力资源`

> How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification

# 摘要

> 当机器学习（ML）算法用于自动化人类决策时，人类代理可能会了解决策策略并采取策略性行为以获得理想结果。战略分类（SC）旨在解决代理与决策者之间的互动。以往研究假设代理是完全或近似理性的，通过最大化效用来响应决策策略。然而，由于收集真实世界代理响应的困难，验证这些假设颇具挑战性。随着大型语言模型（LLMs）的普及，SC环境中的人类代理更可能从这些工具中寻求建议。我们提出使用LLMs生成的战略建议来模拟SC中的人类代理响应。具体而言，我们研究了五个关键SC场景——招聘、贷款申请、学校录取、个人收入和公共援助计划——并模拟了不同背景的人类代理如何从LLMs中寻求建议。随后，我们将生成的代理响应与现有理论模型的最佳响应进行比较。研究发现：（i）在大多数情况下，LLMs和理论模型通常导致代理分数或资格变化方向一致，且两者公平性水平相似；（ii）最先进的商业LLMs（如GPT-3.5、GPT-4）始终提供有用建议，但这些建议通常不会带来最大分数或资格提升；（iii）LLMs倾向于生成更多样化的代理响应，通常更偏好平衡的努力分配策略。这些结果表明，理论模型与LLMs在一定程度上一致，利用LLMs模拟更现实的代理响应为设计可信的ML系统提供了新思路。

> When machine learning (ML) algorithms are used to automate human-related decisions, human agents may gain knowledge of the decision policy and behave strategically to obtain desirable outcomes. Strategic Classification (SC) has been proposed to address the interplay between agents and decision-makers. Prior work on SC has relied on assumptions that agents are perfectly or approximately rational, responding to decision policies by maximizing their utilities. Verifying these assumptions is challenging due to the difficulty of collecting real-world agent responses. Meanwhile, the growing adoption of large language models (LLMs) makes it increasingly likely that human agents in SC settings will seek advice from these tools. We propose using strategic advice generated by LLMs to simulate human agent responses in SC. Specifically, we examine five critical SC scenarios -- hiring, loan applications, school admissions, personal income, and public assistance programs -- and simulate how human agents with diverse profiles seek advice from LLMs. We then compare the resulting agent responses with the best responses generated by existing theoretical models. Our findings reveal that: (i) LLMs and theoretical models generally lead to agent score or qualification changes in the same direction across most settings, with both achieving similar levels of fairness; (ii) state-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide helpful suggestions, though these suggestions typically do not result in maximal score or qualification improvements; and (iii) LLMs tend to produce more diverse agent responses, often favoring more balanced effort allocation strategies. These results suggest that theoretical models align with LLMs to some extent and that leveraging LLMs to simulate more realistic agent responses offers a promising approach to designing trustworthy ML systems.

[Arxiv](https://arxiv.org/abs/2501.16355)