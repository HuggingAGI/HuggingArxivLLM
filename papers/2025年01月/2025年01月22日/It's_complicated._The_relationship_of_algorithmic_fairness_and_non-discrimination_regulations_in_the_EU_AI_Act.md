# 关系错综复杂：算法公平性与欧盟AI法案中的非歧视法规

发布时间：2025年01月22日

`LLM应用

**理由**：这篇论文主要讨论了AI模型（包括大型语言模型）在决策中的公平性问题，并分析了《人工智能法案》对AI系统的监管要求。虽然涉及法律和算法公平的概念，但其核心关注点是如何在AI系统（包括LLM）中实现公平决策，并提出了针对AI系统的审计和测试方法的建议。因此，它属于**LLM应用**的范畴。` `人工智能`

> It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act

# 摘要

> 什么是公平的决策？这个问题不仅对人类来说颇具挑战性，当引入人工智能（AI）模型时，难度更是倍增。鉴于算法歧视问题，欧盟近期通过了《人工智能法案》，该法案为AI模型制定了具体规则，融合了传统的法律非歧视法规和基于机器学习的算法公平概念。本文旨在通过以下方式在《人工智能法案》中桥接这两个概念：首先，针对法律和计算机科学领域的学者，对这两个概念进行高层次的介绍；其次，深入分析《人工智能法案》中法律非歧视法规与算法公平之间的关系。我们的分析揭示了三个关键发现：（1）大多数非歧视法规仅针对高风险AI系统。（2）对高风险系统的监管涵盖了数据输入要求和输出监控，但这些法规常常不一致，并引发了计算可行性的问题。（3）对于通用AI模型（如大型语言模型）的监管，如果它们不同时被归类为高风险系统，目前相比其他法规缺乏具体性。基于这些发现，我们建议为AI系统开发更具体的审计和测试方法。本文旨在为未来法律学者和计算机科学领域的机器学习研究人员在研究AI系统中的歧视问题时进行跨学科合作奠定基础。

> What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for AI models, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First a high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.), most non-discrimination regulations target only high-risk AI systems. (2.), the regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are often inconsistent and raise questions of computational feasibility. (3.) Regulations for General Purpose AI Models, such as Large Language Models that are not simultaneously classified as high-risk systems, currently lack specificity compared to other regulations. Based on these findings, we recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.

[Arxiv](https://arxiv.org/abs/2501.12962)