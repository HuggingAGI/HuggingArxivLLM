# 资源匮乏语言：开放还是封闭的LLM？希腊经验启示

发布时间：2025年01月22日

`LLM应用

理由：这篇论文主要讨论了如何利用开源和闭源的大型语言模型（LLMs）来解决资源较少语言的NLP问题，并展示了这些模型在具体任务（如作者归属、法律NLP等）中的应用。论文的重点在于LLMs在实际任务中的表现和应用，而不是理论上的探讨或LLM的内部机制。因此，它属于LLM应用类别。`

> Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek

# 摘要

> # 摘要
资源较少语言的NLP面临诸多挑战，如数据集有限、高资源语言的偏见继承以及领域特定需求。本研究通过三大贡献为现代希腊语填补这些空白。首先，我们评估了开源（Llama-70b）和闭源（GPT-4o mini）LLMs在七个核心NLP任务上的表现，揭示了任务特定优势和性能对等性。其次，我们将作者归属重新定义为评估LLMs预训练数据使用情况的工具，高0-shot准确率揭示了数据来源的伦理问题。最后，我们通过法律NLP案例展示了总结、翻译和嵌入（STE）方法在长文本聚类上优于传统TF-IDF方法。这些贡献为资源较少语言的NLP发展提供了路线图，弥合了模型评估、任务创新和实际应用之间的鸿沟。

> Natural Language Processing (NLP) for lesser-resourced languages faces persistent challenges, including limited datasets, inherited biases from high-resource languages, and the need for domain-specific solutions. This study addresses these gaps for Modern Greek through three key contributions. First, we evaluate the performance of open-source (Llama-70b) and closed-source (GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset availability, revealing task-specific strengths, weaknesses, and parity in their performance. Second, we expand the scope of Greek NLP by reframing Authorship Attribution as a tool to assess potential data usage by LLMs in pre-training, with high 0-shot accuracy suggesting ethical implications for data provenance. Third, we showcase a legal NLP case study, where a Summarize, Translate, and Embed (STE) methodology outperforms the traditional TF-IDF approach for clustering \emph{long} legal texts. Together, these contributions provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps in model evaluation, task innovation, and real-world impact.

[Arxiv](https://arxiv.org/abs/2501.12826)