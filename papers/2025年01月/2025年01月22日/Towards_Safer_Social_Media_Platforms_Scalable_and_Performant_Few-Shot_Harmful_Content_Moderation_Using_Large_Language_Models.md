# 迈向更安全的社交媒体平台：基于大型语言模型的可扩展高效小样本有害内容审核

发布时间：2025年01月22日

`LLM应用` `社交媒体` `内容审核`

> Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models

# 摘要

> 社交媒体上泛滥的有害内容对用户和社会构成了巨大威胁，亟需更高效、可扩展的内容审核策略。当前方法依赖人工审核、监督分类器和大量训练数据，但在应对内容的动态变化、主观判断和规模化审核时往往力不从心。为此，我们利用大型语言模型（LLMs），通过上下文学习实现少样本动态内容审核。通过对多个LLMs的广泛实验，我们的少样本方法在识别有害内容上超越了现有的专有基线（如Perspective和OpenAI Moderation）以及此前的最先进少样本学习方法。我们还引入了视觉信息（如视频缩略图），并评估了多模态技术对模型性能的提升效果。实验结果表明，基于LLM的方法在实现可扩展、动态化的在线有害内容审核方面具有显著优势。

> The prevalence of harmful content on social media platforms poses significant risks to users and society, necessitating more effective and scalable content moderation strategies. Current approaches rely on human moderators, supervised classifiers, and large volumes of training data, and often struggle with scalability, subjectivity, and the dynamic nature of harmful content (e.g., violent content, dangerous challenge trends, etc.). To bridge these gaps, we utilize Large Language Models (LLMs) to undertake few-shot dynamic content moderation via in-context learning. Through extensive experiments on multiple LLMs, we demonstrate that our few-shot approaches can outperform existing proprietary baselines (Perspective and OpenAI Moderation) as well as prior state-of-the-art few-shot learning methods, in identifying harm. We also incorporate visual information (video thumbnails) and assess if different multimodal techniques improve model performance. Our results underscore the significant benefits of employing LLM based methods for scalable and dynamic harmful content moderation online.

[Arxiv](https://arxiv.org/abs/2501.13976)