# 每张图片都在倾听，每张图片都在舞动：音乐驱动的图像动画

发布时间：2025年01月30日

`其他

理由：这篇论文主要讨论的是图像动画和视频生成，特别是音乐驱动的舞蹈视频生成。虽然它涉及到多模态输入（音乐和文本），但其核心内容并不直接涉及大型语言模型（LLM）、检索增强生成（RAG）或智能体（Agent）等主题。因此，将其分类为“其他”更为合适。` `多媒体`

> Every Image Listens, Every Image Dances: Music-Driven Image Animation

# 摘要

> # 摘要
图像动画在多模态研究中崭露头角，专注于从参考图像生成视频。尽管以往研究多集中于文本引导的通用视频生成，但音乐驱动的舞蹈视频生成仍是一片蓝海。本文推出MuseDance，一款创新的端到端模型，通过音乐和文本双输入为参考图像注入生命。这一设计让MuseDance能够创作出既贴合文本描述又能与音乐节奏同步的个性化视频。不同于现有技术，MuseDance无需复杂的运动引导输入（如姿势或深度序列），使得各层次用户都能轻松实现灵活且富有创意的视频生成。为推动该领域研究，我们发布了一个包含2,904个舞蹈视频及其配套背景音乐和文本描述的多模态数据集。我们的方法采用基于扩散的技术，确保了强大的泛化能力、精确控制及时间一致性，为音乐驱动的图像动画任务树立了新标杆。

> Image animation has become a promising area in multimodal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task.

[Arxiv](https://arxiv.org/abs/2501.18801)