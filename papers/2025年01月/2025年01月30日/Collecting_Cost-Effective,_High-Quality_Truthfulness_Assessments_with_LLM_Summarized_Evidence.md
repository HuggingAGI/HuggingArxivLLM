# 利用LLM总结的证据，高效收集高质量的真实性评估

发布时间：2025年01月30日

`LLM应用` `信息验证`

> Collecting Cost-Effective, High-Quality Truthfulness Assessments with LLM Summarized Evidence

# 摘要

> 随着在线错误信息和虚假信息防护措施的弱化，有效应对这一问题变得愈发重要。本文研究了基于大型语言模型（LLM）生成的在线来源摘要进行众包真实性评估的效率和效果。我们在A/B测试中对比了使用摘要与原始网页的效果，并雇佣了大量多样化的众包工作者进行真实性评估。我们评估了评估质量、执行效率以及参与者的行为和参与度。结果显示，摘要模式在评估准确性上与标准模式无显著差异，但显著提升了评估速度。使用摘要的工作者在相同时间内完成了更多评估，降低了成本。此外，摘要模式还提高了注释者间的一致性以及对证据的依赖和感知有用性，证明了摘要证据的实用性，且未影响评估质量。

> With the degradation of guardrails against mis- and disinformation online, it is more critical than ever to be able to effectively combat it. In this paper, we explore the efficiency and effectiveness of using crowd-sourced truthfulness assessments based on condensed, large language model (LLM) generated summaries of online sources. We compare the use of generated summaries to the use of original web pages in an A/B testing setting, where we employ a large and diverse pool of crowd-workers to perform the truthfulness assessment. We evaluate the quality of assessments, the efficiency with which assessments are performed, and the behavior and engagement of participants. Our results demonstrate that the Summary modality, which relies on summarized evidence, offers no significant change in assessment accuracy over the Standard modality, while significantly increasing the speed with which assessments are performed. Workers using summarized evidence produce a significantly higher number of assessments in the same time frame, reducing the cost needed to acquire truthfulness assessments. Additionally, the Summary modality maximizes both the inter-annotator agreements as well as the reliance on and perceived usefulness of evidence, demonstrating the utility of summarized evidence without sacrificing the quality of assessments.

[Arxiv](https://arxiv.org/abs/2501.18265)