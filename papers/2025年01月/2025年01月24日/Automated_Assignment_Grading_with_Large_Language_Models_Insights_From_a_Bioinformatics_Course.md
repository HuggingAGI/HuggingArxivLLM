# 基于大型语言模型的自动化作业评分：生物信息学课程的实践与洞察

发布时间：2025年01月24日

`LLM应用

理由：这篇论文主要讨论了如何利用大语言模型（LLMs）在教育场景中提供个性化反馈，特别是在大规模班级中自动评分和反馈的应用。论文展示了LLM在实际课堂中的评估结果，并与人类助教的反馈进行了对比。这属于LLM在实际应用中的具体使用案例，因此归类为LLM应用。`

> Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics Course

# 摘要

> # 摘要
个性化反馈是教育中支持学生学习和发展的基石。研究表明，及时、高质量的反馈对提升学习成果至关重要。然而，在大规模班级中，提供个性化反馈往往因耗时耗力而难以实现。近年来，自然语言处理和大语言模型（LLMs）的进展为高效提供个性化反馈带来了希望。这些技术不仅能减轻课程工作人员负担，还能提升学生满意度和学习效果。然而，其成功应用仍需在真实课堂中进行全面评估。我们展示了卢布尔雅那大学生物信息学导论课程2024/25学期中，基于LLM的书面作业评分系统的实际评估结果。超过100名学生回答了36个文本问题，其中大部分由LLM自动评分。在一项盲法研究中，学生收到了来自LLM和人类助教的匿名反馈，并对反馈质量进行了评分。我们对六种商业和开源LLM进行了系统评估，并与人类助教的表现进行了对比。结果显示，通过精心设计的提示，LLM的评分准确性和反馈质量可与人类评分者媲美。此外，开源LLM的表现与商业LLM相当，为学校在保护隐私的同时实施自主评分系统提供了可能。

> Providing students with individualized feedback through assignments is a cornerstone of education that supports their learning and development. Studies have shown that timely, high-quality feedback plays a critical role in improving learning outcomes. However, providing personalized feedback on a large scale in classes with large numbers of students is often impractical due to the significant time and effort required. Recent advances in natural language processing and large language models (LLMs) offer a promising solution by enabling the efficient delivery of personalized feedback. These technologies can reduce the workload of course staff while improving student satisfaction and learning outcomes. Their successful implementation, however, requires thorough evaluation and validation in real classrooms. We present the results of a practical evaluation of LLM-based graders for written assignments in the 2024/25 iteration of the Introduction to Bioinformatics course at the University of Ljubljana. Over the course of the semester, more than 100 students answered 36 text-based questions, most of which were automatically graded using LLMs. In a blind study, students received feedback from both LLMs and human teaching assistants without knowing the source, and later rated the quality of the feedback. We conducted a systematic evaluation of six commercial and open-source LLMs and compared their grading performance with human teaching assistants. Our results show that with well-designed prompts, LLMs can achieve grading accuracy and feedback quality comparable to human graders. Our results also suggest that open-source LLMs perform as well as commercial LLMs, allowing schools to implement their own grading systems while maintaining privacy.

[Arxiv](https://arxiv.org/abs/2501.14499)