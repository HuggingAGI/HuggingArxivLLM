# 通过代表性启发法检验大型语言模型的对齐性：以政治刻板印象为例

发布时间：2025年01月24日

`LLM理论

理由：这篇论文主要探讨了大型语言模型（LLMs）在政治倾向上的对齐问题，特别是其与人类意图和价值观的对齐挑战。研究聚焦于LLMs在政治问题上的偏离经验立场的因素，并提出了基于提示的缓解策略。这些内容涉及LLMs的理论层面，特别是其行为模式和对齐问题，因此归类为LLM理论。` `认知科学`

> Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes

# 摘要

> # 摘要
近年来，大型语言模型（LLMs）的对齐问题日益凸显，尤其是在系统未能按预期运行时。本研究聚焦于LLMs与人类意图和价值观的对齐挑战，特别是其政治倾向。已有研究表明，LLMs不仅倾向于表现出政治倾向，还能模仿某些政党在各种问题上的立场。然而，LLMs在何种程度和条件下偏离经验立场，尚未得到充分探讨。为此，我们系统研究了导致LLMs在政治问题上偏离经验立场的因素，旨在量化这些偏离并识别其触发条件。
  基于认知科学中关于代表性启发法的发现——即个体容易夸大目标群体的代表性属性——我们通过这一视角审视LLM的响应。实验结果表明，尽管LLMs能够模仿某些政党的立场，但其表现往往比人类更为夸张。值得注意的是，LLMs比人类更倾向于过度强调代表性。本研究揭示了LLMs对代表性启发法的易感性，暗示了其对政治刻板印象的潜在脆弱性。我们提出的基于提示的缓解策略，有效减少了LLM响应中代表性的影响。

> Examining the alignment of large language models (LLMs) has become increasingly important, particularly when these systems fail to operate as intended. This study explores the challenge of aligning LLMs with human intentions and values, with specific focus on their political inclinations. Previous research has highlighted LLMs' propensity to display political leanings, and their ability to mimic certain political parties' stances on various issues. However, the extent and conditions under which LLMs deviate from empirical positions have not been thoroughly examined. To address this gap, our study systematically investigates the factors contributing to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them.
  Drawing on cognitive science findings related to representativeness heuristics -- where individuals readily recall the representative attribute of a target group in a way that leads to exaggerated beliefs -- we scrutinize LLM responses through this heuristics lens. We conduct experiments to determine how LLMs exhibit stereotypes by inflating judgments in favor of specific political parties. Our results indicate that while LLMs can mimic certain political parties' positions, they often exaggerate these positions more than human respondents do. Notably, LLMs tend to overemphasize representativeness to a greater extent than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggeseting potential vulnerabilities to political stereotypes. We propose prompt-based mitigation strategies that demonstrate effectiveness in reducing the influence of representativeness in LLM responses.

[Arxiv](https://arxiv.org/abs/2501.14294)