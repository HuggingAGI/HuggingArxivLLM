# HateBench: 基于LLM生成内容与仇恨运动的仇恨言论检测器性能评估

发布时间：2025年01月28日

`LLM应用

**理由**：这篇论文主要讨论了大型语言模型（LLMs）在生成仇恨言论方面的滥用问题，并提出了一个用于评估LLM生成仇恨言论检测器的基准框架（HateBench）。论文的核心是LLM在实际应用中的问题（即生成仇恨言论）以及如何检测和防御这些问题的技术手段。因此，它属于**LLM应用**的范畴。` `社交媒体` `网络安全`

> HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns

# 摘要

> # 摘要
大型语言模型（LLMs）在生成仇恨言论方面的滥用引发了广泛关注。在应对这一问题的众多努力中，仇恨言论检测器扮演着关键角色。然而，不同检测器对LLM生成的仇恨言论的有效性仍不明确。本文提出了HateBench，一个用于评估LLM生成仇恨言论检测器的基准框架。我们首先构建了一个包含7,838个样本的仇恨言论数据集，这些样本由六个广泛使用的LLM生成，涵盖34个身份群体，并由三位标注者精心标注。随后，我们评估了八个代表性检测器在该数据集上的表现。结果显示，尽管检测器在识别LLM生成的仇恨言论方面总体有效，但随着LLM版本的更新，其性能有所下降。我们还揭示了LLM驱动的仇恨活动的潜在威胁，这是LLM给仇恨言论检测领域带来的新挑战。通过对抗攻击和模型窃取攻击等先进技术，攻击者可以绕过检测器，自动化在线仇恨活动。最强大的对抗攻击成功率高达0.966，而通过模型窃取攻击，攻击效率可进一步提升13-21倍，同时保持可接受的攻击性能。我们希望这项研究能激励研究社区和平台管理者加强防御，应对这些新兴威胁。

> Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.

[Arxiv](https://arxiv.org/abs/2501.16750)