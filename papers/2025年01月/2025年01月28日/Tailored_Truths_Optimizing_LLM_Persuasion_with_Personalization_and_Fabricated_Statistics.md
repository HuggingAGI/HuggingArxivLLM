# 定制真相：利用个性化和虚构数据优化LLM的说服力

发布时间：2025年01月28日

`LLM应用

理由：这篇论文主要研究了大型语言模型（LLMs）在辩论环境中的说服力，特别是如何利用LLMs生成个性化论点来影响人类观点。这属于LLM在实际应用中的表现和效果研究，因此归类为LLM应用。` `社交媒体` `信息安全`

> Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics

# 摘要

> 大型语言模型（LLMs）正变得越来越有说服力，能够通过利用个人数据在与人类对话中个性化论点。这可能会对虚假信息活动的规模和有效性产生严重影响。我们通过让人类（n=33）与LLM生成的旨在改变人类观点的论点进行辩论，研究了LLMs在辩论环境中的说服力。我们通过测量人类在辩论前后对假设的同意程度，并分析观点变化的幅度以及向LLM方向更新的可能性，量化了LLM的效果。我们比较了已建立的说服策略的说服力，包括基于用户人口统计和个性的个性化论点、对捏造统计数据的吸引力，以及同时使用个性化论点和捏造统计数据的混合策略。我们发现，由人类和GPT-4o-mini生成的静态论点具有相当的说服力。然而，在互动辩论环境中，当LLM利用混合策略时，它优于静态的人类撰写的论点。这种方法有$\mathbf{51\%}$的机会说服参与者修改他们的初始立场，而静态的人类撰写的论点只有$\mathbf{32\%}$的机会。我们的结果突显了LLMs在实现廉价且具有说服力的大规模虚假信息活动方面的潜在风险。

> Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated arguments intended to change the human's opinion. We quantified the LLM's effect by measuring human agreement with the debate's hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM's direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a $\mathbf{51\%}$ chance of persuading participants to modify their initial position, compared to $\mathbf{32\%}$ for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.

[Arxiv](https://arxiv.org/abs/2501.17273)