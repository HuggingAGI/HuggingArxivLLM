# 基于实例化的逻辑推理任务形式化：语言模型与逻辑求解器的结合

发布时间：2025年01月28日

`LLM应用

理由：这篇论文主要讨论了如何通过语义自验证（SSV）方法提升大型语言模型在推理任务中的鲁棒性和准确性。该方法涉及将自然语言中的推理问题转化为逻辑求解器的形式语言，并通过验证功能来提高推理精度。这属于将大型语言模型应用于具体任务（如推理系统）的范畴，因此归类为“LLM应用”。` `人工智能` `推理系统`

> Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers

# 摘要

> # 摘要
推理的鲁棒性仍是大型语言模型的一大挑战，解决这一问题对AI推理系统的实际应用至关重要。我们提出了语义自验证（SSV），这一创新方法解决了将语言模型与逻辑求解器结合的关键难题：如何将自然语言中的推理问题准确转化为求解器的形式语言。SSV通过基于一致性的方法，利用模型生成并由求解器验证的具体实例，生成问题的强抽象形式化。该方法不仅显著提升了推理精度，还引入了一个关键创新——验证功能，在大量案例中实现了接近完美的精度，正如我们在开放推理基准测试中所展示的那样。我们提出的*接近确定的推理*方法，旨在减少对手动验证的依赖，推动AI推理系统向更可靠、更自主的方向迈进。

> Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such *near-certain reasoning* as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems.

[Arxiv](https://arxiv.org/abs/2501.16961)