# 声音的细语：通过音频与文本情感识别及Llama微调，从抑郁症患者的非结构化数据中提取增强信息

发布时间：2025年01月28日

`其他

理由：这篇论文主要讨论的是多模态融合模型在抑郁症分类中的应用，虽然涉及到了模型的学习和特征提取，但并没有直接涉及到大型语言模型（LLM）、检索增强生成（RAG）或智能体（Agent）等具体的技术。因此，将其归类为“其他”更为合适。` `心理健康`

> Whispers of Sound-Enhancing Information Extraction from Depression Patients' Unstructured Data through Audio and Text Emotion Recognition and Llama Fine-tuning

# 摘要

> 本研究提出了一种基于师生架构的创新多模态融合模型，旨在提升抑郁症分类的准确性。该模型通过引入多头注意力机制和加权多模态迁移学习，克服了传统方法在特征融合和模态权重分配上的不足。基于DAIC-WOZ数据集，学生融合模型在文本和听觉教师模型的指导下，显著提升了分类准确率。消融实验显示，该模型在测试集上的F1分数高达99.1%，远超单模态和传统方法。我们的方法不仅有效捕捉了文本与音频特征的互补性，还动态调整教师模型的贡献，增强了泛化能力。实验结果充分证明了该框架在处理复杂多模态数据时的鲁棒性和适应性。本研究为抑郁症分析中的多模态大模型学习提供了新的技术框架，为解决现有方法在模态融合和特征提取上的局限性提供了新思路。

> This study proposes an innovative multimodal fusion model based on a teacher-student architecture to enhance the accuracy of depression classification. Our designed model addresses the limitations of traditional methods in feature fusion and modality weight allocation by introducing multi-head attention mechanisms and weighted multimodal transfer learning. Leveraging the DAIC-WOZ dataset, the student fusion model, guided by textual and auditory teacher models, achieves significant improvements in classification accuracy. Ablation experiments demonstrate that the proposed model attains an F1 score of 99. 1% on the test set, significantly outperforming unimodal and conventional approaches. Our method effectively captures the complementarity between textual and audio features while dynamically adjusting the contributions of the teacher models to enhance generalization capabilities. The experimental results highlight the robustness and adaptability of the proposed framework in handling complex multimodal data. This research provides a novel technical framework for multimodal large model learning in depression analysis, offering new insights into addressing the limitations of existing methods in modality fusion and feature extraction.

[Arxiv](https://arxiv.org/abs/2501.16813)