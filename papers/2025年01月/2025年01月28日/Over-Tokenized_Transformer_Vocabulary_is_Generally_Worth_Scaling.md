# 过度分词的Transformer：词汇表扩展的价值

发布时间：2025年01月28日

`其他

由于论文摘要翻译失败，无法准确理解其内容，因此无法将其归类到Agent、RAG、LLM应用、LLM理论中的任何一个。建议重新获取或翻译摘要后再进行分类。`

> Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling

# 摘要

> <翻译失败>

> Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.

[Arxiv](https://arxiv.org/abs/2501.16975)