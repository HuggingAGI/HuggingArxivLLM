# Eagle 2: 为前沿视觉-语言模型打造全新训练后数据策略

发布时间：2025年01月20日

`LLM应用` `计算机视觉`

> Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models

# 摘要

> 近期，开源视觉语言模型（VLMs）在能力上逐渐逼近专有前沿模型，取得了显著进展。然而，大多数开源模型仅公开最终模型权重，数据策略和实现细节仍不透明。本研究从数据角度切入，探讨VLM的后训练，揭示了数据策略在开发前沿VLM中的核心作用。通过从头构建后训练数据策略，我们分享了开发过程的详细见解，旨在助力开源社区打造更具竞争力的模型。我们提出的数据策略，结合训练方案和模型设计，催生了名为Eagle2的高性能VLM系列。其中，Eagle2-9B在多模态基准测试中表现卓越，与某些高达70B参数的竞争模型不相上下。

> Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights, leaving the critical details of data strategies and implementation largely opaque. In this work, we address VLM post-training from a data-centric perspective, showing the key role of data strategy in developing frontier VLMs. By studying and building our post-training data strategy from scratch, we share detailed insights into the development processes, aiming to benefit the development of competitive models for the open-source community. Our introduced data strategy, together with training recipes and model design, leads to a family of performant VLMs named Eagle2. Specifically, Eagle2-9B achieves state-of-the-art results across various multimodal benchmarks, matching certain competitive models with up to 70B parameters.

[Arxiv](https://arxiv.org/abs/2501.14818)