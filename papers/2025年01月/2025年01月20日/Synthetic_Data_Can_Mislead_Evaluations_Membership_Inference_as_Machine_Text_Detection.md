# 合成数据可能误导评估：成员推断与机器文本检测

发布时间：2025年01月20日

`LLM理论

理由：这篇论文主要讨论了针对大型语言模型（LLMs）的成员推断攻击（MIAs）的问题，特别是使用合成数据作为替代方案时可能导致的误导性结论。论文通过实验揭示了MIAs在检测机器生成文本时的普遍性，并指出这种方法可能影响对模型记忆和数据泄漏的评估。这些内容主要涉及LLMs的理论研究和评估方法，因此归类为“LLM理论”。` `机器学习` `数据安全`

> Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text Detection

# 摘要

> 近期研究表明，针对大型语言模型（LLMs）的成员推断攻击（MIAs）结果并不明确，部分原因是难以构建无时间偏移的非成员数据集。尽管研究人员转向合成数据作为替代方案，但我们发现这种方法可能具有根本性的误导。实验表明，MIAs 实际上充当了机器生成文本的检测器，无论数据来源如何，都会错误地将合成数据识别为训练样本。这一现象在不同架构和规模的模型中普遍存在，从开源模型到 GPT-3.5 等商业模型。甚至由不同、可能更大的模型生成的合成文本也会被目标模型误判为训练数据。我们的研究揭示了一个严重问题：在成员评估中使用合成数据可能导致对模型记忆和数据泄漏的错误结论。我们提醒，这一问题可能影响其他依赖模型信号（如损失）的评估，尤其是在合成或机器生成的翻译数据替代真实样本时。

> Recent work shows membership inference attacks (MIAs) on large language models (LLMs) produce inconclusive results, partly due to difficulties in creating non-member datasets without temporal shifts. While researchers have turned to synthetic data as an alternative, we show this approach can be fundamentally misleading. Our experiments indicate that MIAs function as machine-generated text detectors, incorrectly identifying synthetic data as training samples regardless of the data source. This behavior persists across different model architectures and sizes, from open-source models to commercial ones such as GPT-3.5. Even synthetic text generated by different, potentially larger models is classified as training data by the target model. Our findings highlight a serious concern: using synthetic data in membership evaluations may lead to false conclusions about model memorization and data leakage. We caution that this issue could affect other evaluations using model signals such as loss where synthetic or machine-generated translated data substitutes for real-world samples.

[Arxiv](https://arxiv.org/abs/2501.11786)