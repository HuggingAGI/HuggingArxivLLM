# 基于日常道德困境的大型语言模型规范性评估

发布时间：2025年01月29日

`LLM应用

**理由**：这篇论文主要探讨了大型语言模型（LLMs）在道德推理和伦理决策方面的应用，特别是通过对比LLMs与人类在复杂道德困境中的判断，揭示了LLMs在道德推理中的独特模式。研究内容涉及LLMs在实际应用中的伦理问题，属于LLM应用的范畴。` `伦理决策` `人工智能`

> Normative Evaluation of Large Language Models with Everyday Moral Dilemmas

# 摘要

> 大型语言模型（LLMs）的快速普及引发了对它们编码的道德规范和决策过程的深入研究。许多研究通过调查式问题提示LLMs，评估其与特定人口群体、道德信仰或政治意识形态的对齐程度。然而，这些方法往往过于依赖表面结构，简化了日常道德困境的复杂性。我们认为，沿着更细致的人类互动轴审核LLMs，对于评估它们对人类信仰和行为的影响至关重要。为此，我们从Reddit的“Am I the Asshole”（AITA）社区中选取了复杂的日常道德困境，评估了七个LLMs对超过10,000个AITA道德困境的归责和解释。我们将LLMs的判断与Redditors的判断进行对比，旨在揭示它们在道德推理中的模式。结果显示，LLMs表现出独特的道德判断模式，与AITA子论坛上的人类评估有显著差异。LLMs在自我一致性上表现中等至高，但模型间的共识较低。进一步分析模型解释，发现它们在引用道德原则时表现出独特模式。这些发现突显了在人工系统中实现一致道德推理的复杂性，以及仔细评估不同模型伦理判断方式的必要性。随着LLMs在治疗师和伴侣等需要伦理决策的角色中广泛应用，仔细评估对于减轻潜在偏见和限制至关重要。

> The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am I the Asshole" (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven LLMs to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs' judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations.

[Arxiv](https://arxiv.org/abs/2501.18081)