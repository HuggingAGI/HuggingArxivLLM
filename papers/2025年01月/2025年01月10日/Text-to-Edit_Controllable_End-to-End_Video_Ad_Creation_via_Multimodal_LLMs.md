# 文本到编辑：利用多模态LLMs实现可控的端到端视频广告创作

发布时间：2025年01月10日

`LLM应用

**理由**：该论文摘要主要讨论了如何利用多模态大型语言模型（MLLMs）来实现视频内容的自动化编辑，并提出了一个端到端的框架。这涉及到LLM在实际应用中的使用，特别是结合多模态输入（如视频和文本）来实现特定的任务（视频编辑）。因此，这篇论文应归类为LLM应用。` `短视频` `视频编辑`

> Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal LLMs

# 摘要

> 短视频内容的爆发式增长催生了对高效、自动化视频编辑解决方案的迫切需求，挑战在于如何理解视频并根据用户需求进行定制化编辑。为此，我们提出了一种创新的端到端基础框架，实现了对视频内容编辑的精准控制。借助多模态大型语言模型（MLLMs）的灵活性和泛化能力，我们定义了清晰的输入-输出映射，以提升视频创作效率。为了增强模型对视频内容的理解和处理能力，我们采用了高帧率与快慢处理相结合的策略，显著提升了时间和空间信息的提取与理解。此外，我们还引入了文本到编辑的机制，用户可通过文本输入实现理想的视频效果，从而提升了编辑视频的质量和可控性。通过大量实验，我们的方法不仅在广告数据集中表现出色，还在公共数据集中得出了普遍适用的结论。

> The exponential growth of short-video content has ignited a surge in the necessity for efficient, automated solutions to video editing, with challenges arising from the need to understand videos and tailor the editing according to user requirements. Addressing this need, we propose an innovative end-to-end foundational framework, ultimately actualizing precise control over the final video content editing. Leveraging the flexibility and generalizability of Multimodal Large Language Models (MLLMs), we defined clear input-output mappings for efficient video creation. To bolster the model's capability in processing and comprehending video content, we introduce a strategic combination of a denser frame rate and a slow-fast processing technique, significantly enhancing the extraction and understanding of both temporal and spatial video information. Furthermore, we introduce a text-to-edit mechanism that allows users to achieve desired video outcomes through textual input, thereby enhancing the quality and controllability of the edited videos. Through comprehensive experimentation, our method has not only showcased significant effectiveness within advertising datasets, but also yields universally applicable conclusions on public datasets.

[Arxiv](https://arxiv.org/abs/2501.05884)