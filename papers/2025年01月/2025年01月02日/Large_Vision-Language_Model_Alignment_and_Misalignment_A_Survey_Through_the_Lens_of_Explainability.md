# 大型视觉-语言模型的对齐与偏差：基于可解释性视角的全面调查

发布时间：2025年01月02日

`LLM理论

**理由**：这篇论文主要探讨了大型视觉-语言模型（LVLMs）中视觉与语言表示的对齐问题，并从可解释性视角进行了深入分析。论文涉及了对齐的基础、不对齐现象的剖析以及缓解策略的回顾，这些都是对大型语言模型（LLM）的理论研究，因此分类为LLM理论。` `计算机视觉`

> Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability

# 摘要

> 大型视觉-语言模型（LVLMs）在视觉和文本信息处理方面展现了卓越能力，但视觉与语言表示的对齐问题仍未被完全理解。本文通过可解释性视角，全面探讨了LVLMs中的对齐与不对齐现象。我们首先分析了对齐的基础，涵盖表示、行为、训练方法及理论基础。接着，我们从对象、属性和关系三个语义层次剖析了不对齐现象，揭示其源于数据、模型和推理层的多重挑战。我们系统回顾了现有缓解策略，将其分为参数冻结和参数调优两类。最后，我们展望了未来研究方向，强调标准化评估协议和深入可解释性研究的重要性。

> Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in processing both visual and textual information. However, the critical challenge of alignment between visual and linguistic representations is not fully understood. This survey presents a comprehensive examination of alignment and misalignment in LVLMs through an explainability lens. We first examine the fundamentals of alignment, exploring its representational and behavioral aspects, training methodologies, and theoretical foundations. We then analyze misalignment phenomena across three semantic levels: object, attribute, and relational misalignment. Our investigation reveals that misalignment emerges from challenges at multiple levels: the data level, the model level, and the inference level. We provide a comprehensive review of existing mitigation strategies, categorizing them into parameter-frozen and parameter-tuning approaches. Finally, we outline promising future research directions, emphasizing the need for standardized evaluation protocols and in-depth explainability studies.

[Arxiv](https://arxiv.org/abs/2501.01346)