# 让大型语言模型在应对相反论点时实现忠实完整性的对齐

发布时间：2025年01月02日

`LLM应用` `对话系统`

> Aligning Large Language Models for Faithful Integrity Against Opposing Argument

# 摘要

> 大型语言模型（LLMs）在复杂推理任务中展现出非凡能力。然而，在对话中，它们容易被不实论点误导，即便最初陈述无误。为此，我们对 LLMs 中保持忠实完整性的问题展开研究。这意味着要确保 LLMs 在面对相悖论点时坚守忠实陈述，遇到忠实论点时能纠正错误陈述。在本研究中，我们提出一个新颖框架，名为具有置信度估计的忠实完整性对齐（AFICE），旨在让 LLM 的响应符合忠实完整性。具体而言，AFICE 首先设计了一种双边置信度估计（BCE）方法，用于评估 LLM 在给定特定上下文时生成的每个响应的不确定性，该方法同时依据解码过程中的内部状态估计模型对问题的置信度，以及依据累积概率比估计对答案的置信度。借助 BCE，我们构建了一个由上下文、原始陈述和论点组成的对话偏好数据集，通过直接偏好优化（DPO）使 LLM 与忠实完整性保持一致。在众多基准测试中的大量实验结果显示，当遭遇相悖论点时，LLM 保持忠实响应的能力显著提升，确保了 LLMs 在复杂交互场景中的实际效用和可信度。代码和数据将通过 https://github.com/zhaoy777/AFICE.git 发布。

> Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks. However, they can be easily misled by unfaithful arguments during conversations, even when their original statements are correct. To this end, we investigate the problem of maintaining faithful integrity in LLMs. This involves ensuring that LLMs adhere to their faithful statements in the face of opposing arguments and are able to correct their incorrect statements when presented with faithful arguments. In this work, we propose a novel framework, named Alignment for Faithful Integrity with Confidence Estimation (AFICE), which aims to align the LLM responses with faithful integrity. Specifically, AFICE first designs a Bilateral Confidence Estimation (BCE) approach for estimating the uncertainty of each response generated by the LLM given a specific context, which simultaneously estimate the model's confidence to the question based on the internal states during decoding as well as to the answer based on cumulative probability ratios. With the BCE, we construct a conversational preference dataset composed of context, original statement, and argument, which is adopted for aligning the LLM for faithful integrity using Direct Preference Optimization (DPO). Extensive experimental results on a wide range of benchmarks demonstrate significant improvements in the LLM's ability to maintain faithful responses when encountering opposing arguments, ensuring both the practical utility and trustworthiness of LLMs in complex interactive settings. Code and data will be released via https://github.com/zhaoy777/AFICE.git

[Arxiv](https://arxiv.org/abs/2501.01336)