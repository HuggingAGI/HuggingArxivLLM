# 多语言漏洞修复预训练模型评估

发布时间：2025年01月13日

`LLM应用

**理由**：这篇论文探讨了预训练语言模型（CodeBERT和CodeT5）在自动漏洞修复中的应用，属于将大型语言模型（LLM）应用于特定领域（软件安全）的研究。因此，它应被归类为LLM应用。` `软件安全` `漏洞修复`

> Evaluating Pre-Trained Models for Multi-Language Vulnerability Patching

# 摘要

> # 摘要
软件漏洞带来了严重的安全风险，亟需有效的应对策略。尽管自动程序修复（APR）的进展主要集中在一般软件错误上，但漏洞修复这一安全关键领域仍未被充分探索。本文探讨了预训练语言模型CodeBERT和CodeT5在跨多个数据集和五种编程语言的自动漏洞修复中的潜力。我们评估了这些模型在准确性、计算效率以及漏洞代码补丁长度对性能的影响。结果显示，CodeT5在复杂漏洞模式的数据集上表现出色，而CodeBERT在处理碎片化或上下文有限的数据集时更具优势。CodeT5还展示了卓越的效率，适合大规模应用。然而，随着补丁长度的增加，两个模型在保持性能方面都面临挑战，突显了修复扩展代码的复杂性。本研究为模型性能提供了基准，指出了关键限制，并为改进实际安全应用中的自动漏洞修复提供了见解。

> Software vulnerabilities pose critical security risks, demanding prompt and effective mitigation strategies. While advancements in Automated Program Repair (APR) have primarily targeted general software bugs, the domain of vulnerability patching, which is a security-critical subset of APR, remains underexplored. This paper investigates the potential of pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across diverse datasets and five programming languages. We evaluate these models on their accuracy, computational efficiency, and how the length of vulnerable code patches impacts performance. Our findings reveal promising accuracy levels, particularly for CodeT5 on datasets with complex vulnerability patterns, while CodeBERT demonstrates strengths in handling fragmented or context-limited datasets. CodeT5 further showcases superior efficiency, making it well-suited for large-scale applications. However, both models face challenges in maintaining performance as patch length increases, highlighting the complexity of addressing extended in program repair specifically aimed at fixing vulnerabilities. This study benchmarks model performance, highlights key limitations, and offers insights to improve automated vulnerability patching for practical security applications.

[Arxiv](https://arxiv.org/abs/2501.07339)