# ChatGPT 模型在糖尿病自我管理中的建议：挑战与优化方向

发布时间：2025年01月14日

`LLM应用

**理由**：这篇论文主要讨论了大型语言模型（如ChatGPT 3.5和4）在医疗管理研究中的应用，特别是针对糖尿病患者查询的响应能力。论文还提出了结合检索增强生成技术（RAG）和常识提示评估层的方法，以提升模型在医疗领域的应用效果。因此，这篇论文属于LLM应用类别。` `慢性病管理`

> Advice for Diabetes Self-Management by ChatGPT Models: Challenges and Recommendations

# 摘要

> 大型语言模型凭借其强大的推理能力、广泛的上下文理解和出色的问答能力，在医疗管理研究中崭露头角。然而，尽管这些模型能够处理多种医疗问题，但在为糖尿病等慢性病提供准确且实用的建议时，仍面临巨大挑战。我们评估了ChatGPT 3.5和4版本对糖尿病患者查询的响应，重点考察了它们的医学知识深度以及提供个性化、情境化建议的能力。研究发现，这些模型在准确性和偏见方面存在显著差异，表明它们在提供定制建议方面的能力有限，除非通过复杂的提示技术激活。此外，这两个模型经常在没有进一步澄清的情况下提供建议，可能导致潜在的危险。这凸显了在没有人类监督的临床环境中，这些模型的实际应用效果有限。为此，我们提出了一种基于常识的提示评估层，并结合先进的检索增强生成技术引入疾病特定的外部记忆。这一方法旨在提升信息质量，降低错误信息风险，从而推动AI在医疗领域的更可靠应用。我们的研究旨在为AI在医疗中的未来发展提供方向，提升其整合的广度和质量。

> Given their ability for advanced reasoning, extensive contextual understanding, and robust question-answering abilities, large language models have become prominent in healthcare management research. Despite adeptly handling a broad spectrum of healthcare inquiries, these models face significant challenges in delivering accurate and practical advice for chronic conditions such as diabetes. We evaluate the responses of ChatGPT versions 3.5 and 4 to diabetes patient queries, assessing their depth of medical knowledge and their capacity to deliver personalized, context-specific advice for diabetes self-management. Our findings reveal discrepancies in accuracy and embedded biases, emphasizing the models' limitations in providing tailored advice unless activated by sophisticated prompting techniques. Additionally, we observe that both models often provide advice without seeking necessary clarification, a practice that can result in potentially dangerous advice. This underscores the limited practical effectiveness of these models without human oversight in clinical settings. To address these issues, we propose a commonsense evaluation layer for prompt evaluation and incorporating disease-specific external memory using an advanced Retrieval Augmented Generation technique. This approach aims to improve information quality and reduce misinformation risks, contributing to more reliable AI applications in healthcare settings. Our findings seek to influence the future direction of AI in healthcare, enhancing both the scope and quality of its integration.

[Arxiv](https://arxiv.org/abs/2501.07931)