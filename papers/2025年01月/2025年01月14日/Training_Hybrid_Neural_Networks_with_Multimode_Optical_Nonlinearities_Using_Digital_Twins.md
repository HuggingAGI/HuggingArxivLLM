# 利用数字孪生技术训练多模光学非线性混合神经网络

发布时间：2025年01月14日

`其他

理由：这篇论文主要讨论的是通过将物理系统集成到神经网络中来提高AI模型的能效和计算效率，虽然涉及神经网络和AI模型，但并没有直接涉及大型语言模型（LLM）、检索增强生成（RAG）、代理（Agent）或LLM理论。因此，将其分类为“其他”更为合适。` `人工智能` `光学计算`

> Training Hybrid Neural Networks with Multimode Optical Nonlinearities Using Digital Twins

# 摘要

> 训练大型神经网络的能力使人工智能成为科技发现的前沿。然而，网络规模的指数级增长也带来了对能源和计算硬件的巨大需求。通过将复杂的物理事件作为高效计算模块引入网络，可以减少可训练层的复杂性，从而满足这一需求。我们利用多模光纤中的超短脉冲传播来实现大规模非线性变换。通过可微近似光学系统的神经模型，我们训练了这种混合架构。训练算法更新神经模拟器，并通过代理反向传播误差信号，优化光学层之前的层。实验结果达到了最先进的图像分类精度和模拟保真度，且该框架对实验漂移表现出极强的抗性。通过将低能耗物理系统集成到神经网络中，这种方法能够实现可扩展、高能效的AI模型，并大幅降低计算需求。

> The ability to train ever-larger neural networks brings artificial intelligence to the forefront of scientific and technical discoveries. However, their exponentially increasing size creates a proportionally greater demand for energy and computational hardware. Incorporating complex physical events in networks as fixed, efficient computation modules can address this demand by decreasing the complexity of trainable layers. Here, we utilize ultrashort pulse propagation in multimode fibers, which perform large-scale nonlinear transformations, for this purpose. Training the hybrid architecture is achieved through a neural model that differentiably approximates the optical system. The training algorithm updates the neural simulator and backpropagates the error signal over this proxy to optimize layers preceding the optical one. Our experimental results achieve state-of-the-art image classification accuracies and simulation fidelity. Moreover, the framework demonstrates exceptional resilience to experimental drifts. By integrating low-energy physical systems into neural networks, this approach enables scalable, energy-efficient AI models with significantly reduced computational demands.

[Arxiv](https://arxiv.org/abs/2501.07991)