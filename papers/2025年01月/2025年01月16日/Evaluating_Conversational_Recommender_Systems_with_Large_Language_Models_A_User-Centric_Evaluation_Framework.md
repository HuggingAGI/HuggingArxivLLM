# 基于大型语言模型的对话推荐系统评估：用户中心评价框架

发布时间：2025年01月16日

`LLM应用

**解释**：这篇论文主要讨论了如何利用大型语言模型（LLM）来自动化评估对话推荐系统（CRS）。论文提出了一个基于LLM的评估框架，结合了人机交互和心理学的研究成果，从多个维度对CRS进行评估。这属于LLM在实际应用中的使用，因此分类为LLM应用。` `推荐系统` `人机交互`

> Evaluating Conversational Recommender Systems with Large Language Models: A User-Centric Evaluation Framework

# 摘要

> 对话推荐系统（CRS）结合了推荐与对话任务，其评估颇具挑战性。尽管已有研究从用户角度探讨了影响CRS交互满意度的因素，但相关评估指标仍较为匮乏。近期研究表明，LLMs能够与人类偏好对齐，并已推出多种基于LLM的文本质量评估方法。然而，LLMs在CRS评估中的应用尚不广泛。为填补这一研究空白并推动以用户为中心的CRS发展，本研究提出了一种基于LLM的自动化CRS评估框架，该框架结合了人机交互与心理学的研究成果，从对话行为、语言表达、推荐项目和响应内容四个维度对CRS进行评估。我们利用该框架对四种不同的CRS进行了评估。

> Conversational recommender systems (CRS) involve both recommendation and dialogue tasks, which makes their evaluation a unique challenge. Although past research has analyzed various factors that may affect user satisfaction with CRS interactions from the perspective of user studies, few evaluation metrics for CRS have been proposed. Recent studies have shown that LLMs can align with human preferences, and several LLM-based text quality evaluation measures have been introduced. However, the application of LLMs in CRS evaluation remains relatively limited. To address this research gap and advance the development of user-centric conversational recommender systems, this study proposes an automated LLM-based CRS evaluation framework, building upon existing research in human-computer interaction and psychology. The framework evaluates CRS from four dimensions: dialogue behavior, language expression, recommendation items, and response content. We use this framework to evaluate four different conversational recommender systems.

[Arxiv](https://arxiv.org/abs/2501.09493)