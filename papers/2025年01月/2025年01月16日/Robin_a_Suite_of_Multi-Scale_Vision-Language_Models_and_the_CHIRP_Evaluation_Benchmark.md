# Robin: 多尺度视觉语言模型套件及CHIRP评估基准

发布时间：2025年01月16日

`LLM应用

理由：这篇论文主要讨论了视觉-语言模型（VLMs）的评估方法和基准，特别是通过融合大型语言模型（LLMs）与视觉编码器（VEs）构建的VLM套件Robin，以及为更全面评估VLM而设计的CHIRP基准。虽然涉及了LLM的应用，但重点在于VLM的评估和改进，因此归类为LLM应用。` `计算机视觉`

> Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark

# 摘要

> 近年来，视觉-语言模型（VLMs）的快速发展催生了对严格且全面评估方法和基准的需求。本文深入分析了现有的VLM评估技术，涵盖自动化指标、AI驱动的评估以及跨多种任务的人类评估。我们首先推出了Robin——一个通过在不同规模上融合大型语言模型（LLMs）与视觉编码器（VEs）构建的创新VLM套件，并利用Robin揭示了当前评估方法在不同规模上的短板。随后，为了突破这些局限，我们推出了CHIRP——一个专为更全面、更稳健的VLM评估而设计的长文本响应基准。我们公开了Robin的训练代码、模型套件及CHIRP基准，旨在促进研究的可重复性，推动VLM领域的进一步发展。

> The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.

[Arxiv](https://arxiv.org/abs/2501.09672)