# 负责任的大型语言模型调查：固有风险、恶意使用与应对策略

发布时间：2025年01月16日

`LLM理论

这篇论文主要讨论了大型语言模型（LLMs）在隐私保护、减少幻觉、价值对齐等方面的理论和技术突破，并提出了一个统一框架来提升LLM在实际应用中的表现。这些内容属于对LLM的理论研究和改进，因此分类为LLM理论。` `人工智能`

> A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy

# 摘要

> 尽管大型语言模型（LLMs）在支持现实应用和推动社会进步方面潜力巨大，但它们仍面临隐私泄露、幻觉输出和价值错位等固有风险，且在被越狱后可能被滥用于生成有害内容。为此，本文全面回顾了近期在缓解这些问题上的进展，围绕LLM的四个关键阶段展开：数据收集与预训练、微调与对齐、提示与推理、以及后处理与审计。我们深入探讨了在隐私保护、减少幻觉、价值对齐、消除毒性和防御越狱等方面的最新技术突破。与以往仅聚焦单一维度的研究不同，本文提出了一个统一框架，涵盖多个维度，为提升LLM在实际应用中的表现提供了全面视角。

> While large language models (LLMs) present significant potential for supporting numerous real-world applications and delivering positive social impacts, they still face significant challenges in terms of the inherent risk of privacy leakage, hallucinated outputs, and value misalignment, and can be maliciously used for generating toxic content and unethical purposes after been jailbroken. Therefore, in this survey, we present a comprehensive review of recent advancements aimed at mitigating these issues, organized across the four phases of LLM development and usage: data collecting and pre-training, fine-tuning and alignment, prompting and reasoning, and post-processing and auditing. We elaborate on the recent advances for enhancing the performance of LLMs in terms of privacy protection, hallucination reduction, value alignment, toxicity elimination, and jailbreak defenses. In contrast to previous surveys that focus on a single dimension of responsible LLMs, this survey presents a unified framework that encompasses these diverse dimensions, providing a comprehensive view of enhancing LLMs to better serve real-world applications.

[Arxiv](https://arxiv.org/abs/2501.09431)