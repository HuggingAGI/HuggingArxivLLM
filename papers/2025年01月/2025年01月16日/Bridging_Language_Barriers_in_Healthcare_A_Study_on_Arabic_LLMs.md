# 跨越医疗语言障碍：阿拉伯语大型语言模型研究

发布时间：2025年01月16日

`LLM应用

理由：这篇论文主要讨论了开发多语言理解和医学知识的大型语言模型（LLMs）的挑战，并探讨了如何通过调整语言比例和预训练方法来优化模型在临床任务中的表现。这些内容涉及到LLMs在实际应用中的优化和调整，因此属于LLM应用类别。` `多语言处理`

> Bridging Language Barriers in Healthcare: A Study on Arabic LLMs

# 摘要

> 本文探讨了开发兼具多语言理解和医学知识的大型语言模型（LLMs）的挑战。我们发现，单纯翻译医学数据并不能确保在目标语言的临床任务中表现优异。实验显示，不同医疗任务所需的最佳语言组合差异显著。精心调整语言比例的大型模型在母语临床任务中表现更佳。此外，仅靠微调可能并非将新语言知识融入LLMs的最佳方式，数据和计算密集型的预训练方法仍是实现多语言医疗环境中最佳性能的关键。这些发现为构建包容性强的医疗AI系统提供了重要指导。

> This paper investigates the challenges of developing large language models (LLMs) proficient in both multilingual understanding and medical knowledge. We demonstrate that simply translating medical data does not guarantee strong performance on clinical tasks in the target language. Our experiments reveal that the optimal language mix in training data varies significantly across different medical tasks. We find that larger models with carefully calibrated language ratios achieve superior performance on native-language clinical tasks. Furthermore, our results suggest that relying solely on fine-tuning may not be the most effective approach for incorporating new language knowledge into LLMs. Instead, data and computationally intensive pretraining methods may still be necessary to achieve optimal performance in multilingual medical settings. These findings provide valuable guidance for building effective and inclusive medical AI systems for diverse linguistic communities.

[Arxiv](https://arxiv.org/abs/2501.09825)