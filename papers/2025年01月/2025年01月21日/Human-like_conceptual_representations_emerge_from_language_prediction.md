# 人类般的概念表示从语言预测中自然浮现

发布时间：2025年01月21日

`LLM理论

理由：这篇论文主要探讨了大型语言模型（LLMs）在概念表示和组织方面的能力，特别是它们如何模拟人类的概念推理和表示。研究通过实验验证了LLMs能够生成与人类认知相似的概念表示，并且这些表示与大脑神经活动模式高度吻合。这些发现不仅揭示了LLMs在模拟人类认知方面的潜力，还为LLMs的理论研究提供了新的视角。因此，这篇论文属于“LLM理论”分类。` `认知科学` `人工智能`

> Human-like conceptual representations emerge from language prediction

# 摘要

> # 摘要
最近大型语言模型（LLMs）的突破为解决一个长期问题提供了新契机：概念在头脑中如何表示和组织，这对揭示人类认知的本质至关重要。我们重新设计了经典的反向词典任务，模拟人类在上下文中的概念推理，并探索了LLMs中类似人类的概念表示的形成。研究发现，LLMs能够从定义性描述中推断概念，并构建出趋向于共享的、上下文无关的表示空间。这些表示不仅有效预测了人类的行为判断，还与大脑神经活动模式高度吻合，为生物合理性提供了证据。这表明，即使没有现实世界的根基，类似人类的概念表示和组织也能从语言预测中自然涌现。我们的研究支持了LLMs作为理解复杂人类认知的有力工具的观点，并为人工智能与人类智能的更好对齐奠定了基础。

> Recent advances in large language models (LLMs) provide a new opportunity to address the long-standing question of how concepts are represented and organized in the mind, which is central to unravelling the nature of human cognition. Here, we reframed the classic reverse dictionary task to simulate human concept inference in context and investigated the emergence of human-like conceptual representations within LLMs. We found that LLMs were able to infer concepts from definitional descriptions and construct representation spaces that converge towards a shared, context-independent structure. These representations effectively predicted human behavioural judgments and aligned well with neural activity patterns in the human brain, offering evidence for biological plausibility. These findings demonstrate that human-like conceptual representations and organization can naturally emerge from language prediction, even without real-world grounding. Our work supports the view that LLMs serve as valuable tools for understanding complex human cognition and paves the way for better alignment between artificial and human intelligence.

[Arxiv](https://arxiv.org/abs/2501.12547)