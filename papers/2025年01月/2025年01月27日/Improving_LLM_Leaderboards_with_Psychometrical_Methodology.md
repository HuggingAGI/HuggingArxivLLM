# 运用心理测量学方法优化LLM排行榜

发布时间：2025年01月27日

`LLM理论

解释：这篇论文主要讨论了如何通过心理测量方法来改进大型语言模型（LLMs）的评估和排名，这涉及到对LLM性能的理论分析和评估方法的改进。因此，它更适合归类为LLM理论。` `人工智能` `心理测量`

> Improving LLM Leaderboards with Psychometrical Methodology

# 摘要

> 随着大型语言模型（LLMs）的迅猛发展，创建评估其性能的基准变得至关重要。这些基准类似于人类的测试和调查，通过一系列问题来衡量系统在认知行为中的涌现特性。然而，与社会科学中明确定义的特质和能力不同，这些基准所衡量的特性往往较为模糊且定义不够严谨。为了方便比较，最著名的基准通常被分组到排行榜中，汇总性能指标。遗憾的是，这些排行榜通常采用简单的聚合方法，例如取基准的平均分数。本文展示了应用当代心理测量方法（最初为人类测试和调查设计）来改进 LLM 在排行榜上排名的优势。以 Hugging Face 排行榜的数据为例，我们比较了传统朴素排名方法与心理测量学指导的排名结果，结果表明，采用心理测量技术能够更稳健、更有意义地评估 LLM 性能。

> The rapid development of large language models (LLMs) has necessitated the creation of benchmarks to evaluate their performance. These benchmarks resemble human tests and surveys, as they consist of sets of questions designed to measure emergent properties in the cognitive behavior of these systems. However, unlike the well-defined traits and abilities studied in social sciences, the properties measured by these benchmarks are often vaguer and less rigorously defined. The most prominent benchmarks are often grouped into leaderboards for convenience, aggregating performance metrics and enabling comparisons between models. Unfortunately, these leaderboards typically rely on simplistic aggregation methods, such as taking the average score across benchmarks. In this paper, we demonstrate the advantages of applying contemporary psychometric methodologies - originally developed for human tests and surveys - to improve the ranking of large language models on leaderboards. Using data from the Hugging Face Leaderboard as an example, we compare the results of the conventional naive ranking approach with a psychometrically informed ranking. The findings highlight the benefits of adopting psychometric techniques for more robust and meaningful evaluation of LLM performance.

[Arxiv](https://arxiv.org/abs/2501.17200)