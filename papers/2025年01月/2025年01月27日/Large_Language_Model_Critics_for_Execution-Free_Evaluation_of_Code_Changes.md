# 大型语言模型批评者：无需执行即可评估代码更改

发布时间：2025年01月27日

`Agent

理由：这篇论文主要讨论了基于大型语言模型（LLMs）的智能体工作流在自动化软件工程任务中的应用，特别是通过设计基于LLM的批评者来评估代码更改的质量。论文的核心内容围绕智能体（Agent）的工作流和评估方法展开，因此将其分类为Agent。` `软件工程` `自动化测试`

> Large Language Model Critics for Execution-Free Evaluation of Code Changes

# 摘要

> 大型语言模型（LLMs）为自动化软件工程任务（如错误修复、功能添加等）提供了一种有前景的途径，通过基于多步骤LLM的智能体工作流。然而，现有的评估指标（主要是构建状态和偶尔的日志分析）过于稀疏，无法全面评估更改质量。为此，我们设计了基于LLM的批评者，用于推导结构良好且严格的中间/步骤级别评估代理，适用于仓库级别的代码更改。我们假设可以访问黄金测试补丁（即参考感知），以评估生成补丁的语义和可执行性。以黄金测试补丁为参考，我们预测所有编辑位置的可执行性，F1得分为91.6%，并能在SWE-bench中84.8%的实例中预测构建状态。这种以执行为重点的LLM批评者比其他无参考和参考感知的LLM批评者表现更好，高出38.9%到72.5%。此外，我们还展示了这种参考感知框架在比较不同智能体工作流生成的补丁时的实用性。最后，我们开源了该项目的库，供其他智能体工作流或基准测试使用。源代码可在https://github.com/amazon-science/code-agent-eval获取。

> Large language models (LLMs) offer a promising way forward for automating software engineering tasks, such as bug fixes, feature additions, etc., via multi-step LLM-based agentic workflows. However, existing metrics for evaluating such workflows, mainly build status and occasionally log analysis, are too sparse and limited in providing the information needed to assess the quality of changes made. In this work, we designed LLM-based critics to derive well-structured and rigorous intermediate/step-level, execution-free evaluation proxies for repo-level code changes. Importantly, we assume access to the gold test patch for the problem (i.e., reference-aware) to assess both semantics and executability of generated patches. With the gold test patch as a reference, we predict executability of all editing locations with an F1 score of 91.6%, aggregating which, we can predict the build status in 84.8% of the instances in SWE-bench. In particular, such an execution-focused LLM critic outperforms other reference-free and reference-aware LLM critics by 38.9% to 72.5%. Moreover, we demonstrate the usefulness of such a reference-aware framework in comparing patches generated by different agentic workflows. Finally, we open-source the library developed for this project, which allows further usage for either other agentic workflows or other benchmarks. The source code is available at https://github.com/amazon-science/code-agent-eval.

[Arxiv](https://arxiv.org/abs/2501.16655)