# LLM 代理系统能否合作：一场关于社会困境的探索

发布时间：2025年01月27日

`Agent

理由：这篇论文主要探讨了大型语言模型（LLM）智能体在战略互动中的集体行为，特别是它们在迭代囚徒困境中的合作倾向。研究通过进化博弈论模拟了不同战略倾向的智能体群体，并观察其进化动态。因此，这篇论文的核心内容是关于智能体（Agent）的行为和策略，而不是直接讨论LLM的理论、应用或其他相关技术。` `人工智能` `博弈论`

> Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma

# 摘要

> 随着自主智能体的普及，理解它们在战略互动中的集体行为变得至关重要。本研究探讨了大型语言模型（LLM）智能体在社会困境中的合作倾向。与以往研究不同，我们让最先进的LLMs生成迭代囚徒困境的完整策略，而非仅输出个体行动。通过进化博弈论，我们模拟了具有不同战略倾向（攻击性、合作性或中性）的智能体群体，并观察其进化动态。研究发现，不同LLMs表现出不同的偏见，影响了攻击性与合作性策略的相对成功。这项研究为基于LLM的自主智能体系统的长期行为提供了洞见，并强调了仔细考虑其运作战略环境的重要性。

> As autonomous agents become more prevalent, understanding their collective behaviour in strategic interactions is crucial. This study investigates the emergent cooperative tendencies of systems of Large Language Model (LLM) agents in a social dilemma. Unlike previous research where LLMs output individual actions, we prompt state-of-the-art LLMs to generate complete strategies for iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate populations of agents with different strategic dispositions (aggressive, cooperative, or neutral) and observe their evolutionary dynamics. Our findings reveal that different LLMs exhibit distinct biases affecting the relative success of aggressive versus cooperative strategies. This research provides insights into the potential long-term behaviour of systems of deployed LLM-based autonomous agents and highlights the importance of carefully considering the strategic environments in which they operate.

[Arxiv](https://arxiv.org/abs/2501.16173)