# CILP-FGDI：基于视觉-语言模型的可泛化人员重识别

发布时间：2025年01月27日

`LLM应用

**理由**：该论文探讨了如何利用CLIP（一种视觉语言模型）来对齐视觉与文本特征，以解决人员重识别任务中的挑战。CLIP是一种基于大规模预训练的模型，属于大型语言模型（LLM）的应用范畴。论文中的方法涉及利用CLIP的跨模态能力来提升任务的性能，因此应归类为LLM应用。` `计算机视觉` `人员重识别`

> CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person Re-Identification

# 摘要

> 视觉语言模型以其强大的跨模态能力著称，广泛应用于各类计算机视觉任务。本文探讨了如何利用CLIP（对比语言-图像预训练）——一种在大规模图像-文本对上预训练的视觉语言模型，来对齐视觉与文本特征，从而在可泛化的人员重识别任务中获取细粒度且领域不变的表示。将CLIP应用于此任务面临两大挑战：一是学习更细粒度的特征以增强判别能力，二是学习更领域不变的特征以提升模型的泛化能力。为解决第一个挑战，我们提出了一种三阶段策略来提升文本描述的准确性。首先，训练图像编码器以适配人员重识别任务；其次，利用图像编码器提取的特征为每张图像生成文本描述（即提示）；最后，使用带有学习提示的文本编码器指导最终图像编码器的训练。为增强模型对未见领域的泛化能力，我们引入了一种双向引导方法，用于学习领域不变的图像特征。具体而言，生成领域不变和领域相关的提示，并通过正视图（拉近图像特征与领域不变提示）和负视图（推远图像特征与领域相关提示）来训练图像编码器。这些策略共同推动了一种创新的基于CLIP的框架，用于学习人员重识别中的细粒度泛化特征。

> The Visual Language Model, known for its robust cross-modal capabilities, has been extensively applied in various computer vision tasks. In this paper, we explore the use of CLIP (Contrastive Language-Image Pretraining), a vision-language model pretrained on large-scale image-text pairs to align visual and textual features, for acquiring fine-grained and domain-invariant representations in generalizable person re-identification. The adaptation of CLIP to the task presents two primary challenges: learning more fine-grained features to enhance discriminative ability, and learning more domain-invariant features to improve the model's generalization capabilities. To mitigate the first challenge thereby enhance the ability to learn fine-grained features, a three-stage strategy is proposed to boost the accuracy of text descriptions. Initially, the image encoder is trained to effectively adapt to person re-identification tasks. In the second stage, the features extracted by the image encoder are used to generate textual descriptions (i.e., prompts) for each image. Finally, the text encoder with the learned prompts is employed to guide the training of the final image encoder. To enhance the model's generalization capabilities to unseen domains, a bidirectional guiding method is introduced to learn domain-invariant image features. Specifically, domain-invariant and domain-relevant prompts are generated, and both positive (pulling together image features and domain-invariant prompts) and negative (pushing apart image features and domain-relevant prompts) views are used to train the image encoder. Collectively, these strategies contribute to the development of an innovative CLIP-based framework for learning fine-grained generalized features in person re-identification.

[Arxiv](https://arxiv.org/abs/2501.16065)