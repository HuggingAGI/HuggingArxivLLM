# 迈向安全的AI临床医生：全面研究大型语言模型在医疗领域的越狱问题

发布时间：2025年01月27日

`LLM应用

理由：这篇论文主要讨论了大型语言模型（LLMs）在医疗领域的应用及其安全性问题，特别是对黑箱越狱技术的脆弱性评估和防御措施。研究内容集中在LLMs的实际应用场景（医疗领域）及其安全性改进，因此应归类为LLM应用。` `人工智能安全`

> Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare

# 摘要

> 大型语言模型（LLMs）在医疗领域的应用日益广泛，但其在临床实践中的部署却带来了显著的安全隐患，如潜在的有害信息传播。本研究系统评估了六种LLMs在医疗背景下对三种高级黑箱越狱技术的脆弱性，并提出了一个自动化和领域适应的智能评估流程来量化这些技术的有效性。实验结果显示，主流商业和开源LLMs对医疗越狱攻击极为脆弱。为提升模型的安全性和可靠性，我们进一步探讨了持续微调（CFT）在防御医疗对抗攻击中的效果。研究结果强调了不断更新攻击方法评估、领域特定的安全对齐以及LLM安全性与效用平衡的重要性。这项研究为提升AI临床医生的安全性和可靠性提供了实用见解，助力医疗保健领域实现道德且有效的AI部署。

> Large language models (LLMs) are increasingly utilized in healthcare applications. However, their deployment in clinical practice raises significant safety concerns, including the potential spread of harmful information. This study systematically assesses the vulnerabilities of six LLMs to three advanced black-box jailbreaking techniques within medical contexts. To quantify the effectiveness of these techniques, we propose an automated and domain-adapted agentic evaluation pipeline. Experiment results indicate that leading commercial and open-source LLMs are highly vulnerable to medical jailbreaking attacks. To bolster model safety and reliability, we further investigate the effectiveness of Continual Fine-Tuning (CFT) in defending against medical adversarial attacks. Our findings underscore the necessity for evolving attack methods evaluation, domain-specific safety alignment, and LLM safety-utility balancing. This research offers actionable insights for advancing the safety and reliability of AI clinicians, contributing to ethical and effective AI deployment in healthcare.

[Arxiv](https://arxiv.org/abs/2501.18632)