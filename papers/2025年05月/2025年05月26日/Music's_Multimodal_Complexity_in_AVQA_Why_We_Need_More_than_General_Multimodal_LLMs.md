# 音乐的多模态复杂性在AVQA：为什么需要更强大的多模态LLMs

发布时间：2025年05月26日

`LLM应用` `视听问答`

> Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs

# 摘要

> 尽管多模态大型语言模型在通用任务中表现出色，但音乐等专业领域仍需量身定制的方法。音乐视听问答（Music AVQA）尤其面临独特挑战，包括连续的视听内容、复杂的时间动态和对领域知识的迫切需求。通过系统分析，我们发现专业输入处理、时空设计架构和音乐建模策略是成功的关键。本研究为研究人员提供了设计模式的实证见解，提出了结合音乐先验的未来方向，并致力于为多模态音乐理解奠定基础。这项工作旨在激发更多关注和研究，支持一个持续更新的GitHub仓库：https://github.com/xid32/Survey4MusicAVQA。

> While recent Multimodal Large Language Models exhibit impressive capabilities for general multimodal tasks, specialized domains like music necessitate tailored approaches. Music Audio-Visual Question Answering (Music AVQA) particularly underscores this, presenting unique challenges with its continuous, densely layered audio-visual content, intricate temporal dynamics, and the critical need for domain-specific knowledge. Through a systematic analysis of Music AVQA datasets and methods, this position paper identifies that specialized input processing, architectures incorporating dedicated spatial-temporal designs, and music-specific modeling strategies are critical for success in this domain. Our study provides valuable insights for researchers by highlighting effective design patterns empirically linked to strong performance, proposing concrete future directions for incorporating musical priors, and aiming to establish a robust foundation for advancing multimodal musical understanding. This work is intended to inspire broader attention and further research, supported by a continuously updated anonymous GitHub repository of relevant papers: https://github.com/xid32/Survey4MusicAVQA.

[Arxiv](https://arxiv.org/abs/2505.20638)