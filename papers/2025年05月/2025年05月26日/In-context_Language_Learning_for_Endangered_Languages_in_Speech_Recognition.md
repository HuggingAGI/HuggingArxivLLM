# 针对濒危语言的语音识别上下文语言学习

发布时间：2025年05月26日

`LLM应用` `语言模型` `语音技术`

> In-context Language Learning for Endangered Languages in Speech Recognition

# 摘要

> 全球约有7,000种语言在使用，但目前的大型语言模型（LLMs）仅支持其中一小部分。先前研究发现，LLMs无需监督数据即可学习某些新语言。我们在此基础上扩展研究，探索LLMs是否能通过上下文学习（ICL）掌握未见过的低资源语言。通过在四种多样化的濒危语言上进行实验（这些语言并未包含在LLMs的训练数据中），我们发现提供更多的相关文本样本能显著提升语言建模和自动语音识别（ASR）任务的表现。此外，我们发现概率驱动的方法在语言学习中比传统的指令驱动方法更有效。最后，我们发现ICL使LLMs在ASR任务中达到与专门针对这些语言训练的模型相当甚至更优的性能，同时保留了LLMs原有的能力。

> With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs.

[Arxiv](https://arxiv.org/abs/2505.20445)