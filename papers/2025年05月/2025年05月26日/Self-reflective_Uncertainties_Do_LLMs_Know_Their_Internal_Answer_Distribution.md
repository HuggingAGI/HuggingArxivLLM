# # 自我反思的不确定性：大型语言模型是否清楚自身内部答案分布？

发布时间：2025年05月26日

`LLM理论` `人工智能`

> Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?

# 摘要

> 要揭示大型语言模型（LLM）何时对某个响应感到不确定，不确定性量化通常会生成与输出相关的百分比数字。但这是否是我们的全部能力？我们提出，在LLM的输出空间——即字符串空间中，存在足够表达的字符串，能够总结LLM认为可能的输出字符串分布。我们为此新途径的不确定性解释奠定了基础，并提出了SelfReflect，这是一个理论驱动的指标，用于评估一个字符串如何忠实总结LLM的内部答案分布。我们展示了SelfReflect能够区分候选摘要字符串之间的细微差别，并且与人类判断一致，优于LLM判断和嵌入比较等替代指标。借助SelfReflect，我们研究了多种自我总结方法，并发现即使是最先进的推理模型也难以解释其内部不确定性。但我们也发现，通过采样和总结可以生成忠实的摘要。我们的指标为未来研究LLM普遍存在的不确定性形式提供了方向。

> To reveal when a large language model (LLM) is uncertain about a response, uncertainty quantification commonly produces percentage numbers along with the output. But is this all we can do? We argue that in the output space of LLMs, the space of strings, exist strings expressive enough to summarize the distribution over output strings the LLM deems possible. We lay a foundation for this new avenue of uncertainty explication and present SelfReflect, a theoretically-motivated metric to assess how faithfully a string summarizes an LLM's internal answer distribution. We show that SelfReflect is able to discriminate even subtle differences of candidate summary strings and that it aligns with human judgement, outperforming alternative metrics such as LLM judges and embedding comparisons. With SelfReflect, we investigate a number of self-summarization methods and find that even state-of-the-art reasoning models struggle to explicate their internal uncertainty. But we find that faithful summarizations can be generated by sampling and summarizing. Our metric enables future works towards this universal form of LLM uncertainties.

[Arxiv](https://arxiv.org/abs/2505.20295)