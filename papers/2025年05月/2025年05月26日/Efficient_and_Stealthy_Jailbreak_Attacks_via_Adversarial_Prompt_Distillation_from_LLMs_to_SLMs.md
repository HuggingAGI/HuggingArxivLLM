# 高效且隐秘的越狱攻击：通过对抗提示蒸馏从大型语言模型到小语言模型

发布时间：2025年05月26日

`LLM应用` `人工智能安全` `网络安全`

> Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs

# 摘要

> 在越狱场景下，针对大型语言模型（LLMs）的攻击引发了诸多安全和伦理问题。目前的越狱攻击方法存在效率低、计算成本高以及跨模型适应性和通用性差等局限性，难以应对LLM的快速发展和新防御策略的挑战。为此，我们提出了一种结合掩码语言建模、强化学习和动态温度控制的对抗性提示蒸馏方法，通过提示生成和蒸馏的方式，使小型语言模型（SLMs）能够对主流LLMs发起越狱攻击。实验结果表明，该方法在攻击成功率和危害性方面表现优异，同时展现了其资源效率和跨模型适应性。本研究不仅探索了将LLM的越狱能力蒸馏至SLM的可行性，还揭示了模型的潜在漏洞，并为LLM的安全研究提供了新的思路。

> Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.

[Arxiv](https://arxiv.org/abs/2506.17231)