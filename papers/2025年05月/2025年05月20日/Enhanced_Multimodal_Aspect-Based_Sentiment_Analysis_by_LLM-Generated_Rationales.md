# 利用LLM生成的理由，提升多模态方面情感分析的效果

发布时间：2025年05月20日

`LLM应用` `情感分析` `多模态`

> Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales

# 摘要

> 近年来，多模态方面情感分析（MABSA）逐渐成为研究热点。现有方法主要依赖预训练的小语言模型（SLMs）从图像和文本中提取与方面和情感相关的信息，并力求实现模态对齐。然而，小规模的SLMs在容量和知识储备上存在局限性，常常导致在文本和视觉数据中对意义、方面、情感及其相互关联的识别不够准确。另一方面，大型语言模型（LLMs）在各种任务中表现出色，能够有效探索多模态数据中的细粒度信息。然而，一些研究表明，与经过微调的小模型相比，LLMs在ABSA领域的表现仍显不足。基于这些研究发现，我们提出了一种新型框架，命名为LRSA，该框架结合了SLMs的决策能力和LLMs提供的额外信息，用于MABSA任务。具体而言，我们将LLMs生成的解释作为推理依据注入到SLMs中，并采用双交叉注意力机制来增强特征交互与融合，从而提升SLMs在识别方面和情感方面的性能。我们使用两个基线模型对我们的方法进行了评估，大量实验结果表明，我们的方法在三个广泛使用的基准测试中表现优异，证明了其通用性和适用于大多数预训练模型的MABSA任务。

> There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.

[Arxiv](https://arxiv.org/abs/2505.14499)