# 跨越语言之旅：多模态大语言模型的跨语言一致性评测

发布时间：2025年05月20日

`LLM应用` `多语言`

> Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs

# 摘要

> 多模态大型语言模型 (MLLMs) 的快速发展显著提升了其在现实世界中的应用。然而，要在不同语言间实现一致的性能，尤其是在整合文化知识方面，仍然是一个重大挑战。为了更好地评估这一问题，我们引入了两个新的基准测试：KnowRecall 和 VisRecall，它们用于评估 MLLMs 的跨语言一致性。KnowRecall 是一个视觉问答基准测试，旨在衡量 15 种语言中的事实知识一致性，重点关注涉及全球地标的文化和历史问题。VisRecall 通过要求模型在不访问图像的情况下描述 9 种语言中的地标外观，评估视觉记忆一致性。实验结果表明，包括专有模型在内的最先进的 MLLMs 仍然难以实现跨语言一致性。这凸显了需要更强大的方法来创建真正多语言且具备文化意识的模型。

> The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.

[Arxiv](https://arxiv.org/abs/2505.15075)