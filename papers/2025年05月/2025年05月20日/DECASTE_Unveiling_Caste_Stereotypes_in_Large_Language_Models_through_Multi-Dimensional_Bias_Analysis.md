# DECASTE：多维度偏见分析揭示大型语言模型中的种姓刻板印象

发布时间：2025年05月20日

`LLM应用` `社会公平`

> DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis

# 摘要

> 大型语言模型（LLMs）的突破性进展彻底改变了自然语言处理（NLP）领域，并将其应用范围扩展至各个领域。然而，尽管LLMs展现出令人瞩目的能力，但它们也被发现会反映并延续有害的社会偏见，包括基于种族、性别和宗教的偏见。一个关键且尚未充分探索的问题是，LLMs强化了基于种姓的偏见，尤其是针对印度边缘化种姓群体，如达利特和首陀罗。本文针对这一研究空白，提出DECASTE——一种创新的多维度框架，旨在检测和评估LLMs中的隐性和显性种姓偏见。我们的方法从社会文化、经济、教育和政治四个维度评估种姓公平性，并采用多种定制提示策略。通过对多个先进的LLMs进行基准测试，我们发现这些模型系统性地强化了种姓偏见，受压迫种姓与优势种姓在模型中的待遇存在显著差异。例如，将达利特和首陀罗与优势种姓群体进行比较时，偏见得分显著升高，反映了社会偏见在模型输出中的持续存在。这些结果揭示了LLMs中隐性且普遍存在的种姓偏见，并强调了开发更全面和包容的偏见评估方法的必要性，以评估在实际应用中部署这些模型的潜在风险。

> Recent advancements in large language models (LLMs) have revolutionized natural language processing (NLP) and expanded their applications across diverse domains. However, despite their impressive capabilities, LLMs have been shown to reflect and perpetuate harmful societal biases, including those based on ethnicity, gender, and religion. A critical and underexplored issue is the reinforcement of caste-based biases, particularly towards India's marginalized caste groups such as Dalits and Shudras. In this paper, we address this gap by proposing DECASTE, a novel, multi-dimensional framework designed to detect and assess both implicit and explicit caste biases in LLMs. Our approach evaluates caste fairness across four dimensions: socio-cultural, economic, educational, and political, using a range of customized prompting strategies. By benchmarking several state-of-the-art LLMs, we reveal that these models systematically reinforce caste biases, with significant disparities observed in the treatment of oppressed versus dominant caste groups. For example, bias scores are notably elevated when comparing Dalits and Shudras with dominant caste groups, reflecting societal prejudices that persist in model outputs. These results expose the subtle yet pervasive caste biases in LLMs and emphasize the need for more comprehensive and inclusive bias evaluation methodologies that assess the potential risks of deploying such models in real-world contexts.

[Arxiv](https://arxiv.org/abs/2505.14971)