# 动态游戏平台 KORGym：专为 LLM 推理能力评测打造

发布时间：2025年05月20日

`LLM应用` `人工智能` `评估平台`

> KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation

# 摘要

> 大型语言模型（LLMs）的最新进展凸显了更全面评估方法的重要性，以准确衡量其推理能力。现有基准测试通常局限于特定领域，无法充分捕捉LLM的通用推理潜力。为应对这一局限，我们推出了Knowledge Orthogonal Reasoning Gymnasium（KORGym），一个受KOR-Bench和Gymnasium启发的动态评估平台。KORGym提供超过五十款以文本或视觉形式呈现的游戏，并支持结合强化学习场景的互动式多轮评估。借助KORGym，我们对19个LLM和8个VLM进行了全面实验，揭示了模型家族内部一致的推理模式，并展示了闭源模型的优越性能。进一步分析探讨了模态、推理策略、强化学习技术及响应长度对模型表现的影响。我们期望KORGym能成为推动LLM推理研究与开发适用于复杂互动环境评估方法的宝贵资源。

> Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.

[Arxiv](https://arxiv.org/abs/2505.14552)