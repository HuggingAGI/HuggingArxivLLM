# 迷失在基准测试中？用项目反应理论重新思考大型语言模型的评估方法。本研究提出了一种基于IRT（项目反应理论）的新颖评估框架，旨在更全面地衡量LLMs的能力。通过IRT分析，我们揭示了传统基准测试在评估模型性能时的局限性，并提出了一种更科学的评估方法。

发布时间：2025年05月20日

`LLM理论` `评估方法` `基准测试`

> Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory

# 摘要

> 尽管通过基准测试评估大型语言模型（LLMs）的做法已十分普遍，但不同排行榜间的不一致以及顶尖模型间的区分度不足，引发了对其能否准确反映真实模型能力的质疑。本文通过对主流知名LLM基准测试的深入分析，探讨了基准测试的有效性。我们提出了一种全新的框架，用于更准确可靠地评估题目特征和模型能力。具体而言，我们开发了基于伪孪生网络的项目反应理论（PSN-IRT），这是一个增强版的项目反应理论框架，能够在架构中整合丰富的题目参数。借助PSN-IRT，我们进行了广泛分析，揭示了现有基准测试在测量质量方面存在显著且多样的缺陷。更重要的是，我们证明，通过PSN-IRT可以构建规模更小但与人类偏好更一致的基准测试。

> The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining main-stream prominent LLM benchmarks using results from diverse models. We first propose a new framework for accurate and reliable estimations of item characteristics and model abilities. Specifically, we propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive analysis which reveals significant and varied shortcomings in the measurement quality of current benchmarks. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.

[Arxiv](https://arxiv.org/abs/2505.15055)