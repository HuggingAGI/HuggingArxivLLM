# 大型语言模型真的能够识别你的名字吗？

发布时间：2025年05月20日

`LLM应用

论文摘要：大型语言模型（LLMs）正被越来越多地用于保护用户隐私，但现有基于LLMs的隐私解决方案假设这些模型能可靠识别个人身份信息（PII），尤其是人名。本文挑战这一假设，揭示了基于LLMs的隐私任务中系统性失败的现象。我们发现，现代LLMs在短文本中经常忽视人名，原因在于模糊上下文导致这些姓名被误读或误处理。为此，我们提出了AMBENCH——一个看似模糊的人类姓名基准数据集，利用名称规则偏见现象，将其嵌入简短文本片段，并辅以良性提示注入。实验表明，与易识别姓名相比，模糊姓名的召回率下降20--40%。更值得关注的是，当存在良性提示注入时，模糊姓名在LLMs生成的隐私保护摘要中被忽视的可能性高出四倍。这些发现凸显了仅依赖LLMs保障隐私的潜在风险，强调了对隐私失败模式进行更系统性研究的必要性。

LLM应用` `隐私保护`

> Can Large Language Models Really Recognize Your Name?

# 摘要

> 大型语言模型（LLMs）正被越来越多地用于保护用户隐私，但现有基于LLMs的隐私解决方案假设这些模型能可靠识别个人身份信息（PII），尤其是人名。本文挑战这一假设，揭示了基于LLMs的隐私任务中系统性失败的现象。我们发现，现代LLMs在短文本中经常忽视人名，原因在于模糊上下文导致这些姓名被误读或误处理。为此，我们提出了AMBENCH——一个看似模糊的人类姓名基准数据集，利用名称规则偏见现象，将其嵌入简短文本片段，并辅以良性提示注入。实验表明，与易识别姓名相比，模糊姓名的召回率下降20--40%。更值得关注的是，当存在良性提示注入时，模糊姓名在LLMs生成的隐私保护摘要中被忽视的可能性高出四倍。这些发现凸显了仅依赖LLMs保障隐私的潜在风险，强调了对隐私失败模式进行更系统性研究的必要性。

> Large language models (LLMs) are increasingly being used to protect sensitive user data. However, current LLM-based privacy solutions assume that these models can reliably detect personally identifiable information (PII), particularly named entities. In this paper, we challenge that assumption by revealing systematic failures in LLM-based privacy tasks. Specifically, we show that modern LLMs regularly overlook human names even in short text snippets due to ambiguous contexts, which cause the names to be misinterpreted or mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous human names, leveraging the name regularity bias phenomenon, embedded within concise text snippets along with benign prompt injections. Our experiments on modern LLMs tasked to detect PII as well as specialized tools show that recall of ambiguous names drops by 20--40% compared to more recognizable names. Furthermore, ambiguous human names are four times more likely to be ignored in supposedly privacy-preserving summaries generated by LLMs when benign prompt injections are present. These findings highlight the underexplored risks of relying solely on LLMs to safeguard user privacy and underscore the need for a more systematic investigation into their privacy failure modes.

[Arxiv](https://arxiv.org/abs/2505.14549)