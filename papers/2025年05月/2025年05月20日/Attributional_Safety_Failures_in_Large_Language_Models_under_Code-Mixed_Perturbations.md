# 大型语言模型面对混合编码扰动时的归因安全故障

发布时间：2025年05月20日

`LLM应用`

> Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations

# 摘要

> 大型语言模型（LLMs）的最新进展引发了显著的安全担忧，特别是在处理代码混合输入和输出时。本研究系统性地探讨了与单语英语提示相比，LLMs在代码混合提示下产生不安全输出的易感性增加。通过采用解释性方法，我们剖析了导致模型有害行为的内部归因变化。此外，我们还通过区分普遍不安全和文化特定不安全查询，探索了文化维度。本文提出了新颖的实验见解，阐明了驱动这一现象的机制。

> Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.

[Arxiv](https://arxiv.org/abs/2505.14469)