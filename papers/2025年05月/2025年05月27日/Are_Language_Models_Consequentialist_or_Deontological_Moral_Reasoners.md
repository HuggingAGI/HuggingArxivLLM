# 语言模型是功利主义者还是义务论者？

发布时间：2025年05月27日

`LLM理论`

> Are Language Models Consequentialist or Deontological Moral Reasoners?

# 摘要

> # 摘要
随着AI系统在医疗、法律和治理等领域的广泛应用，理解其处理复杂道德场景的能力变得至关重要。不同于以往研究主要关注大型语言模型（LLMs）的道德判断，我们聚焦于分析LLMs背后的道德推理过程。通过引入超过600个独特的电车难题作为探针，我们深入揭示了不同LLMs中的推理模式。基于两大规范伦理理论——功利主义和义务论——我们构建了一个系统化的道德推理分类框架。研究发现，LLMs的推理链更倾向于基于道德义务的义务论原则，而事后解释则显著转向强调效用的功利主义推理。这一框架为理解LLMs如何处理和表达道德考量奠定了基础，是实现LLMs在高风险决策环境中安全、可解释部署的关键一步。我们的代码可在https://github.com/keenansamway/moral-lens获取。

> As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .

[Arxiv](https://arxiv.org/abs/2505.21479)