# EasyDistill：一个全面的工具包，实现大型语言模型的有效知识蒸馏。

发布时间：2025年05月27日

`LLM应用` `云计算`

> EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models

# 摘要

> 本文介绍了一个名为EasyDistill的综合性工具包，专为大型语言模型（LLMs）的黑盒与白盒知识蒸馏（KD）设计。该工具包支持多样化的功能，包括数据合成、监督微调、排序优化及强化学习技术，特别针对KD场景进行了优化。无论是System 1（快而直观）还是System 2（慢而分析）模型，EasyDistill都能提供相应的KD支持。凭借模块化设计和用户友好的界面，研究人员和行业从业者可轻松实验并实现前沿的LLMs KD策略。此外，EasyDistill还提供了我们开发的鲁棒蒸馏模型、基于KD的工业解决方案以及开源数据集，满足多种应用场景需求。同时，我们实现了EasyDistill与阿里云AI平台（PAI）的无缝集成。总体而言，EasyDistill工具包使先进的LLMs KD技术在NLP社区中更具可及性和影响力。

> In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.

[Arxiv](https://arxiv.org/abs/2505.20888)