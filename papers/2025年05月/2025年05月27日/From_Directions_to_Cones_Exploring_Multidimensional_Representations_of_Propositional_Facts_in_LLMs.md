# 从方向到锥体：揭示大型语言模型中命题事实的多维表达

发布时间：2025年05月27日

`LLM理论` `信息质量控制` `人工智能`

> From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs

# 摘要

> 大型语言模型（LLMs）虽然拥有强大的对话能力，但生成虚假信息的问题仍然存在。先前研究表明，简单命题的真实性可以表示为模型内部激活中的单一线性方向，但这可能未能完全揭示其潜在的几何结构。在本研究中，我们扩展了近期用于建模拒绝行为的概念锥框架，将其应用到真实性的领域。我们发现，多个维度的概念锥在因果上调节了多个LLM家族中与真实性相关的行为。我们的研究结果得到了三方面的支持：(i)因果干预能可靠地翻转模型对事实陈述的回应，(ii)学习到的概念锥在不同模型架构间具有良好的泛化性，(iii)基于概念锥的干预保留了模型在其他方面的行为。这些发现揭示了简单真/假命题在LLMs中受到的更丰富、多方向结构的控制，并凸显了概念锥作为探索抽象行为的有力工具的潜力。

> Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.

[Arxiv](https://arxiv.org/abs/2505.21800)