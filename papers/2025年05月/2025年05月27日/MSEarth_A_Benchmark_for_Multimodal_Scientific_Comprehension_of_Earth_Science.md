# # MSEarth：专注于地球科学领域的多模态科学理解能力的基准测试

发布时间：2025年05月27日

`LLM应用` `地球科学`

> MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science

# 摘要

> # MSEarth：多模态科学基准的全新突破

多模态大型语言模型 (MLLMs) 的迅猛发展为攻克复杂科学难题带来了全新机遇。然而，尽管取得了诸多进展，MLLMs 在地球科学领域的应用，特别是在研究生层次的研究中，仍未得到充分挖掘。

当前，地球科学领域面临一个关键挑战：缺乏能够准确反映地球科学推理深度和情境复杂性的基准测试。现有的基准测试往往依赖于合成数据集或简单的图表配对，这与真实科学应用中所需的复杂推理和领域知识相去甚远。

为了解决这一难题，我们推出了 MSEarth —— 一个精心打造的多模态科学基准。MSEarth 从高质量开放获取的科学出版物中精选内容，涵盖地球科学的五大核心领域：大气圈、冰冻圈、水圈、岩石圈和生物圈，包含超过 7,000 个图表，并配有精心优化的说明。

MSEarth 的独特之处在于其说明文本。这些说明不仅基于原始图表说明，还融入了论文中的深入讨论和推理，确保基准能够准确捕捉到复杂推理过程和知识密集型内容，这对于高级科学任务的开展至关重要。

MSEarth 支持多种任务类型，包括：
- 科学图表说明
- 多项选择题
- 开放性推理挑战

通过填补研究生水平基准测试的空白，MSEarth 为提升 MLLMs 在科学推理方面的开发和评估提供了一个可扩展且高保真的资源。这一基准测试已向公众开放，旨在推动该领域的进一步研究和创新。

探索 MSEarth 的更多资源，欢迎访问：
- Hugging Face: https://huggingface.co/MSEarth
- GitHub: https://github.com/xiangyu-mm/MSEarth


> The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 7K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field. Resources related to this benchmark can be found at https://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.

[Arxiv](https://arxiv.org/abs/2505.20740)