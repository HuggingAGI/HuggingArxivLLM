# One-shot Entropy Minimization
单次熵最小化方法

发布时间：2025年05月27日

`LLM理论` `机器学习`

> One-shot Entropy Minimization

# 摘要

> 我们训练了13,440个大型语言模型，发现熵最小化只需一个无标签数据和10步优化，即可实现与基于规则的强化学习中使用数千个数据和精心设计奖励所达到的性能提升相媲美甚至更优的效果。这一引人注目的结果可能促使我们重新思考大型语言模型的后训练范式。代码已开源，访问链接：https://github.com/zitian-gao/one-shot-em。

> We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.

[Arxiv](https://arxiv.org/abs/2505.20282)