# # 在多模态大语言模型时代，实现高效无词汇细粒度视觉识别

发布时间：2025年05月02日

`LLM应用

理由：这篇论文探讨了如何将多模态大型语言模型（MLLMs）应用于无词汇表的细粒度视觉识别（VF-FGVR）任务。具体来说，作者提出了一种通过MLLM生成标签并微调下游模型的方法，以解决在缺乏标注数据的情况下进行高效视觉识别的问题。这一研究属于将大型语言模型应用于特定任务的范畴，因此归类为LLM应用。` `计算机视觉`

> Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs

# 摘要

> 细粒度视觉识别（FGVR）在区分视觉相似类别时面临固有挑战，主要源于微小的类间差异和对大规模专业标注数据集的需求。在医学成像等领域，受限于隐私问题和高昂标注成本，难以获得理想的数据集。面对缺乏标注数据的情况，FGVR模型无法依赖预定义的训练标签，其预测输出空间因而无约束。我们将这一任务定义为无词汇表FGVR（VF-FGVR），即模型需在无先验标签信息的情况下从无约束输出空间中进行预测。尽管多模态大型语言模型（MLLMs）在VF-FGVR领域展现出潜力，但因成本和推理时间限制，直接将其应用于每个测试输入并不现实。为解决此问题，我们提出了	extbf{Nea}rest-Neighbor Label 	extbf{R}efinement（NeaR），一种通过MLLM生成标签微调下游CLIP模型的新方法。该方法从小型未标注训练集构建弱监督数据集，并利用MLLM生成标签。NeaR专门针对MLLM生成标签中的噪声、随机性和开放性设计，为高效VF-FGVR设定了新基准。

> Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

[Arxiv](https://arxiv.org/abs/2505.01064)