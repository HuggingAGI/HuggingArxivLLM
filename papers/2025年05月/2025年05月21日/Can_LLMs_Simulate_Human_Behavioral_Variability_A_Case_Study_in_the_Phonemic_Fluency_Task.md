# 大型语言模型能模拟人类行为变异性吗？一项语音流畅性任务的案例研究

发布时间：2025年05月21日

`LLM应用` `认知科学` `人工智能`

> Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task

# 摘要

> 大型语言模型（LLMs）正越来越多地被研究作为认知任务中人类参与者的替代方案，但它们在模拟人类行为变异性方面的表现仍有待明确。本研究探讨了LLMs是否能够模拟音位流畅性任务中的个体差异，其中参与者需要生成以目标字母开头的单词。我们评估了34种不同的模型配置，涉及提示的特异性、采样的温度以及模型类型，并将模型输出与106名人类参与者的回答进行了比较。尽管某些配置，尤其是Claude 3.7 Sonnet，能够接近人类的平均表现和词汇偏好，但没有一种配置能够完全再现人类行为的多样性。LLM的输出始终缺乏多样性，结构上也较为僵化，且通过集成多个LLM也无法提升多样性。网络分析进一步揭示了人类与模型在检索结构上的根本差异。这些结果突显了使用LLMs模拟人类认知和行为的关键局限性。


> Large language models (LLMs) are increasingly explored as substitutes for human participants in cognitive tasks, but their ability to simulate human behavioral variability remains unclear. This study examines whether LLMs can approximate individual differences in the phonemic fluency task, where participants generate words beginning with a target letter. We evaluated 34 model configurations, varying prompt specificity, sampling temperature, and model type, and compared outputs to responses from 106 human participants. While some configurations, especially Claude 3.7 Sonnet, matched human averages and lexical preferences, none reproduced the scope of human variability. LLM outputs were consistently less diverse and structurally rigid, and LLM ensembles failed to increase diversity. Network analyses further revealed fundamental differences in retrieval structure between humans and models. These results highlight key limitations in using LLMs to simulate human cognition and behavior.

[Arxiv](https://arxiv.org/abs/2505.16164)