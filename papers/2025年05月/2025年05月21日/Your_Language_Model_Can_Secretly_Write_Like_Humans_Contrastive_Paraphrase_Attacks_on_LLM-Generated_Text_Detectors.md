# 你的语言模型竟能悄悄写出人类风格的文章：对比式改写攻击如何绕过LLM文本检测器

发布时间：2025年05月21日

`LLM应用`

> Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors

# 摘要

> 大型语言模型（LLMs）的滥用，如学术剽窃，催生了检测器来识别由LLMs生成的文本。为了绕过这些检测器，改写攻击应运而生，通过故意重写文本以逃避检测。尽管已有方法在一定程度上取得成功，但它们需要大量数据和计算资源来训练专用改写器，且在面对先进检测算法时，其攻击效果显著下降。为此，我们提出了**对比式改写攻击（CoPA）**，这是一种无需训练的方法，能够利用现成的LLMs有效欺骗文本检测器。首先，我们精心设计指令，促使LLMs生成更具人似性的文本。然而，LLMs固有的统计偏见可能导致某些生成文本带有机器化特征，这些特征可能被检测器捕捉到。为克服这一问题，CoPA构建了一个辅助的机器化词汇分布，与LLM生成的人似分布形成对比。通过在解码过程中从人似分布中减去机器化模式，CoPA能够生成更难以被文本检测器分辨的句子。理论分析表明，CoPA在攻击效果上具有显著优势。大量实验验证了CoPA在各种场景下欺骗文本检测器的有效性。

> The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \textbf{Co}ntrastive \textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.

[Arxiv](https://arxiv.org/abs/2505.15337)