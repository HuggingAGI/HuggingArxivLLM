# 文化价值观对齐在大型语言模型中的研究：基于提示的 Schwartz 价值观分析，应用于 Gemini、ChatGPT 和 DeepSeek

发布时间：2025年05月21日

`LLM理论

论文摘要：本研究通过分析 Gemini、ChatGPT 和 DeepSeek 对施瓦茨价值观框架中价值观的优先排序，探讨大型语言模型 (LLMs) 中的文化价值观对齐问题。我们使用包含 40 个项目的《人物价值观问卷》，评估了经过中文数据训练的 DeepSeek 是否在价值观偏好上与西方模型存在显著差异。贝叶斯序贯回归模型的结果显示，自我超越价值观（如利他主义、普世主义）在所有模型中均被高度优先考虑，这反映了大型语言模型普遍倾向于强调亲社会价值观。然而，与 ChatGPT 和 Gemini 相比，DeepSeek 独特地降低了自我提升价值观（如权力、成就）的优先级，这与集体主义文化倾向相一致。这些发现表明，大型语言模型反映了文化特定的偏见，而非遵循普遍的伦理框架。为解决大型语言模型中的价值观失衡问题，我们提出了多角度推理、自我反思反馈和动态情境化的方法。这项研究为人工智能公平性、文化中立性以及构建整合多元道德视角的多元人工智能对齐框架的讨论做出了贡献。` `人工智能`

> Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek

# 摘要

> 本研究通过分析 Gemini、ChatGPT 和 DeepSeek 对施瓦茨价值观框架中价值观的优先排序，探讨大型语言模型 (LLMs) 中的文化价值观对齐问题。我们使用包含 40 个项目的《人物价值观问卷》，评估了经过中文数据训练的 DeepSeek 是否在价值观偏好上与西方模型存在显著差异。贝叶斯序贯回归模型的结果显示，自我超越价值观（如利他主义、普世主义）在所有模型中均被高度优先考虑，这反映了大型语言模型普遍倾向于强调亲社会价值观。然而，与 ChatGPT 和 Gemini 相比，DeepSeek 独特地降低了自我提升价值观（如权力、成就）的优先级，这与集体主义文化倾向相一致。这些发现表明，大型语言模型反映了文化特定的偏见，而非遵循普遍的伦理框架。为解决大型语言模型中的价值观失衡问题，我们提出了多角度推理、自我反思反馈和动态情境化的方法。这项研究为人工智能公平性、文化中立性以及构建整合多元道德视角的多元人工智能对齐框架的讨论做出了贡献。

> This study examines cultural value alignment in large language models (LLMs) by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct value preferences compared to Western models. Results of a Bayesian ordinal regression model show that self-transcendence values (e.g., benevolence, universalism) were highly prioritized across all models, reflecting a general LLM tendency to emphasize prosocial values. However, DeepSeek uniquely downplayed self-enhancement values (e.g., power, achievement) compared to ChatGPT and Gemini, aligning with collectivist cultural tendencies. These findings suggest that LLMs reflect culturally situated biases rather than a universal ethical framework. To address value asymmetries in LLMs, we propose multi-perspective reasoning, self-reflective feedback, and dynamic contextualization. This study contributes to discussions on AI fairness, cultural neutrality, and the need for pluralistic AI alignment frameworks that integrate diverse moral perspectives.

[Arxiv](https://arxiv.org/abs/2505.17112)