# 工具嵌入循环：量化大型语言模型问答系统在工具使用中的不确定性

发布时间：2025年05月21日

`LLM应用` `问答系统`

> Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools

# 摘要

> 现代大型语言模型（LLMs）在面对知识盲区时，往往需要借助外部工具（如机器学习分类器或知识检索系统）来提供准确答案。这种结合虽然扩展了LLMs的应用范围，但也带来了新的挑战：如何判断系统输出结果的可信度。在医疗决策等高风险场景中，我们不仅要评估LLM生成文本的不确定性，还要考量外部工具输出结果的可靠性，以确保最终响应的可信度。然而，现有的不确定性量化方法并未考虑到工具调用场景下，LLM和外部工具共同作用带来的系统不确定性。为此，我们提出了一种全新的框架，用于建模工具调用型LLMs，通过综合考量LLM和外部工具的预测不确定性来量化整体不确定性。我们扩展了先前针对token序列不确定性量化的方法，并提出了高效的近似算法，使不确定性计算在实际应用中成为可能。我们在两个全新的合成问答数据集上评估了我们的框架，这些数据集源自知名机器学习数据集，需要工具调用才能获得准确答案。此外，我们将方法应用于增强型检索生成（RAG）系统，并通过概念验证实验展示了在需要外部信息检索的场景下，我们的不确定性指标的有效性。实验结果表明，本框架能够有效提升基于LLM系统的可信度，特别是在LLM内部知识不足且需要借助外部工具的情况下。


> Modern Large Language Models (LLMs) often require external tools, such as machine learning classifiers or knowledge retrieval systems, to provide accurate answers in domains where their pre-trained knowledge is insufficient. This integration of LLMs with external tools expands their utility but also introduces a critical challenge: determining the trustworthiness of responses generated by the combined system. In high-stakes applications, such as medical decision-making, it is essential to assess the uncertainty of both the LLM's generated text and the tool's output to ensure the reliability of the final response. However, existing uncertainty quantification methods do not account for the tool-calling scenario, where both the LLM and external tool contribute to the overall system's uncertainty. In this work, we present a novel framework for modeling tool-calling LLMs that quantifies uncertainty by jointly considering the predictive uncertainty of the LLM and the external tool. We extend previous methods for uncertainty quantification over token sequences to this setting and propose efficient approximations that make uncertainty computation practical for real-world applications. We evaluate our framework on two new synthetic QA datasets, derived from well-known machine learning datasets, which require tool-calling for accurate answers. Additionally, we apply our method to retrieval-augmented generation (RAG) systems and conduct a proof-of-concept experiment demonstrating the effectiveness of our uncertainty metrics in scenarios where external information retrieval is needed. Our results show that the framework is effective in enhancing trust in LLM-based systems, especially in cases where the LLM's internal knowledge is insufficient and external tools are required.

[Arxiv](https://arxiv.org/abs/2505.16113)