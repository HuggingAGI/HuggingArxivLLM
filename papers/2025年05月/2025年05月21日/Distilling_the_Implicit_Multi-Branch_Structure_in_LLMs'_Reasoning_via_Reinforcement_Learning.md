# 通过强化学习萃取大型语言模型推理中的多分支结构
揭示LLMs推理中的隐式多分支结构，助力提升模型推理能力与可解释性

发布时间：2025年05月21日

`LLM理论

摘要讨论了通过监督微调从教师模型向学生模型蒸馏推理路径的方法，并提出了一个新的框架RLKD，结合强化学习和生成结构奖励模型来提升小型LLMs的推理能力。该工作主要集中在改进蒸馏方法和推理结构的优化，属于LLM理论的研究。` `人工智能` `机器学习`

> Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning

# 摘要

> 通过监督微调 (SFT) 从教师模型向学生模型蒸馏推理路径，为提升小型大型语言模型 (LLMs) 的推理能力提供了一种快捷方式。然而，教师模型生成的推理路径通常仅反映其底层真实推理的表面痕迹。认知神经科学研究表明，真实推理涉及元推理（从多个候选中选择合适子问题）与问题解决（处理子问题）之间的复杂交织，这表明真实推理具有隐含的多分支结构。然而，监督微调会将这种丰富的结构压缩为教师推理路径中令牌预测的平坦序列，阻碍了这一结构的有效蒸馏。为解决这一限制，我们提出了 RLKD，一种由新型生成结构奖励模型 (GSRM) 引导的基于强化学习 (RL) 的蒸馏框架。GSRM 将推理路径分解为多个元推理-解决步骤，并通过计算奖励衡量学生与教师推理的结构对齐。RLKD 将此奖励与 RL 结合，使学生 LLM 不仅能够模仿固定输出路径，更能内化教师的隐含多分支推理结构。实验结果表明，即使在仅基于 RL 的训练模式下使用 0.1% 的数据，RLKD 也超越了标准的 SFT-RL 管道，展现出比基于 SFT 的蒸馏更大的学生推理潜力。

> Distilling reasoning paths from teacher to student models via supervised fine-tuning (SFT) provides a shortcut for improving the reasoning ability of smaller Large Language Models (LLMs). However, the reasoning paths generated by teacher models often reflect only surface-level traces of their underlying authentic reasoning. Insights from cognitive neuroscience suggest that authentic reasoning involves a complex interweaving between meta-reasoning (which selects appropriate sub-problems from multiple candidates) and solving (which addresses the sub-problem). This implies authentic reasoning has an implicit multi-branch structure. Supervised fine-tuning collapses this rich structure into a flat sequence of token prediction in the teacher's reasoning path, preventing effective distillation of this structure to students. To address this limitation, we propose RLKD, a reinforcement learning (RL)-based distillation framework guided by a novel Generative Structure Reward Model (GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving steps and computes rewards to measure structural alignment between student and teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to internalize the teacher's implicit multi-branch reasoning structure rather than merely mimicking fixed output paths. Experiments show RLKD surpasses standard SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime, unlocking greater student reasoning potential than SFT-based distillation.

[Arxiv](https://arxiv.org/abs/2505.16142)