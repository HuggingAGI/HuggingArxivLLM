# 大型语言模型中的锚定效应研究：实证分析、机制探讨与缓解策略

发布时间：2025年05月21日

`LLM理论` `认知偏见`

> An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations

# 摘要

> 大型语言模型（LLMs）如ChatGPT的崛起推动了自然语言处理的发展，但对认知偏见的担忧也在日益增加。本文聚焦于“锚定效应”这一认知偏见，即人们往往过度依赖初始信息作为判断依据的现象。我们深入探讨了LLMs是否也会受到这种效应的影响，其背后的原因，以及可能的缓解方法。为了更系统地研究锚定效应，我们开发了一个全新数据集——SynAnchors。结合优化的评估指标，我们对目前广受欢迎的LLMs进行了全面测试。研究发现，LLMs普遍存在着“浅层锚定偏见”，常规缓解手段效果有限，但通过推理可以一定程度上减轻这一问题。这一发现提醒我们，评估LLMs时不应仅依赖标准基准测试或过度优化的鲁棒性检验，而是需要建立更关注认知偏见的可信评估体系。

> The rise of Large Language Models (LLMs) like ChatGPT has advanced natural language processing, yet concerns about cognitive biases are growing. In this paper, we investigate the anchoring effect, a cognitive bias where the mind relies heavily on the first information as anchors to make affected judgments. We explore whether LLMs are affected by anchoring, the underlying mechanisms, and potential mitigation strategies. To facilitate studies at scale on the anchoring effect, we introduce a new dataset, SynAnchors. Combining refined evaluation metrics, we benchmark current widely used LLMs. Our findings show that LLMs' anchoring bias exists commonly with shallow-layer acting and is not eliminated by conventional strategies, while reasoning can offer some mitigation. This recontextualization via cognitive psychology urges that LLM evaluations focus not on standard benchmarks or over-optimized robustness tests, but on cognitive-bias-aware trustworthy evaluation.

[Arxiv](https://arxiv.org/abs/2505.15392)