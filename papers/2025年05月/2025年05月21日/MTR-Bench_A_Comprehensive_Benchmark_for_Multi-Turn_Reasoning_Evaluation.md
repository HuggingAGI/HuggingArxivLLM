# MTR-Bench：全面的多轮推理评估基准测试工具

发布时间：2025年05月21日

`LLM应用

论文摘要讨论了大型语言模型在多轮推理任务中的应用，并提出了一种评估框架MTR-Bench，专注于评估模型的多轮推理能力。这属于将大型语言模型应用于特定任务的范畴，因此归类为LLM应用。` `人工智能` `评估体系`

> MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation

# 摘要

> 大型语言模型（LLMs）在复杂推理任务中展现了令人瞩目的潜力。然而，当前的评估主要集中在单轮推理场景，而对交互式任务的探索却相对较少。我们将其归因于缺乏全面的数据集和可扩展的自动评估协议。为了解决这一问题，我们提出了MTR-Bench，专注于评估LLMs的多轮推理能力。MTR-Bench涵盖4个类别、40个任务和3600个实例，不仅包含了多样化的推理能力，还设置了精细的难度级别，并要求与环境进行多轮交互。此外，MTR-Bench提供了一个完全自动化的框架，从数据集构建到模型评估，实现了无需人工干预的可扩展评估。通过大量实验我们发现，即使是前沿的推理模型也难以应对多轮交互推理任务。对这些结果的深入分析为未来交互式AI系统的研究提供了宝贵的启示。

> Recent advances in Large Language Models (LLMs) have shown promising results in complex reasoning tasks. However, current evaluations predominantly focus on single-turn reasoning scenarios, leaving interactive tasks largely unexplored. We attribute it to the absence of comprehensive datasets and scalable automatic evaluation protocols. To fill these gaps, we present MTR-Bench for LLMs' Multi-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600 instances, MTR-Bench covers diverse reasoning capabilities, fine-grained difficulty granularity, and necessitates multi-turn interactions with the environments. Moreover, MTR-Bench features fully-automated framework spanning both dataset constructions and model evaluations, which enables scalable assessment without human interventions. Extensive experiments reveal that even the cutting-edge reasoning models fall short of multi-turn, interactive reasoning tasks. And the further analysis upon these results brings valuable insights for future research in interactive AI systems.

[Arxiv](https://arxiv.org/abs/2505.17123)