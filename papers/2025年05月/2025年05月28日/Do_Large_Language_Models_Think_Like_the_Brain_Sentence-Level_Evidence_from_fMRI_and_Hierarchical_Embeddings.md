# 大型语言模型是否像大脑一样思考？句子级别的证据来自fMRI和层次嵌入

发布时间：2025年05月28日

`LLM理论

理由：这篇论文探讨了大型语言模型（LLMs）的神经机制与人类大脑的对齐情况，属于对模型内部工作原理和理论的深入研究。` `认知神经科学` `人工智能`

> Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings

# 摘要

> 大型语言模型（LLMs）与人类大脑是否遵循相似的计算原则，仍是认知神经科学和AI领域的重要课题。LLMs中出现的类似大脑的模式，是单纯由规模效应产生，还是反映了与人类语言处理架构的深层对齐？本研究专注于语言模型的句子层面神经机制，系统性地探究了LLMs中的层级表示与人类句子理解过程中动态神经反应的对齐情况。通过对比14个公开LLMs的层级嵌入与参与者在接触自然叙事故事时的fMRI数据，我们构建了句子层面的神经预测模型，精准识别出与大脑区域激活高度相关的模型层。研究结果表明，模型性能的提升推动了表示架构向大脑类层级的演变，尤其在更高语义抽象水平上实现了更强的功能和解剖对应关系。

> Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in LLMs align with the dynamic neural responses during human sentence comprehension. By comparing hierarchical embeddings from 14 publicly available LLMs with fMRI data collected from participants, who were exposed to a naturalistic narrative story, we constructed sentence-level neural prediction models to precisely identify the model layers most significantly correlated with brain region activations. Results show that improvements in model performance drive the evolution of representational architectures toward brain-like hierarchies, particularly achieving stronger functional and anatomical correspondence at higher semantic abstraction levels.

[Arxiv](https://arxiv.org/abs/2505.22563)