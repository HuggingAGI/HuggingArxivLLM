# 探究大型语言模型中的AI评分者效应：以GPT、Claude、Gemini和DeepSeek为例

发布时间：2025年05月28日

`LLM应用` `评分系统`

> Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model

# 摘要

> 大型语言模型（LLMs）在低风险评估中的自动评分应用已被广泛探索，以促进学习和教学。在实际应用中使用LLMs进行自动评分之前，需要收集哪些LLM能产生最可靠评分并引发最少评分者效应的实证证据。本研究将十种LLMs（包括ChatGPT 3.5、ChatGPT 4、ChatGPT 4o、OpenAI o1、Claude 3.5 Sonnet、Gemini 1.5、Gemini 1.5 Pro、Gemini 2.0、DeepSeek V3和DeepSeek R1）与人类专家评分者在两种写作任务的评分上进行了比较。通过Quadratic Weighted Kappa评估了LLMs的总体和分析评分与人类评分者的一致性。采用Cronbach Alpha比较了不同提示下的评分者一致性。利用Many-Facet Rasch模型评估并比较了LLMs的评分者效应与人类评分者。总体结果支持使用ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet，因其具有高评分准确性、更好的评分者可靠性和较少的评分者效应。

> Large language models (LLMs) have been widely explored for automated scoring in low-stakes assessment to facilitate learning and instruction. Empirical evidence related to which LLM produces the most reliable scores and induces least rater effects needs to be collected before the use of LLMs for automated scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4, ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini 2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in scoring two types of writing tasks. The accuracy of the holistic and analytic scores from LLMs compared with human raters was evaluated in terms of Quadratic Weighted Kappa. Intra-rater consistency across prompts was compared in terms of Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human raters using the Many-Facet Rasch model. The results in general supported the use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring accuracy, better rater reliability, and less rater effects.

[Arxiv](https://arxiv.org/abs/2505.18486)