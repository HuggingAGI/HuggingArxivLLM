# VIRAL: 视觉引导的奖励设计与学习整合

发布时间：2025年05月28日

`LLM应用` `人工智能` `机器学习`

> VIRAL: Vision-grounded Integration for Reward design And Learning

# 摘要

> 人机对齐是当前人工智能领域的重要挑战。强化学习旨在最大化奖励函数，但其对设计欠佳的奖励函数存在显著风险。近期研究表明，大型语言模型（LLMs）在奖励生成方面已超越人类水平。我们推出VIRAL，一个利用多模态LLMs生成和优化奖励函数的流水线。VIRAL基于给定环境和目标提示或标注图像，自主创建并交互式改进奖励函数。优化过程可整合人类反馈，或由视频LLM生成的描述引导，该描述以视频形式解释代理策略。我们在五个Gymnasium环境中评估了VIRAL，结果表明它不仅加速新行为学习，还显著提升了与用户意图的对齐。源代码和演示视频已开放获取，详情请访问：https://github.com/VIRAL-UCBL1/VIRAL 和 https://youtu.be/t4_BXugBm9Q。


> The alignment between humans and machines is a critical challenge in artificial intelligence today. Reinforcement learning, which aims to maximize a reward function, is particularly vulnerable to the risks associated with poorly designed reward functions. Recent advancements has shown that Large Language Models (LLMs) for reward generation can outperform human performance in this context. We introduce VIRAL, a pipeline for generating and refining reward functions through the use of multi-modal LLMs. VIRAL autonomously creates and interactively improves reward functions based on a given environment and a goal prompt or annotated image. The refinement process can incorporate human feedback or be guided by a description generated by a video LLM, which explains the agent's policy in video form. We evaluated VIRAL in five Gymnasium environments, demonstrating that it accelerates the learning of new behaviors while ensuring improved alignment with user intent. The source-code and demo video are available at: https://github.com/VIRAL-UCBL1/VIRAL and https://youtu.be/t4_BXugBm9Q.

[Arxiv](https://arxiv.org/abs/2505.22092)