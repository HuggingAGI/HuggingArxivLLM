# 借助推理型LLMs评审科学论文中的关键问题：探索基线方法与自动评估方案

发布时间：2025年05月28日

`LLM应用` `学术出版` `人工智能`

> Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation

# 摘要

> 大型语言模型的近期进展激发了人们利用其辅助科学出版物的同行评审流程的兴趣。与让AI模型像人类审稿人那样生成评审意见不同，我们提出将其作为手稿质量检查工具。我们引入了几个基线方法，并构建了一个可扩展的自动评估框架，利用顶尖的大型语言模型作为评审员，以解决手动评估中招募领域专家的难题。我们利用从arXiv撤回的论文，通过多家供应商提供的领先推理型大型语言模型验证了我们的方法，并评估了它们在识别关键错误和不严谨问题上的性能和API成本。在我们的评估中，OpenAI的o3模型表现最佳，而o4-mini则是成本效益最高的选择。本文为基于文档的科学理解/推理提供了见解，并为未来应用奠定了基础。

> Recent advancements in large language models have sparked interest in utilizing them to assist the peer review process of scientific publication. Instead of having AI models generate reviews in the same way as human reviewers, we propose adopting them as manuscript quality checkers. We introduce several baseline approaches and an extendable automatic evaluation framework using top LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from different providers and assessed their performance and API costs for identifying critical errors and unsoundness problems. The OpenAI o3 model performed the best, while o4-mini was the most cost-effective one in our evaluation. This paper provides insights into document-based scientific understanding/reasoning and lays the foundation for future applications.

[Arxiv](https://arxiv.org/abs/2505.23824)