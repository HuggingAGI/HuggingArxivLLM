# 以游戏为探针：评估大型语言模型概念知识的游戏化基准评测

发布时间：2025年05月23日

`LLM应用` `人工智能` `交互系统`

> Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs

# 摘要

> 概念是人类高效分类和推理的普遍抽象，但大型语言模型（LLMs）对这些语义关系的理解程度仍是个谜。现有评估基准大多专注于事实记忆和孤立任务，忽视了模型对概念边界的理解能力。为填补这一空白，我们推出了CK-Arena——一个基于Undercover游戏打造的多智能体交互平台，专门用于评测LLMs在动态交互环境中运用概念进行推理的能力。通过要求模型基于部分信息描述、区分和推断概念边界，CK-Arena鼓励模型深入探究密切相关概念间的共性与差异。凭借对现实交互的模拟，CK-Arena为评估动态环境中的概念推理提供了一个既可扩展又贴近真实的评测基准。实验结果揭示，LLMs对概念知识的理解存在显著的类别差异，且与其参数规模或通用能力并无必然关联。更多数据和代码请访问项目官网：https://ck-arena.site。


> Concepts represent generalized abstractions that enable humans to categorize and reason efficiently, yet it is unclear to what extent Large Language Models (LLMs) comprehend these semantic relationships. Existing benchmarks typically focus on factual recall and isolated tasks, failing to evaluate the ability of LLMs to understand conceptual boundaries. To address this gap, we introduce CK-Arena, a multi-agent interaction game built upon the Undercover game, designed to evaluate the capacity of LLMs to reason with concepts in interactive settings. CK-Arena challenges models to describe, differentiate, and infer conceptual boundaries based on partial information, encouraging models to explore commonalities and distinctions between closely related concepts. By simulating real-world interaction, CK-Arena provides a scalable and realistic benchmark for assessing conceptual reasoning in dynamic environments. Experimental results show that LLMs' understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with parameter size or general model capabilities. The data and code are available at the project homepage: https://ck-arena.site.

[Arxiv](https://arxiv.org/abs/2505.17512)