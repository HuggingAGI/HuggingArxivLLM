# # 自信推理驱动的大型语言模型自训练方法

发布时间：2025年05月23日

`LLM理论

理由：这篇论文探讨了大型语言模型（LLMs）的自训练方法，提出了一种新的策略优化方法CORE-PO，用于提升模型的推理路径质量。它专注于改进LLM的训练机制和推理能力，属于对LLM理论的深入研究。` `人工智能` `机器学习`

> Self-Training Large Language Models with Confident Reasoning

# 摘要

> 大型语言模型（LLMs）在生成推理路径和最终答案方面表现出色，但学习这些推理路径需要昂贵的人工监督。为了解决这一问题， recent studies have explored self-training methods that improve reasoning capabilities using pseudo-labels generated by the LLMs themselves. 在这些方法中，基于信心的自训练通过多数投票估计置信度，微调LLMs以偏好具有高置信度答案的推理路径。然而，这类方法仅关注最终答案的质量，可能忽视推理路径的质量，因为即使一个错误的推理路径也可能偶然得到正确答案。相反，我们主张使用推理级别的信心来识别自训练中的高质量推理路径，这一观点得到了我们实证观察的支持。我们随后提出了一种新的自训练方法CORE-PO，通过策略优化微调LLMs，使其偏好高置信度推理路径。实验表明，与现有自训练方法相比，CORE-PO在四个分布内和两个分布外基准上的输出准确性得到了提升。

> Large language models (LLMs) have shown impressive performance by generating reasoning paths before final answers, but learning such a reasoning path requires costly human supervision. To address this issue, recent studies have explored self-training methods that improve reasoning capabilities using pseudo-labels generated by the LLMs themselves. Among these, confidence-based self-training fine-tunes LLMs to prefer reasoning paths with high-confidence answers, where confidence is estimated via majority voting. However, such methods exclusively focus on the quality of the final answer and may ignore the quality of the reasoning paths, as even an incorrect reasoning path leads to a correct answer by chance. Instead, we advocate the use of reasoning-level confidence to identify high-quality reasoning paths for self-training, supported by our empirical observations. We then propose a new self-training method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths through Policy Optimization. Our experiments show that CORE-PO improves the accuracy of outputs on four in-distribution and two out-of-distribution benchmarks, compared to existing self-training methods.

[Arxiv](https://arxiv.org/abs/2505.17454)