# BARREL：边界感知推理，构建事实可靠的大语言模型

发布时间：2025年05月18日

`LLM理论` `人工智能` `大型语言模型`

> BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs

# 摘要

> 近期，大型推理模型（LRMs）在数学和逻辑推理方面展现了卓越的能力。然而，这些模型很少承认自己的局限性或回答"I don't know"，反而常常自信满满地给出错误答案，这引发了对其事实可靠性的担忧。本研究发现，两种由过度思考引起的病理推理模式——最后一刻猜测和二次思考螺旋——是导致这些错误和过度自信回答的原因。为解决这一问题，我们提出了BARREL框架，它通过促进简洁且具有边界意识的事实推理，显著提升了模型的可靠性。实验结果显示，经过BARREL训练后，DeepSeek-R1-Distill-Llama-8B模型的可靠性从39.33%提升至61.48%，同时保持了与基于R1推理数据微调模型相当的准确性。这一成果表明，我们的研究为构建更可靠、更基于事实的系统2型LRMs提供了重要启发。

> Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.

[Arxiv](https://arxiv.org/abs/2505.13529)