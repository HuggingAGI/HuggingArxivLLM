# ExpertSteer：借助专业知识引导大型语言模型

发布时间：2025年05月18日

`LLM应用

摘要中的论文提出了一种新的方法，通过外部专家模型生成引导向量来控制大型语言模型的推理过程。这种方法属于LLM的应用层面，因为它展示了如何在实际任务中改进或增强LLM的行为。` `人工智能`

> ExpertSteer: Intervening in LLMs through Expert Knowledge

# 摘要

> 大型语言模型（LLMs）在多种任务中表现卓越，但引导其在推理过程中遵循期望行为仍具挑战。激活引导通过修改内部激活来控制生成过程，展现潜力。然而，现有方法受限于模型自身生成的引导向量，无法充分利用外部专家模型。为此，我们提出ExpertSteer，一种基于任意专家模型生成引导向量的新方法，可干预任何LLMs。通过四步连贯流程：自动编码器对齐维度、互信息分析确定干预层对、递归特征机器生成向量、推理时应用向量选择性引导，实现跨模型知识转移且无需更新参数。我们在三个LLMs上针对四个领域的15个基准进行全面实验，结果表明ExpertSteer显著超越现有基线，且成本极低。

> Large Language Models (LLMs) exhibit remarkable capabilities across various tasks, yet guiding them to follow desired behaviours during inference remains a significant challenge. Activation steering offers a promising method to control the generation process of LLMs by modifying their internal activations. However, existing methods commonly intervene in the model's behaviour using steering vectors generated by the model itself, which constrains their effectiveness to that specific model and excludes the possibility of leveraging powerful external expert models for steering. To address these limitations, we propose ExpertSteer, a novel approach that leverages arbitrary specialized expert models to generate steering vectors, enabling intervention in any LLMs. ExpertSteer transfers the knowledge from an expert model to a target LLM through a cohesive four-step process: first aligning representation dimensions with auto-encoders to enable cross-model transfer, then identifying intervention layer pairs based on mutual information analysis, next generating steering vectors from the expert model using Recursive Feature Machines, and finally applying these vectors on the identified layers during inference to selectively guide the target LLM without updating model parameters. We conduct comprehensive experiments using three LLMs on 15 popular benchmarks across four distinct domains. Experiments demonstrate that ExpertSteer significantly outperforms established baselines across diverse tasks at minimal cost.

[Arxiv](https://arxiv.org/abs/2505.12313)