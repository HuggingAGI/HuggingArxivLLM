# 理解看似不可理解之物：大型语言模型中可解释人工智能的反思、调查与挑战，迈向人机协同的人工智能

发布时间：2025年05月18日

`LLM应用

摘要主要探讨了大型语言模型（LLMs）在实际应用中的可解释性和负责任使用，属于LLM的应用层面。`

> Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI

# 摘要

> 随着大型语言模型（LLMs）在医疗、法律和教育等敏感领域的广泛应用，对透明、可解释且负责任的人工智能系统的呼声日益高涨。可解释性人工智能（XAI）充当了 LLMs 不透明推理过程与依赖其输出进行高风险决策的多元利益相关者之间的关键接口。本文围绕三个核心问题，对 LLMs 的 XAI 进行全面反思和综述：为什么解释性至关重要？它涉及哪些技术与伦理维度？以及如何在实际部署中发挥其作用？

    我们强调了 LLMs 中解释性四个核心维度：保真性、真实度、合理性和对比性，这些维度共同揭示了关键设计张力，并指导开发既技术严谨又情境恰当的解释策略。文章探讨了 XAI 如何支持知识清晰度、合规性以及不同利益相关者角色和决策场景下的受众特定可理解性。

    我们进一步分析了解释性如何被评估，以及受众敏感型 XAI、机制可解释性、因果推理和自适应解释系统等新兴发展的现状。我们强调从表层透明度向治理就绪设计的转变，识别出确保 LLMs 在复杂社会背景下负责任使用的重大挑战和未来研究方向。我们主张，解释性必须演变为一种 civic infrastructure，促进信任、支持可争议性，并使 AI 系统与机构问责制和以人为本的决策过程相协调。

> As large language models (LLMs) are increasingly deployed in sensitive domains such as healthcare, law, and education, the demand for transparent, interpretable, and accountable AI systems becomes more urgent. Explainable AI (XAI) acts as a crucial interface between the opaque reasoning of LLMs and the diverse stakeholders who rely on their outputs in high-risk decisions. This paper presents a comprehensive reflection and survey of XAI for LLMs, framed around three guiding questions: Why is explainability essential? What technical and ethical dimensions does it entail? And how can it fulfill its role in real-world deployment?
  We highlight four core dimensions central to explainability in LLMs, faithfulness, truthfulness, plausibility, and contrastivity, which together expose key design tensions and guide the development of explanation strategies that are both technically sound and contextually appropriate. The paper discusses how XAI can support epistemic clarity, regulatory compliance, and audience-specific intelligibility across stakeholder roles and decision settings.
  We further examine how explainability is evaluated, alongside emerging developments in audience-sensitive XAI, mechanistic interpretability, causal reasoning, and adaptive explanation systems. Emphasizing the shift from surface-level transparency to governance-ready design, we identify critical challenges and future research directions for ensuring the responsible use of LLMs in complex societal contexts. We argue that explainability must evolve into a civic infrastructure fostering trust, enabling contestability, and aligning AI systems with institutional accountability and human-centered decision-making.

[Arxiv](https://arxiv.org/abs/2505.20305)