# 诠释不可诠释：面向大型语言模型的可解释性AI的反思与展望，迈向以人类为中心的AI

发布时间：2025年05月18日

`LLM理论` `人工智能`

> Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI

# 摘要

> 随着大型语言模型（LLMs）在医疗、法律和教育等敏感领域的广泛应用，对透明、可解释和负责任的人工智能系统的需求日益迫切。可解释的人工智能（XAI）作为连接LLMs不透明推理与依赖其输出进行高风险决策的多元利益相关者的桥梁，具有重要意义。本文围绕三个核心问题，对LLMs的XAI进行了全面的反思和综述：为什么可解释性至关重要？它涉及哪些技术与伦理维度？以及如何在实际部署中发挥其作用？

我们强调了LLMs可解释性中的四个核心维度：保真性、真实性、合理性和对比性，这些维度揭示了关键设计矛盾，并指导了既在技术上 robust 又在情境中恰当的解释策略的开发。本文探讨了XAI如何支持知识清晰度、合规监管以及在不同利益相关者角色和决策场景下的受众特定可理解性。

我们进一步探讨了可解释性如何被评估，以及受众敏感型XAI、机制可解释性、因果推理和自适应解释系统等新兴发展的现状。我们强调了从表面透明度向治理就绪型设计的转变，识别了在复杂社会背景下确保LLMs负责任使用的关键挑战和未来研究方向。我们主张，可解释性必须演变为一种 civic infrastructure，促进信任、使 contestability 成为可能，并使AI系统与制度问责制和以人为本的决策制定保持一致。


> As large language models (LLMs) are increasingly deployed in sensitive domains such as healthcare, law, and education, the demand for transparent, interpretable, and accountable AI systems becomes more urgent. Explainable AI (XAI) acts as a crucial interface between the opaque reasoning of LLMs and the diverse stakeholders who rely on their outputs in high-risk decisions. This paper presents a comprehensive reflection and survey of XAI for LLMs, framed around three guiding questions: Why is explainability essential? What technical and ethical dimensions does it entail? And how can it fulfill its role in real-world deployment?
  We highlight four core dimensions central to explainability in LLMs, faithfulness, truthfulness, plausibility, and contrastivity, which together expose key design tensions and guide the development of explanation strategies that are both technically sound and contextually appropriate. The paper discusses how XAI can support epistemic clarity, regulatory compliance, and audience-specific intelligibility across stakeholder roles and decision settings.
  We further examine how explainability is evaluated, alongside emerging developments in audience-sensitive XAI, mechanistic interpretability, causal reasoning, and adaptive explanation systems. Emphasizing the shift from surface-level transparency to governance-ready design, we identify critical challenges and future research directions for ensuring the responsible use of LLMs in complex societal contexts. We argue that explainability must evolve into a civic infrastructure fostering trust, enabling contestability, and aligning AI systems with institutional accountability and human-centered decision-making.

[Arxiv](https://arxiv.org/abs/2505.20305)