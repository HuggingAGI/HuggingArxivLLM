# # 解构无感：理解不可理解之处——大型语言模型中可解释AI的反思、综述与挑战，迈向以人为中心的AI的思考

发布时间：2025年05月18日

`LLM理论`

> Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI

# 摘要

> 随着大型语言模型（LLMs）在医疗、法律和教育等敏感领域的广泛应用，对透明、可解释且负责任的AI系统的迫切需求日益增加。可解释性AI（XAI）作为LLMs不透明推理与依赖其输出进行高风险决策的多样化利益相关者之间的关键接口，发挥着重要作用。本文围绕三个核心问题，对LLMs的XAI进行了全面的反思与综述：为什么可解释性至关重要？它涉及哪些技术与伦理维度？以及如何在实际部署中实现其作用？

我们强调了LLMs可解释性中四个核心维度：保真性、真实度、合理性和对比性，这四个方面揭示了关键的设计张力，并指导了在技术上合理且情境适宜的解释策略的开发。本文探讨了XAI如何在不同利益相关者角色和决策场景下支持知识清晰性、合规性和受众特定的可理解性。

我们进一步分析了可解释性如何被评估，以及受众敏感型XAI、机制可解释性、因果推理和自适应解释系统等新兴发展的现状。强调从表面透明度向治理就绪设计的转变，我们指出了在复杂社会背景下确保LLMs负责任使用的关键挑战和未来研究方向。我们主张，可解释性必须演变为一种 civic infrastructure，促进信任、支持可争议性，并使AI系统与制度问责制及以人为中心的决策制定相一致。


> As large language models (LLMs) are increasingly deployed in sensitive domains such as healthcare, law, and education, the demand for transparent, interpretable, and accountable AI systems becomes more urgent. Explainable AI (XAI) acts as a crucial interface between the opaque reasoning of LLMs and the diverse stakeholders who rely on their outputs in high-risk decisions. This paper presents a comprehensive reflection and survey of XAI for LLMs, framed around three guiding questions: Why is explainability essential? What technical and ethical dimensions does it entail? And how can it fulfill its role in real-world deployment?
  We highlight four core dimensions central to explainability in LLMs, faithfulness, truthfulness, plausibility, and contrastivity, which together expose key design tensions and guide the development of explanation strategies that are both technically sound and contextually appropriate. The paper discusses how XAI can support epistemic clarity, regulatory compliance, and audience-specific intelligibility across stakeholder roles and decision settings.
  We further examine how explainability is evaluated, alongside emerging developments in audience-sensitive XAI, mechanistic interpretability, causal reasoning, and adaptive explanation systems. Emphasizing the shift from surface-level transparency to governance-ready design, we identify critical challenges and future research directions for ensuring the responsible use of LLMs in complex societal contexts. We argue that explainability must evolve into a civic infrastructure fostering trust, enabling contestability, and aligning AI systems with institutional accountability and human-centered decision-making.

[Arxiv](https://arxiv.org/abs/2505.20305)