# 神经热力学 I：熵力在深度学习与通用表示学习中的作用

发布时间：2025年05月18日

`LLM理论` `人工智能` `机器学习`

> Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning

# 摘要

> 深度学习和大型语言模型中涌现现象的迅速发现，使得解释和理解这些现象的成因变得尤为迫切。我们提出了一种严谨的熵力理论，旨在揭示使用随机梯度下降（SGD）及其变体训练的神经网络的学习动态。基于参数对称性理论和熵损失景观，我们发现，表示学习的核心驱动力源于随机性和离散时间更新所引发的新兴熵力。这些熵力系统性地打破连续参数对称性，同时保留离散对称性，从而引发了一系列梯度平衡现象，这些现象与热系统的等分性质颇为相似。这些现象不仅（a）阐明了AI模型间神经表示的普遍对齐，并为柏拉图表示假设提供了证明，还（b）化解了深度学习优化中尖锐性与平坦性寻求行为看似矛盾的观察结果。通过理论推导与实验验证，我们证实，熵力与对称性破缺的结合是解读深度学习中涌现现象的关键所在。

> With the rapid discovery of emergent phenomena in deep learning and large language models, explaining and understanding their cause has become an urgent need. Here, we propose a rigorous entropic-force theory for understanding the learning dynamics of neural networks trained with stochastic gradient descent (SGD) and its variants. Building on the theory of parameter symmetries and an entropic loss landscape, we show that representation learning is crucially governed by emergent entropic forces arising from stochasticity and discrete-time updates. These forces systematically break continuous parameter symmetries and preserve discrete ones, leading to a series of gradient balance phenomena that resemble the equipartition property of thermal systems. These phenomena, in turn, (a) explain the universal alignment of neural representations between AI models and lead to a proof of the Platonic Representation Hypothesis, and (b) reconcile the seemingly contradictory observations of sharpness- and flatness-seeking behavior of deep learning optimization. Our theory and experiments demonstrate that a combination of entropic forces and symmetry breaking is key to understanding emergent phenomena in deep learning.

[Arxiv](https://arxiv.org/abs/2505.12387)