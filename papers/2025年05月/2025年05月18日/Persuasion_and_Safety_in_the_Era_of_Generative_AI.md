# # 劝说与安全：生成式 AI 时代的新挑战

发布时间：2025年05月18日

`LLM应用` `AI伦理` `AI安全`

> Persuasion and Safety in the Era of Generative AI

# 摘要

> 随着大型语言模型（LLMs）说服能力的不断提升，对其潜在风险的担忧也在增加。欧盟《AI法案》明确禁止那些利用 manipulative 或 deceptive 技巧破坏知情决策的 AI 系统，这凸显了区分理性劝说（基于理性）与操控（利用认知偏见）的重要性。我的博士论文通过构建说服技巧分类体系、创建人工标注的数据集以及评估 LLMs 区分这些方法的能力，填补了该领域实证研究的空白。这项研究不仅为缓解说服型 AI 的风险提供了资源，还推动了生成式 AI 时代关于伦理劝说的讨论，为 AI 安全做出了贡献。

> As large language models (LLMs) achieve advanced persuasive capabilities, concerns about their potential risks have grown. The EU AI Act prohibits AI systems that use manipulative or deceptive techniques to undermine informed decision-making, highlighting the need to distinguish between rational persuasion, which engages reason, and manipulation, which exploits cognitive biases. My dissertation addresses the lack of empirical studies in this area by developing a taxonomy of persuasive techniques, creating a human-annotated dataset, and evaluating LLMs' ability to distinguish between these methods. This work contributes to AI safety by providing resources to mitigate the risks of persuasive AI and fostering discussions on ethical persuasion in the age of generative AI.

[Arxiv](https://arxiv.org/abs/2505.12248)