# RealMath：用于研究级数学评估的语言模型持续性基准测试

发布时间：2025年05月18日

`LLM应用

论文摘要：现有的用于评估大型语言模型（LLMs）数学推理能力的基准测试，主要依赖竞赛题目、正式证明或人工设计的难题，但这些未能捕捉到实际研究环境中遇到的数学本质。我们引入了RealMath，一个直接从研究论文和数学论坛中提取的全新基准测试，用于评估LLMs在真实数学任务中的能力。我们的方法解决了三个关键挑战：如何获取多样化的研究级内容，如何通过可验证的陈述实现可靠的自动化评估，以及如何设计一个持续更新的数据集以降低污染风险。实验结果表明，现有模型在处理研究数学方面的能力，比竞赛题更令人惊讶，尽管在高度挑战性的问题上仍有限制，但它们可能已经可以作为数学家的有用助手。RealMath的代码和数据集已公开可用。

LLM应用` `数学建模`

> RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics

# 摘要

> 现有的用于评估大型语言模型（LLMs）数学推理能力的基准测试，主要依赖竞赛题目、正式证明或人工设计的难题，但这些未能捕捉到实际研究环境中遇到的数学本质。我们引入了RealMath，一个直接从研究论文和数学论坛中提取的全新基准测试，用于评估LLMs在真实数学任务中的能力。我们的方法解决了三个关键挑战：如何获取多样化的研究级内容，如何通过可验证的陈述实现可靠的自动化评估，以及如何设计一个持续更新的数据集以降低污染风险。实验结果表明，现有模型在处理研究数学方面的能力，比竞赛题更令人惊讶，尽管在高度挑战性的问题上仍有限制，但它们可能已经可以作为数学家的有用助手。RealMath的代码和数据集已公开可用。

> Existing benchmarks for evaluating mathematical reasoning in large language models (LLMs) rely primarily on competition problems, formal proofs, or artificially challenging questions -- failing to capture the nature of mathematics encountered in actual research environments. We introduce RealMath, a novel benchmark derived directly from research papers and mathematical forums that assesses LLMs' abilities on authentic mathematical tasks. Our approach addresses three critical challenges: sourcing diverse research-level content, enabling reliable automated evaluation through verifiable statements, and designing a continually refreshable dataset to mitigate contamination risks. Experimental results across multiple LLMs reveal surprising capabilities in handling research mathematics compared to competition problems, suggesting current models may already serve as valuable assistants for working mathematicians despite limitations on highly challenging problems. The code and dataset for RealMath are publicly available.

[Arxiv](https://arxiv.org/abs/2505.12575)