# 泄露机密：利用 CPU 缓存旁路通道从大型语言模型中窃取令牌

发布时间：2025年05月01日

`LLM理论`

> Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models

# 摘要

> 针对共享硬件资源的侧信道攻击正日益威胁数据机密性，尤其在大型语言模型（LLMs）日益普及的背景下。本文介绍了一种名为“Spill The Beans”的创新方法，利用缓存侧信道泄露LLM生成的令牌。通过将攻击进程与目标模型部署在同一硬件上，我们清空并重新加载嵌入层中的嵌入向量，每个令牌对应唯一的嵌入向量。当令牌生成过程中访问这些向量时，会在共享低级缓存中产生可检测的缓存命中。

    一个主要挑战在于LLMs的巨大规模，其计算密集型的特性导致嵌入向量迅速被逐出缓存。我们通过平衡监控令牌数量与泄露信息量来应对这一问题。监控更多令牌会增加潜在词汇泄露，但也会提高因缓存逐出而错失缓存命中的风险；反之，监控较少令牌虽能提高检测可靠性，却会限制词汇覆盖范围。

    通过大量实验，我们证明了利用缓存侧信道从LLMs中泄露令牌的可行性。研究发现揭示了LLM部署中的一种新漏洞，表明即使是复杂模型也易受传统侧信道攻击。我们探讨了这一发现对LLM服务基础设施隐私与安全的影响，并提出了缓解此类威胁的建议。作为概念验证，我们考虑了两个具体攻击场景：实验表明，攻击者通过单次监控可恢复高达80%-90%的高熵API密钥。对于英文文本，我们实现了40%的恢复率。需要注意的是，恢复率高度依赖于监控的令牌集，通过针对更专业的输出领域，这些恢复率可以进一步提升。
    

> Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.
  A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.
  Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.

[Arxiv](https://arxiv.org/abs/2505.00817)