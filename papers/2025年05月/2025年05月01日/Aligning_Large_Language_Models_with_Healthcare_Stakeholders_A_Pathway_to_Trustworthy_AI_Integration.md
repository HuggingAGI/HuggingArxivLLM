# 构建值得信赖的AI整合路径：大型语言模型与医疗领域利益相关者的对齐之道

发布时间：2025年05月01日

`LLM应用` `人工智能`

> Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration

# 摘要

> 大型语言模型（LLMs）的广泛应用提高了医疗相关者偏好与模型输出之间对齐的意识。这种对齐成为有效、安全和负责任地赋能医疗工作流程的关键基础。然而，LLMs 的不同行为可能并不总是与医疗相关者的知识、需求和价值观相匹配。为了实现人与AI的对齐，医疗相关者需要在指导和提升LLMs性能方面发挥关键作用。医疗专业人员必须参与LLMs在医疗领域应用的整个生命周期，包括训练数据整理、模型训练和推理过程。在本综述中，我们探讨了医疗相关者与LLMs之间对齐的方法、工具和应用。我们展示了通过适当增强医疗知识整合、任务理解和人类指导，LLMs可以更好地遵循人类价值观。我们展望了提升人类与LLMs对齐的方法，以构建值得信赖的现实医疗应用。

> The wide exploration of large language models (LLMs) raises the awareness of alignment between healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to empower the healthcare workflow effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not always match with healthcare stakeholders' knowledge, demands, and values. To enable a human-AI alignment, healthcare stakeholders will need to perform essential roles in guiding and enhancing the performance of LLMs. Human professionals must participate in the entire life cycle of adopting LLM in healthcare, including training data curation, model training, and inference. In this review, we discuss the approaches, tools, and applications of alignments between healthcare stakeholders and LLMs. We demonstrate that LLMs can better follow human values by properly enhancing healthcare knowledge integration, task understanding, and human guidance. We provide outlooks on enhancing the alignment between humans and LLMs to build trustworthy real-world healthcare applications.

[Arxiv](https://arxiv.org/abs/2505.02848)