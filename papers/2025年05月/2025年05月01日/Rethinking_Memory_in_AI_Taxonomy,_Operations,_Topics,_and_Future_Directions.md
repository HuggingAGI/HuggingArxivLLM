# 重新审视AI的记忆机制：分类体系、运作机制、研究主题与未来发展

发布时间：2025年05月01日

`LLM理论` `人工智能`

> Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

# 摘要

> 记忆作为AI系统的基石，支撑着大语言模型（LLMs）代理的运行。尽管先前的研究集中于LLMs的记忆应用，但往往忽视了记忆动态中的基础操作。本综述首先将记忆表示划分为参数化、上下文结构化和上下文非结构化三类，并系统性地介绍了六种核心记忆操作：固化、更新、索引、遗忘、检索与压缩。我们进一步将这些操作与长期记忆、长上下文、参数修改及多源记忆等研究领域进行了系统性关联。通过聚焦于原子操作与表示类型的视角，本综述为AI记忆相关的研究、基准数据集及工具提供了一个结构清晰且动态视角的框架，揭示了LLMs代理中功能交互的内在机制，并为未来研究指明了方向。

> Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

[Arxiv](https://arxiv.org/abs/2505.00675)