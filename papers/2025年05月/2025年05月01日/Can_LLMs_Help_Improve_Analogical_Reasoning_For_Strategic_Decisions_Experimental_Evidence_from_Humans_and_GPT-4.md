# LLMs能否助力提升战略决策中的类比推理能力？来自人类与GPT-4的实验证据

发布时间：2025年05月01日

`LLM应用` `人工智能` `战略决策`

> Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4

# 摘要

> 本研究探讨大型语言模型（以GPT4为例）是否能与人类在战略决策情境下的类比推理能力相媲美。通过新颖的实验设计，采用源到目标匹配的方法，我们发现GPT4能够检索到所有合理的类比，从而实现高召回率，但其精确度较低，经常基于表面相似性应用错误的类比。相比之下，人类参与者表现出高精确度但低召回率，尽管选择的类比较少，但因果对齐更强。这些发现通过识别匹配（类比推理的评估阶段）作为需要超越简单检索的准确因果映射的独立步骤，推动了理论的发展。虽然当前的LLMs在生成候选类比方面表现出色，但人类在识别跨领域深层次结构相似性方面仍具有比较优势。错误分析表明，AI错误源于表面级别的匹配，而人类错误则源于对因果结构的误解。综合来看，这些结果表明在AI辅助的组织决策中可以实现有效的劳动分工：LLMs可以作为广泛的类比生成器，而人类则可以作为关键评估者，将最符合上下文的类比应用到战略问题中。

> This study investigates whether large language models, specifically GPT4, can match human capabilities in analogical reasoning within strategic decision making contexts. Using a novel experimental design involving source to target matching, we find that GPT4 achieves high recall by retrieving all plausible analogies but suffers from low precision, frequently applying incorrect analogies based on superficial similarities. In contrast, human participants exhibit high precision but low recall, selecting fewer analogies yet with stronger causal alignment. These findings advance theory by identifying matching, the evaluative phase of analogical reasoning, as a distinct step that requires accurate causal mapping beyond simple retrieval. While current LLMs are proficient in generating candidate analogies, humans maintain a comparative advantage in recognizing deep structural similarities across domains. Error analysis reveals that AI errors arise from surface level matching, whereas human errors stem from misinterpretations of causal structure. Taken together, the results suggest a productive division of labor in AI assisted organizational decision making where LLMs may serve as broad analogy generators, while humans act as critical evaluators, applying the most contextually appropriate analogies to strategic problems.

[Arxiv](https://arxiv.org/abs/2505.00603)