# 善用大型语言模型的解释，增强表格数据分类中的替代模型

发布时间：2025年05月08日

`LLM应用

理由：这篇论文探讨了如何将大型语言模型（LLMs）应用于表格预测任务，并提出了一种新的上下文学习框架，结合解释生成和代理模型来提升性能和可解释性。这属于将LLMs应用于具体任务的范畴，因此归类为LLM应用。` `数据分析` `人工智能`

> Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification

# 摘要

> 大型语言模型（LLMs）在解决复杂任务方面展现出卓越能力，为提升表格学习提供了有前途的工具。然而，现有基于LLMs的方法面临高资源需求、次优演示选择和有限可解释性等挑战，严重限制了其预测性能及现实应用。为解决这些问题，我们提出了一种针对表格预测的新颖上下文学习框架。该框架的核心理念是利用LLMs生成的解释，引导一个更小、本地可部署的代理语言模型（SLM），实现可解释的表格预测。具体而言，框架包含三个主要阶段：(i) 事后解释生成，LLMs为候选演示中的问答对生成解释，揭示答案背后的推理过程。 (ii) 事后解释引导的演示选择，利用LLMs生成的解释从候选演示中选择最优演示。 (iii) 事后解释引导的可解释SLM预测，利用步骤(ii)中获得的演示作为上下文，并将相应解释合并为理由，以提升SLM性能并引导模型生成可解释输出。实验结果凸显了该框架的有效性，在不同领域的各种表格数据集上，平均准确率提高了5.31%。

> Large Language Models (LLMs) have shown remarkable ability in solving complex tasks, making them a promising tool for enhancing tabular learning. However, existing LLM-based methods suffer from high resource requirements, suboptimal demonstration selection, and limited interpretability, which largely hinder their prediction performance and application in the real world. To overcome these problems, we propose a novel in-context learning framework for tabular prediction. The core idea is to leverage the explanations generated by LLMs to guide a smaller, locally deployable Surrogate Language Model (SLM) to make interpretable tabular predictions. Specifically, our framework mainly involves three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to generate explanations for question-answer pairs in candidate demonstrations, providing insights into the reasoning behind the answer. (ii) Post Hoc Explanation-Guided Demonstrations Selection, which utilizes explanations generated by LLMs to guide the process of demonstration selection from candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM Prediction, which utilizes the demonstrations obtained in step (ii) as in-context and merges corresponding explanations as rationales to improve the performance of SLM and guide the model to generate interpretable outputs. Experimental results highlight the framework's effectiveness, with an average accuracy improvement of 5.31% across various tabular datasets in diverse domains.

[Arxiv](https://arxiv.org/abs/2505.05744)