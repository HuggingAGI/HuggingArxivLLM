# Hunty不像我们：少数化拟人化提示对大型语言模型感知与行为的影响

发布时间：2025年05月08日

`LLM应用` `社会学`

> Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs

# 摘要

> 随着大型语言模型（LLMs）逐步适应并个性化以满足多样化的用户群体需求，系统滥用社会方言的风险也在增加，即使用与特定少数群体生活经历相关的语言风格或方言（例如非裔美国英语、酷儿俚语）。在本研究中，我们探讨了LLM代理使用社会方言是否会影响用户对其输出结果的依赖程度以及用户感知（包括满意度、挫败感、信任度和社会临场感）。我们设计并开展了一项用户研究，其中498名非裔美国英语（AAE）使用者和487名酷儿俚语使用者在LLM建议的辅助下完成了一系列问答任务，这些建议分别以标准美式英语（SAE）或用户自识别的社会方言呈现。

研究结果表明，LLMs使用社会方言会影响用户的依赖程度和感知，尽管在某些方面结果出乎意料。研究发现，无论是AAE使用者还是酷儿俚语使用者都更倾向于依赖SAE代理，并对SAE代理有更积极的感知。然而，只有酷儿俚语使用者认为酷儿俚语代理比SAE代理更具社会临场感，而只有AAE使用者更偏爱并信任SAE代理而非AAE代理。

这些发现强调了测试行为结果的重要性，而不仅仅是假设个性化会导致更好的依赖结果。它们还突显了少数群体语言在机器交互中的复杂动态，强调了LLMs需要精心设计，以尊重文化与语言边界，同时促进真实用户的参与和信任。


> As large language models (LLMs) increasingly adapt and personalize to diverse sets of users, there is an increased risk of systems appropriating sociolects, i.e., language styles or dialects that are associated with specific minoritized lived experiences (e.g., African American English, Queer slang). In this work, we examine whether sociolect usage by an LLM agent affects user reliance on its outputs and user perception (satisfaction, frustration, trust, and social presence). We designed and conducted user studies where 498 African American English (AAE) speakers and 487 Queer slang speakers performed a set of question-answering tasks with LLM-based suggestions in either standard American English (SAE) or their self-identified sociolect. Our findings showed that sociolect usage by LLMs influenced both reliance and perceptions, though in some surprising ways. Results suggest that both AAE and Queer slang speakers relied more on the SAE agent, and had more positive perceptions of the SAE agent. Yet, only Queer slang speakers felt more social presence from the Queer slang agent over the SAE one, whereas only AAE speakers preferred and trusted the SAE agent over the AAE one. These findings emphasize the need to test for behavioral outcomes rather than simply assume that personalization would lead to a better and safer reliance outcome. They also highlight the nuanced dynamics of minoritized language in machine interactions, underscoring the need for LLMs to be carefully designed to respect cultural and linguistic boundaries while fostering genuine user engagement and trust.

[Arxiv](https://arxiv.org/abs/2505.05660)