# GesPrompt：在虚拟现实环境中，通过协同语言的手势增强基于LLM的交互体验

发布时间：2025年05月08日

`LLM应用` `扩展现实` `用户界面`

> GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based Interaction in Virtual Reality

# 摘要

> 基于大型语言模型（LLM）的副驾驶在扩展现实（XR）应用中展现出巨大潜力。然而，用户在向副驾驶描述3D环境时面临挑战，因为仅通过文本或语音传达时空信息的复杂性。为了解决这一问题，我们引入了GesPrompt，这是一种结合伴随语言手势和语音的多模态XR界面，使最终用户能够在XR环境中与基于LLM的副驾驶进行更自然、更准确的沟通。通过整合手势，GesPrompt从伴随语言的手势中提取时空参考，减少了对精确文本提示的需求，从而降低了最终用户的认知负荷。我们的贡献包括（1）在XR环境中整合手势和语音输入的工作流程，（2）实现该工作流程的VR系统原型，以及（3）一项用户研究，证明其在提升VR环境中用户沟通效果方面的有效性。

> Large Language Model (LLM)-based copilots have shown great potential in Extended Reality (XR) applications. However, the user faces challenges when describing the 3D environments to the copilots due to the complexity of conveying spatial-temporal information through text or speech alone. To address this, we introduce GesPrompt, a multimodal XR interface that combines co-speech gestures with speech, allowing end-users to communicate more naturally and accurately with LLM-based copilots in XR environments. By incorporating gestures, GesPrompt extracts spatial-temporal reference from co-speech gestures, reducing the need for precise textual prompts and minimizing cognitive load for end-users. Our contributions include (1) a workflow to integrate gesture and speech input in the XR environment, (2) a prototype VR system that implements the workflow, and (3) a user study demonstrating its effectiveness in improving user communication in VR environments.

[Arxiv](https://arxiv.org/abs/2505.05441)