# 超越语言先验：多模态模型中视觉理解与注意力的增强

发布时间：2025年05月08日

`LLM应用` `计算机视觉`

> Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models

# 摘要

> 视觉与语言的深度对齐仍是多模态大型语言模型（MLLMs）的核心挑战。现有模型往往过度依赖语言先验，未能充分发挥视觉输入的价值。我们的研究首先揭示了MLLMs内部构建图像区域视觉理解的机制，并提出了一系列增强这一能力的技术。具体来说，我们开发了旨在深化模型对视觉内容理解并确保这些视觉洞察能主动指导语言生成的方法。通过详细分析，我们验证了我们模型在多模态理解上的显著优势，不仅体现在其预测视觉依赖令牌的能力上，还在视觉挑战任务上实现了10分的性能提升。

> Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks.

[Arxiv](https://arxiv.org/abs/2505.05626)