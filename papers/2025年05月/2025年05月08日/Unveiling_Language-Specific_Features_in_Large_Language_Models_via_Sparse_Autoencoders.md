# 借助稀疏自动编码器，揭示大型语言模型中的语言独特特性

发布时间：2025年05月08日

`LLM理论` `跨语言处理`

> Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders

# 摘要

> LLMs的多语言能力机制通常通过基于神经元或内部激活的方法进行研究，但这些方法常受叠加效应和分层激活方差的限制，影响了可靠性。稀疏自动编码器（SAEs）通过将LLMs的激活分解为SAE特征的稀疏线性组合，提供了更细致的分析。我们引入了一种新型指标来评估从SAEs获得的特征的单语性，发现某些特征与特定语言密切相关。进一步研究发现，仅消除这些SAE特征只会显著降低LLMs在一种语言上的能力，对其他语言影响微乎其微。有趣的是，某些语言具有多个协同作用的SAE特征，同时消除这些特征能带来比单独消除更大的改进。此外，我们利用这些源自SAE的特定语言特征来增强引导向量，实现了对LLMs生成语言的控制。

> The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs.

[Arxiv](https://arxiv.org/abs/2505.05111)