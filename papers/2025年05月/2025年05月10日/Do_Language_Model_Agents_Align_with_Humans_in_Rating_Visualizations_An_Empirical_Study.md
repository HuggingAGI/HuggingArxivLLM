# # 摘要
语言模型代理在评估可视化时是否与人类保持一致？一项实证研究。
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年05月10日

`LLM应用` `可视化设计` `设计研究`

> Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study

# 摘要

> 大型语言模型在多个领域编码知识，并展现出理解可视化的 ability。它们也可能捕捉到可视化设计知识，并有可能帮助减少形成性研究的成本。然而，大型语言模型是否能够预测人类对可视化的反馈仍然存疑。

为了研究这一问题，我们进行了三项研究，以考察基于大型模型的代理能否模拟人类在可视化任务中的评分。第一项研究复制了一项已发表的人类被试研究，结果显示代理在进行类人推理和评分方面具有潜力，其结果指导了后续的实验设计。第二项研究重复了文献中报告的六项关于主观评分的人类被试研究，但用代理替代了人类参与者。咨询了五位人类专家后，本研究证明代理评分与人类评分的一致性与专家在实验前的信心水平呈正相关。第三项研究测试了增强代理的常用技术，包括预处理视觉和文本输入以及知识注入。结果显示这些技术在稳健性和潜在偏见诱导方面存在问题。

这三项研究表明，在高信心假设的指导下，基于语言模型的代理有可能模拟人类在可视化实验中的评分，这些假设来自专家评估者。此外，我们展示了使用代理快速评估原型的使用场景。我们讨论了评估和改进代理评分与人类评分一致性方面的见解和未来方向。我们指出，模拟只能作为补充，不能取代用户研究。

> Large language models encode knowledge in various domains and demonstrate the ability to understand visualizations. They may also capture visualization design knowledge and potentially help reduce the cost of formative studies. However, it remains a question whether large language models are capable of predicting human feedback on visualizations. To investigate this question, we conducted three studies to examine whether large model-based agents can simulate human ratings in visualization tasks. The first study, replicating a published study involving human subjects, shows agents are promising in conducting human-like reasoning and rating, and its result guides the subsequent experimental design. The second study repeated six human-subject studies reported in literature on subjective ratings, but replacing human participants with agents. Consulting with five human experts, this study demonstrates that the alignment of agent ratings with human ratings positively correlates with the confidence levels of the experts before the experiments. The third study tests commonly used techniques for enhancing agents, including preprocessing visual and textual inputs, and knowledge injection. The results reveal the issues of these techniques in robustness and potential induction of biases. The three studies indicate that language model-based agents can potentially simulate human ratings in visualization experiments, provided that they are guided by high-confidence hypotheses from expert evaluators. Additionally, we demonstrate the usage scenario of swiftly evaluating prototypes with agents. We discuss insights and future directions for evaluating and improving the alignment of agent ratings with human ratings. We note that simulation may only serve as complements and cannot replace user studies.

[Arxiv](https://arxiv.org/abs/2505.06702)