# 通过公理约束，从大型语言模型嵌入中恢复事件概率

发布时间：2025年05月10日

`LLM理论

摘要中讨论了大型语言模型生成概率的一致性问题，并提出了一种理论方法来施加公理约束以恢复一致的事件概率，属于对模型理论的研究。` `概率论` `机器学习`

> Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints

# 摘要

> 在不确定性决策中，对事件概率的一致信念至关重要。然而，大型语言模型（LLMs）生成的概率往往违背概率论的公理，表现出不一致性。这引发了一个问题：是否能从模型使用的嵌入向量中恢复一致的事件概率？如果可行，这些概率将为涉及不确定性的事件提供更准确的估计。为此，我们提出了一种方法，在扩展的变分自编码器（VAE）学习的潜在空间中施加公理约束，如概率论的加法法则，应用于LLM的嵌入向量。通过这种方法，事件概率能够在VAE学习过程中自然显现，既重建原始嵌入，又预测语义相关事件的嵌入。我们以互补事件（事件A及其补事件，非A事件）为例进行评估，其中两者的概率之和必须为1。实验结果表明，通过嵌入向量恢复的概率比模型直接输出的概率更具一致性，并与真实概率高度吻合。

> Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities.

[Arxiv](https://arxiv.org/abs/2505.07883)