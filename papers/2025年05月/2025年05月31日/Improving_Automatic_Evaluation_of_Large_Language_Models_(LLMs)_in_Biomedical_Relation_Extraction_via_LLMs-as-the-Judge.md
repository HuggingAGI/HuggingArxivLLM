# 利用LLMs作为评估者提升大型语言模型在生物医学关系抽取中的自动评估效果

发布时间：2025年05月31日

`LLM应用

理由：这篇论文探讨了将大型语言模型（LLMs）作为评估工具在生物医学关系抽取任务中的应用。研究者测试了多种LLMs作为评估者的性能，并提出了改进评估方法的策略，如结构化输出格式和领域适应技术。这些内容都属于将LLMs应用于具体任务并优化其表现的研究，因此归类为LLM应用。` `生物医学` `关系抽取`

> Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge

# 摘要

> 大型语言模型（LLMs）在生物医学关系抽取任务中表现优异，即使在零-shot场景下也不例外。然而，由于LLMs生成的文本常与标准答案存在同义或缩略表达，传统自动评估指标可靠性不足，因此在这一任务中评估LLMs仍具挑战性。人工评估虽更可靠，但成本高且耗时，难以推广。本文探讨了将LLMs作为评估者（LLM-as-the-Judge）这一替代方法在生物医学关系抽取中的应用。我们选取8种LLMs作为评估者，对其生成的响应在3个生物医学关系抽取数据集上进行了基准测试。与其它文本生成任务不同，我们发现基于LLMs的评估者在该任务中表现欠佳（通常准确率低于50%）。研究发现，这主要归因于LLMs提取的关系未遵循标准格式。为解决这一问题，我们提出了针对LLM生成响应的结构化输出格式，平均提升了LLM评估者性能约15%。我们还引入了领域适应技术，通过迁移数据集间知识，进一步增强了评估性能。我们公开发布了人工标注和LLM标注的评估数据（总计36,000个样本）：https://github.com/tahmedge/llm_judge_biomedical_re。

> Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text, often producing synonyms or abbreviations of gold-standard answers, making traditional automatic evaluation metrics unreliable. On the other hand, while human evaluation is more reliable, it is costly and time-consuming, making it impractical for real-world applications. This paper investigates the use of LLMs-as-the-Judge as an alternative evaluation method for biomedical relation extraction. We benchmark 8 LLMs as judges to evaluate the responses generated by 5 other LLMs across 3 biomedical relation extraction datasets. Unlike other text-generation tasks, we observe that LLM-based judges perform quite poorly (usually below 50% accuracy) in the biomedical relation extraction task. Our findings reveal that it happens mainly because relations extracted by LLMs do not adhere to any standard format. To address this, we propose structured output formatting for LLM-generated responses that helps LLM-Judges to improve their performance by about 15% (on average). We also introduce a domain adaptation technique to further enhance LLM-Judge performance by effectively transferring knowledge between datasets. We release both our human-annotated and LLM-annotated judgment data (36k samples in total) for public use here: https://github.com/tahmedge/llm_judge_biomedical_re.

[Arxiv](https://arxiv.org/abs/2506.00777)