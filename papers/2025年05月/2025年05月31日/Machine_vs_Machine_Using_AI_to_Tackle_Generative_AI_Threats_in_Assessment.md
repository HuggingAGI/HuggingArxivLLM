# AI对决：用AI技术破解评估中的生成式AI威胁

发布时间：2025年05月31日

`其他` `评估方法`

> Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment

# 摘要

> 本文提出了一种机器对机器的方法框架，用于应对生成式人工智能 (AI) 在高等教育评估中带来的挑战。GPT-4、Claude 和 Llama 等大型语言模型展现出生成复杂学术内容的能力，使传统评估方法面临生存威胁。调查显示，74-92% 的学生曾尝试使用这些工具进行学术用途。当前的应对措施，包括检测软件和手动评估的重新设计，都存在明显局限：检测工具对非英语母语者有偏见且易被绕过，而手动框架依赖主观判断并假设 AI 能力静态不变。本文提出了一种结合静态分析和动态测试的双重策略范式，构建了一个全面的评估漏洞评估理论框架。静态分析部分包含八个要素：具体性与情境化、时间相关性、过程可见性要求、个性化要素、资源可访问性、多模态整合、伦理推理要求和协作要素。每个要素针对生成式 AI 的特定限制，创建障碍以区分真实人类学习与 AI 生成模拟。动态测试部分通过基于模拟的漏洞评估提供互补方法，弥补了基于模式分析的不足。本文提出了一种漏洞评分理论框架，涵盖定量评估的概念基础、加权框架和阈值确定理论。

> This paper presents a theoretical framework for addressing the challenges posed by generative artificial intelligence (AI) in higher education assessment through a machine-versus-machine approach. Large language models like GPT-4, Claude, and Llama increasingly demonstrate the ability to produce sophisticated academic content, traditional assessment methods face an existential threat, with surveys indicating 74-92% of students experimenting with these tools for academic purposes. Current responses, ranging from detection software to manual assessment redesign, show significant limitations: detection tools demonstrate bias against non-native English writers and can be easily circumvented, while manual frameworks rely heavily on subjective judgment and assume static AI capabilities. This paper introduces a dual strategy paradigm combining static analysis and dynamic testing to create a comprehensive theoretical framework for assessment vulnerability evaluation. The static analysis component comprises eight theoretically justified elements: specificity and contextualization, temporal relevance, process visibility requirements, personalization elements, resource accessibility, multimodal integration, ethical reasoning requirements, and collaborative elements. Each element addresses specific limitations in generative AI capabilities, creating barriers that distinguish authentic human learning from AI-generated simulation. The dynamic testing component provides a complementary approach through simulation-based vulnerability assessment, addressing limitations in pattern-based analysis. The paper presents a theoretical framework for vulnerability scoring, including the conceptual basis for quantitative assessment, weighting frameworks, and threshold determination theory.

[Arxiv](https://arxiv.org/abs/2506.02046)