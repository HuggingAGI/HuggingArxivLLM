# 评估语言模型预测器时的常见误区

发布时间：2025年05月31日

`LLM理论` `评估方法` `模型评估`

> Pitfalls in Evaluating Language Model Forecasters

# 摘要

> 大型语言模型（LLMs）近期在预测任务中崭露头角，有研究声称其表现可与人类比肩。然而，本文主张，作为研究社区，我们需对这些结论保持谨慎，因为评估 LLM 预测器面临独特挑战。我们归纳了两大类问题：（1）由于多种时间泄漏形式，导致评估结果难以令人信服；（2）从评估表现推断实际预测能力存在困难。通过系统性分析与实例研究，我们揭示了评估缺陷如何动摇当前及未来性能声明的可信度。我们认为，唯有采用更为严谨的评估方法，方能准确评估 LLM 的预测实力。

> Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.

[Arxiv](https://arxiv.org/abs/2506.00723)