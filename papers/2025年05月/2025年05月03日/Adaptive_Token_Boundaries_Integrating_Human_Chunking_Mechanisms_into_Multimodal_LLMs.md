# 自适应分词边界：将人类分块机制融入多模态大语言模型

发布时间：2025年05月03日

`LLM理论` `人工智能` `认知科学`

> Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs

# 摘要

> # 摘要
多模态大型语言模型（MLLMs）的最新进展展示了在处理多种数据类型方面的卓越能力，但人类认知过程与计算方法在多模态信息整合方面仍存在显著差异。本研究系统性地探讨了人类跨模态分块机制与MLLMs中的token表示方法之间的相似性。通过比较人类在视觉语言任务中的表现模式与模型行为的实证研究，我们证明了传统静态token化方案从根本上限制了当前模型模拟人类信息处理动态、上下文敏感特性的能力。我们提出了一种基于认知科学原理的新型动态跨模态token化框架，该框架集成了自适应边界、层次化表示和对齐机制。定量评估表明，与现有最优模型相比，我们的方法在基准任务上取得了统计显著的提升（视觉问答任务提升+7.8%，复杂场景描述任务提升+5.3%），同时展现出更符合人类认知的错误模式和注意力分布。这些发现增进了我们对人类认知与人工智能之间关系的理论理解，同时为开发更具认知合理性的AI系统提供了实证依据。

> Recent advancements in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing diverse data types, yet significant disparities persist between human cognitive processes and computational approaches to multimodal information integration. This research presents a systematic investigation into the parallels between human cross-modal chunking mechanisms and token representation methodologies in MLLMs. Through empirical studies comparing human performance patterns with model behaviors across visual-linguistic tasks, we demonstrate that conventional static tokenization schemes fundamentally constrain current models' capacity to simulate the dynamic, context-sensitive nature of human information processing. We propose a novel framework for dynamic cross-modal tokenization that incorporates adaptive boundaries, hierarchical representations, and alignment mechanisms grounded in cognitive science principles. Quantitative evaluations demonstrate that our approach yields statistically significant improvements over state-of-the-art models on benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene Description) while exhibiting more human-aligned error patterns and attention distributions. These findings contribute to the theoretical understanding of the relationship between human cognition and artificial intelligence, while providing empirical evidence for developing more cognitively plausible AI systems.

[Arxiv](https://arxiv.org/abs/2505.04637)