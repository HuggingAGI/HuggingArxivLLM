# # 大型语言模型的隐私风险与保护研究综述

发布时间：2025年05月03日

`LLM理论` `人工智能`

> A Survey on Privacy Risks and Protection in Large Language Models

# 摘要

> 大型语言模型（LLMs）在各类应用中的重要性日益凸显，但其功能也引发诸多隐私问题。本研究旨在全面概述与LLMs相关的隐私风险，并探讨当前可用于缓解这些挑战的解决方案。

首先，我们分析LLMs中的隐私泄露与攻击，重点关注模型如何无意中通过模型反转、训练数据提取和成员推断等技术泄露敏感信息。我们深入研究了隐私泄露的机制，包括未经授权的训练数据提取，以及这些漏洞可能被恶意行为者滥用的风险。

接下来，我们回顾了现有的隐私保护措施，如推断检测、联邦学习、后门缓解和机密计算，并评估了这些方法在防止隐私泄露方面的有效性。

此外，我们还强调了实践中面临的关键挑战，并提出了未来的研究方向，以开发安全且保护隐私的LLMs，重点关注隐私风险评估、模型间安全知识迁移以及隐私治理的跨学科框架。

最终，本研究旨在为应对LLMs领域日益严峻的隐私挑战制定明确的路线图。

> Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.

[Arxiv](https://arxiv.org/abs/2505.01976)