# 重新定义提示优化器：从优点到实践

发布时间：2025年05月14日

`LLM应用` `人工智能`

> Rethinking Prompt Optimizers: From Prompt Merits to Optimization

# 摘要

> 提示优化（PO）为大型语言模型（LLMs）提供了一种无需微调的实用方法，能够在不改变模型权重的情况下提升性能。现有方法通常依赖于像GPT-4这样的先进大规模LLMs来生成优化提示。然而，由于向下兼容性有限，这些高级LLMs生成的冗长、指令密集型提示可能会对轻量级推理模型造成过载，从而降低响应质量。本研究通过可解释性设计的视角重新思考提示优化。我们首先识别出一组模型无关的提示质量优势，并通过实证验证了它们在提升提示和响应质量方面的有效性。随后，我们引入了MePO——一种基于优势引导、轻量级且可本地部署的提示优化器，它基于我们使用轻量级LLM生成的与优势对齐的提示构建的偏好数据集进行训练。与以往工作不同，MePO避免了对在线优化的依赖，降低了成本和隐私问题，并通过学习清晰、可解释的优势，能够有效推广到大规模和轻量级推理模型。实验表明，MePO在各种任务和模型类型中实现了更好的结果，为现实世界的部署提供了一种可扩展且稳健的解决方案。我们的模型和数据集可在以下链接获取：https://github.com/MidiyaZhu/MePO

> Prompt optimization (PO) offers a practical alternative to fine-tuning large language models (LLMs), enabling performance improvements without altering model weights. Existing methods typically rely on advanced, large-scale LLMs like GPT-4 to generate optimized prompts. However, due to limited downward compatibility, verbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight inference models and degrade response quality. In this work, we rethink prompt optimization through the lens of interpretable design. We first identify a set of model-agnostic prompt quality merits and empirically validate their effectiveness in enhancing prompt and response quality. We then introduce MePO, a merit-guided, lightweight, and locally deployable prompt optimizer trained on our preference dataset built from merit-aligned prompts generated by a lightweight LLM. Unlike prior work, MePO avoids online optimization reliance, reduces cost and privacy concerns, and, by learning clear, interpretable merits, generalizes effectively to both large-scale and lightweight inference models. Experiments demonstrate that MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution for real-world deployment. Our model and dataset are available at: https://github.com/MidiyaZhu/MePO

[Arxiv](https://arxiv.org/abs/2505.09930)