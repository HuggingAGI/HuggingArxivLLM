# # 测试 GPT 与推理型大语言模型解答物理竞赛题：超越人类水平及对教育评估的启示

发布时间：2025年05月14日

`LLM应用` `人工智能`

> Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment

# 摘要

> 大型语言模型（LLMs）如今已广泛应用，涵盖各个教育层次的学习者。这一发展引发了担忧，担心它们的使用可能会规避关键的学习过程，削弱现有评估形式的完整性。在物理教育领域，问题解决在教学和评估中占据核心地位，因此有必要了解LLMs在物理问题解决方面的特定能力。这种理解对于制定负责任且符合教学原则的方法，将LLMs整合到教学和评估中至关重要。

本研究比较了通用型LLM（GPT-4o，采用不同提示技术）和一个优化推理模型（o1-preview）与德国物理奥林匹克参赛者在一组明确的奥林匹克问题上的问题解决表现。除了评估生成解决方案的正确性外，本研究还分析了LLM生成解决方案的典型优势和局限性。

研究结果表明，两种测试的LLMs（GPT-4o和o1-preview）在奥林匹克类型的物理问题上展示了高级的问题解决能力，平均表现优于人类参与者。提示技术对GPT-4o的表现影响不大，而o1-preview几乎一致优于GPT-4o和人类基准。基于这些发现，本研究讨论了在物理教育中设计总结性和形成性评估的含义，包括如何维护评估的完整性以及如何帮助学生批判性地使用LLMs。

> Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.

[Arxiv](https://arxiv.org/abs/2505.09438)