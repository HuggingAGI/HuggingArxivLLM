# 基于人类反馈的强化学习中的伦理与劝说：一种程序修辞方法

发布时间：2025年05月14日

`LLM应用` `AI伦理` `修辞学`

> Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach

# 摘要

> 自2022年以来，生成式AI聊天机器人如ChatGPT和Claude采用了名为“从人类反馈中强化学习”（RLHF）的技术进行训练，利用人类标注者的反馈来优化语言模型输出。这一技术的引入显著提升了大型语言模型的表现，使其交互和回应相较于仅依赖监督学习的旧版本更加“类人化”。然而，随着人类与机器生成文本的趋同，潜在的伦理、社会技术及教育隐忧也随之浮现，涉及透明度、信任、偏见和人际关系等多个层面。本文通过修辞学分析，探讨了RLHF增强型生成式AI聊天机器人对语言规范维护、信息寻求实践及社会关系期望等方面的影响。目前，关于生成式AI和LLMs的修辞学研究多集中于内容说服力，而本文则采用Ian Bogost的程序性修辞概念，将研究重点从内容分析转向RLHF增强型LLMs中的内在说服机制。这一探索为AI伦理研究开辟了新方向，关注AI驱动技术如何可能强化霸权语言、 perpetuate偏见、脱离语境化学习及影响人际关系。本文将吸引教育工作者、研究人员、学者以及越来越多的生成式AI用户。

> Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more "human-like" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.

[Arxiv](https://arxiv.org/abs/2505.09576)