# 大型语言模型的说服力超越激励型人类劝说者

发布时间：2025年05月14日

`LLM应用` `人工智能` `对话系统`

> Large Language Models Are More Persuasive Than Incentivized Human Persuaders

# 摘要

> 我们直接对比了前沿大型语言模型（LLM；Claude Sonnet 3.5）与激励型人类说服者在互动实时对话问答场景中的说服能力。在这一预先注册的大规模激励实验中，参与者（答题者）完成了一项在线问答，其中说服者（人类或LLM）试图说服答题者选择正确或错误答案。结果显示，LLM说服者在引导答题者时的说服成功率显著高于人类说服者，无论是在引导答题者选择正确答案的真实情境，还是在引导答题者选择错误答案的欺骗情境中，LLM的说服能力均表现更优。此外，当引导答题者选择正确答案时，LLM说服者显著提高了答题者的准确率，使其收益增加；而当引导答题者选择错误答案时，则显著降低了答题者的准确率，使其收益减少。总体而言，我们的研究结果表明，AI的说服能力已经超越了那些与绩效挂钩、获得真实金钱奖励的人类说服者。因此，AI说服能力的日益增强凸显了对新兴对齐与治理框架的迫切需求。

> We directly compare the persuasion capabilities of a frontier large language model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an interactive, real-time conversational quiz setting. In this preregistered, large-scale incentivized experiment, participants (quiz takers) completed an online quiz where persuaders (either humans or LLMs) attempted to persuade quiz takers toward correct or incorrect answers. We find that LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than incentivized human persuaders, demonstrating superior persuasive capabilities in both truthful (toward correct answers) and deceptive (toward incorrect answers) contexts. We also find that LLM persuaders significantly increased quiz takers' accuracy, leading to higher earnings, when steering quiz takers toward correct answers, and significantly decreased their accuracy, leading to lower earnings, when steering them toward incorrect answers. Overall, our findings suggest that AI's persuasion capabilities already exceed those of humans that have real-money bonuses tied to performance. Our findings of increasingly capable AI persuaders thus underscore the urgency of emerging alignment and governance frameworks.

[Arxiv](https://arxiv.org/abs/2505.09662)