# # 评估大型语言模型生成单元测试的能力，采用等价类划分与边界值方法

发布时间：2025年05月14日

`LLM应用

LLM应用` `软件工程` `人工智能`

> Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values

# 摘要

> 单元测试的设计与实现常被程序员忽视，因其复杂性而令人望而却步。本研究探索了大型语言模型（LLMs）在自动生成测试用例方面的潜力，并将其与传统手动测试进行了对比。我们开发了一种优化提示方法，整合代码与需求，覆盖了等价类划分和边界值等关键测试案例。通过定量指标和人工定性分析，我们揭示了LLMs与训练有素的程序员之间的优劣。研究结果显示，LLMs的效果取决于精心设计的提示、稳健的实现和精确的需求。尽管LLMs灵活且前景光明，但它们仍需人工监督。本研究强调了人工定性分析在单元测试评估中的重要性，作为自动化的重要补充。

> The design and implementation of unit tests is a complex task many programmers neglect. This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests. An optimized prompt was developed, that integrates code and requirements, covering critical cases such as equivalence partitions and boundary values. The strengths and weaknesses of LLMs versus trained programmers were compared through quantitative metrics and manual qualitative analysis. The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. Although flexible and promising, LLMs still require human supervision. This work highlights the importance of manual qualitative analysis as an essential complement to automation in unit test evaluation.

[Arxiv](https://arxiv.org/abs/2505.09830)