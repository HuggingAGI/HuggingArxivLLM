# 轻便智能可穿戴平台，搭载多模态基础模型，专为视力障碍人群打造，助力提升空间推理能力

发布时间：2025年05月16日

`LLM应用` `无障碍技术` `可穿戴设备`

> A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision

# 摘要

> 视障人士在日常生活中面临重大挑战，尤其是在导航和定位物体方面，因为他们缺乏足够的视觉线索。空间推理能力对这些个体至关重要，因为它使他们能够理解周围环境中的空间关系，从而增强他们更安全、更独立地导航和互动的能力。然而，目前针对低视力人群的多模态大型语言模型（MLLM）在空间推理能力方面仍有欠缺，难以有效支持这些任务。此外，市场上缺乏轻量级、易于使用的系统，帮助视障人士有效感知和与周围环境互动。在本文中，我们提出了一种基于空间增强型多模态大型语言模型的创新方法，旨在为视障人士提供更有效的支持。通过微调MLLM以整合空间推理能力，我们的方法显著提升了对环境背景的理解，这对于导航和物体识别至关重要。这一创新还扩展到一个硬件组件，设计为眼镜附件，确保了更高的可访问性和易用性。该集成利用先进的视觉语言模型（VLMs）来解释视觉数据，并为用户提供实时、具有空间意识的反馈。我们的方法旨在弥合先进机器学习模型与实用、用户友好的辅助设备之间的差距，为视障用户提供一个强大的解决方案，使他们能够更有效地独立导航周围环境。本文通过使用VizWiz数据集进行了深入的评估，展示了准确性和用户体验的显著提升。此外，我们设计了一个全面的数据集，以评估我们的方法在现实世界情况下的有效性，展示了准确性和用户体验的显著提升。

> People with blindness and low vision (pBLV) face significant challenges, struggling to navigate environments and locate objects due to limited visual cues. Spatial reasoning is crucial for these individuals, as it enables them to understand and interpret the spatial relationships in their surroundings, enhancing their ability to navigate and interact more safely and independently. Current multi-modal large language (MLLM) models for low vision people lack the spatial reasoning capabilities needed to effectively assist in these tasks. Moreover, there is a notable absence of lightweight, easy-to-use systems that allow pBLV to effectively perceive and interact with their surrounding environment. In this paper, we propose a novel spatial enhanced multi-modal large language model based approach for visually impaired individuals. By fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method significantly improves the understanding of environmental context, which is critical for navigation and object recognition. The innovation extends to a hardware component, designed as an attachment for glasses, ensuring increased accessibility and ease of use. This integration leverages advanced VLMs to interpret visual data and provide real-time, spatially aware feedback to the user. Our approach aims to bridge the gap between advanced machine learning models and practical, user-friendly assistive devices, offering a robust solution for visually impaired users to navigate their surroundings more effectively and independently. The paper includes an in-depth evaluation using the VizWiz dataset, demonstrating substantial improvements in accuracy and user experience. Additionally, we design a comprehensive dataset to evaluate our method's effectiveness in realworld situations, demonstrating substantial improvements in accuracy and user experience.

[Arxiv](https://arxiv.org/abs/2505.10875)