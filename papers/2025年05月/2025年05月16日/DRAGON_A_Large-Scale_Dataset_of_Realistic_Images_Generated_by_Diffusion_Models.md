# DRAGON：一个由扩散模型生成的高质量图像大规模数据集

发布时间：2025年05月16日

`LLM应用` `图像生成` `图像检测`

> DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models

# 摘要

> 扩散模型在图像生成领域的易用性令人瞩目，但也导致大量合成内容在网络上泛滥。这些模型虽常用于正途，但也被滥用来生成虚假信息和仇恨言论的图像。因此，开发可靠的工具来检测图像是否由模型生成变得尤为重要。然而，现有检测方法通常需要大量样本图像进行训练。遗憾的是，由于技术日新月异，现有数据集往往只能涵盖有限的模型，很快便过时。为此，我们推出了DRAGON数据集，它包含来自25种扩散模型生成的图像，横跨最新技术与经典架构。数据集中的图像主题丰富多样。为了提升图像的真实性，我们设计了一套简单而有效的流程，借助大型语言模型扩展输入提示，从而生成更多样化、高质量的输出，这一点在标准质量指标上的提升得到了验证。该数据集提供从小到大多种尺寸，以适应不同研究需求。DRAGON旨在助力取证领域开发和评估合成内容的检测与归属技术。此外，数据集还附带专用测试集，为评估新方法性能提供了基准。

> The remarkable ease of use of diffusion models for image generation has led to a proliferation of synthetic content online. While these models are often employed for legitimate purposes, they are also used to generate fake images that support misinformation and hate speech. Consequently, it is crucial to develop robust tools capable of detecting whether an image has been generated by such models. Many current detection methods, however, require large volumes of sample images for training. Unfortunately, due to the rapid evolution of the field, existing datasets often cover only a limited range of models and quickly become outdated. In this work, we introduce DRAGON, a comprehensive dataset comprising images from 25 diffusion models, spanning both recent advancements and older, well-established architectures. The dataset contains a broad variety of images representing diverse subjects. To enhance image realism, we propose a simple yet effective pipeline that leverages a large language model to expand input prompts, thereby generating more diverse and higher-quality outputs, as evidenced by improvements in standard quality metrics. The dataset is provided in multiple sizes (ranging from extra-small to extra-large) to accomodate different research scenarios. DRAGON is designed to support the forensic community in developing and evaluating detection and attribution techniques for synthetic content. Additionally, the dataset is accompanied by a dedicated test set, intended to serve as a benchmark for assessing the performance of newly developed methods.

[Arxiv](https://arxiv.org/abs/2505.11257)