# 创造力还是蛮力？通过趣味难题透视大型语言模型的问题解决能力

发布时间：2025年05月16日

`LLM理论` `人工智能` `推理策略`

> Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models

# 摘要

> 准确性仍是评估AI系统的重要指标，但它未能充分揭示模型的解题思路。本研究引入了一个基于长篇叙述形式脑筋急转弯的基准测试，旨在深入探究模型所采用的推理策略。脑筋急转弯因可运用多种解题方法而成为理想选择，例如几步之内运用创意见解，或通过更多暴力手段的长篇解答。我们从多个推理层面研究大型语言模型（LLMs），不仅关注答案的准确性，更注重其解决方案的质量与创意。我们深入探究了推理过程的五个关键方面：（1）将脑筋急转弯转化为精确的数学竞赛风格格式；（2）基于数学形式生成解决方案；（3）根据黄金解决方案进行自我校正；（4）生成逐步解决方案的草稿；（5）有效利用提示。研究发现，LLMs在许多情况下能够提出富有创意且见解独到的解决方案，这表明它们具备以创新方式解决新颖问题的能力。然而，仍存在某些情况下，模型倾向于依赖暴力手段，即便有更高效、更具创意的解决方案可供选择，这揭示了LLMs推理能力有待改进的方向。

> Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.

[Arxiv](https://arxiv.org/abs/2505.10844)