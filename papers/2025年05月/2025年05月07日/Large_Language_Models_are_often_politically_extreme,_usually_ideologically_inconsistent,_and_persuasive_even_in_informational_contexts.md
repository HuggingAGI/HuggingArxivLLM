# 大型语言模型常展现出政治立场极端、意识形态不一致的特点，甚至在信息环境中也展现出强大的说服力。

发布时间：2025年05月07日

`LLM应用

理由：这篇论文探讨了大型语言模型（LLMs）在政治环境中的应用及其潜在影响，特别是LLMs的偏见和说服力。它分析了LLMs在实际使用中的效果和潜在问题，属于LLM的实际应用和影响分析，因此归类为LLM应用。` `政治传播`

> Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts

# 摘要

> 大型语言模型（LLMs）正在以一种革命性的方式改变人们获取信息和与世界互动的方式。随着LLMs在各种任务中的广泛应用，学术界对其内在偏见，尤其是政治偏见的研究日益增多，但通常认为这些偏见微乎其微。我们对此提出挑战。

首先，通过将31个LLMs与立法者、法官以及美国全国代表性选民样本进行比较，我们发现：LLMs看似微小的整体党派偏好，其实是特定主题上极端观点相互抵消的结果，这与温和选民的行为模式十分相似。

其次，在一项随机实验中，我们发现：即使是在信息寻求的环境中，LLMs也能够将自己的偏好转化为政治说服力。具体来说，随机分配与LLM聊天机器人讨论政治问题的选民，有高达5个百分点的可能性表达与该聊天机器人相同的偏好。

令人意外的是，这些说服效果不受用户对LLMs的熟悉程度、新闻消费习惯或对政治的兴趣等因素的影响。特别是那些由私人公司或政府控制的LLMs，可能成为一种强大而有针对性的政治影响力的载体。

> Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.

[Arxiv](https://arxiv.org/abs/2505.04171)