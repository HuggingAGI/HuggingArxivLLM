# 两个身份的故事：人类与AI塑造的人设伦理审查

发布时间：2025年05月07日

`LLM应用` `生成模型` `身份生成`

> A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas

# 摘要

> 随着大型语言模型 (LLMs) 在生成合成角色方面的应用日益广泛，特别是在健康、隐私和人机交互等数据有限的领域，理解这些叙述如何呈现身份，尤其是少数群体的身份，变得尤为重要。本文通过代表性的伤害视角，审核了 3 个 LLM（GPT4o、Gemini 1.5 Pro、Deepseek 2.5）生成的合成角色，重点关注种族身份。通过结合近距离阅读、词汇分析和参数化创造力框架的混合方法，我们将 1512 个 LLM 生成的角色与人工撰写的角色进行了比较。我们的研究发现，LLMs 过分强调种族标记，过度生成文化编码语言，并构建出句法复杂但叙述简化的角色。这些模式导致了一系列社会技术伤害，包括刻板印象、异域情调、消除和善意偏见，而这些伤害往往被表面上积极的叙述所掩盖。我们将这一现象形式化为算法他者化，其中少数群体身份被过度可见但缺乏真实性。基于这些发现，我们提出了面向叙述感知评估指标和以社区为中心的验证协议的设计建议，用于合成身份生成。

> As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.

[Arxiv](https://arxiv.org/abs/2505.07850)