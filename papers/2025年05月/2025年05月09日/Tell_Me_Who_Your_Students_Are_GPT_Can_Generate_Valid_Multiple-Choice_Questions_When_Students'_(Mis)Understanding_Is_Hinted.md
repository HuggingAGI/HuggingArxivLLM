# 你的学生是谁？当学生的误解被揭示，GPT就能生成有效的选择题

发布时间：2025年05月09日

`LLM应用`

> Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted

# 摘要

> 本研究旨在开发并评估一种创新的提示技术 AnaQuest，用于利用预训练大型语言模型生成多项选择题 (MCQ)。在 AnaQuest 中，选项是关于复杂概念的句子级别陈述。该技术整合了形成性评估和总结性评估。在形成性评估阶段，学生以自由文本形式回答目标概念的开放性问题。在总结性评估中，AnaQuest 分析这些回答以生成正确和错误的陈述。为了评估生成的 MCQ 的有效性，应用了项目反应理论 (IRT) 来比较 AnaQuest 生成的 MCQ、基线 ChatGPT 提示以及人工编制的项目之间的项目特征。实证研究表明，专家教师认为 AI 模型生成的 MCQ 与人工教师创建的同样有效。然而，基于 IRT 的分析表明，AnaQuest 生成的问题——特别是包含错误陈述（干扰项）的问题——在难度和区分度方面比 ChatGPT 生成的问题更接近人工编制的项目。

> The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT.

[Arxiv](https://arxiv.org/abs/2505.05815)