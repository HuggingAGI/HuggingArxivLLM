# InteractAnything: 基于大语言模型反馈与物体可用性解析的零样本人类-物体交互合成

发布时间：2025年05月30日

`LLM应用` `计算机视觉` `3D人体感知`

> InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing

# 摘要

> 近期研究在3D人体感知生成领域取得显著进展，但现有方法在从文本生成新颖的人-物交互（HOI）时仍面临三大挑战，尤其是针对开放集物体。这些挑战包括：精准的人-物关系推理；任意物体的可操作性解析；以及与描述和物体几何相匹配的详细人-物交互姿态合成。

我们提出了一种全新的零样本3D HOI生成框架，无需特定数据集训练，而是通过大规模预训练模型的知识迁移实现。具体方法如下：首先，我们从大型语言模型（LLMs）中推断人-物关系，以此初始化物体属性并指导优化流程。然后，利用预训练的2D图像扩散模型解析未见过的物体并提取接触点，突破现有3D资产知识的局限。接着，基于输入文本和物体几何信息，通过多视图采样生成多个初始人体姿态假设。最后，引入细致优化模块，生成精确且自然的交互效果，确保3D物体与身体部位（包括抓取时的手部）实现真实接触。这一创新通过从LLMs中提炼人类级别的反馈，精准捕捉文本指令中的详细人-物关系得以实现。

实验结果表明，我们的方法在交互细节处理和开放集3D物体生成方面显著优于先前工作。


> Recent advances in 3D human-aware generation have made significant progress. However, existing methods still struggle with generating novel Human Object Interaction (HOI) from text, particularly for open-set objects. We identify three main challenges of this task: precise human-object relation reasoning, affordance parsing for any object, and detailed human interaction pose synthesis aligning description and object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework without training on specific datasets, leveraging the knowledge from large-scale pre-trained models. Specifically, the human-object relations are inferred from large language models (LLMs) to initialize object properties and guide the optimization process. Then we utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. The initial human pose is generated by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. Finally, we introduce a detailed optimization to generate fine-grained, precise, and natural interaction, enforcing realistic 3D contact between the 3D object and the involved body parts, including hands in grasping. This is achieved by distilling human-level feedback from LLMs to capture detailed human-object relations from the text instruction. Extensive experiments validate the effectiveness of our approach compared to prior works, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects.

[Arxiv](https://arxiv.org/abs/2505.24315)