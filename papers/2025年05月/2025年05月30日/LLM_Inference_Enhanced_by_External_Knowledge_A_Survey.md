# # 摘要
借助外部知识提升 LLM 推理能力：综述

发布时间：2025年05月30日

`LLM应用` `人工智能`

> LLM Inference Enhanced by External Knowledge: A Survey

# 摘要

> 大型语言模型（LLMs）在自然语言推理方面取得了显著进展。然而，其参数化记忆的局限性以及易产生幻觉的特性，使其在需要基于上下文进行准确推理的任务中仍然面临挑战。为了解决这些问题，越来越多的研究提出通过利用外部知识来增强 LLMs。本研究系统性地探索了利用外部知识增强 LLMs 的策略，首先将外部知识划分为非结构化数据与结构化数据。接着，我们聚焦于结构化知识，分别针对表格和知识图谱（KGs）提出了不同的分类法，详细阐述了它们与 LLMs 的集成范式，并回顾了代表性方法。通过比较分析，我们揭示了可解释性、可扩展性和性能之间的权衡，为开发可信且通用的知识增强型 LLMs 提供了见解。

> Recent advancements in large language models (LLMs) have enhanced natural-language reasoning. However, their limited parametric memory and susceptibility to hallucination present persistent challenges for tasks requiring accurate, context-based inference. To overcome these limitations, an increasing number of studies have proposed leveraging external knowledge to enhance LLMs. This study offers a systematic exploration of strategies for using external knowledge to enhance LLMs, beginning with a taxonomy that categorizes external knowledge into unstructured and structured data. We then focus on structured knowledge, presenting distinct taxonomies for tables and knowledge graphs (KGs), detailing their integration paradigms with LLMs, and reviewing representative methods. Our comparative analysis further highlights the trade-offs among interpretability, scalability, and performance, providing insights for developing trustworthy and generalizable knowledge-enhanced LLMs.

[Arxiv](https://arxiv.org/abs/2505.24377)