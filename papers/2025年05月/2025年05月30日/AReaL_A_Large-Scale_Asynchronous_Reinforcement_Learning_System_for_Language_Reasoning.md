# AReaL：面向语言推理的大规模异步强化学习系统

发布时间：2025年05月30日

`LLM应用

理由：这篇论文主要讨论了强化学习（RL）在训练大型语言模型（LLMs）中的应用，特别是提出了一种新的异步RL系统AReaL，用于提高训练效率。它专注于训练方法和系统的优化，属于LLM的应用层面。` `人工智能` `高性能计算`

> AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning

# 摘要

> 强化学习（RL）正在成为训练大型语言模型（LLMs）的热门范式，尤其在推理任务中表现突出。然而，针对LLMs的高效RL训练需要大规模并行化和高效的训练系统支持。当前大多数大规模RL系统采用同步方式运行，通过批处理设置下交替生成和训练来完成任务，其中每个训练批次的回放由相同的（或最新的）模型生成。这种方式虽然稳定了RL训练，但存在严重的系统级低效问题。生成必须等待批次中最长的输出完成才能进行模型更新，导致GPU利用率低下。

我们提出AReaL，一个\emph{全异步}的RL系统，彻底解耦了生成与训练过程。在AReaL中，回放工作节点持续生成新输出而不必等待，同时训练工作节点在收集到一批数据后即可更新模型。AReaL还集成了多项系统级优化，显著提升了GPU利用率。为稳定RL训练，AReaL通过平衡回放与训练工作节点的负载来控制数据陈旧程度，并采用一种增强型PPO变体来更好地处理过时的训练样本。

在数学与代码推理基准测试中的大量实验表明，与相同GPU数量的最佳同步系统相比，AReaL实现了	extbf{最高2.57倍的训练加速}，同时达到匹配甚至超越的最终性能。AReaL的代码可在https://github.com/inclusionAI/AReaL/获取。

> Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.

[Arxiv](https://arxiv.org/abs/2505.24298)