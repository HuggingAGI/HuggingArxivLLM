# # Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games
心理理论与亲社会信念对LLMs在最后通牒游戏中引导人类对齐行为的影响

发布时间：2025年05月30日

`LLM应用` `人工智能` `社交互动`

> Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games

# 摘要

> 大型语言模型（LLMs）在模拟人类行为和心智理论（ToM）推理方面展现出潜力，这对于复杂的社交互动至关重要。本研究聚焦于ToM推理在使智能体行为与人类规范保持一致方面的作用，特别是在谈判任务中，我们以最后通牒博弈作为受控环境进行研究。我们初始化了具备不同亲社会信念（包括贪婪型、公平型和无私型）以及不同推理方法（如思维链（CoT）和不同水平的ToM）的LLM智能体，并在多种LLMs中考察了它们的决策过程，其中包括推理模型如o3-mini和DeepSeek-R1 Distilled Qwen 32B。基于2700次模拟实验的结果表明，ToM推理能够显著提升行为一致性、决策稳定性和谈判结果。与不具备ToM推理能力的推理模型相比，具备ToM推理能力的模型在行为一致性、决策稳定性和谈判结果上表现更优，且游戏收益的不同顺序对ToM推理的影响也不同。我们的研究发现为理解ToM在提升人机交互和合作决策中的作用提供了重要见解。实验代码可访问https://github.com/Stealth-py/UltimatumToM。

> Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

[Arxiv](https://arxiv.org/abs/2505.24255)