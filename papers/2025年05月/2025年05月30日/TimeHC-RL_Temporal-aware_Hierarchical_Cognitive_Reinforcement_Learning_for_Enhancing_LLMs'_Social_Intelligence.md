# TimeHC-RL：时间感知的层次化认知强化学习，助力于提升大型语言模型（LLMs）的社会智能

发布时间：2025年05月30日

`LLM应用` `社交智能` `人工智能`

> TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence

# 摘要

> 大型语言模型（LLMs）在需要审慎思考的IQ相关领域（如数学和编码）取得了显著进展，但如何从后训练视角提升其在社交领域的认知能力仍是一个亟待探索的方向。与主要依赖系统2思维（谨慎、逐步推理）的数学不同，社交世界遵循着独特的时序规律，需要更丰富的认知模式（从直觉反应（系统1）和浅层思考到深思熟虑（系统2））。为此，我们提出了基于时间感知的分层认知强化学习（TimeHC-RL），旨在提升LLMs的社交智能。通过五种后训练范式和两种测试时干预范式，在包含八种不同数据模式的数据集上，我们系统性地探索了提升LLMs社交智能的方法，并验证了TimeHC-RL方法的有效性。实验结果表明，与广泛采用的系统2强化学习方法相比，TimeHC-RL具有显著优势。它赋予了70亿参数的基础模型更强的能力，使其能够与DeepSeek-R1和OpenAI-O3等先进模型相媲美。此外，从后训练和测试时干预的视角系统性探索提升LLMs社交智能的方法，为我们带来了许多有价值的见解。

> Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.

[Arxiv](https://arxiv.org/abs/2505.24500)