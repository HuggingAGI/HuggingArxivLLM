# 统一语言代理算法与基于图的编排引擎，实现可复现的代理研究

发布时间：2025年05月30日

`Agent` `人工智能`

> Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research

# 摘要

> 大型语言模型 (LLMs) 驱动的语言代理在理解、推理和执行复杂任务方面展现了非凡的能力。然而，开发稳健的代理面临重大挑战：显著的工程开销、缺乏标准化组件以及用于公平比较的评估框架不足。我们引入了 AGORA (Agent Graph-based Orchestration for Reasoning and Assessment)，这是一个灵活且可扩展的框架，通过以下三个关键贡献解决这些挑战：

(1) 模块化架构，包含基于图的工作流引擎、高效的内存管理和清晰的组件抽象；  
(2) 一套全面的可重用代理算法，实现最先进的推理方法；  
(3) 严格的评估框架，支持多维度的系统化比较。  

通过在数学推理和多模态任务上的广泛实验，我们评估了不同 LLM 上的各种代理算法，揭示了它们相对优势和适用性的关键见解。我们的结果表明，尽管复杂的推理方法可以提升代理能力，但像 Chain-of-Thought 这样的简单方法通常表现出稳健的性能，且计算开销显著降低。AGORA 不仅简化了语言代理的开发，还通过标准化的评估协议为可重复的代理研究奠定了基础。

> Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.

[Arxiv](https://arxiv.org/abs/2505.24354)