# GuidedQuant: 借助末端损失指导实现大型语言模型量化

发布时间：2025年05月11日

`LLM应用

理由：这篇论文专注于后训练量化技术，提出了一种新的量化方法GuidedQuant，旨在优化大型语言模型的内存占用和推理延迟。这种方法属于模型优化和应用层面的改进，因此归类为LLM应用。` `移动计算`

> GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance

# 摘要

> 后训练量化是一种通过量化权重和激活来降低大型语言模型内存占用和推理延迟的关键技术，无需重新训练即可实现。然而，现有方法存在两大局限：要么未能考虑隐藏特征对最终损失的不同重要性，要么在引入最终损失时忽略了模型权重间的交互作用。为了解决这些问题，我们提出了GuidedQuant，这是一种创新的量化方法。它将最终损失的梯度信息整合到量化目标中，同时保留了输出通道内权重间的依赖关系。实验结果表明，GuidedQuant在仅权重标量、仅权重向量以及权重和激活量化方面，均显著提升了现有先进量化方法的性能。此外，我们还提出了一种新的非均匀标量量化算法，该算法保证了量化目标值的单调递减，并在同类方法中优于现有方法。我们的代码已发布在GitHub上，地址为https://github.com/snu-mllab/GuidedQuant。


> Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.

[Arxiv](https://arxiv.org/abs/2505.07004)