# # 大型语言模型的角色扮演评估

发布时间：2025年05月19日

`LLM应用` `语言模型` `评估框架`

> Role-Playing Evaluation for Large Language Models

# 摘要

> 大型语言模型（LLMs）在角色扮演方面展现出显著的能力，但对其能力的评估却面临两大难题：人工评估耗时耗力，自动化评估又可能带有偏见。为解决这一问题，我们推出了Role-Playing Eval（RPEval），这是一个全新设计的评估框架，从情感理解、决策能力、道德对齐和角色一致性四大维度全面评测LLM的角色扮演能力。本文将详细介绍RPEval的构建过程，并呈现基线评估结果。我们的代码和数据集已开源，地址为https://github.com/yelboudouri/RPEval。

> Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval

[Arxiv](https://arxiv.org/abs/2505.13157)