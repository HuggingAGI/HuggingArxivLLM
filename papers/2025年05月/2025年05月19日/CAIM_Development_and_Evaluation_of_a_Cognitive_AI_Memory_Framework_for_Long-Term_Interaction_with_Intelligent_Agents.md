# CAIM: 一种支持长期与智能体交互的认知AI记忆框架的开发与评估

发布时间：2025年05月19日

`LLM应用

摘要主要讨论了大型语言模型（LLMs）在长期人机交互中的应用，特别是通过整体记忆建模和认知人工智能框架来改善模型的记忆能力和交互性能。这属于LLM的应用层面，因此归类为LLM应用。` `交互系统` `认知人工智能`

> CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents

# 摘要

> 大型语言模型（LLMs）在人工智能领域取得了重大进展，为交互系统提供了强大支持。然而，它们在长期互动中仍面临挑战，尤其是在适应用户需求和理解动态环境方面。为解决这些问题，我们提出了整体记忆建模，以在交互过程中高效检索和存储信息，生成合适响应。认知人工智能（Cognitive AI）通过模拟人类思维过程，提供了改进LLMs记忆建模的思路，如思维、记忆机制和决策等。受此启发，我们提出了CAIM记忆框架。CAIM由三个模块组成：1) 作为核心决策单元的记忆控制器；2) 根据请求过滤相关数据用于交互的记忆检索模块；3) 维护记忆存储的后思考模块。通过与现有方法对比，我们发现CAIM在检索准确性、响应正确性、上下文连贯性和记忆存储等指标上均表现优异，突显了其对上下文的感知能力以及在改善长期人机交互方面的潜力。

> Large language models (LLMs) have advanced the field of artificial intelligence (AI) and are a powerful enabler for interactive systems. However, they still face challenges in long-term interactions that require adaptation towards the user as well as contextual knowledge and understanding of the ever-changing environment. To overcome these challenges, holistic memory modeling is required to efficiently retrieve and store relevant information across interaction sessions for suitable responses. Cognitive AI, which aims to simulate the human thought process in a computerized model, highlights interesting aspects, such as thoughts, memory mechanisms, and decision-making, that can contribute towards improved memory modeling for LLMs. Inspired by these cognitive AI principles, we propose our memory framework CAIM. CAIM consists of three modules: 1.) The Memory Controller as the central decision unit; 2.) the Memory Retrieval, which filters relevant data for interaction upon request; and 3.) the Post-Thinking, which maintains the memory storage. We compare CAIM against existing approaches, focusing on metrics such as retrieval accuracy, response correctness, contextual coherence, and memory storage. The results demonstrate that CAIM outperforms baseline frameworks across different metrics, highlighting its context-awareness and potential to improve long-term human-AI interactions.

[Arxiv](https://arxiv.org/abs/2505.13044)