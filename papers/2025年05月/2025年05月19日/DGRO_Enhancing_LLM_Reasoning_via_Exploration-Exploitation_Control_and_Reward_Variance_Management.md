# DGRO：通过探索-利用调控与奖励方差优化提升LLM推理能力

发布时间：2025年05月19日

`LLM理论

摘要中讨论了强化学习算法在大型语言模型推理中的应用，特别是奖励函数的设计和优化，属于理论层面的创新，因此归类为LLM理论。` `人工智能` `机器学习`

> DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management

# 摘要

> 推理扩展进一步推动大型语言模型向人工通用智能迈进，通过大规模强化学习释放长期的链式推理能力。当前大多数推理方法依赖于手工设计的基于规则的奖励函数，然而强化学习算法中探索与利用的权衡涉及多方面的复杂考量，手动设计奖励函数的理论和实际影响仍需深入研究。本文提出了一种针对大型语言模型推理的通用强化学习算法——解耦组奖励优化（DGRO）。一方面，DGRO将传统的正则化系数解耦为两个独立的超参数：一个用于调整策略梯度项，另一个用于调节采样策略的距离。这种解耦不仅实现了对探索与利用平衡的精准控制，还可以无缝扩展至Kimi k1.5和直接奖励优化中的在线策略镜像下降算法。另一方面，我们发现奖励方差对收敛速度和最终模型性能有显著影响。我们通过理论分析和广泛的实证验证来评估DGRO，包括详细的研究其性能和优化动态的消融实验。实验结果显示，DGRO在逻辑数据集上实现了96.9%的平均准确率，达到了目前最优水平，并在数学基准测试中展现了强大的泛化能力。

> Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9\%, and demonstrates strong generalization across mathematical benchmarks.

[Arxiv](https://arxiv.org/abs/2505.12951)