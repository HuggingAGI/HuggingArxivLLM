# 戏弄机器：人格化设置如何影响LLM的脆弱性

发布时间：2025年05月19日

`LLM应用` `网络安全` `人工智能伦理`

> Bullying the Machine: How Personas Increase LLM Vulnerability

# 摘要

> 大型语言模型（LLMs）在需要模型扮演特定角色的交互场景中被广泛应用。本文研究了这种角色设定是否会影响模型在遭受网络欺凌时的安全性，这是一种通过施加心理压力迫使受害者服从的对抗性操纵行为。我们提出了一种仿真框架，其中攻击者LLM利用基于心理学的欺凌战术与受害者LLM互动，而受害者则采用与大五人格特质模型一致的人格设定。通过使用多个开源LLMs和多种对抗性目标进行的实验表明，某些人格配置——例如降低宜人性或尽责性——会显著增加受害者产生不安全输出的易感性。涉及情绪化或讽刺性操纵的欺凌战术（如煤气灯效应和嘲讽）特别有效。这些发现表明，基于角色的交互为LLMs的安全风险引入了一种新型的传播途径，并强调了需要开发具备角色感知的安全评估和对齐策略的重要性。

> Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.

[Arxiv](https://arxiv.org/abs/2505.12692)