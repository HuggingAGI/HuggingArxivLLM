# 欺凌机器：如何通过人格化攻击让大型语言模型更容易被攻破

发布时间：2025年05月19日

`LLM应用` `人工智能`

> Bullying the Machine: How Personas Increase LLM Vulnerability

# 摘要

> 大型语言模型（LLMs）广泛应用于需要采用特定角色设定的交互场景中。本文探讨了角色设定条件化在遭受欺凌时对模型安全性的影响，即攻击者通过施加心理压力迫使受害者服从的对抗性操控。我们提出了一种仿真框架，其中攻击者LLM利用基于心理学的欺凌策略与受害者LLM互动，而受害者则采用了与五大人格特质模型相符的角色设定。实验结果表明，某些人格设定配置——如降低宜人性或尽责性——显著增加了受害者产生不安全输出的易感性。涉及情绪化或讽刺性操控的欺凌策略，如精神操控和讽刺，尤其有效。这些发现揭示了基于角色设定的交互为LLMs安全风险带来的全新途径，并强调了开发具备角色意识的安全评估和对齐策略的必要性。

> Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.

[Arxiv](https://arxiv.org/abs/2505.12692)