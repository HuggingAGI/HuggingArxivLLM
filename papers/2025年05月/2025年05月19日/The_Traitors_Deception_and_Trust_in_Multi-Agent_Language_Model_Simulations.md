# # 背叛者：多智能体语言模型模拟实验中的欺骗与信任

发布时间：2025年05月19日

`Agent` `人工智能` `多智能体系统`

> The Traitors: Deception and Trust in Multi-Agent Language Model Simulations

# 摘要

> 随着AI系统在需要信任与人类价值观对齐的关键领域中发挥越来越重要的作用，理解其何时及为何会产生欺骗行为已成为研究重点。我们推出了The Traitors——一个基于社交推理游戏设计的多智能体模拟框架，旨在深入探究信息不对称环境下大型语言模型（LLM）智能体间的欺骗、信任构建与战略沟通机制。少数“叛徒”智能体试图误导多数派，而忠诚智能体则需通过对话与推理揭示隐藏身份。我们的主要贡献包括：(1) 基于博弈论、行为经济学与社会认知理论构建了严谨的理论框架；(2) 设计了一套评估指标体系，涵盖欺骗成功率、信任演变与集体推理质量；(3) 开发了一个支持异构智能体、特色属性与自适应行为的全自动模拟平台，使LLMs能够基于持续记忆与动态社交关系进行推理。在DeepSeek-V3、GPT-4o-mini与GPT-4o（每模型运行10次）上的实验揭示了显著的不对称现象：尽管GPT-4o等先进模型展现出卓越的欺骗能力，却对其他模型的虚假信息表现出过度敏感。这表明欺骗能力的提升速度可能快于检测能力。总体而言，The Traitors为研究LLM在复杂社交互动中的行为提供了一个聚焦且灵活的实验平台。我们希望通过这项工作推动对欺骗机制、对齐挑战及AI系统社会可靠性的深入研究。

> As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.

[Arxiv](https://arxiv.org/abs/2505.12923)