# 资源受限环境下大型语言模型的优化：模型压缩技术综述

发布时间：2025年05月04日

`LLM应用` `移动计算` `边缘计算`

> Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques

# 摘要

> 大型语言模型（LLMs）在 AI 领域带来了革命性变革，但其较高的资源需求限制了在移动和边缘设备上的应用。本文综述了压缩 LLM 的技术，以实现资源受限环境下的高效推理。我们重点探讨了知识蒸馏、模型量化和模型剪枝三种主要方法。针对每种技术，我们将深入浅出地介绍其工作原理，分析不同变体，并分享成功应用案例。此外，我们还简要介绍了混合专家模型和早期退出策略等辅助技术。最后，我们将展望未来研究方向，为研究人员和从业者提供优化 LLM 边缘部署的有益参考。

> Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.

[Arxiv](https://arxiv.org/abs/2505.02309)