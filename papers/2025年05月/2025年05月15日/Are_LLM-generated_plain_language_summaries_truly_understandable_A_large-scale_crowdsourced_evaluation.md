# LLM生成的平实语言摘要是否真的易懂？一项大规模的众包评估。

发布时间：2025年05月15日

`LLM应用` `健康信息`

> Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation

# 摘要

> # 摘要
Plain Language Summaries（PLSs）在促进医患沟通中发挥着关键作用，它们能让非专业人士更轻松地理解复杂医学信息并采取行动。近期，大型语言模型（LLMs）在自动化生成 PLS 方面展现出巨大潜力，但其对健康信息理解的支持效果仍有待验证。以往研究多依赖于无法直接衡量可理解性的自动化评分，或来自便利样本的主观 Likert 量表评分，这些方法的适用性有限。为填补这一空白，我们通过 Amazon Mechanical Turk 招募了 150 名参与者，开展了一项大规模众包评估，对 LLM 生成的 PLS 进行全面评估。我们从简洁性、信息量、连贯性和忠实性四个方面进行主观评分，并通过客观的多项选择题测试评估读者的理解效果。此外，我们还对比了 10 个自动化评估指标与人类判断的一致性。研究发现表明，尽管 LLM 生成的 PLS 在主观评估中与人工撰写的作品难分高下，但人工撰写的 PLS 能带来显著更好的理解效果。此外，自动化评估指标未能准确反映人类判断，这引发了它们是否适合用于评估 PLS 的疑问。这是首个基于读者偏好和理解效果，系统性评估 LLM 生成 PLS 的研究。我们的发现强调了建立超越表面质量的评估框架，并开发出明确优化非专业人士理解的生成方法的重要性。


> Plain language summaries (PLSs) are essential for facilitating effective communication between clinicians and patients by making complex medical information easier for laypeople to understand and act upon. Large language models (LLMs) have recently shown promise in automating PLS generation, but their effectiveness in supporting health information comprehension remains unclear. Prior evaluations have generally relied on automated scores that do not measure understandability directly, or subjective Likert-scale ratings from convenience samples with limited generalizability. To address these gaps, we conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using Amazon Mechanical Turk with 150 participants. We assessed PLS quality through subjective Likert-scale ratings focusing on simplicity, informativeness, coherence, and faithfulness; and objective multiple-choice comprehension and recall measures of reader understanding. Additionally, we examined the alignment between 10 automated evaluation metrics and human judgments. Our findings indicate that while LLMs can generate PLSs that appear indistinguishable from human-written ones in subjective evaluations, human-written PLSs lead to significantly better comprehension. Furthermore, automated evaluation metrics fail to reflect human judgment, calling into question their suitability for evaluating PLSs. This is the first study to systematically evaluate LLM-generated PLSs based on both reader preferences and comprehension outcomes. Our findings highlight the need for evaluation frameworks that move beyond surface-level quality and for generation methods that explicitly optimize for layperson comprehension.

[Arxiv](https://arxiv.org/abs/2505.10409)