# 可行性真的重要吗？解析其对合成训练数据的影响

发布时间：2025年05月15日

`其他` `图像生成` `计算机视觉`

> Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data

# 摘要

> 随着 photorealistic diffusion models 的发展，基于合成数据训练的模型逐步取得了更好的效果。然而，扩散模型仍然经常生成现实中不可能存在的图像，例如漂浮在空中的狗或具有不现实纹理缺陷的图像。我们定义可行性为合成图像中的属性是否能够在现实世界中真实存在；包含违反该标准属性的合成图像被认为是不可行的。直观上，不可行的图像通常被视为分布外数据，因此使用这些图像进行训练预计会阻碍模型对现实世界数据的泛化能力，因此应尽可能将它们排除在训练集之外。然而，可行性真的重要吗？在本文中，我们研究了在为基于 CLIP 的分类器生成合成训练数据时，强制执行可行性是否必要，重点关注背景、颜色和纹理这三个目标属性。我们引入了 VariReal 管道，它可以通过最小编辑给定的源图像，以包含由大型语言模型生成的文本提示中给定的可行或不可行属性。我们的实验表明，可行性对 LoRA 微调的 CLIP 性能影响微乎其微，在三个细粒度数据集上的 top-1 准确率差异大多小于 0.3%。此外，属性是否会影响可行/不可行图像对分类性能的对抗性影响。最后，将可行和不可行图像混合在训练数据集中与仅使用完全可行或不可行的数据集相比，对性能没有显著影响。

> With the development of photorealistic diffusion models, models trained in part or fully on synthetic data achieve progressively better results. However, diffusion models still routinely generate images that would not exist in reality, such as a dog floating above the ground or with unrealistic texture artifacts. We define the concept of feasibility as whether attributes in a synthetic image could realistically exist in the real-world domain; synthetic images containing attributes that violate this criterion are considered infeasible. Intuitively, infeasible images are typically considered out-of-distribution; thus, training on such images is expected to hinder a model's ability to generalize to real-world data, and they should therefore be excluded from the training set whenever possible. However, does feasibility really matter? In this paper, we investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture. We introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model. Our experiments show that feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Also, the attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Finally, mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.

[Arxiv](https://arxiv.org/abs/2505.10551)