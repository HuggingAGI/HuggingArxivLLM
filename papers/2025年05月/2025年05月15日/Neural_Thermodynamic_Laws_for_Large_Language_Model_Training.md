# 大型语言模型训练中的神经热力学定律

发布时间：2025年05月15日

`LLM理论` `人工智能` `计算智能`

> Neural Thermodynamic Laws for Large Language Model Training

# 摘要

> 除了神经缩放定律，我们对大型语言模型 (LLMs) 的潜在规律知之甚少。为此，我们提出了神经热力学法则 (NTL) —— 一个全新的框架，为理解 LLM 的训练动态提供了崭新的视角。从理论层面，关键热力学量（如温度、熵、热容量、热传导）和经典热力学原理（如热力学三定律及均分定理）在河流-山谷损失景观假设下自然浮现。从实践层面，这一科学视角为设计学习率调度策略提供了直观的指导原则。

> Beyond neural scaling laws, little is known about the laws underlying large language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new framework that offers fresh insights into LLM training dynamics. On the theoretical side, we demonstrate that key thermodynamic quantities (e.g., temperature, entropy, heat capacity, thermal conduction) and classical thermodynamic principles (e.g., the three laws of thermodynamics and the equipartition theorem) naturally emerge under river-valley loss landscape assumptions. On the practical side, this scientific perspective yields intuitive guidelines for designing learning rate schedules.

[Arxiv](https://arxiv.org/abs/2505.10559)