# Multi-RAG：一个多模态检索增强生成系统，专为自适应视频理解设计

发布时间：2025年05月29日

`LLM应用` `多模态` `人机协作`

> Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding

# 摘要

> 在瞬息万变的现代社会中，适应环境、筛选信息和做出明智决策的能力至关重要。随着机器人和智能体逐渐融入人类生活，将认知压力从人类转移到这些系统的需求日益增长，尤其是在信息丰富且动态变化的场景中。
    为此，我们提出了Multi-RAG——一个多模态增强生成系统，专为信息密集型环境设计，旨在为人提供自适应支持。通过整合和推理多源信息（包括视频、音频和文本），Multi-RAG力求提升情境理解并降低认知压力。作为实现长期人机协作的重要一步，Multi-RAG探索了多模态信息理解如何为人本化、动态环境中的自适应机器人辅助奠定基础。
    为了评估其在现实人类辅助任务中的能力，我们在具有挑战性的多模态视频理解基准MMBench-Video数据集上对Multi-RAG进行了基准测试。结果表明，Multi-RAG不仅性能优于现有的开源视频大语言模型（Video-LLMs）和大型视觉语言模型（LVLMs），还采用了更少的资源和输入数据。这证明Multi-RAG有望成为未来动态现实环境中人机自适应辅助系统实用且高效的基石。
    

> To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information understanding can serve as a foundation for adaptive robotic assistance in dynamic, human-centered situations. To evaluate its capability in a realistic human-assistance proxy task, we benchmarked Multi-RAG on the MMBench-Video dataset, a challenging multimodal video understanding benchmark. Our system achieves superior performance compared to existing open-source video large language models (Video-LLMs) and large vision-language models (LVLMs), while utilizing fewer resources and less input data. The results demonstrate Multi- RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts.

[Arxiv](https://arxiv.org/abs/2505.23990)