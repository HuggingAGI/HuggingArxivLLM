# 用于评估和优化代码效率的性能约束合成方法

发布时间：2025年05月29日

`LLM应用` `软件工程` `代码优化`

> Synthesizing Performance Constraints for Evaluating and Improving Code Efficiency

# 摘要

> 大型语言模型（LLMs）在代码效率优化中发挥着越来越重要的作用。评估其效果并提出优化建议，通常需要通过高质量的测试来揭示程序中的性能瓶颈。然而，现有方法往往受限于手动编写的输入或LLM生成的缺乏深度的测试，难以发现更细微的优化机会。为此，我们提出了WEDGE框架，用于根据待测试程序生成具有性能压力的输入。WEDGE通过显式的性能特征约束（以分支条件形式）将程序的执行空间划分为特定性能的区域。当与覆盖率引导的模糊测试工具结合使用时，访问不同的区域会为测试生成带来显式的奖励，从而帮助发现低效实现。我们的评估结果显示，与CodeContests中的测试及现有方法声称优化的测试相比，WEDGE引入了显著的性能减速。从实用性角度看，整合我们的测试能够显著提升现有依赖于测试驱动执行反馈的代码优化方法。我们发布了由WEDGE生成的性能测试集PERFFORGE，用于评估未来高效的代码生成方法，地址为https://github.com/UChiSeclab/perfforge。

> Large Language Models (LLMs) have been increasingly used to optimize code efficiency. Evaluating their effectiveness and further suggesting optimization opportunities often rely on high-quality tests to demonstrate the performance bottlenecks presented in the program. However, existing approaches rely on a limited set of hand-curated inputs or LLM-generated uninteresting length-stressing tests, failing to reveal more nuanced optimization opportunities. We present WEDGE, a framework for generating performance-stressing input given the program under test. WEDGE synthesizes explicit performance-characterizing constraints in the form of branch conditions to partition the programs' execution space into performance-specific regions. When integrated with the coverage-guided fuzzer, reaching different regions introduces explicit rewards for test generation to explore inefficient implementations. Our evaluation shows that WEDGE introduces a significant slowdown compared to the tests in CodeContests and those claimed to be optimized by existing approaches. From the utility perspective, integrating our tests substantially improves the existing code optimization approaches that rely on test-driven execution feedback. We release PERFFORGE, the performance tests generated by WEDGE, to benchmark future approaches for efficient code generation at https://github.com/UChiSeclab/perfforge.

[Arxiv](https://arxiv.org/abs/2505.23471)