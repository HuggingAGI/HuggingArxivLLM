# # 基于大型语言模型语义理解能力的自适应调整策略
基于大型语言模型（LLMs）的语义理解能力，我们提出了一种自适应调整策略。

发布时间：2025年05月29日

`LLM应用

理由：这篇论文探讨了如何绕过大型语言模型的安全和伦理约束，属于对大型语言模型的应用研究，特别是安全攻击方面的应用。` `AI安全` `网络安全`

> Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models

# 摘要

> # 引言
近年来，针对大型语言模型（LLMs）的越狱攻击——通过绕过其内置安全和伦理约束的方法——已成为AI安全领域的一项重大挑战。这些攻击通过利用LLMs理解能力中的固有弱点，严重威胁了其可靠性。本文深入探讨了专门针对不同LLMs理解水平而设计的越狱策略的有效性。我们提出了一种创新的框架——基于大型语言模型语义理解能力的自适应越狱策略，该框架根据LLMs的语义理解能力，将其分为类型I和类型II两类。针对每一类，我们精心设计了专门的越狱策略，旨在精准利用其漏洞，从而实现高效的攻击。通过在多个LLMs上进行的广泛实验，我们发现，我们的自适应策略显著提升了越狱的成功率。特别值得一提的是，我们的方法在针对GPT-4o（2025年5月29日发布版本）的越狱攻击中，达到了惊人的98.9%成功率。

> Adversarial attacks on Large Language Models (LLMs) via jailbreaking techniques-methods that circumvent their built-in safety and ethical constraints-have emerged as a critical challenge in AI security. These attacks compromise the reliability of LLMs by exploiting inherent weaknesses in their comprehension capabilities. This paper investigates the efficacy of jailbreaking strategies that are specifically adapted to the diverse levels of understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models, a novel framework that classifies LLMs into Type I and Type II categories according to their semantic comprehension abilities. For each category, we design tailored jailbreaking strategies aimed at leveraging their vulnerabilities to facilitate successful attacks. Extensive experiments conducted on multiple LLMs demonstrate that our adaptive strategy markedly improves the success rate of jailbreaking. Notably, our approach achieves an exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)

[Arxiv](https://arxiv.org/abs/2505.23404)