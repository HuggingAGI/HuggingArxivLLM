# 利用大型语言模型生成多样化的训练样本用于关系抽取

发布时间：2025年05月29日

`LLM应用` `数据科学`

> Generating Diverse Training Samples for Relation Extraction with Large Language Models

# 摘要

> 利用大型语言模型 (LLMs) 生成训练数据可能是提升零样本或少样本 NLP 任务的更优途径。然而，这一方向仍有许多问题亟待探索。针对关系抽取 (RE) 任务，我们发现直接提示 LLMs 生成的样本往往结构相似度高。它们在表达一对实体间的关系时，倾向于使用有限多样的措辞。因此，在本文中，我们研究如何有效提升 LLMs 生成的关系抽取训练样本多样性，同时保持其正确性。我们首先尝试通过在 In-Context Learning (ICL) 提示中直接给出指令，使 LLMs 生成差异性样本。然后，我们提出一种基于 Direct Preference Optimization (DPO) 的方法，用于微调 LLMs 以生成多样化的训练样本。我们在常用的关系抽取数据集上的实验表明，这两种尝试均能提升生成训练数据的质量。我们还发现，与直接使用 LLM 进行关系抽取相比，利用生成样本训练非 LLM 的关系抽取模型，可能会获得更好的性能。

> Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE), we find that samples generated by directly prompting LLMs may easily have high structural similarities with each other. They tend to use a limited variety of phrasing while expressing the relation between a pair of entities. Therefore, in this paper, we study how to effectively improve the diversity of the training samples generated with LLMs for RE, while also maintaining their correctness. We first try to make the LLMs produce dissimilar samples by directly giving instructions in In-Context Learning (ICL) prompts. Then, we propose an approach to fine-tune LLMs for diversity training sample generation through Direct Preference Optimization (DPO). Our experiments on commonly used RE datasets show that both attempts can improve the quality of the generated training data. We also find that comparing with directly performing RE with an LLM, training a non-LLM RE model with its generated samples may lead to better performance.

[Arxiv](https://arxiv.org/abs/2505.23108)