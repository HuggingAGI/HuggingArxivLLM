# # SafeScientist: 通过 LLM 代理实现风险感知的科学研究

发布时间：2025年05月29日

`LLM应用` `科学研究` `伦理安全`

> SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents

# 摘要

> 大型语言模型（LLM）代理的最新进展显著加速了科学发现的自动化，但同时也引发了重要的伦理和安全问题。为系统性应对这些挑战，我们推出了	extbf{SafeScientist}——一个专为提升AI驱动科学探索中的安全性和伦理责任而设计的创新AI科学家框架。SafeScientist不仅主动拒绝不道德或高风险任务，还在整个研究过程中严格强调安全性。我们集成了多种防御机制，包括提示监控、代理协作监控、工具使用监控以及伦理审查组件，以实现全面的安全监管。作为SafeScientist的补充，我们提出了	extbf{SciSafetyBench}——一个专门用于评估科学领域中AI安全性的新型基准，包含6个领域内的240个高风险科学任务，以及30个特别设计的科学工具和120个与工具相关的风险任务。大量实验表明，与传统AI科学家框架相比，SafeScientist在安全性方面提升了35%，同时保持了科学产出质量。此外，我们严格验证了我们的安全管道在面对多种对抗攻击方法时的稳健性，进一步证实了我们集成方法的有效性。代码和数据可在https://github.com/ulab-uiuc/SafeScientist获取。	extcolor{red}{警告：本文包含可能令人反感或有害的示例数据。}

> Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}

[Arxiv](https://arxiv.org/abs/2505.23559)