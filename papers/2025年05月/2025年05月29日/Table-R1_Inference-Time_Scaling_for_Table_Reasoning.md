# Table-R1：推理时间缩放在表格推理中的应用

发布时间：2025年05月29日

`LLM应用` `数据处理` `人工智能`

> Table-R1: Inference-Time Scaling for Table Reasoning

# 摘要

> 本研究首次探索了表格推理任务中的推理效率缩放问题。我们开发并评估了两种后训练策略，分别是基于前沿模型推理轨迹的知识蒸馏和具有可验证奖励的强化学习（RLVR）。在蒸馏策略中，我们构建了一个由DeepSeek-R1生成的大型推理轨迹数据集，并通过微调大型语言模型，成功打造了Table-R1-SFT模型。针对RLVR策略，我们设计了特定任务的可验证奖励函数，并结合GRPO算法，开发出了Table-R1-Zero模型。我们在包括简短问答、事实核查和自由问答等多样化任务上评估了Table-R1系列模型。值得注意的是，Table-R1-Zero模型在仅使用70亿参数规模的大型语言模型情况下，其性能可与GPT-4.1和DeepSeek-R1相媲美甚至超越。此外，该模型在跨领域数据集上也展现出强大的泛化能力。通过广泛的消融实验和定性分析，我们揭示了指令微调、模型架构选择以及跨任务泛化能力的重要性，并观察到在强化学习训练过程中表格推理关键技能的逐步浮现。

> In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training.

[Arxiv](https://arxiv.org/abs/2505.23621)