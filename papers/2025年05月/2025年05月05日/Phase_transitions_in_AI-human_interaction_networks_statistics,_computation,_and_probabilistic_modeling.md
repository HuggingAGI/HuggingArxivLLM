# AI-人类交互网络中的相变：统计、计算与概率建模

发布时间：2025年05月05日

`LLM理论

理由：这篇论文探讨了大语言模型（LLMs）的内在机制，特别是使用自旋玻璃理论来分析模型的不确定性、噪声和涌现行为。研究重点在于理论框架和模型的内在机制，而非具体的应用场景。因此，归类为LLM理论。` `社交网络` `文学创作`

> Phase transitions in AI-human interaction networks: statistics, computation, and probabilistic modeling

# 摘要

> 近年来，大语言模型（LLMs）凭借生成类人文本的能力，彻底革新了自然语言处理领域。然而，理解驱动其涌现行为的潜在机制，尤其是其输出中的随机性，仍然是一个基本挑战。本文探讨了将自旋玻璃理论作为数学框架，用于量化大语言模型的不确定性。此外，我们分析了大语言模型内部噪声与社交网络噪声之间的相互作用，如何塑造系统中涌现的集体行为。通过将大语言模型与自旋玻璃模型相联系，我们对大语言模型的高维优化景观、输出中的不确定性以及噪声在其学习过程中的作用获得了新的见解。

我们特别关注大语言模型在复制人类书写的flitzes方面的能力，这是一种达特茅斯学院特有的挑逗性诗歌形式，用于邀请同侪或潜在的浪漫伴侣参加社交活动。鉴于flitzes的俏皮语气、个人引用以及在复杂社交网络中的作用，它们代表了一种独特创造性的语言形式，使它们成为探索大语言模型中温度参数如何影响AI生成内容的创意性和真实性的理想选择。

为了更好地理解温度如何影响模型行为，我们通过统计分析、计算方法以及我们自旋玻璃模型的模拟，寻找温度依赖的相变。我们的研究发现表明，温度不仅控制着大语言模型输出中的随机性，还调解了语言结构、感知质量和人机对齐方面的更深层次转变。通过将统计物理与语言生成相结合，我们为理解大语言模型中的涌现行为及其与复杂社交网络的互动提供了一个新颖的框架。

> In recent years, Large Language Models (LLMs) have revolutionized Natural Language Processing with their ability to generate human-like texts. However, a fundamental challenge remains in understanding the underlying mechanisms driving their emergent behaviors, particularly the randomness in their outputs. This paper investigates the application of spin glass theory as a mathematical framework to quantify the uncertainty of LLMs. Moreover, we analyze how the interaction between the noise in LLMs and from social networks shape emergent collective behaviors of the system. By making connections between LLMs and spin glass models, we gain insights into the high-dimensional optimization landscapes of LLMs, the uncertainty in their outputs, and the role of noise in their learning process. We focus on LLMs' ability to replicate human-written flitzes, a form of flirtatious poems unique to Dartmouth College, used to invite peers or a potentially romantic partner to social events. Given flitzes' playful tone, personal references, and role in complex social networks, they represent a uniquely creative form of language, making them ideal for exploring how the temperature parameter in LLMs affects the creativity and verisimilitude of AI-generated content. To better understand where temperature affects model behavior, we look for temperature-based phase transitions through statistical analysis, computational methods, and simulation of our spin glass model. Our findings demonstrate that temperature not only governs randomness in LLM output, but also mediates deeper transitions in linguistic structure, perceived quality, and human-machine alignment. By connecting statistical physics with language generation, we provide a novel framework for understanding emergent behavior in LLMs and their interaction with complex social networks.

[Arxiv](https://arxiv.org/abs/2505.02879)