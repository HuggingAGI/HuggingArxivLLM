# # 摘要
最近大型语言模型（LLMs）的突破性进展引领了一场从机器人流程自动化到智能体流程自动化的革命性转变，这一转变通过基于LLMs自动化工作流编排过程实现了。

发布时间：2025年05月05日

`LLM应用` `数据分析` `多模态`

> Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era

# 摘要

> 边缘设备的普及在各领域产生了海量时序数据，催生了众多量身定制的方法。近年来，大型语言模型（LLMs）通过利用文本数据与时序数据共有的序列特性，为时序数据分析开辟了全新范式。然而，时序数据与大型语言模型之间存在根本性的跨模态差距——大型语言模型基于文本语料库预训练，未针对时序数据进行优化。近期许多研究提案致力于解决这一问题。本次综述旨在为基于大型语言模型的时序数据分析跨模态建模提供全面概述。首先，我们根据用于时序建模的文本数据类型，将现有方法归类为四组。随后，我们总结了跨模态策略中的关键方法，如对齐与融合，并探讨了它们在多种下游任务中的应用。此外，我们在不同应用领域的多模态数据集上开展实验，探究文本数据与跨模态策略的有效组合，以提升时序数据分析能力。最后，我们提出了未来研究的几个有前景的方向。本次综述为对基于大型语言模型的时序建模感兴趣的各类专业人士、研究人员和实践者提供了有价值的参考。


> The proliferation of edge devices has generated an unprecedented volume of time series data across different domains, motivating various well-customized methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm for time series analytics by leveraging the shared sequential nature of textual data and time series. However, a fundamental cross-modality gap between time series and LLMs exists, as LLMs are pre-trained on textual corpora and are not inherently optimized for time series. Many recent proposals are designed to address this issue. In this survey, we provide an up-to-date overview of LLMs-based cross-modality modeling for time series analytics. We first introduce a taxonomy that classifies existing approaches into four groups based on the type of textual data employed for time series modeling. We then summarize key cross-modality strategies, e.g., alignment and fusion, and discuss their applications across a range of downstream tasks. Furthermore, we conduct experiments on multimodal datasets from different application domains to investigate effective combinations of textual data and cross-modality strategies for enhancing time series analytics. Finally, we suggest several promising directions for future research. This survey is designed for a range of professionals, researchers, and practitioners interested in LLM-based time series modeling.

[Arxiv](https://arxiv.org/abs/2505.02583)