# 哥伦比亚女服务员与加拿大法官：大型语言模型中的职业推荐偏见研究

发布时间：2025年05月05日

`LLM理论

摘要中探讨了大型语言模型中的偏见问题，分析了模型在不同语言和文化背景下的公平性表现，以及模型结构对偏见的影响，属于对模型理论的深入研究。` `公平性`

> Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs

# 摘要

> 在NLP领域的公平性研究中，一个重要目标是识别并减少由NLP系统传播的刻板印象偏见。然而，现有研究往往局限于单一维度的偏见（如性别偏见）和英语语言。为突破这些限制，我们开展了首个关于多语言交叉国家与性别偏见的研究，特别关注大型语言模型生成的职业推荐。我们构建了一个包含英语、西班牙语和德语提示的基准测试集，系统性地考察了25个国家和四组代词的组合。通过对5款基于Llama的模型进行评估，我们发现LLM模型中存在显著的性别和国家偏见。值得注意的是，即使模型在单独考虑性别或国家时表现公平，基于两者交叉的职场偏见依然存在。此外，我们发现提示语言显著影响偏见程度，而经过指令微调的模型始终表现出最低且最稳定的偏见水平。这些发现凸显了在公平性研究中采用交叉和多语言视角的重要性。

> One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.

[Arxiv](https://arxiv.org/abs/2505.02456)