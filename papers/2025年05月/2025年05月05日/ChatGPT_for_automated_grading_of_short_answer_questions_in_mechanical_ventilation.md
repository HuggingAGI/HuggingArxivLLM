# ChatGPT 应用于机械通气领域中的简答题自动评分

发布时间：2025年05月05日

`LLM应用`

> ChatGPT for automated grading of short answer questions in mechanical ventilation

# 摘要

> 研究生教育中广泛采用标准化考试，其中简答题（SAQs）是常见的评估方式。大型语言模型（LLMs）因其能够模拟对话语言并准确解释非结构化自由文本回答，成为自动化评分的理想选择。本研究评估了ChatGPT 4o在研究生医学教育中的简答题评分能力，数据来自215名学生（557份简答题回答），这些学生参与了一项关于机械通气的在线课程（2020--2024年）。我们将匿名化处理后的三个案例场景回答提交给ChatGPT，并附上了标准化的评分提示和评分标准。通过混合效应模型、方差分量分析、组内相关系数（ICC）、Cohen's kappa、Kendall's W以及Bland-Altman统计方法对评分结果进行了深入分析。结果显示，ChatGPT的评分普遍低于人工评分，平均分差（偏差）为-1.34分（满分10分）。组内相关系数（ICC1 = 0.086）表明个体层面的评分一致性较差，而Cohen's kappa值（-0.0786）则显示几乎没有实质性的评分一致性。方差分量分析显示，五个ChatGPT评分回合之间的差异很小（G值 = 0.87），这表明内部一致性较好，但与人工评分存在偏差。评分一致性最差的情况出现在评估性和分析性题目上，而清单式和指导性评分标准的题目则分歧较小。我们建议谨慎使用大型语言模型进行研究生课程的评分。研究发现，超过60%的ChatGPT评分与人工评分的差异超出了高利害评估可接受的范围。

> Standardised tests using short answer questions (SAQs) are common in postgraduate education. Large language models (LLMs) simulate conversational language and interpret unstructured free-text responses in ways aligning with applying SAQ grading rubrics, making them attractive for automated grading. We evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data from 215 students (557 short-answer responses) enrolled in an online course on mechanical ventilation (2020--2024). Deidentified responses to three case-based scenarios were presented to ChatGPT with a standardised grading prompt and rubric. Outputs were analysed using mixed-effects modelling, variance component analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks than human graders with a mean difference (bias) of -1.34 on a 10-point scale. ICC values indicated poor individual-level agreement (ICC1 = 0.086), and Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component analysis showed minimal variability among the five ChatGPT sessions (G-value = 0.87), indicating internal consistency but divergence from the human grader. The poorest agreement was observed for evaluative and analytic items, whereas checklist and prescriptive rubric items had less disagreement. We caution against the use of LLMs in grading postgraduate coursework. Over 60% of ChatGPT-assigned grades differed from human grades by more than acceptable boundaries for high-stakes assessments.

[Arxiv](https://arxiv.org/abs/2505.04645)