# 评估与提升用于评估大型语言模型在软件工程任务中的基准

发布时间：2025年05月13日

`LLM应用` `软件工程` `基准测试`

> Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks

# 摘要

> 大型语言模型（LLMs）凭借其卓越性能在软件工程（SE）领域迅速崛起，广泛应用于需求工程与设计、代码分析与生成、软件维护及质量保证等多个环节。随着 LLMs 在 SE 中的应用不断深化，对其效能的评估成为理解其潜力的关键。近年来，针对 LLM 在 SE 任务中的表现，研究者们开发了众多基准测试。本文对 191 个基准进行了全面梳理，聚焦三大核心问题：现有基准有哪些、它们是如何构建的，以及未来发展趋势如何。我们从需求工程与设计、编码辅助、软件测试、AIOPs、软件维护和质量管理等典型 SE 任务入手，深入分析了各类基准及其构建过程，揭示了现有基准的局限性。同时，我们探讨了 LLMs 在不同软件任务中的成败案例，并展望了 SE 相关基准的未来机遇与挑战。本文旨在为 SE 领域的基准研究提供全面视角，助力更高效的评估工具开发。

> Large language models (LLMs) are gaining increasing popularity in software engineering (SE) due to their unprecedented performance across various applications. These models are increasingly being utilized for a range of SE tasks, including requirements engineering and design, code analysis and generation, software maintenance, and quality assurance. As LLMs become more integral to SE, evaluating their effectiveness is crucial for understanding their potential in this field. In recent years, substantial efforts have been made to assess LLM performance in various SE tasks, resulting in the creation of several benchmarks tailored to this purpose. This paper offers a thorough review of 191 benchmarks, addressing three main aspects: what benchmarks are available, how benchmarks are constructed, and the future outlook for these benchmarks. We begin by examining SE tasks such as requirements engineering and design, coding assistant, software testing, AIOPs, software maintenance, and quality management. We then analyze the benchmarks and their development processes, highlighting the limitations of existing benchmarks. Additionally, we discuss the successes and failures of LLMs in different software tasks and explore future opportunities and challenges for SE-related benchmarks. We aim to provide a comprehensive overview of benchmark research in SE and offer insights to support the creation of more effective evaluation tools.

[Arxiv](https://arxiv.org/abs/2505.08903)