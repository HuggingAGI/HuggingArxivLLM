# 隐藏状态背后是否还有未解之谜？探索LLMs事实编码能力的边界

发布时间：2025年05月22日

`LLM理论` `内容审核`

> Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs

# 摘要

> 事实性幻觉是大型语言模型（LLMs）面临的主要挑战之一。它们通过生成不准确或捏造的内容削弱了模型的可靠性和用户信任度。近期研究表明，当生成虚假陈述时，LLMs的内部状态中包含了关于陈述真实性的信息。然而，这些研究通常依赖于缺乏现实感的合成数据集，这在评估模型生成文本的事实准确性时限制了其通用性。本文通过研究真实性编码能力，对先前工作的发现提出挑战，并生成了更具现实性和挑战性的数据集。具体而言，我们通过以下方式扩展了先前的工作：(1) 从表格数据中抽样真实-虚假的事实性句子的策略；(2) 从问答集合中生成现实的、依赖于LLMs的真实-虚假数据集的程序。我们对两个开源LLMs的分析表明，尽管先前研究的某些发现得到了部分验证，但将其推广到LLM生成的数据集仍然具有挑战性。这项研究为未来关于LLMs事实性的研究奠定了基础，并为更有效的评估提供了实用指南。

> Factual hallucinations are a major challenge for Large Language Models (LLMs). They undermine reliability and user trust by generating inaccurate or fabricated content. Recent studies suggest that when generating false statements, the internal states of LLMs encode information about truthfulness. However, these studies often rely on synthetic datasets that lack realism, which limits generalization when evaluating the factual accuracy of text generated by the model itself. In this paper, we challenge the findings of previous work by investigating truthfulness encoding capabilities, leading to the generation of a more realistic and challenging dataset. Specifically, we extend previous work by introducing: (1) a strategy for sampling plausible true-false factoid sentences from tabular data and (2) a procedure for generating realistic, LLM-dependent true-false datasets from Question Answering collections. Our analysis of two open-source LLMs reveals that while the findings from previous studies are partially validated, generalization to LLM-generated datasets remains challenging. This study lays the groundwork for future research on factuality in LLMs and offers practical guidelines for more effective evaluation.

[Arxiv](https://arxiv.org/abs/2505.16520)