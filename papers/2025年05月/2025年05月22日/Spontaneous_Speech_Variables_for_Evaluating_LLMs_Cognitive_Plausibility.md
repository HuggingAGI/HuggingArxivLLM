# 自发性语言变量助力评估大型语言模型的认知合理性

发布时间：2025年05月22日

`LLM应用` `语音处理`

> Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility

# 摘要

> 大型语言模型在自然语言处理领域的卓越表现，尤其是在高资源语言方面，促使我们从认知角度深入理解其特性。研究人员尝试通过评估模型在语言处理过程中预测行为（如眼动追踪固定）和生理（如大脑反应）变量的能力来评价这些模型。例如，在阅读或听力过程中。本文中，我们提出利用自发性演讲语料库提取生产变量（如语音缩减、语调重音），并以类似方式应用它们。更准确地说，我们提取这些变量。然后，我们测试使用标准训练过程在不同预训练数据集（书面、口语和混合类型）上训练的模型，评估它们预测这些变量的能力。我们的结果显示，在一些微调之后，模型能够显著超越基线预测这些生产变量。我们还观察到，基于口语类型的训练数据比书面类型的数据能提供更准确的预测。这些结果为利用高质量语音语料库作为大型语言模型的基准测试贡献了更广泛的努力。

> The achievements of Large Language Models in Natural Language Processing, especially for high-resource languages, call for a better understanding of their characteristics from a cognitive perspective. Researchers have attempted to evaluate artificial models by testing their ability to predict behavioral (e.g., eye-tracking fixations) and physiological (e.g., brain responses) variables during language processing (e.g., reading/listening). In this paper, we propose using spontaneous speech corpora to derive production variables (speech reductions, prosodic prominences) and applying them in a similar fashion. More precisely, we extract. We then test models trained with a standard procedure on different pretraining datasets (written, spoken, and mixed genres) for their ability to predict these two variables. Our results show that, after some fine-tuning, the models can predict these production variables well above baselines. We also observe that spoken genre training data provides more accurate predictions than written genres. These results contribute to the broader effort of using high-quality speech corpora as benchmarks for LLMs.

[Arxiv](https://arxiv.org/abs/2505.16277)