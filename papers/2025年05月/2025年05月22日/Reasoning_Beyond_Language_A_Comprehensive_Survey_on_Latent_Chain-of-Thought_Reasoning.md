# 超越语言局限的推理：潜在链式思维推理的全面探索

发布时间：2025年05月22日

`LLM理论` `认知科学` `人工智能`

> Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning

# 摘要

> 大型语言模型（LLMs）在复杂推理任务中通过链式思维（CoT）提示展现了令人瞩目的性能。然而，传统的 CoT 依赖于自然语言中显式表达的推理步骤，这导致了低效并限制了其在抽象推理中的应用。为了解决这一问题，研究者们逐渐转向潜在的 CoT 推理，其中推理发生在潜在空间中。通过将推理与语言解耦，潜在推理有望实现更丰富的认知表示和更灵活、更快的推理过程。

研究者在这一充满前景的领域中探索了多个方向，包括训练方法、结构创新和内部推理机制。本文对这一推理范式进行了全面的概述与分析。我们首先从四个角度提出了一种统一的分类法：逐个标记的策略、内部机制、分析和应用。接着，我们对具有代表性的方法进行了深入讨论和比较分析，强调了它们的设计模式、优势和开放挑战。我们旨在为推进 LLM 推理这一新兴方向提供一个结构化的基础。相关论文将定期更新至 https://github.com/EIT-NLP/Awesome-Latent-CoT。

> Large Language Models (LLMs) have achieved impressive performance on complex reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional CoT relies on reasoning steps explicitly verbalized in natural language, introducing inefficiencies and limiting its applicability to abstract reasoning. To address this, there has been growing research interest in latent CoT reasoning, where inference occurs within latent spaces. By decoupling reasoning from language, latent reasoning promises richer cognitive representations and more flexible, faster inference. Researchers have explored various directions in this promising field, including training methodologies, structural innovations, and internal reasoning mechanisms. This paper presents a comprehensive overview and analysis of this reasoning paradigm. We begin by proposing a unified taxonomy from four perspectives: token-wise strategies, internal mechanisms, analysis, and applications. We then provide in-depth discussions and comparative analyses of representative methods, highlighting their design patterns, strengths, and open challenges. We aim to provide a structured foundation for advancing this emerging direction in LLM reasoning. The relevant papers will be regularly updated at https://github.com/EIT-NLP/Awesome-Latent-CoT.

[Arxiv](https://arxiv.org/abs/2505.16782)