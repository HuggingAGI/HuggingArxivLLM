# # 理解超越语言：链式思维推理的系统性综述

发布时间：2025年05月22日

`LLM理论` `人工智能`

> Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning

# 摘要

> 大型语言模型（LLMs）在链式思维（CoT）提示下实现了复杂推理任务的出色表现。然而，传统CoT依赖显式表达的自然语言推理步骤，导致效率低下且限制了其在抽象推理中的应用。为解决这一问题，研究者们逐渐对潜在CoT推理产生了浓厚兴趣，这种推理在潜在空间中进行。通过将推理与语言解耦，潜在推理有望实现更丰富的认知表示和更灵活、更快的推理过程。研究者们在这一充满前景的领域中探索了多个方向，包括训练方法、结构创新和内部推理机制。本文对这一推理范式进行了全面概述和分析。我们首先从四个视角提出了一种统一的分类法：逐个标记的策略、内部机制、分析和应用。然后，我们对具有代表性的方法进行了深入讨论和比较分析，强调了它们的设计模式、优势和开放挑战。我们旨在为推进LLM推理这一新兴方向奠定结构化的基础。相关论文将定期更新至https://github.com/EIT-NLP/Awesome-Latent-CoT。

> Large Language Models (LLMs) have achieved impressive performance on complex reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional CoT relies on reasoning steps explicitly verbalized in natural language, introducing inefficiencies and limiting its applicability to abstract reasoning. To address this, there has been growing research interest in latent CoT reasoning, where inference occurs within latent spaces. By decoupling reasoning from language, latent reasoning promises richer cognitive representations and more flexible, faster inference. Researchers have explored various directions in this promising field, including training methodologies, structural innovations, and internal reasoning mechanisms. This paper presents a comprehensive overview and analysis of this reasoning paradigm. We begin by proposing a unified taxonomy from four perspectives: token-wise strategies, internal mechanisms, analysis, and applications. We then provide in-depth discussions and comparative analyses of representative methods, highlighting their design patterns, strengths, and open challenges. We aim to provide a structured foundation for advancing this emerging direction in LLM reasoning. The relevant papers will be regularly updated at https://github.com/EIT-NLP/Awesome-Latent-CoT.

[Arxiv](https://arxiv.org/abs/2505.16782)