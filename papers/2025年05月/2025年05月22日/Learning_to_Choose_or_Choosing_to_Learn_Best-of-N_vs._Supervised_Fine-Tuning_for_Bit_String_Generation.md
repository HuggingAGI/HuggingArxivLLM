# 学习选择，还是选择学习？Best-of-N 与监督微调在比特串生成中的对比研究

发布时间：2025年05月22日

`LLM理论` `人工智能`

> Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation

# 摘要

> 以位串生成问题为例，我们从理论上比较了两种将大型语言模型适配到新任务的标准方法。第一种方法称为监督微调，它通过在优质生成样本上训练一个新的下一个词预测器来实现。第二种方法，即Best-of-N（BoN），则通过训练一个奖励模型，从未经修改的基础模型生成的候选集合中选择优质响应。如果学习环境是可实现的，监督微调在收敛速率上优于BoN，原因在于其对响应长度的依赖更为出色。如果可实现性不成立，那么根据失效模式的不同，BoN可能在n的收敛速率上表现更优，或者在对响应长度的依赖上获得更好的收敛速率。

> Using the bit string generation problem as a case study, we theoretically compare two standard methods for adapting large language models to new tasks. The first, referred to as supervised fine-tuning, involves training a new next token predictor on good generations. The second method, Best-of-N, trains a reward model to select good responses from a collection generated by an unaltered base model. If the learning setting is realizable, we find that supervised fine-tuning outperforms BoN through a better dependence on the response length in its rate of convergence. If realizability fails, then depending on the failure mode, BoN can enjoy a better rate of convergence in either n or a rate of convergence with better dependence on the response length.

[Arxiv](https://arxiv.org/abs/2505.17288)