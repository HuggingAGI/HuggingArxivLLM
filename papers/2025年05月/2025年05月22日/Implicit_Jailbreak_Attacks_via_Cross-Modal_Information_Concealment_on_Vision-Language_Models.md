# 视觉语言模型遭遇隐式越狱攻击：跨模态信息隐藏的威胁

发布时间：2025年05月22日

`LLM应用` `人工智能安全` `网络安全`

> Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models

# 摘要

> 多模态大型语言模型（MLLMs）拥有强大的跨模态推理能力。然而，输入空间的扩展也带来了新的安全挑战。传统的越狱攻击通常通过文本将恶意指令注入到视觉等对齐度较低的模态中。随着MLLMs逐渐引入跨模态一致性和对齐机制，这类显式攻击更容易被检测和拦截。本研究提出了一种名为IJA的新型隐式越狱框架，该框架巧妙地将恶意指令嵌入图像中，并与看似无害的文本提示相结合。为了提升攻击在各类MLLMs中的效果，我们引入了对抗后缀和模板优化模块，可根据模型反馈对提示和嵌入进行迭代优化。在GPT-4o和Gemini-1.5 Pro等商用模型上，我们的方法仅需平均3次查询即可实现超过90%的攻击成功率。

> Multimodal large language models (MLLMs) enable powerful cross-modal reasoning capabilities. However, the expanded input space introduces new attack surfaces. Previous jailbreak attacks often inject malicious instructions from text into less aligned modalities, such as vision. As MLLMs increasingly incorporate cross-modal consistency and alignment mechanisms, such explicit attacks become easier to detect and block. In this work, we propose a novel implicit jailbreak framework termed IJA that stealthily embeds malicious instructions into images via least significant bit steganography and couples them with seemingly benign, image-related textual prompts. To further enhance attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes generated by a surrogate model and introduce a template optimization module that iteratively refines both the prompt and embedding based on model feedback. On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack success rates of over 90% using an average of only 3 queries.

[Arxiv](https://arxiv.org/abs/2505.16446)