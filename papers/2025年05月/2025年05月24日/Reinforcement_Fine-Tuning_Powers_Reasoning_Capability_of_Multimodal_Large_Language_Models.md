# 强化微调助力多模态大型语言模型推理能力提升

发布时间：2025年05月24日

`LLM理论

这篇论文探讨了强化微调（RFT）如何提升多模态大型语言模型（MLLMs）的推理能力，属于模型优化和训练方法的理论层面。` `人工智能`

> Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models

# 摘要

> 2025年，正值通用人工智能（AGI）追求的关键节点，强化微调（RFT）在提升大型语言模型（LLMs）推理能力方面展现出巨大潜力，孕育了如OpenAI-o1和DeepSeek-R1等尖端AI模型。同时，RFT在多模态大型语言模型（MLLMs）推理能力上的高效应用也引发了研究界的浓厚兴趣。本文旨在探讨强化微调如何赋能多模态大型语言模型的推理能力。首先，我们将为领域研究者详细介绍所需掌握的基础背景知识。其次，我们将RFT在提升MLLMs推理能力方面的贡献归纳为五大要点：多模态多样性、跨任务与跨领域、优化训练算法、丰富基准测试及工程框架的蓬勃发展。最后，我们提出了五个值得探索的未来研究方向。希望本文能在AGI发展的关键时期为研究社区提供有益的洞察。更多关于RFT在MLLMs上的研究总结，请访问https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs。

> Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.

[Arxiv](https://arxiv.org/abs/2505.18536)